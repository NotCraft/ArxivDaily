{"2024-03-20T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.13804v1","updated":"2024-03-20T17:59:43Z","published":"2024-03-20T17:59:43Z","title":"Learning from Models and Data for Visual Grounding","summary":"  We introduce SynGround, a novel framework that combines data-driven learning\nand knowledge transfer from various large-scale pretrained models to enhance\nthe visual grounding capabilities of a pretrained vision-and-language model.\nThe knowledge transfer from the models initiates the generation of image\ndescriptions through an image description generator. These descriptions serve\ndual purposes: they act as prompts for synthesizing images through a\ntext-to-image generator, and as queries for synthesizing text, from which\nphrases are extracted using a large language model. Finally, we leverage an\nopen-vocabulary object detector to generate synthetic bounding boxes for the\nsynthetic images and texts. We finetune a pretrained vision-and-language model\non this dataset by optimizing a mask-attention consistency objective that\naligns region annotations with gradient-based model explanations. The resulting\nmodel improves the grounding capabilities of an off-the-shelf\nvision-and-language model. Particularly, SynGround improves the pointing game\naccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on\nRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to\n63.67%.\n","authors":["Ruozhen He","Paola Cascante-Bonilla","Ziyan Yang","Alexander C. Berg","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2403.13804v1.pdf","comment":"Project Page: https://catherine-r-he.github.io/SynGround/"},{"id":"http://arxiv.org/abs/2403.13802v1","updated":"2024-03-20T17:59:14Z","published":"2024-03-20T17:59:14Z","title":"ZigMa: Zigzag Mamba Diffusion Model","summary":"  The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$. Code will be released at https://taohu.me/zigma/\n","authors":["Vincent Tao Hu","Stefan Andreas Baumann","Ming Gui","Olga Grebenkova","Pingchuan Ma","Johannes Fischer","Bjorn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13802v1.pdf","comment":"Project Page: https://taohu.me/zigma/"},{"id":"http://arxiv.org/abs/2403.13801v1","updated":"2024-03-20T17:58:12Z","published":"2024-03-20T17:58:12Z","title":"Natural Language as Polices: Reasoning for Coordinate-Level Embodied\n  Control with LLMs","summary":"  We demonstrate experimental results with LLMs that address robotics action\nplanning problems. Recently, LLMs have been applied in robotics action\nplanning, particularly using a code generation approach that converts complex\nhigh-level instructions into mid-level policy codes. In contrast, our approach\nacquires text descriptions of the task and scene objects, then formulates\naction planning through natural language reasoning, and outputs coordinate\nlevel control commands, thus reducing the necessity for intermediate\nrepresentation code as policies. Our approach is evaluated on a multi-modal\nprompt simulation benchmark, demonstrating that our prompt engineering\nexperiments with natural language reasoning significantly enhance success rates\ncompared to its absence. Furthermore, our approach illustrates the potential\nfor natural language descriptions to transfer robotics skills from known tasks\nto previously unseen tasks.\n","authors":["Yusuke Mikami","Andrew Melnik","Jun Miura","Ville Hautamäki"],"pdf_url":"https://arxiv.org/pdf/2403.13801v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.07923v4","updated":"2024-03-20T17:55:48Z","published":"2023-10-11T22:35:18Z","title":"The Expressive Power of Transformers with Chain of Thought","summary":"  Recent theoretical work has identified surprisingly simple reasoning\nproblems, such as checking if two nodes in a graph are connected or simulating\nfinite-state machines, that are provably unsolvable by standard transformers\nthat answer immediately after reading their input. However, in practice,\ntransformers' reasoning can be improved by allowing them to use a \"chain of\nthought\" or \"scratchpad\", i.e., generate and condition on a sequence of\nintermediate tokens before answering. Motivated by this, we ask: Does such\nintermediate generation fundamentally extend the computational power of a\ndecoder-only transformer? We show that the answer is yes, but the amount of\nincrease depends crucially on the amount of intermediate generation. For\ninstance, we find that transformer decoders with a logarithmic number of\ndecoding steps (w.r.t. the input length) push the limits of standard\ntransformers only slightly, while a linear number of decoding steps, assuming a\nslight generalization to standard pre-norm, adds a clear new ability (under\nstandard complexity conjectures): recognizing all regular languages. Our\nresults also imply that linear steps keep transformer decoders within\ncontext-sensitive languages, and polynomial steps with generalized pre-norm\nmake them recognize exactly the class of polynomial-time solvable problems --\nthe first exact characterization of a type of transformers in terms of standard\ncomplexity classes. Together, our results provide a nuanced framework for\nunderstanding how the length of a transformer's chain of thought or scratchpad\nimpacts its reasoning power.\n","authors":["William Merrill","Ashish Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2310.07923v4.pdf","comment":"9-page preprint. Updated March 20 after ICLR acceptance"},{"id":"http://arxiv.org/abs/2403.13799v1","updated":"2024-03-20T17:55:35Z","published":"2024-03-20T17:55:35Z","title":"Reverse Training to Nurse the Reversal Curse","summary":"  Large language models (LLMs) have a surprising failure: when trained on \"A\nhas a feature B\", they do not generalize to \"B is a feature of A\", which is\ntermed the Reversal Curse. Even when training with trillions of tokens this\nissue still appears due to Zipf's law - hence even if we train on the entire\ninternet. This work proposes an alternative training scheme, called reverse\ntraining, whereby all words are used twice, doubling the amount of available\ntokens. The LLM is trained in both forward and reverse directions by reversing\nthe training strings while preserving (i.e., not reversing) chosen substrings,\nsuch as entities. We show that data-matched reverse-trained models provide\nsuperior performance to standard models on standard tasks, and compute-matched\nreverse-trained models provide far superior performance on reversal tasks,\nhelping resolve the reversal curse issue.\n","authors":["Olga Golovneva","Zeyuan Allen-Zhu","Jason Weston","Sainbayar Sukhbaatar"],"pdf_url":"https://arxiv.org/pdf/2403.13799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13786v1","updated":"2024-03-20T17:47:49Z","published":"2024-03-20T17:47:49Z","title":"Chain-of-Interaction: Enhancing Large Language Models for Psychiatric\n  Behavior Understanding by Dyadic Contexts","summary":"  Automatic coding patient behaviors is essential to support decision making\nfor psychotherapists during the motivational interviewing (MI), a collaborative\ncommunication intervention approach to address psychiatric issues, such as\nalcohol and drug addiction. While the behavior coding task has rapidly adapted\nmachine learning to predict patient states during the MI sessions, lacking of\ndomain-specific knowledge and overlooking patient-therapist interactions are\nmajor challenges in developing and deploying those models in real practice. To\nencounter those challenges, we introduce the Chain-of-Interaction (CoI)\nprompting method aiming to contextualize large language models (LLMs) for\npsychiatric decision support by the dyadic interactions. The CoI prompting\napproach systematically breaks down the coding task into three key reasoning\nsteps, extract patient engagement, learn therapist question strategies, and\nintegrates dyadic interactions between patients and therapists. This approach\nenables large language models to leverage the coding scheme, patient state, and\ndomain knowledge for patient behavioral coding. Experiments on real-world\ndatasets can prove the effectiveness and flexibility of our prompting method\nwith multiple state-of-the-art LLMs over existing prompting baselines. We have\nconducted extensive ablation analysis and demonstrate the critical role of\ndyadic interactions in applying LLMs for psychotherapy behavior understanding.\n","authors":["Guangzeng Han","Weisi Liu","Xiaolei Huang","Brian Borsari"],"pdf_url":"https://arxiv.org/pdf/2403.13786v1.pdf","comment":"Accepted to IEEE ICHI 2024"},{"id":"http://arxiv.org/abs/2403.13780v1","updated":"2024-03-20T17:42:08Z","published":"2024-03-20T17:42:08Z","title":"Information-Theoretic Distillation for Reference-less Summarization","summary":"  The current winning recipe for automatic summarization is using proprietary\nlarge-scale language models (LLMs) such as ChatGPT as is, or imitation learning\nfrom them as teacher models. While increasingly ubiquitous dependence on such\nlarge-scale language models is convenient, there remains an important question\nof whether small-scale models could have achieved competitive results, if we\nwere to seek an alternative learning method -- that allows for a more\ncost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a\nnovel framework to distill a powerful summarizer based on the\ninformation-theoretic objective for summarization, without relying on either\nthe LLM's capability or human-written references. To achieve this, we first\npropose a novel formulation of the desiderata of summarization (saliency,\nfaithfulness and brevity) through the lens of mutual information between the\noriginal document and the summary. Based on this formulation, we start off from\nPythia-2.8B as the teacher model, which is not yet capable of summarization,\nthen self-train the model to optimize for the information-centric measures of\nideal summaries. Distilling from the improved teacher, we arrive at a compact\nbut powerful summarizer with only 568M parameters that performs competitively\nagainst ChatGPT, without ever relying on ChatGPT's capabilities. Extensive\nanalysis demonstrates that our approach outperforms in-domain supervised models\nin human evaluation, let alone state-of-the-art unsupervised methods, and wins\nover ChatGPT in controllable summarization.\n","authors":["Jaehun Jung","Ximing Lu","Liwei Jiang","Faeze Brahman","Peter West","Pang Wei Koh","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2403.13780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11085v2","updated":"2024-03-20T17:35:15Z","published":"2024-03-17T04:36:18Z","title":"m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks","summary":"  Real-world multi-modal problems are rarely solved by a single machine\nlearning model, and often require multi-step computational plans that involve\nstitching several models. Tool-augmented LLMs hold tremendous promise for\nautomating the generation of such computational plans. However, the lack of\nstandardized benchmarks for evaluating LLMs as planners for multi-step\nmulti-modal tasks has prevented a systematic study of planner design decisions.\nShould LLMs generate a full plan in a single shot or step-by-step? Should they\ninvoke tools directly with Python code or through structured data formats like\nJSON? Does feedback improve planning? To answer these questions and more, we\nintroduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks\ninvolving 33 tools that include multi-modal models, (free) public APIs, and\nimage processing modules. For each of these task queries, we provide\nautomatically generated plans using this realistic toolset. We further provide\na high-quality subset of 1,565 task plans that are human-verified and correctly\nexecutable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies\n(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3\ntypes of feedback (parsing/verification/execution). Finally, we summarize\ntakeaways from our extensive experiments. Our dataset and code are available on\nHuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github\n(https://github.com/RAIVNLab/mnms).\n","authors":["Zixian Ma","Weikai Huang","Jieyu Zhang","Tanmay Gupta","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.11085v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14456v4","updated":"2024-03-20T17:16:37Z","published":"2023-05-23T18:27:51Z","title":"Having Beer after Prayer? Measuring Cultural Bias in Large Language\n  Models","summary":"  As the reach of large language models (LMs) expands globally, their ability\nto cater to diverse cultural contexts becomes crucial. Despite advancements in\nmultilingual capabilities, models are not designed with appropriate cultural\nnuances. In this paper, we show that multilingual and Arabic monolingual LMs\nexhibit bias towards entities associated with Western culture. We introduce\nCAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities\nspanning eight types that contrast Arab and Western cultures. CAMeL provides a\nfoundation for measuring cultural biases in LMs through both extrinsic and\nintrinsic evaluations. Using CAMeL, we examine the cross-cultural performance\nin Arabic of 16 different LMs on tasks such as story generation, NER, and\nsentiment analysis, where we find concerning cases of stereotyping and cultural\nunfairness. We further test their text-infilling performance, revealing the\nincapability of appropriate adaptation to Arab cultural contexts. Finally, we\nanalyze 6 Arabic pre-training corpora and find that commonly used sources such\nas Wikipedia may not be best suited to build culturally aware LMs, if used as\nthey are without adjustment. We will make CAMeL publicly available at:\nhttps://github.com/tareknaous/camel\n","authors":["Tarek Naous","Michael J. Ryan","Alan Ritter","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14456v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13754v1","updated":"2024-03-20T17:01:56Z","published":"2024-03-20T17:01:56Z","title":"Different Tokenization Schemes Lead to Comparable Performance in Spanish\n  Number Agreement","summary":"  The relationship between language model tokenization and performance is an\nopen area of research. Here, we investigate how different tokenization schemes\nimpact number agreement in Spanish plurals. We find that\nmorphologically-aligned tokenization performs similarly to other tokenization\nschemes, even when induced artificially for words that would not be tokenized\nthat way during training. We then present exploratory analyses demonstrating\nthat language model embeddings for different plural tokenizations have similar\ndistributions along the embedding space axis that maximally distinguishes\nsingular and plural nouns. Our results suggest that morphologically-aligned\ntokenization is a viable tokenization approach, and existing models already\ngeneralize some morphological patterns to new items. However, our results\nindicate that morphological tokenization is not strictly required for\nperformance.\n","authors":["Catherine Arnett","Pamela D. Rivière","Tyler A. Chang","Sean Trott"],"pdf_url":"https://arxiv.org/pdf/2403.13754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02706v5","updated":"2024-03-20T16:56:48Z","published":"2023-09-06T04:38:16Z","title":"HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models","summary":"  Large language models (LLMs) trained on massive corpora demonstrate\nimpressive capabilities in a wide range of tasks. While there are ongoing\nefforts to adapt these models to languages beyond English, the attention given\nto their evaluation methodologies remains limited. Current multilingual\nbenchmarks often rely on back translations or re-implementations of English\ntests, limiting their capacity to capture unique cultural and linguistic\nnuances. To bridge this gap for the Korean language, we introduce the HAE-RAE\nBench, a dataset curated to challenge models lacking Korean cultural and\ncontextual depth. The dataset encompasses six downstream tasks across four\ndomains: vocabulary, history, general knowledge, and reading comprehension.\nUnlike traditional evaluation suites focused on token and sequence\nclassification or mathematical and logical reasoning, the HAE-RAE Bench\nemphasizes a model's aptitude for recalling Korean-specific knowledge and\ncultural contexts. Comparative analysis with prior Korean benchmarks indicates\nthat the HAE-RAE Bench presents a greater challenge to non-Korean models by\ndisturbing abilities and knowledge learned from English being transferred.\n","authors":["Guijin Son","Hanwool Lee","Suwan Kim","Huiseo Kim","Jaecheol Lee","Je Won Yeom","Jihyu Jung","Jung Woo Kim","Songseong Kim"],"pdf_url":"https://arxiv.org/pdf/2309.02706v5.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.13737v1","updated":"2024-03-20T16:43:42Z","published":"2024-03-20T16:43:42Z","title":"EthioLLM: Multilingual Large Language Models for Ethiopian Languages\n  with Task Evaluation","summary":"  Large language models (LLMs) have gained popularity recently due to their\noutstanding performance in various downstream Natural Language Processing (NLP)\ntasks. However, low-resource languages are still lagging behind current\nstate-of-the-art (SOTA) developments in the field of NLP due to insufficient\nresources to train LLMs. Ethiopian languages exhibit remarkable linguistic\ndiversity, encompassing a wide array of scripts, and are imbued with profound\nreligious and cultural significance. This paper introduces EthioLLM --\nmultilingual large language models for five Ethiopian languages (Amharic,\nGe'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a\nnew benchmark dataset for various downstream NLP tasks. We evaluate the\nperformance of these models across five downstream NLP tasks. We open-source\nour multilingual language models, new benchmark datasets for various downstream\ntasks, and task-specific fine-tuned language models and discuss the performance\nof the models. Our dataset and models are available at the\nhttps://huggingface.co/EthioNLP repository.\n","authors":["Atnafu Lambebo Tonja","Israel Abebe Azime","Tadesse Destaw Belay","Mesay Gemeda Yigezu","Moges Ahmed Mehamed","Abinew Ali Ayele","Ebrahim Chekol Jibril","Michael Melese Woldeyohannis","Olga Kolesnikova","Philipp Slusallek","Dietrich Klakow","Shengwu Xiong","Seid Muhie Yimam"],"pdf_url":"https://arxiv.org/pdf/2403.13737v1.pdf","comment":"Accepted at LREC-Coling 2024"},{"id":"http://arxiv.org/abs/2308.08739v2","updated":"2024-03-20T16:41:11Z","published":"2023-08-17T02:26:30Z","title":"Enhancing Phrase Representation by Information Bottleneck Guided Text\n  Diffusion Process for Keyphrase Extraction","summary":"  Keyphrase extraction (KPE) is an important task in Natural Language\nProcessing for many scenarios, which aims to extract keyphrases that are\npresent in a given document. Many existing supervised methods treat KPE as\nsequential labeling, span-level classification, or generative tasks. However,\nthese methods lack the ability to utilize keyphrase information, which may\nresult in biased results. In this study, we propose Diff-KPE, which leverages\nthe supervised Variational Information Bottleneck (VIB) to guide the text\ndiffusion process for generating enhanced keyphrase representations. Diff-KPE\nfirst generates the desired keyphrase embeddings conditioned on the entire\ndocument and then injects the generated keyphrase embeddings into each phrase\nrepresentation. A ranking network and VIB are then optimized together with rank\nloss and classification loss, respectively. This design of Diff-KPE allows us\nto rank each candidate phrase by utilizing both the information of keyphrases\nand the document. Experiments show that Diff-KPE outperforms existing KPE\nmethods on a large open domain keyphrase extraction benchmark, OpenKP, and a\nscientific domain dataset, KP20K.\n","authors":["Yuanzhen Luo","Qingyu Zhou","Feng Zhou"],"pdf_url":"https://arxiv.org/pdf/2308.08739v2.pdf","comment":"10 pages, 2 figures, accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2310.12963v3","updated":"2024-03-20T16:36:06Z","published":"2023-10-19T17:57:39Z","title":"AutoMix: Automatically Mixing Language Models","summary":"  Large language models (LLMs) are now available from cloud API providers in\nvarious sizes and configurations. While this diversity offers a broad spectrum\nof choices, effectively leveraging the options to optimize computational cost\nand performance remains challenging. In this work, we present AutoMix, an\napproach that strategically routes queries to larger LMs, based on the\napproximate correctness of outputs from a smaller LM. Central to AutoMix is a\nfew-shot self-verification mechanism, which estimates the reliability of its\nown outputs without requiring training. Given that verifications can be noisy,\nwe employ a meta-verifier in AutoMix to refine the accuracy of these\nassessments. Our experiments using LLAMA2-13B and GPT-4, on five\ncontext-grounded reasoning datasets demonstrate that AutoMix surpasses\nestablished baselines, improving the incremental benefit per cost by up to 86%.\nOur code and data are available at https://github.com/automix-llm/automix.\n","authors":["Aman Madaan","Pranjal Aggarwal","Ankit Anand","Srividya Pranavi Potharaju","Swaroop Mishra","Pei Zhou","Aditya Gupta","Dheeraj Rajagopal","Karthik Kappaganthu","Yiming Yang","Shyam Upadhyay"," Mausam","Manaal Faruqui"],"pdf_url":"https://arxiv.org/pdf/2310.12963v3.pdf","comment":"The first two authors contributed equally. Work started and partly\n  done during Aman's internship at Google. This version adds results on\n  additional models and datasets"},{"id":"http://arxiv.org/abs/2309.07915v3","updated":"2024-03-20T16:17:02Z","published":"2023-09-14T17:59:17Z","title":"MMICL: Empowering Vision-language Model with Multi-Modal In-Context\n  Learning","summary":"  Since the resurgence of deep learning, vision-language models (VLMs) enhanced\nby large language models (LLMs) have grown exponentially in popularity.\nHowever, while LLMs can utilize extensive background knowledge and task\ninformation with in-context learning, most VLMs still struggle with\nunderstanding complex multi-modal prompts with multiple images, making VLMs\nless effective in downstream vision-language tasks. In this paper, we address\nthe limitation above by 1) introducing vision-language Model with Multi-Modal\nIn-Context Learning(MMICL), a new approach to allow the VLM to deal with\nmulti-modal inputs efficiently; 2) proposing a novel context scheme to augment\nthe in-context learning ability of the VLM; 3) constructing the Multi-modal\nIn-Context Learning (MIC) dataset, designed to enhance the VLM's ability to\nunderstand complex multi-modal prompts. Our experiments confirm that MMICL\nachieves new state-of-the-art zero-shot performance on a wide range of general\nvision-language tasks, especially for complex benchmarks, including MME and\nMMBench. Our analysis demonstrates that MMICL effectively tackles the challenge\nof complex multi-modal prompt understanding and emerges the impressive ICL\nability. Furthermore, we observe that MMICL successfully alleviates language\nbias in VLMs, a common issue for VLMs that often leads to hallucination when\nfaced with extensive textual context. Our code, dataset, dataset tool, and\nmodel are available at https://github.com/PKUnlp-icler/MIC\n","authors":["Haozhe Zhao","Zefan Cai","Shuzheng Si","Xiaojian Ma","Kaikai An","Liang Chen","Zixuan Liu","Sheng Wang","Wenjuan Han","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2309.07915v3.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2306.17447v3","updated":"2024-03-20T15:53:37Z","published":"2023-06-30T07:44:49Z","title":"Correct Like Humans: Progressive Learning Framework for Chinese Text\n  Error Correction","summary":"  Chinese Text Error Correction (CTEC) aims to detect and correct errors in the\ninput text, which benefits human daily life and various downstream tasks.\nRecent approaches mainly employ Pre-trained Language Models (PLMs) to resolve\nCTEC. Although PLMs have achieved remarkable success in CTEC, we argue that\nprevious studies still overlook the importance of human thinking patterns. To\nenhance the development of PLMs for CTEC, inspired by humans' daily\nerror-correcting behavior, we propose a novel model-agnostic progressive\nlearning framework, named ProTEC, which guides PLMs-based CTEC models to learn\nto correct like humans. During the training process, ProTEC guides the model to\nlearn text error correction by incorporating these sub-tasks into a progressive\nparadigm. During the inference process, the model completes these sub-tasks in\nturn to generate the correction results. Extensive experiments and detailed\nanalyses demonstrate the effectiveness and efficiency of our proposed\nmodel-agnostic ProTEC framework.\n","authors":["Yinghui Li","Shirong Ma","Shaoshen Chen","Haojing Huang","Shulin Huang","Yangning Li","Hai-Tao Zheng","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2306.17447v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09098v2","updated":"2024-03-20T15:41:07Z","published":"2023-05-16T01:51:22Z","title":"Weight-Inherited Distillation for Task-Agnostic BERT Compression","summary":"  Knowledge Distillation (KD) is a predominant approach for BERT compression.\nPrevious KD-based methods focus on designing extra alignment losses for the\nstudent model to mimic the behavior of the teacher model. These methods\ntransfer the knowledge in an indirect way. In this paper, we propose a novel\nWeight-Inherited Distillation (WID), which directly transfers knowledge from\nthe teacher. WID does not require any additional alignment loss and trains a\ncompact student by inheriting the weights, showing a new perspective of\nknowledge distillation. Specifically, we design the row compactors and column\ncompactors as mappings and then compress the weights via structural\nre-parameterization. Experimental results on the GLUE and SQuAD benchmarks show\nthat WID outperforms previous state-of-the-art KD-based baselines. Further\nanalysis indicates that WID can also learn the attention patterns from the\nteacher model without any alignment loss on attention distributions. The code\nis available at https://github.com/wutaiqiang/WID-NAACL2024.\n","authors":["Taiqiang Wu","Cheng Hou","Shanshan Lao","Jiayi Li","Ngai Wong","Zhe Zhao","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2305.09098v2.pdf","comment":"9 pages, 4 figures, NAACL2024 findings"},{"id":"http://arxiv.org/abs/2403.13681v1","updated":"2024-03-20T15:39:54Z","published":"2024-03-20T15:39:54Z","title":"PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned\n  Language Model for Indian Legal Case Documents","summary":"  In this paper, we present PARAMANU-AYN, a language model based exclusively on\ncase documents of the Supreme Court of India, the Constitution of India, and\nthe Indian Penal Code. The novel Auto Regressive (AR) decoder based model is\npretrained from scratch at a context size of 8192. We evaluated our pretrained\nlegal model on perplexity metrics. We also instruction-tuned our pretrained\nmodel on a set of 10,763 instructions covering various legal tasks such as\nlegal reasoning, judgement explanation, legal clause generation, legal\ndrafting, legal contract drafting, case summarization, constitutional\nquestion-answering, etc. We also evaluated the responses of prompts for\ninstruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness,\nand legal reasoning metrics in a scale of 10. Our model can be run on CPU and\nachieved 42.46 tokens/sec CPU inference speed. We found that our models,\ndespite not being pretrained on legal books, various legal contracts, and legal\ndocuments, were able to learn the domain knowledge required for drafting\nvarious legal contracts and legal clauses, and generalize to draft legal\ncontracts and legal clauses with limited instruction tuning. Hence, we conclude\nthat for a strong domain-specialized generative language model (such as legal),\nvery large amounts of data are not required to develop models from scratch. We\nbelieve that this work is the first attempt to make a dedicated generative\nlegal language model from scratch for Indian Supreme Court jurisdiction or in\nlegal NLP overall. We plan to release our Paramanu-Ayn model at\nhttps://www.bharatgpts.com.\n","authors":["Mitodru Niyogi","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2403.13681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13679v1","updated":"2024-03-20T15:38:36Z","published":"2024-03-20T15:38:36Z","title":"RoleInteract: Evaluating the Social Interaction of Role-Playing Agents","summary":"  Large language models (LLMs) have advanced the development of various AI\nconversational agents, including role-playing conversational agents that mimic\ndiverse characters and human behaviors. While prior research has predominantly\nfocused on enhancing the conversational capability, role-specific knowledge,\nand stylistic attributes of these agents, there has been a noticeable gap in\nassessing their social intelligence. In this paper, we introduce RoleInteract,\nthe first benchmark designed to systematically evaluate the sociality of\nrole-playing conversational agents at both individual and group levels of\nsocial interactions. The benchmark is constructed from a variety of sources and\ncovers a wide range of 500 characters and over 6,000 question prompts and\n30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations\non this benchmark using mainstream open-source and closed-source LLMs. We find\nthat agents excelling in individual level does not imply their proficiency in\ngroup level. Moreover, the behavior of individuals may drift as a result of the\ninfluence exerted by other agents within the group. Experimental results on\nRoleInteract confirm its significance as a testbed for assessing the social\ninteraction of role-playing conversational agents. The benchmark is publicly\naccessible at https://github.com/X-PLUG/RoleInteract.\n","authors":["Hongzhan Chen","Hehong Chen","Ming Yan","Wenshen Xu","Xing Gao","Weizhou Shen","Xiaojun Quan","Chenliang Li","Ji Zhang","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.13679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13666v1","updated":"2024-03-20T15:20:30Z","published":"2024-03-20T15:20:30Z","title":"Grounding Spatial Relations in Text-Only Language Models","summary":"  This paper shows that text-only Language Models (LM) can learn to ground\nspatial relations like \"left of\" or \"below\" if they are provided with explicit\nlocation information of objects and they are properly trained to leverage those\nlocations. We perform experiments on a verbalized version of the Visual Spatial\nReasoning (VSR) dataset, where images are coupled with textual statements which\ncontain real or fake spatial relations between two objects of the image. We\nverbalize the images using an off-the-shelf object detector, adding location\ntokens to every object label to represent their bounding boxes in textual form.\nGiven the small size of VSR, we do not observe any improvement when using\nlocations, but pretraining the LM over a synthetic dataset automatically\nderived by us improves results significantly when using location tokens. We\nthus show that locations allow LMs to ground spatial relations, with our\ntext-only LMs outperforming Vision-and-Language Models and setting the new\nstate-of-the-art for the VSR dataset. Our analysis show that our text-only LMs\ncan generalize beyond the relations seen in the synthetic dataset to some\nextent, learning also more useful information than that encoded in the spatial\nrules we used to create the synthetic dataset itself.\n","authors":["Gorka Azkune","Ander Salaberria","Eneko Agirre"],"pdf_url":"https://arxiv.org/pdf/2403.13666v1.pdf","comment":"Accepted in Neural Networks"},{"id":"http://arxiv.org/abs/2403.13638v1","updated":"2024-03-20T14:41:01Z","published":"2024-03-20T14:41:01Z","title":"Do Not Worry if You Do Not Have Data: Building Pretrained Language\n  Models Using Translationese","summary":"  In this paper, we explore the utility of \\textit{Translationese} as synthetic\ndata created using machine translation for pre-training language models (LMs).\nPre-training requires vast amounts of monolingual data, which is mostly\nunavailable for languages other than English. Recently, there has been a\ngrowing interest in using synthetic data to address this data scarcity. We take\nthe case of English and Indic languages and translate web-crawled monolingual\ndocuments (clean) into the target language. Then, we train language models\ncontaining 28M and 85M parameters on this translationese data (synthetic). We\nshow that their performance on downstream natural language understanding and\ngenerative tasks is only 3.56\\% poorer on NLU tasks and 1.51\\% on NLG tasks\nthan LMs pre-trained on clean data. Further, we propose the use of lightweight\n\\textit{TinyLMs} pre-trained on clean data to filter synthetic data efficiently\nwhich significantly improves the performance of our models. We also find that\nLMs trained on synthetic data strongly benefit from extended pretraining on a\ntiny fraction (10\\%) of clean data. We release the data we collected and\ncreated as a part of this work, \\textit{IndicMonoDoc}, the largest collection\nof monolingual document-level corpora, which we hope will help bridge the gap\nbetween English and non-English performance for large language models.\n","authors":["Meet Doshi","Raj Dabre","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2403.13638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09002v3","updated":"2024-03-20T14:08:39Z","published":"2024-01-17T06:42:44Z","title":"AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on\n  Large Language Models","summary":"  In our research, we pioneer a novel approach to evaluate the effectiveness of\njailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2,\ndiverging from traditional robustness-focused binary evaluations. Our study\nintroduces two distinct evaluation frameworks: a coarse-grained evaluation and\na fine-grained evaluation. Each framework, using a scoring range from 0 to 1,\noffers a unique perspective, enabling a more comprehensive and nuanced\nevaluation of attack effectiveness and empowering attackers to refine their\nattack prompts with greater understanding. Furthermore, we have developed a\ncomprehensive ground truth dataset specifically tailored for jailbreak tasks.\nThis dataset not only serves as a crucial benchmark for our current study but\nalso establishes a foundational resource for future research, enabling\nconsistent and comparative analyses in this evolving field. Upon meticulous\ncomparison with traditional evaluation methods, we discovered that our\nevaluation aligns with the baseline's trend while offering a more profound and\ndetailed assessment. We believe that by accurately evaluating the effectiveness\nof attack prompts in the Jailbreak task, our work lays a solid foundation for\nassessing a wider array of similar or even more complex tasks in the realm of\nprompt injection, potentially revolutionizing this field.\n","authors":["Dong shu","Mingyu Jin","Suiyuan Zhu","Beichen Wang","Zihao Zhou","Chong Zhang","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.09002v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13592v1","updated":"2024-03-20T13:42:57Z","published":"2024-03-20T13:42:57Z","title":"Llama meets EU: Investigating the European Political Spectrum through\n  the Lens of LLMs","summary":"  Instruction-finetuned Large Language Models inherit clear political leanings\nthat have been shown to influence downstream task performance. We expand this\nline of research beyond the two-party system in the US and audit Llama Chat in\nthe context of EU politics in various settings to analyze the model's political\nknowledge and its ability to reason in context. We adapt, i.e., further\nfine-tune, Llama Chat on speeches of individual euro-parties from debates in\nthe European Parliament to reevaluate its political leaning based on the EUandI\nquestionnaire. Llama Chat shows considerable knowledge of national parties'\npositions and is capable of reasoning in context. The adapted, party-specific,\nmodels are substantially re-aligned towards respective positions which we see\nas a starting point for using chat-based LLMs as data-driven conversational\nengines to assist research in political science.\n","authors":["Ilias Chalkidis","Stephanie Brandl"],"pdf_url":"https://arxiv.org/pdf/2403.13592v1.pdf","comment":"accepted to NAACL 2024 as a short paper"},{"id":"http://arxiv.org/abs/2403.13590v1","updated":"2024-03-20T13:38:07Z","published":"2024-03-20T13:38:07Z","title":"Teacher-Student Training for Debiasing: General Permutation Debiasing\n  for Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated impressive zero-shot\ncapabilities and versatility in NLP tasks, however they sometimes fail to\nmaintain crucial invariances for specific tasks. One example is permutation\nsensitivity, where LLMs' outputs may significantly vary depending on the order\nof the input options. While debiasing techniques can mitigate these issues, and\nyield better performance and reliability, they often come with a high\ncomputational cost at inference. This paper addresses this inefficiency at\ninference time. The aim is to distill the capabilities of a computationally\nintensive, debiased, teacher model into a more compact student model. We\nexplore two variants of student models: one based on pure distillation, and the\nother on an error-correction approach for more complex tasks, where the student\ncorrects a single biased decision from the teacher to achieve a debiased\noutput. Our approach is general and can be applied to both black-box and\nwhite-box LLMs. Furthermore, we demonstrate that our compact, encoder-only\nstudent models can outperform their larger, biased teacher counterparts,\nachieving better results with significantly fewer parameters.\n","authors":["Adian Liusie","Yassir Fathullah","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2403.13590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13588v1","updated":"2024-03-20T13:37:00Z","published":"2024-03-20T13:37:00Z","title":"Genetic Auto-prompt Learning for Pre-trained Code Intelligence Language\n  Models","summary":"  As Pre-trained Language Models (PLMs), a popular approach for code\nintelligence, continue to grow in size, the computational cost of their usage\nhas become prohibitively expensive. Prompt learning, a recent development in\nthe field of natural language processing, emerges as a potential solution to\naddress this challenge. In this paper, we investigate the effectiveness of\nprompt learning in code intelligence tasks. We unveil its reliance on manually\ndesigned prompts, which often require significant human effort and expertise.\nMoreover, we discover existing automatic prompt design methods are very limited\nto code intelligence tasks due to factors including gradient dependence, high\ncomputational demands, and limited applicability. To effectively address both\nissues, we propose Genetic Auto Prompt (GenAP), which utilizes an elaborate\ngenetic algorithm to automatically design prompts. With GenAP, non-experts can\neffortlessly generate superior prompts compared to meticulously manual-designed\nones. GenAP operates without the need for gradients or additional computational\ncosts, rendering it gradient-free and cost-effective. Moreover, GenAP supports\nboth understanding and generation types of code intelligence tasks, exhibiting\ngreat applicability. We conduct GenAP on three popular code intelligence PLMs\nwith three canonical code intelligence tasks including defect prediction, code\nsummarization, and code translation. The results suggest that GenAP can\neffectively automate the process of designing prompts. Specifically, GenAP\noutperforms all other methods across all three tasks (e.g., improving accuracy\nby an average of 2.13% for defect prediction). To the best of our knowledge,\nGenAP is the first work to automatically design prompts for code intelligence\nPLMs.\n","authors":["Chengzhe Feng","Yanan Sun","Ke Li","Pan Zhou","Jiancheng Lv","Aojun Lu"],"pdf_url":"https://arxiv.org/pdf/2403.13588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13583v1","updated":"2024-03-20T13:33:55Z","published":"2024-03-20T13:33:55Z","title":"CONLINE: Complex Code Generation and Refinement with Online Searching\n  and Correctness Testing","summary":"  Large Language Models (LLMs) have revolutionized code generation ability by\nconverting natural language descriptions into executable code. However,\ngenerating complex code within real-world scenarios remains challenging due to\nintricate structures, subtle bugs, understanding of advanced data types, and\nlack of supplementary contents. To address these challenges, we introduce the\nCONLINE framework, which enhances code generation by incorporating planned\nonline searches for information retrieval and automated correctness testing for\niterative refinement. CONLINE also serializes the complex inputs and outputs to\nimprove comprehension and generate test case to ensure the framework's\nadaptability for real-world applications. CONLINE is validated through rigorous\nexperiments on the DS-1000 and ClassEval datasets. It shows that CONLINE\nsubstantially improves the quality of complex code generation, highlighting its\npotential to enhance the practicality and reliability of LLMs in generating\nintricate code.\n","authors":["Xinyi He","Jiaru Zou","Yun Lin","Mengyu Zhou","Shi Han","Zejian Yuan","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.13583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08015v2","updated":"2024-03-20T13:33:19Z","published":"2024-02-12T19:25:11Z","title":"Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and\n  Generative Datasets","summary":"  Large language models (LLMs) have received a lot of attention in natural\nlanguage processing (NLP) research because of their exceptional performance in\nunderstanding and generating human languages. However, low-resource languages\nare left behind due to the unavailability of resources. In this work, we focus\non enhancing the LLaMA-2-Amharic model by integrating task-specific and\ngenerative datasets to improve language model performance for Amharic. We\ncompile an Amharic instruction fine-tuning dataset and fine-tuned\nLLaMA-2-Amharic model. The fine-tuned model shows promising results in\ndifferent NLP tasks. We open-source our dataset creation pipeline, instruction\ndatasets, trained models, and evaluation outputs to promote language-specific\nstudies on these models.\n","authors":["Israel Abebe Azime","Atnafu Lambebo Tonja","Tadesse Destaw Belay","Mitiku Yohannes Fuge","Aman Kassahun Wassie","Eyasu Shiferaw Jada","Yonas Chanie","Walelign Tewabe Sewunetie","Seid Muhie Yimam"],"pdf_url":"https://arxiv.org/pdf/2402.08015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13578v1","updated":"2024-03-20T13:24:41Z","published":"2024-03-20T13:24:41Z","title":"Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for\n  Counselor Reflection Generation","summary":"  In this paper, we study the problem of multi-reward reinforcement learning to\njointly optimize for multiple text qualities for natural language generation.\nWe focus on the task of counselor reflection generation, where we optimize the\ngenerators to simultaneously improve the fluency, coherence, and reflection\nquality of generated counselor responses. We introduce two novel bandit\nmethods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining\nrewards into a single value and optimizing them simultaneously. Specifically,\nwe employ non-contextual and contextual multi-arm bandits to dynamically adjust\nmultiple reward weights during training. Through automatic and manual\nevaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt,\noutperform existing naive and bandit baselines, showcasing their potential for\nenhancing language models.\n","authors":["Do June Min","Veronica Perez-Rosas","Kenneth Resnicow","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2403.13578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18248v3","updated":"2024-03-20T13:12:48Z","published":"2023-05-29T17:12:03Z","title":"Do Language Models Know When They're Hallucinating References?","summary":"  State-of-the-art language models (LMs) are notoriously susceptible to\ngenerating hallucinated information. Such inaccurate outputs not only undermine\nthe reliability of these models but also limit their use and raise serious\nconcerns about misinformation and propaganda. In this work, we focus on\nhallucinated book and article references and present them as the \"model\norganism\" of language model hallucination research, due to their frequent and\neasy-to-discern nature. We posit that if a language model cites a particular\nreference in its output, then it should ideally possess sufficient information\nabout its authors and content, among other relevant details. Using this basic\ninsight, we illustrate that one can identify hallucinated references without\never consulting any external resources, by asking a set of direct or indirect\nqueries to the language model about the references. These queries can be\nconsidered as \"consistency checks.\" Our findings highlight that while LMs,\nincluding GPT-4, often produce inconsistent author lists for hallucinated\nreferences, they also often accurately recall the authors of real references.\nIn this sense, the LM can be said to \"know\" when it is hallucinating\nreferences. Furthermore, these findings show how hallucinated references can be\ndissected to shed light on their nature. Replication code and results can be\nfound at https://github.com/microsoft/hallucinated-references.\n","authors":["Ayush Agrawal","Mirac Suzgun","Lester Mackey","Adam Tauman Kalai"],"pdf_url":"https://arxiv.org/pdf/2305.18248v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13560v1","updated":"2024-03-20T12:52:38Z","published":"2024-03-20T12:52:38Z","title":"eRST: A Signaled Graph Theory of Discourse Relations and Organization","summary":"  In this article we present Enhanced Rhetorical Structure Theory (eRST), a new\ntheoretical framework for computational discourse analysis, based on an\nexpansion of Rhetorical Structure Theory (RST). The framework encompasses\ndiscourse relation graphs with tree-breaking, nonprojective and concurrent\nrelations, as well as implicit and explicit signals which give explainable\nrationales to our analyses. We survey shortcomings of RST and other existing\nframeworks, such as Segmented Discourse Representation Theory (SDRT), the Penn\nDiscourse Treebank (PDTB) and Discourse Dependencies, and address these using\nconstructs in the proposed theory. We provide annotation, search and\nvisualization tools for data, and present and evaluate a freely available\ncorpus of English annotated according to our framework, encompassing 12 spoken\nand written genres with over 200K tokens. Finally, we discuss automatic\nparsing, evaluation metrics and applications for data in our framework.\n","authors":["Amir Zeldes","Tatsuya Aoyama","Yang Janet Liu","Siyao Peng","Debopam Das","Luke Gessler"],"pdf_url":"https://arxiv.org/pdf/2403.13560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13537v1","updated":"2024-03-20T12:14:54Z","published":"2024-03-20T12:14:54Z","title":"What explains the success of cross-modal fine-tuning with ORCA?","summary":"  ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning,\ni.e., applying pre-trained transformer models to modalities beyond their\ntraining data. The technique consists primarily of training an embedder and\nfine-tuning the embedder and model. Despite its high performance on a variety\nof downstream tasks, we do not understand precisely how each of these\ncomponents contribute to ORCA's success. Therefore, we run a series of\nablations and find that embedder training does not help 2D tasks at all,\ncontrary to what the original paper posits. In 1D tasks, some amount of\nembedder training is necessary but more is not better. In 4 out of 6 datasets\nwe experiment with, it is model fine-tuning that makes the biggest difference.\nThrough our ablations and baselines, we contribute a better understanding of\nthe individual components of ORCA.\n","authors":["Paloma García-de-Herreros","Vagrant Gautam","Philipp Slusallek","Dietrich Klakow","Marius Mosbach"],"pdf_url":"https://arxiv.org/pdf/2403.13537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05113v3","updated":"2024-03-20T11:56:52Z","published":"2023-07-11T08:45:46Z","title":"Piecing Together Clues: A Benchmark for Evaluating the Detective Skills\n  of Large Language Models","summary":"  Detectives frequently engage in information detection and reasoning\nsimultaneously when making decisions across various cases, especially when\nconfronted with a vast amount of information. With the rapid development of\nlarge language models~(LLMs), evaluating how these models identify key\ninformation and reason to solve questions becomes increasingly relevant. We\nintroduces the DetectBench, a reading comprehension dataset designed to assess\na model's ability to jointly ability in key information detection and multi-hop\nreasoning when facing complex and implicit information. The DetectBench\ncomprises 3,928 questions, each paired with a paragraph averaging 190 tokens in\nlength. To enhance model's detective skills, we propose the Detective Thinking\nFramework. These methods encourage models to identify all possible clues within\nthe context before reasoning. Our experiments reveal that existing models\nperform poorly in both information detection and multi-hop reasoning. However,\nthe Detective Thinking Framework approach alleviates this issue.\n","authors":["Zhouhong Gu","Lin Zhang","Jiangjie Chen","Haoning Ye","Xiaoxuan Zhu","Zihan Li","Zheyu Ye","Yan Gao","Yao Hu","Yanghua Xiao","Hongwei Feng"],"pdf_url":"https://arxiv.org/pdf/2307.05113v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13518v1","updated":"2024-03-20T11:38:30Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate motion sequences from given textual\ndescriptions, where a model should explore the interactions between natural\nlanguage instructions and human body movements. While most existing works are\nconfined to coarse-grained motion descriptions (e.g., \"A man squats.\"),\nfine-grained ones specifying movements of relevant body parts are barely\nexplored. Models trained with coarse texts may not be able to learn mappings\nfrom fine-grained motion-related words to motion primitives, resulting in the\nfailure in generating motions from unseen descriptions. In this paper, we build\na large-scale language-motion dataset with fine-grained textual descriptions,\nFineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, which makes full use of\nfine-grained textual information. Our experiments show that FineMotionDiffuse\ntrained on FineHumanML3D acquires good results in quantitative evaluation. We\nalso find this model can better generate spatially/chronologically composite\nmotions by learning the implicit mappings from simple descriptions to the\ncorresponding basic motions.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11117v5","updated":"2024-03-20T11:35:04Z","published":"2023-03-20T13:58:35Z","title":"EmotionIC: emotional inertia and contagion-driven dependency modeling\n  for emotion recognition in conversation","summary":"  Emotion Recognition in Conversation (ERC) has attracted growing attention in\nrecent years as a result of the advancement and implementation of\nhuman-computer interface technologies. In this paper, we propose an emotional\ninertia and contagion-driven dependency modeling approach (EmotionIC) for ERC\ntask. Our EmotionIC consists of three main components, i.e., Identity Masked\nMulti-Head Attention (IMMHA), Dialogue-based Gated Recurrent Unit (DiaGRU), and\nSkip-chain Conditional Random Field (SkipCRF). Compared to previous ERC models,\nEmotionIC can model a conversation more thoroughly at both the\nfeature-extraction and classification levels. The proposed model attempts to\nintegrate the advantages of attention- and recurrence-based methods at the\nfeature-extraction level. Specifically, IMMHA is applied to capture\nidentity-based global contextual dependencies, while DiaGRU is utilized to\nextract speaker- and temporal-aware local contextual information. At the\nclassification level, SkipCRF can explicitly mine complex emotional flows from\nhigher-order neighboring utterances in the conversation. Experimental results\nshow that our method can significantly outperform the state-of-the-art models\non four benchmark datasets. The ablation studies confirm that our modules can\neffectively model emotional inertia and contagion.\n","authors":["Yingjian Liu","Jiang Li","Xiaoping Wang","Zhigang Zeng"],"pdf_url":"https://arxiv.org/pdf/2303.11117v5.pdf","comment":"Accepted by SCIENCE CHINA Information Sciences (SCIS)"},{"id":"http://arxiv.org/abs/2403.13514v1","updated":"2024-03-20T11:30:45Z","published":"2024-03-20T11:30:45Z","title":"How Gender Interacts with Political Values: A Case Study on Czech BERT\n  Models","summary":"  Neural language models, which reach state-of-the-art results on most natural\nlanguage processing tasks, are trained on large text corpora that inevitably\ncontain value-burdened content and often capture undesirable biases, which the\nmodels reflect. This case study focuses on the political biases of pre-trained\nencoders in Czech and compares them with a representative value survey. Because\nCzech is a gendered language, we also measure how the grammatical gender\ncoincides with responses to men and women in the survey. We introduce a novel\nmethod for measuring the model's perceived political values. We find that the\nmodels do not assign statement probability following value-driven reasoning,\nand there is no systematic difference between feminine and masculine sentences.\nWe conclude that BERT-sized models do not manifest systematic alignment with\npolitical values and that the biases observed in the models are rather due to\nsuperficial imitation of training data patterns than systematic value beliefs\nencoded in the models.\n","authors":["Adnan Al Ali","Jindřich Libovický"],"pdf_url":"https://arxiv.org/pdf/2403.13514v1.pdf","comment":"11 pages, 2 figures; LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.13513v1","updated":"2024-03-20T11:27:20Z","published":"2024-03-20T11:27:20Z","title":"What if...?: Counterfactual Inception to Mitigate Hallucination Effects\n  in Large Multimodal Models","summary":"  This paper presents a way of enhancing the reliability of Large Multimodal\nModels (LMMs) in addressing hallucination effects, where models generate\nincorrect or unrelated responses. Without additional instruction tuning\nparadigm, we introduce Counterfactual Inception, a novel method that implants\ncounterfactual thoughts into LMMs using carefully chosen, misaligned\ncounterfactual keywords. This method is grounded in the concept of\ncounterfactual thinking, a cognitive process where humans consider alternative\nrealities and outcomes. By applying this human-like reasoning mechanism to\nLMMs, we aim to reduce hallucination effects and improve the models'\ntrustworthiness. We also propose Dual-modality Verification Process (DVP), a\nrigorous framework for selecting optimal counterfactual keywords to trigger\ncounterfactual thinking into LMMs, concurrently considering visual and\nlinguistic context. Our extensive experiments across various LMMs, including\nboth open-source and proprietary models, corroborate that our method\nsignificantly mitigates hallucination phenomena across different datasets.\n","authors":["Junho Kim","Yeon Ju Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2403.13513v1.pdf","comment":"under review, code available:\n  https://github.com/IVY-LVLM/Counterfactual-Inception"},{"id":"http://arxiv.org/abs/2310.20204v3","updated":"2024-03-20T10:52:03Z","published":"2023-10-31T06:04:18Z","title":"General-Purpose Retrieval-Enhanced Medical Prediction Model Using\n  Near-Infinite History","summary":"  Developing clinical prediction models (e.g., mortality prediction) based on\nelectronic health records (EHRs) typically relies on expert opinion for feature\nselection and adjusting observation window size. This burdens experts and\ncreates a bottleneck in the development process. We propose Retrieval-Enhanced\nMedical prediction model (REMed) to address such challenges. REMed can\nessentially evaluate an unlimited number of clinical events, select the\nrelevant ones, and make predictions. This approach effectively eliminates the\nneed for manual feature selection and enables an unrestricted observation\nwindow. We verified these properties through experiments on 27 clinical tasks\nand two independent cohorts from publicly available EHR datasets, where REMed\noutperformed other contemporary architectures that aim to handle as many events\nas possible. Notably, we found that the preferences of REMed align closely with\nthose of medical experts. We expect our approach to significantly expedite the\ndevelopment of EHR prediction models by minimizing clinicians' need for manual\ninvolvement.\n","authors":["Junu Kim","Chaeeun Shim","Bosco Seong Kyu Yang","Chami Im","Sung Yoon Lim","Han-Gil Jeong","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2310.20204v3.pdf","comment":"The source codes corresponding to this paper are available at:\n  https://github.com/starmpcc/REMed"},{"id":"http://arxiv.org/abs/2403.13485v1","updated":"2024-03-20T10:40:01Z","published":"2024-03-20T10:40:01Z","title":"An Entropy-based Text Watermarking Detection Method","summary":"  Currently, text watermarking algorithms for large language models (LLMs) can\nembed hidden features to texts generated by LLMs to facilitate subsequent\ndetection, thus alleviating the problem of misuse of LLMs. Although the current\ntext watermarking algorithms perform well in most high-entropy scenarios, its\nperformance in low-entropy scenarios still needs to be improved. In this work,\nwe proposed that the influence of token entropy should be fully considered in\nthe watermark detection process, that is, the weight of each token should be\nadjusted according to its entropy during watermark detection, rather than\nsetting the weight of all tokens to the same value as in previous methods.\nSpecifically, we proposed an Entropy-based Watermark Detection (EWD) that gives\nhigher-entropy tokens higher weights during watermark detection, so as to\nbetter reflect the degree of watermarking. Furthermore, the proposed detection\nprocess is training-free and fully automated. %In actual detection, we use a\nproxy-LLM to calculate the entropy of each token, without the need to use the\noriginal LLM. In the experiment, we found that our method can achieve better\ndetection performance in low-entropy scenarios, and our method is also general\nand can be applied to texts with different entropy distributions. Our code and\ndata will be available online.\n","authors":["Yijian Lu","Aiwei Liu","Dianzhi Yu","Jingjing Li","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2403.13485v1.pdf","comment":"8 pages, 5 figures, submitted to ARR Feb 2024"},{"id":"http://arxiv.org/abs/2309.09783v2","updated":"2024-03-20T10:33:24Z","published":"2023-09-18T14:01:06Z","title":"The ParlaSent Multilingual Training Dataset for Sentiment Identification\n  in Parliamentary Proceedings","summary":"  The paper presents a new training dataset of sentences in 7 languages,\nmanually annotated for sentiment, which are used in a series of experiments\nfocused on training a robust sentiment identifier for parliamentary\nproceedings. The paper additionally introduces the first domain-specific\nmultilingual transformer language model for political science applications,\nwhich was additionally pre-trained on 1.72 billion words from parliamentary\nproceedings of 27 European parliaments. We present experiments demonstrating\nhow the additional pre-training on parliamentary data can significantly improve\nthe model downstream performance, in our case, sentiment identification in\nparliamentary proceedings. We further show that our multilingual model performs\nvery well on languages not seen during fine-tuning, and that additional\nfine-tuning data from other languages significantly improves the target\nparliament's results. The paper makes an important contribution to multiple\ndisciplines inside the social sciences, and bridges them with computer science\nand computational linguistics. Lastly, the resulting fine-tuned language model\nsets up a more robust approach to sentiment analysis of political texts across\nlanguages, which allows scholars to study political sentiment from a\ncomparative perspective using standardized tools and techniques.\n","authors":["Michal Mochtak","Peter Rupnik","Nikola Ljubešić"],"pdf_url":"https://arxiv.org/pdf/2309.09783v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06832v2","updated":"2024-03-20T10:02:54Z","published":"2024-03-11T15:48:43Z","title":"The Power of Noise: Toward a Unified Multi-modal Knowledge Graph\n  Representation Framework","summary":"  The advancement of Multi-modal Pre-training highlights the necessity for a\nrobust Multi-Modal Knowledge Graph (MMKG) representation learning framework.\nThis framework is crucial for integrating structured knowledge into multi-modal\nLarge Language Models (LLMs) at scale, aiming to alleviate issues like\nknowledge misconceptions and multi-modal hallucinations. In this work, to\nevaluate models' ability to accurately embed entities within MMKGs, we focus on\ntwo widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and\nMulti-modal Entity Alignment (MMEA). Building on this foundation, we propose a\nnovel SNAG method that utilizes a Transformer-based architecture equipped with\nmodality-level noise masking for the robust integration of multi-modal entity\nfeatures in KGs. By incorporating specific training objectives for both MKGC\nand MMEA, our approach achieves SOTA performance across a total of ten datasets\n(three for MKGC and seven for MEMA), demonstrating its robustness and\nversatility. Besides, SNAG can not only function as a standalone model but also\nenhance other existing methods, providing stable performance improvements. Our\ncode and data are available at: https://github.com/zjukg/SNAG.\n","authors":["Zhuo Chen","Yin Fang","Yichi Zhang","Lingbing Guo","Jiaoyan Chen","Huajun Chen","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06832v2.pdf","comment":"Ongoing work; 10 pages, 6 Tables, 2 Figures; Repo is available at\n  https://github.com/zjukg/SNAG"},{"id":"http://arxiv.org/abs/2403.02889v2","updated":"2024-03-20T09:53:17Z","published":"2024-03-05T11:50:01Z","title":"In Search of Truth: An Interrogation Approach to Hallucination Detection","summary":"  Despite the many advances of Large Language Models (LLMs) and their\nunprecedented rapid evolution, their impact and integration into every facet of\nour daily lives is limited due to various reasons. One critical factor\nhindering their widespread adoption is the occurrence of hallucinations, where\nLLMs invent answers that sound realistic, yet drift away from factual truth. In\nthis paper, we present a novel method for detecting hallucinations in large\nlanguage models, which tackles a critical issue in the adoption of these models\nin various real-world scenarios. Through extensive evaluations across multiple\ndatasets and LLMs, including Llama-2, we study the hallucination levels of\nvarious recent LLMs and demonstrate the effectiveness of our method to\nautomatically detect them. Notably, we observe up to 62% hallucinations for\nLlama-2 in a specific experiment, where our method achieves a Balanced Accuracy\n(B-ACC) of 87%, all without relying on external knowledge.\n","authors":["Yakir Yehuda","Itzik Malkiel","Oren Barkan","Jonathan Weill","Royi Ronen","Noam Koenigstein"],"pdf_url":"https://arxiv.org/pdf/2403.02889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13447v1","updated":"2024-03-20T09:42:43Z","published":"2024-03-20T09:42:43Z","title":"HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal\n  Large Language Models","summary":"  Recent advancements indicate that scaling up Multimodal Large Language Models\n(MLLMs) effectively enhances performance on downstream multimodal tasks. The\nprevailing MLLM paradigm, \\emph{e.g.}, LLaVA, transforms visual features into\ntext-like tokens using a \\emph{static} vision-language mapper, thereby enabling\n\\emph{static} LLMs to develop the capability to comprehend visual information\nthrough visual instruction tuning. Although promising, the \\emph{static} tuning\nstrategy~\\footnote{The static tuning refers to the trained model with static\nparameters.} that shares the same parameters may constrain performance across\ndifferent downstream multimodal tasks. In light of this, we introduce\nHyperLLaVA, which involves adaptive tuning of the projector and LLM parameters,\nin conjunction with a dynamic visual expert and language expert, respectively.\nThese experts are derived from HyperNetworks, which generates adaptive\nparameter shifts through visual and language guidance, enabling dynamic\nprojector and LLM modeling in two-stage training.\n  Our experiments demonstrate that our solution significantly surpasses LLaVA\non existing MLLM benchmarks, including MME, MMBench, SEED-Bench, and\nLLaVA-Bench. ~\\footnote{Our project is available on the link\nhttps://github.com/DCDmllm/HyperLLaVA}.\n","authors":["Wenqiao Zhang","Tianwei Lin","Jiang Liu","Fangxun Shu","Haoyuan Li","Lei Zhang","He Wanggui","Hao Zhou","Zheqi Lv","Hao Jiang","Juncheng Li","Siliang Tang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.13447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07726v2","updated":"2024-03-20T09:36:13Z","published":"2024-03-12T15:06:22Z","title":"SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and\n  Related Observable Overgeneration Mistakes","summary":"  This paper presents the results of the SHROOM, a shared task focused on\ndetecting hallucinations: outputs from natural language generation (NLG)\nsystems that are fluent, yet inaccurate. Such cases of overgeneration put in\njeopardy many NLG applications, where correctness is often mission-critical.\nThe shared task was conducted with a newly constructed dataset of 4000 model\noutputs labeled by 5 annotators each, spanning 3 NLP tasks: machine\ntranslation, paraphrase generation and definition modeling.\n  The shared task was tackled by a total of 58 different users grouped in 42\nteams, out of which 27 elected to write a system description paper;\ncollectively, they submitted over 300 prediction sets on both tracks of the\nshared task. We observe a number of key trends in how this approach was tackled\n-- many participants rely on a handful of model, and often rely either on\nsynthetic data for fine-tuning or zero-shot prompting strategies. While a\nmajority of the teams did outperform our proposed baseline system, the\nperformances of top-scoring systems are still consistent with a random handling\nof the more challenging items.\n","authors":["Timothee Mickus","Elaine Zosa","Raúl Vázquez","Teemu Vahtola","Jörg Tiedemann","Vincent Segonne","Alessandro Raganato","Marianna Apidianaki"],"pdf_url":"https://arxiv.org/pdf/2403.07726v2.pdf","comment":"SemEval 2024 shared task. Pre-review version"},{"id":"http://arxiv.org/abs/2403.13433v1","updated":"2024-03-20T09:21:32Z","published":"2024-03-20T09:21:32Z","title":"Agent Group Chat: An Interactive Group Chat Simulacra For Better\n  Eliciting Collective Emergent Behavior","summary":"  To investigate the role of language in human collective behaviors, we\ndeveloped the Agent Group Chat simulation to simulate linguistic interactions\namong multi-agent in different settings. Agents are asked to free chat in this\nsimulation for their own purposes based on their character setting, aiming to\nsee agents exhibit emergent behaviours that are both unforeseen and\nsignificant. Four narrative scenarios, Inheritance Disputes, Law Court Debates,\nPhilosophical Discourses, Movie Casting Contention, are integrated into Agent\nGroup Chat to evaluate its support for diverse storylines. By configuring\nspecific environmental settings within Agent Group Chat, we are able to assess\nwhether agents exhibit behaviors that align with human expectations. We\nevaluate the disorder within the environment by computing the n-gram Shannon\nentropy of all the content speak by characters. Our findings reveal that under\nthe premise of agents possessing substantial alignment with human expectations,\nfacilitating more extensive information exchange within the simulation ensures\ngreater orderliness amidst diversity, which leads to the emergence of more\nunexpected and meaningful emergent behaviors. The code is open source in\nhttps://github.com/MikeGu721/AgentGroup, and online platform will be open soon.\n","authors":["Zhouhong Gu","Xiaoxuan Zhu","Haoran Guo","Lin Zhang","Yin Cai","Hao Shen","Jiangjie Chen","Zheyu Ye","Yifei Dai","Yan Gao","Yao Hu","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.13433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14166v3","updated":"2024-03-20T08:52:42Z","published":"2024-01-25T13:20:47Z","title":"BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on\n  Few-shot Inference via Debiased Domain Abstraction","summary":"  As a novel and effective fine-tuning paradigm based on large-scale\npre-trained language models (PLMs), prompt-tuning aims to reduce the gap\nbetween downstream tasks and pre-training objectives. While prompt-tuning has\nyielded continuous advancements in various tasks, such an approach still\nremains a persistent defect: prompt-tuning methods fail to generalize to\nspecific few-shot patterns. From the perspective of distribution analyses, we\ndisclose that the intrinsic issues behind the phenomenon are the\nover-multitudinous conceptual knowledge contained in PLMs and the abridged\nknowledge for target downstream domains, which jointly result in that PLMs\nmis-locate the knowledge distributions corresponding to the target domains in\nthe universal knowledge embedding space. To this end, we intuitively explore to\napproximate the unabridged target domains of downstream tasks in a debiased\nmanner, and then abstract such domains to generate discriminative prompts,\nthereby providing the de-ambiguous guidance for PLMs. Guided by such an\nintuition, we propose a simple yet effective approach, namely BayesPrompt, to\nlearn prompts that contain the domain discriminative information against the\ninterference from domain-irrelevant knowledge. BayesPrompt primitively\nleverages known distributions to approximate the debiased factual distributions\nof target domains and further uniformly samples certain representative features\nfrom the approximated distributions to generate the ultimate prompts for PLMs.\nWe provide theoretical insights with the connection to domain adaptation.\nEmpirically, our method achieves state-of-the-art performance on benchmarks.\n","authors":["Jiangmeng Li","Fei Song","Yifan Jin","Wenwen Qiang","Changwen Zheng","Fuchun Sun","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2401.14166v3.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2305.13888v2","updated":"2024-03-20T08:37:42Z","published":"2023-05-23T10:11:56Z","title":"PaD: Program-aided Distillation Can Teach Small Models Reasoning Better\n  than Chain-of-thought Fine-tuning","summary":"  While large language models (LLMs) excel in various natural language\nprocessing tasks, their huge size and the inaccessibility of parameters present\nchallenges for practical deployment. Previous studies try to distill\ntask-specific ability from LLMs to smaller models, using data synthesis and\nchain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains\nfaulty reasoning, which deteriorates the quality of distillation, especially in\nreasoning capabilities. In this work, we propose Program-aided Distillation\n(PaD), which introduces reasoning programs to suppress the errors in distilled\ndata, and thus achieves better distillation quality for reasoning tasks. In\nPaD, we utilize the reasoning program to substitute the CoT, allowing automated\nerror checking of synthetic data. Further, through error injecting and further\ntraining, the small distilling model could iteratively self-refine the\nreasoning. Moreover, we conduct a step-wise beam search by step-by-step\nverifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic\nreasoning, symbolic reasoning, and general ability. Experimental results\ndemonstrate that smaller models using PaD can not only outperform certain\nLLMs~(e.g., LLaMA-1 13B) but also achieve strong improvement over baselines\nwith a significantly smaller scale of parameters and data. The source code is\npublicly available at https://github.com/Xuekai-Zhu/pad.\n","authors":["Xuekai Zhu","Biqing Qi","Kaiyan Zhang","Xinwei Long","Zhouhan Lin","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2305.13888v2.pdf","comment":"NAACL 2024 Long Paper; Code and data are available at\n  https://github.com/Xuekai-Zhu/pad"},{"id":"http://arxiv.org/abs/2309.13243v2","updated":"2024-03-20T08:16:14Z","published":"2023-09-23T03:28:25Z","title":"ChEDDAR: Student-ChatGPT Dialogue in EFL Writing Education","summary":"  The integration of generative AI in education is expanding, yet empirical\nanalyses of large-scale, real-world interactions between students and AI\nsystems still remain limited. In this study, we present ChEDDAR, ChatGPT & EFL\nLearner's Dialogue Dataset As Revising an essay, which is collected from a\nsemester-long longitudinal experiment involving 212 college students enrolled\nin English as Foreign Langauge (EFL) writing courses. The students were asked\nto revise their essays through dialogues with ChatGPT. ChEDDAR includes a\nconversation log, utterance-level essay edit history, self-rated satisfaction,\nand students' intent, in addition to session-level pre-and-post surveys\ndocumenting their objectives and overall experiences. We analyze students'\nusage patterns and perceptions regarding generative AI with respect to their\nintent and satisfaction. As a foundational step, we establish baseline results\nfor two pivotal tasks in task-oriented dialogue systems within educational\ncontexts: intent detection and satisfaction estimation. We finally suggest\nfurther research to refine the integration of generative AI into education\nsettings, outlining potential scenarios utilizing ChEDDAR. ChEDDAR is publicly\navailable at https://github.com/zeunie/ChEDDAR.\n","authors":["Jieun Han","Haneul Yoo","Junho Myung","Minsun Kim","Tak Yeon Lee","So-Yeon Ahn","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2309.13243v2.pdf","comment":"The new version of this paper is on arXiv as arXiv:2403.08272"},{"id":"http://arxiv.org/abs/2403.13372v1","updated":"2024-03-20T08:08:54Z","published":"2024-03-20T08:08:54Z","title":"LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models","summary":"  Efficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It allows users\nto flexibly customize the fine-tuning of 100+ LLMs without the need for coding\nthrough the built-in web UI LlamaBoard. We empirically validate the efficiency\nand effectiveness of our framework on language modeling and text generation\ntasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and\nalready received over 13,000 stars and 1,600 forks.\n","authors":["Yaowei Zheng","Richong Zhang","Junhao Zhang","Yanhan Ye","Zheyan Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13372v1.pdf","comment":"12 pages, preprint"},{"id":"http://arxiv.org/abs/2403.13369v1","updated":"2024-03-20T08:01:33Z","published":"2024-03-20T08:01:33Z","title":"Clinical information extraction for Low-resource languages with Few-shot\n  learning using Pre-trained language models and Prompting","summary":"  Automatic extraction of medical information from clinical documents poses\nseveral challenges: high costs of required clinical expertise, limited\ninterpretability of model predictions, restricted computational resources and\nprivacy regulations. Recent advances in domain-adaptation and prompting methods\nshowed promising results with minimal training data using lightweight masked\nlanguage models, which are suited for well-established interpretability\nmethods. We are first to present a systematic evaluation of these methods in a\nlow-resource setting, by performing multi-class section classification on\nGerman doctor's letters. We conduct extensive class-wise evaluations supported\nby Shapley values, to validate the quality of our small training data set and\nto ensure the interpretability of model predictions. We demonstrate that a\nlightweight, domain-adapted pretrained model, prompted with just 20 shots,\noutperforms a traditional classification model by 30.5% accuracy. Our results\nserve as a process-oriented guideline for clinical information extraction\nprojects working with low-resource.\n","authors":["Phillip Richter-Pechanski","Philipp Wiesenbach","Dominic M. Schwab","Christina Kiriakou","Nicolas Geis","Christoph Dieterich","Anette Frank"],"pdf_url":"https://arxiv.org/pdf/2403.13369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13368v1","updated":"2024-03-20T08:01:22Z","published":"2024-03-20T08:01:22Z","title":"Computational Models to Study Language Processing in the Human Brain: A\n  Survey","summary":"  Despite differing from the human language processing mechanism in\nimplementation and algorithms, current language models demonstrate remarkable\nhuman-like or surpassing language capabilities. Should computational language\nmodels be employed in studying the brain, and if so, when and how? To delve\ninto this topic, this paper reviews efforts in using computational models for\nbrain research, highlighting emerging trends. To ensure a fair comparison, the\npaper evaluates various computational models using consistent metrics on the\nsame dataset. Our analysis reveals that no single model outperforms others on\nall datasets, underscoring the need for rich testing datasets and rigid\nexperimental control to draw robust conclusions in studies involving\ncomputational models.\n","authors":["Shaonan Wang","Jingyuan Sun","Yunhao Zhang","Nan Lin","Marie-Francine Moens","Chengqing Zong"],"pdf_url":"https://arxiv.org/pdf/2403.13368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13362v1","updated":"2024-03-20T07:44:06Z","published":"2024-03-20T07:44:06Z","title":"Incentivizing News Consumption on Social Media Platforms Using Large\n  Language Models and Realistic Bot Accounts","summary":"  Polarization, declining trust, and wavering support for democratic norms are\npressing threats to U.S. democracy. Exposure to verified and quality news may\nlower individual susceptibility to these threats and make citizens more\nresilient to misinformation, populism, and hyperpartisan rhetoric. This project\nexamines how to enhance users' exposure to and engagement with verified and\nideologically balanced news in an ecologically valid setting. We rely on a\nlarge-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on\n28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users\ntweeting about sports, entertainment, or lifestyle with a contextual reply\ncontaining two hardcoded elements: a URL to the topic-relevant section of\nquality news organization and an encouragement to follow its Twitter account.\nTo further test differential effects by gender of the bots, treated users were\nrandomly assigned to receive responses by bots presented as female or male. We\nexamine whether our over-time intervention enhances the following of news media\norganization, the sharing and the liking of news content and the tweeting about\npolitics and the liking of political content. We find that the treated users\nfollowed more news accounts and the users in the female bot treatment were more\nlikely to like news content than the control. Most of these results, however,\nwere small in magnitude and confined to the already politically interested\nTwitter users, as indicated by their pre-treatment tweeting about politics.\nThese findings have implications for social media and news organizations, and\nalso offer direction for future work on how Large Language Models and other\ncomputational interventions can effectively enhance individual on-platform\nengagement with quality news and public affairs.\n","authors":["Hadi Askari","Anshuman Chhabra","Bernhard Clemm von Hohenberg","Michael Heseltine","Magdalena Wojcieszak"],"pdf_url":"https://arxiv.org/pdf/2403.13362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03512v3","updated":"2024-03-20T07:39:48Z","published":"2024-01-07T15:00:36Z","title":"CharPoet: A Chinese Classical Poetry Generation System Based on\n  Token-free LLM","summary":"  Automatic Chinese classical poetry generation has attracted much research\ninterest, but achieving effective control over format and content\nsimultaneously remains challenging. Traditional systems usually accept keywords\nas user inputs, resulting in limited control over content. Large language\nmodels (LLMs) improve content control by allowing unrestricted user\ninstructions, but the token-by-token generation process frequently makes format\nerrors. Motivated by this, we propose CharPoet, a Chinese classical poetry\ngeneration system based on token-free LLM, which provides effective control\nover both format and content. Our token-free architecture generates in a\ncharacter-by-character manner, enabling precise control over the number of\ncharacters. Pruned from existing token-based LLMs, CharPoet inherits their\npretrained capabilities and can generate poetry following instructions like\n\"Write me a poem for my mother's birthday.\" CharPoet achieves format accuracy\nabove 0.96, outperforming Jiuge-GPT-2 (0.91) and GPT-4 (0.38). In terms of\ncontent quality, CharPoet surpasses traditional systems including Jiuge, and is\ncomparable to other LLMs. Our system is open source and available at\nhttps://modelscope.cn/models/CharPoet/CharPoet. A video demonstration of\nCharPoet is available at https://youtu.be/voZ25qEp3Dc.\n","authors":["Chengyue Yu","Lei Zang","Jiaotuan Wang","Chenyi Zhuang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2401.03512v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00758v3","updated":"2024-03-20T07:37:24Z","published":"2024-03-01T18:55:20Z","title":"Mitigating Reversal Curse in Large Language Models via Semantic-aware\n  Permutation Training","summary":"  While large language models (LLMs) have achieved impressive performance\nacross diverse tasks, recent studies showcase that causal LLMs suffer from the\n\"reversal curse\". It is a typical example that the model knows \"A's father is\nB\", but is unable to reason \"B's child is A\". This limitation poses a challenge\nto the advancement of artificial general intelligence (AGI), as it suggests a\ngap in the models' ability to comprehend and apply bidirectional reasoning. In\nthis paper, we first conduct substantial evaluation and identify that the root\ncause of the reversal curse lies in the different word order between the\ntraining and inference stage, namely, the poor ability of causal language\nmodels to predict antecedent words within the training data. Accordingly,\npermutation on the training data is considered as a potential solution, since\nthis can make the model predict antecedent words or tokens. However, previous\npermutation methods may disrupt complete phrases or entities, thereby posing\nchallenges for the model to comprehend and learn from training data. To address\nthis issue, we propose Semantic-aware Permutation Training (SPT), which\naddresses this issue by segmenting the training sentences into semantic units\n(i.e., entities or phrases) with an assistant language model and permuting\nthese units before feeding into the model. Extensive experiments demonstrate\nthat SPT effectively mitigates the reversal curse since the performance on\nreversed questions approximates that on the forward ones, and significantly\nadvances the performance of existing works.\n","authors":["Qingyan Guo","Rui Wang","Junliang Guo","Xu Tan","Jiang Bian","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2403.00758v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11467v2","updated":"2024-03-20T07:08:22Z","published":"2024-01-21T11:42:18Z","title":"Over-Reasoning and Redundant Calculation of Large Language Models","summary":"  Large language models (LLMs) can solve problems step-by-step. While this\nchain-of-thought (CoT) reasoning boosts LLMs' performance, it is unclear if\nLLMs \\textit{know} when to use CoT and whether those CoT are always necessary\nto answer the question. This paper shows that LLMs tend to generate redundant\ncalculations and reasoning on a manually constructed math QA dataset,\nGSM8K-Zero. GSM8K-Zero is constructed such that the questions can be answered\nwithout any calculations, but LLMs, including Llama-2 models and Claude-2, tend\nto generate lengthy and unnecessary calculations to answer the questions. We\nalso conduct experiments to explain why LLMs generate redundant calculations\nand reasonings. GSM8K-Zero is publicly available at\nhttps://github.com/d223302/Over-Reasoning-of-LLMs and\nhttps://huggingface.co/datasets/dcml0714/GSM8K-Zero.\n","authors":["Cheng-Han Chiang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2401.11467v2.pdf","comment":"EACL 2024 main conference paper. Camera-ready version"},{"id":"http://arxiv.org/abs/2403.13344v1","updated":"2024-03-20T07:05:19Z","published":"2024-03-20T07:05:19Z","title":"USE: Dynamic User Modeling with Stateful Sequence Models","summary":"  User embeddings play a crucial role in user engagement forecasting and\npersonalized services. Recent advances in sequence modeling have sparked\ninterest in learning user embeddings from behavioral data. Yet behavior-based\nuser embedding learning faces the unique challenge of dynamic user modeling. As\nusers continuously interact with the apps, user embeddings should be\nperiodically updated to account for users' recent and long-term behavior\npatterns. Existing methods highly rely on stateless sequence models that lack\nmemory of historical behavior. They have to either discard historical data and\nuse only the most recent data or reprocess the old and new data jointly. Both\ncases incur substantial computational overhead. To address this limitation, we\nintroduce User Stateful Embedding (USE). USE generates user embeddings and\nreflects users' evolving behaviors without the need for exhaustive reprocessing\nby storing previous model states and revisiting them in the future.\nFurthermore, we introduce a novel training objective named future W-behavior\nprediction to transcend the limitations of next-token prediction by forecasting\na broader horizon of upcoming user behaviors. By combining it with the Same\nUser Prediction, a contrastive learning-based objective that predicts whether\ndifferent segments of behavior sequences belong to the same user, we further\nimprove the embeddings' distinctiveness and representativeness. We conducted\nexperiments on 8 downstream tasks using Snapchat users' behavioral logs in both\nstatic (i.e., fixed user behavior sequences) and dynamic (i.e., periodically\nupdated user behavior sequences) settings. We demonstrate USE's superior\nperformance over established baselines. The results underscore USE's\neffectiveness and efficiency in integrating historical and recent user behavior\nsequences into user embeddings in dynamic user modeling.\n","authors":["Zhihan Zhou","Qixiang Fang","Leonardo Neves","Francesco Barbieri","Yozen Liu","Han Liu","Maarten W. Bos","Ron Dotsch"],"pdf_url":"https://arxiv.org/pdf/2403.13344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13334v1","updated":"2024-03-20T06:37:59Z","published":"2024-03-20T06:37:59Z","title":"Hyacinth6B: A large language model for Traditional Chinese","summary":"  This research's primary motivation of this study is to address the high\nhardware and computational demands typically associated with LLMs.Therefore,our\ngoal is to find a balance between model lightness and performance,striving to\nmaximize performance while using a comparatively lightweight model. Hyacinth6B\nwas developed with this objective in mind,aiming to fully leverage the core\ncapabilities of LLMs without incurring substantial resource costs, effectively\npushing the boundaries of smaller model's performance. The training approach\ninvolves parameter efficient finetuning using the LoRA method.\n","authors":["Chih-Wei Song","Yin-Te Tsai"],"pdf_url":"https://arxiv.org/pdf/2403.13334v1.pdf","comment":"14pages"},{"id":"http://arxiv.org/abs/2402.15506v3","updated":"2024-03-20T06:00:14Z","published":"2024-02-23T18:56:26Z","title":"AgentOhana: Design Unified Data and Training Pipeline for Effective\n  Agent Learning","summary":"  Autonomous agents powered by large language models (LLMs) have garnered\nsignificant research attention. However, fully harnessing the potential of LLMs\nfor agent-based tasks presents inherent challenges due to the heterogeneous\nnature of diverse data sources featuring multi-turn trajectories. In this\npaper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address\nthese challenges. \\textit{AgentOhana} aggregates agent trajectories from\ndistinct environments, spanning a wide array of scenarios. It meticulously\nstandardizes and unifies these trajectories into a consistent format,\nstreamlining the creation of a generic data loader optimized for agent\ntraining. Leveraging the data unification, our training pipeline maintains\nequilibrium across different data sources and preserves independent randomness\nacross devices during dataset partitioning and model training. Additionally, we\npresent \\textbf{xLAM-v0.1}, a large action model tailored for AI agents, which\ndemonstrates exceptional performance across various benchmarks. Begin the\nexploration at \\url{https://github.com/SalesforceAIResearch/xLAM}.\n","authors":["Jianguo Zhang","Tian Lan","Rithesh Murthy","Zhiwei Liu","Weiran Yao","Juntao Tan","Thai Hoang","Liangwei Yang","Yihao Feng","Zuxin Liu","Tulika Awalgaonkar","Juan Carlos Niebles","Silvio Savarese","Shelby Heinecke","Huan Wang","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2402.15506v3.pdf","comment":"Add GitHub repo link at\n  \\url{https://github.com/SalesforceAIResearch/xLAM} and HuggingFace model link\n  at \\url{https://huggingface.co/Salesforce/xLAM-v0.1-r}"},{"id":"http://arxiv.org/abs/2403.13313v1","updated":"2024-03-20T05:34:03Z","published":"2024-03-20T05:34:03Z","title":"Polaris: A Safety-focused LLM Constellation Architecture for Healthcare","summary":"  We develop Polaris, the first safety-focused LLM constellation for real-time\npatient-AI healthcare conversations. Unlike prior LLM works in healthcare\nfocusing on tasks like question answering, our work specifically focuses on\nlong multi-turn voice conversations. Our one-trillion parameter constellation\nsystem is composed of several multibillion parameter LLMs as co-operative\nagents: a stateful primary agent that focuses on driving an engaging\nconversation and several specialist support agents focused on healthcare tasks\nperformed by nurses to increase safety and reduce hallucinations. We develop a\nsophisticated training protocol for iterative co-training of the agents that\noptimize for diverse objectives. We train our models on proprietary data,\nclinical care plans, healthcare regulatory documents, medical manuals, and\nother medical reasoning documents. We align our models to speak like medical\nprofessionals, using organic healthcare conversations and simulated ones\nbetween patient actors and experienced nurses. This allows our system to\nexpress unique capabilities such as rapport building, trust building, empathy\nand bedside manner. Finally, we present the first comprehensive clinician\nevaluation of an LLM system for healthcare. We recruited over 1100 U.S.\nlicensed nurses and over 130 U.S. licensed physicians to perform end-to-end\nconversational evaluations of our system by posing as patients and rating the\nsystem on several measures. We demonstrate Polaris performs on par with human\nnurses on aggregate across dimensions such as medical safety, clinical\nreadiness, conversational quality, and bedside manner. Additionally, we conduct\na challenging task-based evaluation of the individual specialist support\nagents, where we demonstrate our LLM agents significantly outperform a much\nlarger general-purpose LLM (GPT-4) as well as from its own medium-size class\n(LLaMA-2 70B).\n","authors":["Subhabrata Mukherjee","Paul Gamble","Markel Sanz Ausin","Neel Kant","Kriti Aggarwal","Neha Manjunath","Debajyoti Datta","Zhengliang Liu","Jiayuan Ding","Sophia Busacca","Cezanne Bianco","Swapnil Sharma","Rae Lasko","Michelle Voisard","Sanchay Harneja","Darya Filippova","Gerry Meixiong","Kevin Cha","Amir Youssefi","Meyhaa Buvanesh","Howard Weingram","Sebastian Bierman-Lytle","Harpreet Singh Mangat","Kim Parikh","Saad Godil","Alex Miller"],"pdf_url":"https://arxiv.org/pdf/2403.13313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13312v1","updated":"2024-03-20T05:29:06Z","published":"2024-03-20T05:29:06Z","title":"LeanReasoner: Boosting Complex Logical Reasoning with Lean","summary":"  Large language models (LLMs) often struggle with complex logical reasoning\ndue to logical inconsistencies and the inherent difficulty of such reasoning.\nWe use Lean, a theorem proving framework, to address these challenges. By\nformalizing logical reasoning problems into theorems within Lean, we can solve\nthem by proving or disproving the corresponding theorems. This method reduces\nthe risk of logical inconsistencies with the help of Lean's symbolic solver. It\nalso enhances our ability to treat complex reasoning tasks by using Lean's\nextensive library of theorem proofs. Our method achieves state-of-the-art\nperformance on the FOLIO dataset and achieves performance near this level on\nProofWriter. Notably, these results were accomplished by fine-tuning on fewer\nthan 100 in-domain samples for each dataset.\n","authors":["Dongwei Jiang","Marcio Fonseca","Shay B. Cohen"],"pdf_url":"https://arxiv.org/pdf/2403.13312v1.pdf","comment":"Accepted to NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2311.07838v2","updated":"2024-03-20T05:04:06Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v2.pdf","comment":"Accepted by NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.13301v1","updated":"2024-03-20T04:57:32Z","published":"2024-03-20T04:57:32Z","title":"Reading Users' Minds from What They Say: An Investigation into LLM-based\n  Empathic Mental Inference","summary":"  In human-centered design, developing a comprehensive and in-depth\nunderstanding of user experiences, i.e., empathic understanding, is paramount\nfor designing products that truly meet human needs. Nevertheless, accurately\ncomprehending the real underlying mental states of a large human population\nremains a significant challenge today. This difficulty mainly arises from the\ntrade-off between depth and scale of user experience research: gaining in-depth\ninsights from a small group of users does not easily scale to a larger\npopulation, and vice versa. This paper investigates the use of Large Language\nModels (LLMs) for performing mental inference tasks, specifically inferring\nusers' underlying goals and fundamental psychological needs (FPNs). Baseline\nand benchmark datasets were collected from human users and designers to develop\nan empathic accuracy metric for measuring the mental inference performance of\nLLMs. The empathic accuracy of inferring goals and FPNs of different LLMs with\nvaried zero-shot prompt engineering techniques are experimented against that of\nhuman designers. Experimental results suggest that LLMs can infer and\nunderstand the underlying goals and FPNs of users with performance comparable\nto that of human designers, suggesting a promising avenue for enhancing the\nscalability of empathic design approaches through the integration of advanced\nartificial intelligence technologies. This work has the potential to\nsignificantly augment the toolkit available to designers during human-centered\ndesign, enabling the development of both large-scale and in-depth understanding\nof users' experiences.\n","authors":["Qihao Zhu","Leah Chong","Maria Yang","Jianxi Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13301v1.pdf","comment":"Submitted to IDETC-CIE2024"},{"id":"http://arxiv.org/abs/2306.12245v4","updated":"2024-03-20T03:51:23Z","published":"2023-06-21T13:04:30Z","title":"Bidirectional End-to-End Learning of Retriever-Reader Paradigm for\n  Entity Linking","summary":"  Entity Linking (EL) is a fundamental task for Information Extraction and\nKnowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first\nfind mentions in the given input document and then link the mentions to\ncorresponding entities in a specific knowledge base. Recently, the paradigm of\nretriever-reader promotes the progress of end-to-end EL, benefiting from the\nadvantages of dense entity retrieval and machine reading comprehension.\nHowever, the existing study only trains the retriever and the reader separately\nin a pipeline manner, which ignores the benefit that the interaction between\nthe retriever and the reader can bring to the task. To advance the\nretriever-reader paradigm to perform more perfectly on end-to-end EL, we\npropose BEER$^2$, a Bidirectional End-to-End training framework for Retriever\nand Reader. Through our designed bidirectional end-to-end training, BEER$^2$\nguides the retriever and the reader to learn from each other, make progress\ntogether, and ultimately improve EL performance. Extensive experiments on\nbenchmarks of multiple domains demonstrate the effectiveness of our proposed\nBEER$^2$.\n","authors":["Yinghui Li","Yong Jiang","Yangning Li","Xingyu Lu","Pengjun Xie","Ying Shen","Hai-Tao Zheng"],"pdf_url":"https://arxiv.org/pdf/2306.12245v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01931v2","updated":"2024-03-20T03:33:32Z","published":"2023-06-02T22:12:05Z","title":"Exploring semantic information in disease: Simple Data Augmentation\n  Techniques for Chinese Disease Normalization","summary":"  Disease name normalization is an important task in the medical domain. It\nclassifies disease names written in various formats into standardized names,\nserving as a fundamental component in smart healthcare systems for various\ndisease-related functions. Nevertheless, the most significant obstacle to\nexisting disease name normalization systems is the severe shortage of training\ndata. While data augmentation is a powerful approach for addressing data\nscarcity, our findings reveal that conventional data augmentation techniques\noften impede task performance, primarily due to the multi-axis and\nmulti-granularity nature of disease names. Consequently, we introduce a set of\ncustomized data augmentation techniques designed to leverage the semantic\ninformation inherent in disease names. These techniques aim to enhance the\nmodel's understanding of the semantic intricacies and classification structure\nof disease names. Through extensive experimentation, we illustrate that our\nproposed plug-and-play methods not only surpass general data augmentation\ntechniques but also exhibit significant performance improvements across various\nbaseline models and training objectives, particularly in scenarios with limited\ntraining data. This underscores its potential for widespread application in\nmedical language processing tasks.\n","authors":["Wenqian Cui","Xiangling Fu","Shaohui Liu","Mingjun Gu","Xien Liu","Ji Wu","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2306.01931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.00415v3","updated":"2024-03-20T03:23:11Z","published":"2022-05-01T07:51:22Z","title":"Don't Blame the Annotator: Bias Already Starts in the Annotation\n  Instructions","summary":"  In recent years, progress in NLU has been driven by benchmarks. These\nbenchmarks are typically collected by crowdsourcing, where annotators write\nexamples based on annotation instructions crafted by dataset creators. In this\nwork, we hypothesize that annotators pick up on patterns in the crowdsourcing\ninstructions, which bias them to write many similar examples that are then\nover-represented in the collected data. We study this form of bias, termed\ninstruction bias, in 14 recent NLU benchmarks, showing that instruction\nexamples often exhibit concrete patterns, which are propagated by crowdworkers\nto the collected data. This extends previous work (Geva et al., 2019) and\nraises a new concern of whether we are modeling the dataset creator's\ninstructions, rather than the task. Through a series of experiments, we show\nthat, indeed, instruction bias can lead to overestimation of model performance,\nand that models struggle to generalize beyond biases originating in the\ncrowdsourcing instructions. We further analyze the influence of instruction\nbias in terms of pattern frequency and model size, and derive concrete\nrecommendations for creating future NLU benchmarks.\n","authors":["Mihir Parmar","Swaroop Mishra","Mor Geva","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2205.00415v3.pdf","comment":"EACL 2023 (Outstanding Paper Award)"},{"id":"http://arxiv.org/abs/2403.13272v1","updated":"2024-03-20T03:14:54Z","published":"2024-03-20T03:14:54Z","title":"Community Needs and Assets: A Computational Analysis of Community\n  Conversations","summary":"  A community needs assessment is a tool used by non-profits and government\nagencies to quantify the strengths and issues of a community, allowing them to\nallocate their resources better. Such approaches are transitioning towards\nleveraging social media conversations to analyze the needs of communities and\nthe assets already present within them. However, manual analysis of\nexponentially increasing social media conversations is challenging. There is a\ngap in the present literature in computationally analyzing how community\nmembers discuss the strengths and needs of the community. To address this gap,\nwe introduce the task of identifying, extracting, and categorizing community\nneeds and assets from conversational data using sophisticated natural language\nprocessing methods. To facilitate this task, we introduce the first dataset\nabout community needs and assets consisting of 3,511 conversations from Reddit,\nannotated using crowdsourced workers. Using this dataset, we evaluate an\nutterance-level classification model compared to sentiment classification and a\npopular large language model (in a zero-shot setting), where we find that our\nmodel outperforms both baselines at an F1 score of 94% compared to 49% and 61%\nrespectively. Furthermore, we observe through our study that conversations\nabout needs have negative sentiments and emotions, while conversations about\nassets focus on location and entities. The dataset is available at\nhttps://github.com/towhidabsar/CommunityNeeds.\n","authors":["Md Towhidul Absar Chowdhury","Naveen Sharma","Ashiqur R. KhudaBukhsh"],"pdf_url":"https://arxiv.org/pdf/2403.13272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13269v1","updated":"2024-03-20T03:07:50Z","published":"2024-03-20T03:07:50Z","title":"AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient\n  Fine-Tuning of Large Models","summary":"  We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as\nAdaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each\npre-trained frozen weight tensor, we add a parallel path of trainable low-rank\nmatrices, namely a down-projection and an up-projection matrix, each of which\nis followed by a feature transformation vector. Based on a novel freezing\nscore, we the incrementally freeze these projection matrices during fine-tuning\nto reduce the computation and alleviate over-fitting. Our experimental results\ndemonstrate that we can achieve state-of-the-art performance with an average\nimprovement of up to $0.85\\%$ as evaluated on GLUE benchmark while yeilding up\nto $9.5\\times$ fewer average trainable parameters. While compared in terms of\nruntime, AFLoRA can yield up to $1.86\\times$ improvement as opposed to similar\nPEFT alternatives. Besides the practical utility of our approach, we provide\ninsights on the trainability requirements of LoRA paths at different modules\nand the freezing schedule for the different projection matrices. Code will be\nreleased.\n","authors":["Zeyu Liu","Souvik Kundu","Anni Li","Junrui Wan","Lianghao Jiang","Peter Anthony Beerel"],"pdf_url":"https://arxiv.org/pdf/2403.13269v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2210.03963v2","updated":"2024-03-20T03:06:16Z","published":"2022-10-08T08:07:47Z","title":"SDA: Simple Discrete Augmentation for Contrastive Sentence\n  Representation Learning","summary":"  Contrastive learning has recently achieved compelling performance in\nunsupervised sentence representation. As an essential element, data\naugmentation protocols, however, have not been well explored. The pioneering\nwork SimCSE resorting to a simple dropout mechanism (viewed as continuous\naugmentation) surprisingly dominates discrete augmentations such as cropping,\nword deletion, and synonym replacement as reported. To understand the\nunderlying rationales, we revisit existing approaches and attempt to\nhypothesize the desiderata of reasonable data augmentation methods: balance of\nsemantic consistency and expression diversity. We then develop three simple yet\neffective discrete sentence augmentation schemes: punctuation insertion, modal\nverbs, and double negation. They act as minimal noises at lexical level to\nproduce diverse forms of sentences. Furthermore, standard negation is\ncapitalized on to generate negative samples for alleviating feature suppression\ninvolved in contrastive learning. We experimented extensively with semantic\ntextual similarity on diverse datasets. The results support the superiority of\nthe proposed methods consistently.\n","authors":["Dongsheng Zhu","Zhenyu Mao","Jinghui Lu","Rui Zhao","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2210.03963v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.07081v2","updated":"2024-03-20T03:04:47Z","published":"2023-09-13T16:46:27Z","title":"Can Whisper perform speech-based in-context learning?","summary":"  This paper investigates the in-context learning abilities of the Whisper\nautomatic speech recognition (ASR) models released by OpenAI. A novel\nspeech-based in-context learning (SICL) approach is proposed for test-time\nadaptation, which can reduce the word error rates (WERs) with only a small\nnumber of labelled speech samples without gradient descent. Language-level\nadaptation experiments using Chinese dialects showed that when applying SICL to\nisolated word ASR, consistent and considerable relative WER reductions can be\nachieved using Whisper models of any size on two dialects, which is on average\n32.3%. A k-nearest-neighbours-based in-context example selection technique can\nbe applied to further improve the efficiency of SICL, which can increase the\naverage relative WER reduction to 36.4%. The findings are verified using\nspeaker adaptation or continuous speech recognition tasks, and both achieved\nconsiderable relative WER reductions. Detailed quantitative analyses are also\nprovided to shed light on SICL's adaptability to phonological variances and\ndialect-specific lexical nuances.\n","authors":["Siyin Wang","Chao-Han Huck Yang","Ji Wu","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.07081v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.10943v2","updated":"2024-03-20T02:52:42Z","published":"2024-03-16T15:14:15Z","title":"MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent\n  Recognition and Out-of-scope Detection in Conversations","summary":"  Multimodal intent recognition poses significant challenges, requiring the\nincorporation of non-verbal modalities from real-world contexts to enhance the\ncomprehension of human intentions. Existing benchmark datasets are limited in\nscale and suffer from difficulties in handling out-of-scope samples that arise\nin multi-turn conversational interactions. We introduce MIntRec2.0, a\nlarge-scale benchmark dataset for multimodal intent recognition in multi-party\nconversations. It contains 1,245 dialogues with 15,040 samples, each annotated\nwithin a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope\nsamples, it also includes 5,736 out-of-scope samples appearing in multi-turn\ncontexts, which naturally occur in real-world scenarios. Furthermore, we\nprovide comprehensive information on the speakers in each utterance, enriching\nits utility for multi-party conversational research. We establish a general\nframework supporting the organization of single-turn and multi-turn dialogue\ndata, modality feature extraction, multimodal fusion, as well as in-scope\nclassification and out-of-scope detection. Evaluation benchmarks are built\nusing classic multimodal fusion methods, ChatGPT, and human evaluators. While\nexisting methods incorporating nonverbal information yield improvements,\neffectively leveraging context information and detecting out-of-scope samples\nremains a substantial challenge. Notably, large language models exhibit a\nsignificant performance gap compared to humans, highlighting the limitations of\nmachine learning methods in the cognitive intent understanding task. We believe\nthat MIntRec2.0 will serve as a valuable resource, providing a pioneering\nfoundation for research in human-machine conversational interactions, and\nsignificantly facilitating related applications. The full dataset and codes are\navailable at https://github.com/thuiar/MIntRec2.0.\n","authors":["Hanlei Zhang","Xin Wang","Hua Xu","Qianrui Zhou","Kai Gao","Jianhua Su","jinyue Zhao","Wenrui Li","Yanting Chen"],"pdf_url":"https://arxiv.org/pdf/2403.10943v2.pdf","comment":"Published in ICLR 2024; The abstract is slightly modified due to the\n  length limitation"},{"id":"http://arxiv.org/abs/2403.13257v1","updated":"2024-03-20T02:38:01Z","published":"2024-03-20T02:38:01Z","title":"Arcee's MergeKit: A Toolkit for Merging Large Language Models","summary":"  The rapid expansion of the open-source language model landscape presents an\nopportunity to merge the competencies of these model checkpoints by combining\ntheir parameters. Advances in transfer learning, the process of fine-tuning\npre-trained models for specific tasks, has resulted in the development of vast\namounts of task-specific models, typically specialized in individual tasks and\nunable to utilize each other's strengths. Model merging facilitates the\ncreation of multitask models without the need for additional training, offering\na promising avenue for enhancing model performance and versatility. By\npreserving the intrinsic capabilities of the original models, model merging\naddresses complex challenges in AI - including the difficulties of catastrophic\nforgetting and multi-task learning. To support this expanding area of research,\nwe introduce MergeKit, a comprehensive, open-source library designed to\nfacilitate the application of model merging strategies. MergeKit offers an\nextensible framework to efficiently merge models on any hardware, providing\nutility to researchers and practitioners. To date, thousands of models have\nbeen merged by the open-source community, leading to the creation of some of\nthe worlds most powerful open-source model checkpoints, as assessed by the Open\nLLM Leaderboard. The library is accessible at\nhttps://github.com/arcee-ai/MergeKit.\n","authors":["Charles Goddard","Shamane Siriwardhana","Malikeh Ehghaghi","Luke Meyers","Vlad Karpukhin","Brian Benedict","Mark McQuade","Jacob Solawetz"],"pdf_url":"https://arxiv.org/pdf/2403.13257v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.13253v1","updated":"2024-03-20T02:32:24Z","published":"2024-03-20T02:32:24Z","title":"Document Author Classification Using Parsed Language Structure","summary":"  Over the years there has been ongoing interest in detecting authorship of a\ntext based on statistical properties of the text, such as by using occurrence\nrates of noncontextual words. In previous work, these techniques have been\nused, for example, to determine authorship of all of \\emph{The Federalist\nPapers}. Such methods may be useful in more modern times to detect fake or AI\nauthorship. Progress in statistical natural language parsers introduces the\npossibility of using grammatical structure to detect authorship. In this paper\nwe explore a new possibility for detecting authorship using grammatical\nstructural information extracted using a statistical natural language parser.\nThis paper provides a proof of concept, testing author classification based on\ngrammatical structure on a set of \"proof texts,\" The Federalist Papers and\nSanditon which have been as test cases in previous authorship detection\nstudies. Several features extracted from the statistical natural language\nparser were explored: all subtrees of some depth from any level; rooted\nsubtrees of some depth, part of speech, and part of speech by level in the\nparse tree. It was found to be helpful to project the features into a lower\ndimensional space. Statistical experiments on these documents demonstrate that\ninformation from a statistical parser can, in fact, assist in distinguishing\nauthors.\n","authors":["Todd K Moon","Jacob H. Gunther"],"pdf_url":"https://arxiv.org/pdf/2403.13253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13250v1","updated":"2024-03-20T02:29:09Z","published":"2024-03-20T02:29:09Z","title":"Facilitating Pornographic Text Detection for Open-Domain Dialogue\n  Systems via Knowledge Distillation of Large Language Models","summary":"  Pornographic content occurring in human-machine interaction dialogues can\ncause severe side effects for users in open-domain dialogue systems. However,\nresearch on detecting pornographic language within human-machine interaction\ndialogues is an important subject that is rarely studied. To advance in this\ndirection, we introduce CensorChat, a dialogue monitoring dataset aimed at\ndetecting whether the dialogue session contains pornographic content. To this\nend, we collect real-life human-machine interaction dialogues in the wild and\nbreak them down into single utterances and single-turn dialogues, with the last\nutterance spoken by the chatbot. We propose utilizing knowledge distillation of\nlarge language models to annotate the dataset. Specifically, first, the raw\ndataset is annotated by four open-source large language models, with the\nmajority vote determining the label. Second, we use ChatGPT to update the empty\nlabel from the first step. Third, to ensure the quality of the validation and\ntest sets, we utilize GPT-4 for label calibration. If the current label does\nnot match the one generated by GPT-4, we employ a self-criticism strategy to\nverify its correctness. Finally, to facilitate the detection of pornographic\ntext, we develop a series of text classifiers using a pseudo-labeled dataset.\nDetailed data analysis demonstrates that leveraging knowledge distillation\ntechniques with large language models provides a practical and cost-efficient\nmethod for developing pornographic text detectors.\n","authors":["Huachuan Qiu","Shuai Zhang","Hongliang He","Anqi Li","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2403.13250v1.pdf","comment":"Accepted to CSCWD 2024 (27th International Conference on Computer\n  Supported Cooperative Work in Design). arXiv admin note: text overlap with\n  arXiv:2309.09749"},{"id":"http://arxiv.org/abs/2311.14648v3","updated":"2024-03-20T02:21:20Z","published":"2023-11-24T18:29:50Z","title":"Calibrated Language Models Must Hallucinate","summary":"  Recent language models generate false but plausible-sounding text with\nsurprising frequency. Such \"hallucinations\" are an obstacle to the usability of\nlanguage-based AI systems and can harm people who rely upon their outputs. This\nwork shows that there is an inherent statistical lower-bound on the rate that\npretrained language models hallucinate certain types of facts, having nothing\nto do with the transformer LM architecture or data quality. For \"arbitrary\"\nfacts whose veracity cannot be determined from the training data, we show that\nhallucinations must occur at a certain rate for language models that satisfy a\nstatistical calibration condition appropriate for generative language models.\nSpecifically, if the maximum probability of any fact is bounded, we show that\nthe probability of generating a hallucination is close to the fraction of facts\nthat occur exactly once in the training data (a \"Good-Turing\" estimate), even\nassuming ideal training data without errors.\n  One conclusion is that models pretrained to be sufficiently good predictors\n(i.e., calibrated) may require post-training to mitigate hallucinations on the\ntype of arbitrary facts that tend to appear once in the training set. However,\nour analysis also suggests that there is no statistical reason that pretraining\nwill lead to hallucination on facts that tend to appear more than once in the\ntraining data (like references to publications such as articles and books,\nwhose hallucinations have been particularly notable and problematic) or on\nsystematic facts (like arithmetic calculations). Therefore, different\narchitectures and learning algorithms may mitigate these latter types of\nhallucinations.\n","authors":["Adam Tauman Kalai","Santosh S. Vempala"],"pdf_url":"https://arxiv.org/pdf/2311.14648v3.pdf","comment":"In Proceedings of the 56th Annual ACM Symposium on Theory of\n  Computing (STOC) 2024"},{"id":"http://arxiv.org/abs/2309.08150v2","updated":"2024-03-20T02:17:16Z","published":"2023-09-15T04:34:40Z","title":"Unimodal Aggregation for CTC-based Speech Recognition","summary":"  This paper works on non-autoregressive automatic speech recognition. A\nunimodal aggregation (UMA) is proposed to segment and integrate the feature\nframes that belong to the same text token, and thus to learn better feature\nrepresentations for text tokens. The frame-wise features and weights are both\nderived from an encoder. Then, the feature frames with unimodal weights are\nintegrated and further processed by a decoder. Connectionist temporal\nclassification (CTC) loss is applied for training. Compared to the regular CTC,\nthe proposed method learns better feature representations and shortens the\nsequence length, resulting in lower recognition error and computational\ncomplexity. Experiments on three Mandarin datasets show that UMA demonstrates\nsuperior or comparable performance to other advanced non-autoregressive\nmethods, such as self-conditioned CTC. Moreover, by integrating\nself-conditioned CTC into the proposed framework, the performance can be\nfurther noticeably improved.\n","authors":["Ying Fang","Xiaofei Li"],"pdf_url":"https://arxiv.org/pdf/2309.08150v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.13244v1","updated":"2024-03-20T02:15:55Z","published":"2024-03-20T02:15:55Z","title":"Instruction Multi-Constraint Molecular Generation Using a\n  Teacher-Student Large Language Model","summary":"  While various models and computational tools have been proposed for structure\nand property analysis of molecules, generating molecules that conform to all\ndesired structures and properties remains a challenge. Here, we introduce a\nmulti-constraint molecular generation large language model, TSMMG, which, akin\nto a student, incorporates knowledge from various small models and tools,\nnamely, the 'teachers'. To train TSMMG, we construct a large set of\ntext-molecule pairs by extracting molecular knowledge from these 'teachers',\nenabling it to generate novel molecules that conform to the descriptions\nthrough various text prompts. We experimentally show that TSMMG remarkably\nperforms in generating molecules meeting complex, natural language-described\nproperty requirements across two-, three-, and four-constraint tasks, with an\naverage molecular validity of over 99% and success ratio of 88.08%, 65.27%, and\n61.44%, respectively. The model also exhibits adaptability through zero-shot\ntesting, creating molecules that satisfy combinations of properties that have\nnot been encountered. It can comprehend text inputs with various language\nstyles, extending beyond the confines of outlined prompts, as confirmed through\nempirical validation. Additionally, the knowledge distillation feature of TSMMG\ncontributes to the continuous enhancement of small models, while the innovative\napproach to dataset construction effectively addresses the issues of data\nscarcity and quality, which positions TSMMG as a promising tool in the domains\nof drug discovery and materials science. Code is available at\nhttps://github.com/HHW-zhou/TSMMG.\n","authors":["Peng Zhou","Jianmin Wang","Chunyan Li","Zixu Wang","Yiping Liu","Siqi Sun","Jianxin Lin","Longyue Wang","Xiangxiang Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.13244v1.pdf","comment":"25 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.13240v1","updated":"2024-03-20T02:04:42Z","published":"2024-03-20T02:04:42Z","title":"SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual\n  Summarization","summary":"  Cross-lingual summarization (XLS) generates summaries in a language different\nfrom that of the input documents (e.g., English to Spanish), allowing speakers\nof the target language to gain a concise view of their content. In the present\nday, the predominant approach to this task is to take a performing, pretrained\nmultilingual language model (LM) and fine-tune it for XLS on the language pairs\nof interest. However, the scarcity of fine-tuning samples makes this approach\nchallenging in some cases. For this reason, in this paper we propose revisiting\nthe summarize-and-translate pipeline, where the summarization and translation\ntasks are performed in a sequence. This approach allows reusing the many,\npublicly-available resources for monolingual summarization and translation,\nobtaining a very competitive zero-shot performance. In addition, the proposed\npipeline is completely differentiable end-to-end, allowing it to take advantage\nof few-shot fine-tuning, where available. Experiments over two contemporary and\nwidely adopted XLS datasets (CrossSum and WikiLingua) have shown the remarkable\nzero-shot performance of the proposed approach, and also its strong few-shot\nperformance compared to an equivalent multilingual LM baseline, that the\nproposed approach has been able to outperform in many languages with only 10%\nof the fine-tuning samples.\n","authors":["Jacob Parnell","Inigo Jauregi Unanue","Massimo Piccardi"],"pdf_url":"https://arxiv.org/pdf/2403.13240v1.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2312.04262v2","updated":"2024-03-20T01:59:39Z","published":"2023-12-07T12:40:00Z","title":"PsyChat: A Client-Centric Dialogue System for Mental Health Support","summary":"  Dialogue systems are increasingly integrated into mental health support to\nhelp clients facilitate exploration, gain insight, take action, and ultimately\nheal themselves. A practical and user-friendly dialogue system should be\nclient-centric, focusing on the client's behaviors. However, existing dialogue\nsystems publicly available for mental health support often concentrate solely\non the counselor's strategies rather than the behaviors expressed by clients.\nThis can lead to unreasonable or inappropriate counseling strategies and\ncorresponding responses generated by the dialogue system. To address this\nissue, we propose PsyChat, a client-centric dialogue system that provides\npsychological support through online chat. The client-centric dialogue system\ncomprises five modules: client behavior recognition, counselor strategy\nselection, input packer, response generator, and response selection. Both\nautomatic and human evaluations demonstrate the effectiveness and practicality\nof our proposed dialogue system for real-life mental health support.\nFurthermore, the case study demonstrates that the dialogue system can predict\nthe client's behaviors, select appropriate counselor strategies, and generate\naccurate and suitable responses.\n","authors":["Huachuan Qiu","Anqi Li","Lizhi Ma","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2312.04262v2.pdf","comment":"Accepted to CSCWD 2024 (27th International Conference on Computer\n  Supported Cooperative Work in Design)"},{"id":"http://arxiv.org/abs/2403.13233v1","updated":"2024-03-20T01:46:06Z","published":"2024-03-20T01:46:06Z","title":"Technical Report: Competition Solution For BetterMixture","summary":"  In the era of flourishing large-scale models, the challenge of selecting and\noptimizing datasets from the vast and complex sea of data, to enhance the\nperformance of large language models within the constraints of limited\ncomputational resources, has become paramount. This paper details our solution\nfor the BetterMixture challenge, which focuses on the fine-tuning data mixing\nfor large language models. Our approach, which secured third place,\nincorporates data deduplication, low-level and high-level quality filtering,\nand diversity selection. The foundation of our solution is Ke-Data-Juicer, an\nextension of Data-Juicer, demonstrating its robust capabilities in handling and\noptimizing data for large language models.\n","authors":["Shuaijiang Zhao","Xiaoquan Fang"],"pdf_url":"https://arxiv.org/pdf/2403.13233v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2306.12725v4","updated":"2024-03-20T01:30:41Z","published":"2023-06-22T07:57:19Z","title":"Generative Multimodal Entity Linking","summary":"  Multimodal Entity Linking (MEL) is the task of mapping mentions with\nmultimodal contexts to the referent entities from a knowledge base. Existing\nMEL methods mainly focus on designing complex multimodal interaction mechanisms\nand require fine-tuning all model parameters, which can be prohibitively costly\nand difficult to scale in the era of Large Language Models (LLMs). In this\nwork, we propose GEMEL, a Generative Multimodal Entity Linking framework based\non LLMs, which directly generates target entity names. We keep the vision and\nlanguage model frozen and only train a feature mapper to enable cross-modality\ninteractions. To adapt LLMs to the MEL task, we leverage the in-context\nlearning capability of LLMs by retrieving multimodal instances as\ndemonstrations. Extensive experiments show that, with only ~0.3% of the model\nparameters fine-tuned, GEMEL achieves state-of-the-art results on two\nwell-established MEL datasets (7.7% accuracy gains on WikiDiverse and 8.8%\naccuracy gains on WikiMEL). The performance gain stems from mitigating the\npopularity bias of LLM predictions and disambiguating less common entities\neffectively. Further analysis verifies the generality and scalability of GEMEL.\nOur framework is compatible with any off-the-shelf language model, paving the\nway towards an efficient and general solution for utilizing LLMs in the MEL\ntask. Our code is available at https://github.com/HITsz-TMG/GEMEL.\n","authors":["Senbao Shi","Zhenran Xu","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.12725v4.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2308.00221v3","updated":"2024-03-20T01:04:11Z","published":"2023-08-01T01:27:40Z","title":"Advancing Beyond Identification: Multi-bit Watermark for Large Language\n  Models","summary":"  We show the viability of tackling misuses of large language models beyond the\nidentification of machine-generated text. While existing zero-bit watermark\nmethods focus on detection only, some malicious misuses demand tracing the\nadversary user for counteracting them. To address this, we propose Multi-bit\nWatermark via Position Allocation, embedding traceable multi-bit information\nduring language model generation. Through allocating tokens onto different\nparts of the messages, we embed longer messages in high corruption settings\nwithout added latency. By independently embedding sub-units of messages, the\nproposed method outperforms the existing works in terms of robustness and\nlatency. Leveraging the benefits of zero-bit watermarking, our method enables\nrobust extraction of the watermark without any model access, embedding and\nextraction of long messages ($\\geq$ 32-bit) without finetuning, and maintaining\ntext quality, while allowing zero-bit detection all at the same time. Code is\nreleased here: https://github.com/bangawayoo/mb-lm-watermarking\n","authors":["KiYoon Yoo","Wonhyuk Ahn","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2308.00221v3.pdf","comment":"NAACL 2024 main. 9 pages and appendix"},{"id":"http://arxiv.org/abs/2403.13213v1","updated":"2024-03-20T00:22:38Z","published":"2024-03-20T00:22:38Z","title":"From Representational Harms to Quality-of-Service Harms: A Case Study on\n  Llama 2 Safety Safeguards","summary":"  Recent progress in large language models (LLMs) has led to their widespread\nadoption in various domains. However, these advancements have also introduced\nadditional safety risks and raised concerns regarding their detrimental impact\non already marginalized populations. Despite growing mitigation efforts to\ndevelop safety safeguards, such as supervised safety-oriented fine-tuning and\nleveraging safe reinforcement learning from human feedback, multiple concerns\nregarding the safety and ingrained biases in these models remain. Furthermore,\nprevious work has demonstrated that models optimized for safety often display\nexaggerated safety behaviors, such as a tendency to refrain from responding to\ncertain requests as a precautionary measure. As such, a clear trade-off between\nthe helpfulness and safety of these models has been documented in the\nliterature. In this paper, we further investigate the effectiveness of safety\nmeasures by evaluating models on already mitigated biases. Using the case of\nLlama 2 as an example, we illustrate how LLMs' safety responses can still\nencode harmful assumptions. To do so, we create a set of non-toxic prompts,\nwhich we then use to evaluate Llama models. Through our new taxonomy of LLMs\nresponses to users, we observe that the safety/helpfulness trade-offs are more\npronounced for certain demographic groups which can lead to quality-of-service\nharms for marginalized populations.\n","authors":["Khaoula Chehbouni","Megha Roshan","Emmanuel Ma","Futian Andrew Wei","Afaf Taïk","Jackie CK Cheung","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2403.13213v1.pdf","comment":"9 pages, 4 figures, submitted to the 62nd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2024)"},{"id":"http://arxiv.org/abs/2312.04302v2","updated":"2024-03-20T23:32:08Z","published":"2023-12-07T13:53:29Z","title":"Prompt Highlighter: Interactive Control for Multi-Modal LLMs","summary":"  This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs)\ninference: explicit controllable text generation. Multi-modal LLMs empower\nmulti-modality understanding with the capability of semantic generation yet\nbring less explainability and heavier reliance on prompt contents due to their\nautoregressive generative nature. While manipulating prompt formats could\nimprove outputs, designing specific and precise prompts per task can be\nchallenging and ineffective. To tackle this issue, we introduce a novel\ninference method, Prompt Highlighter, which enables users to highlight specific\nprompt spans to interactively control the focus during generation. Motivated by\nthe classifier-free diffusion guidance, we form regular and unconditional\ncontext pairs based on highlighted tokens, demonstrating that the\nautoregressive generation in models can be guided in a classifier-free way.\nNotably, we find that, during inference, guiding the models with highlighted\ntokens through the attention weights leads to more desired outputs. Our\napproach is compatible with current LLMs and VLMs, achieving impressive\ncustomized generation results without training. Experiments confirm its\neffectiveness in focusing on input contexts and generating reliable content.\nWithout tuning on LLaVA-v1.5, our method secured 70.7 in the MMBench test and\n1552.5 in MME-perception. The code is available at:\nhttps://github.com/dvlab-research/Prompt-Highlighter/\n","authors":["Yuechen Zhang","Shengju Qian","Bohao Peng","Shu Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2312.04302v2.pdf","comment":"CVPR 2024; Project Page:\n  https://julianjuaner.github.io/projects/PromptHighlighter"},{"id":"http://arxiv.org/abs/2403.08002v2","updated":"2024-03-20T23:31:22Z","published":"2024-03-12T18:12:02Z","title":"Training Small Multimodal Models to Bridge Biomedical Competency Gap: A\n  Case Study in Radiology Imaging","summary":"  The scaling laws and extraordinary performance of large foundation models\nmotivate the development and utilization of such large models in biomedicine.\nHowever, despite early promising results on some biomedical benchmarks, there\nare still major challenges that need to be addressed before these models can be\nused in real-world applications. Frontier models such as GPT-4V still have\nmajor competency gaps in multimodal capabilities for biomedical applications.\nMoreover, pragmatic issues such as access, cost, latency, and compliance make\nit hard for clinicians to use privately-hosted state-of-the-art large models\ndirectly on private patient data. In this paper, we explore training\nopen-source small multimodal models (SMMs) to bridge biomedical competency gaps\nfor unmet clinical needs. To maximize data efficiency, we adopt a modular\napproach by incorporating state-of-the-art pre-trained models for image and\ntext modalities, and focusing on training a lightweight adapter to ground each\nmodality to the text embedding space. We conduct a comprehensive study of this\napproach on radiology imaging. For training, we assemble a large dataset with\nover 1 million image-text pairs. For evaluation, we propose a clinically driven\nnovel approach using GPT-4 and demonstrate its parity with expert evaluation.\nWe also study grounding qualitatively using attention. For best practice, we\nconduct a systematic ablation study on various choices in data engineering and\nmultimodal training. The resulting LLaVA-Rad (7B) model attains\nstate-of-the-art results on radiology tasks such as report generation and\ncross-modal retrieval, even outperforming much larger models such as GPT-4V and\nMed-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in\nprivate settings, offering a promising state-of-the-art tool for real-world\nclinical applications.\n","authors":["Juan Manuel Zambrano Chaves","Shih-Cheng Huang","Yanbo Xu","Hanwen Xu","Naoto Usuyama","Sheng Zhang","Fei Wang","Yujia Xie","Mahmoud Khademi","Ziyi Yang","Hany Awadalla","Julia Gong","Houdong Hu","Jianwei Yang","Chunyuan Li","Jianfeng Gao","Yu Gu","Cliff Wong","Mu Wei","Tristan Naumann","Muhao Chen","Matthew P. Lungren","Serena Yeung-Levy","Curtis P. Langlotz","Sheng Wang","Hoifung Poon"],"pdf_url":"https://arxiv.org/pdf/2403.08002v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14037v1","updated":"2024-03-20T23:21:35Z","published":"2024-03-20T23:21:35Z","title":"Ax-to-Grind Urdu: Benchmark Dataset for Urdu Fake News Detection","summary":"  Misinformation can seriously impact society, affecting anything from public\nopinion to institutional confidence and the political horizon of a state. Fake\nNews (FN) proliferation on online websites and Online Social Networks (OSNs)\nhas increased profusely. Various fact-checking websites include news in English\nand barely provide information about FN in regional languages. Thus the Urdu FN\npurveyors cannot be discerned using factchecking portals. SOTA approaches for\nFake News Detection (FND) count upon appropriately labelled and large datasets.\nFND in regional and resource-constrained languages lags due to the lack of\nlimited-sized datasets and legitimate lexical resources. The previous datasets\nfor Urdu FND are limited-sized, domain-restricted, publicly unavailable and not\nmanually verified where the news is translated from English into Urdu. In this\npaper, we curate and contribute the first largest publicly available dataset\nfor Urdu FND, Ax-to-Grind Urdu, to bridge the identified gaps and limitations\nof existing Urdu datasets in the literature. It constitutes 10,083 fake and\nreal news on fifteen domains collected from leading and authentic Urdu\nnewspapers and news channel websites in Pakistan and India. FN for the\nAx-to-Grind dataset is collected from websites and crowdsourcing. The dataset\ncontains news items in Urdu from the year 2017 to the year 2023. Expert\njournalists annotated the dataset. We benchmark the dataset with an ensemble\nmodel of mBERT,XLNet, and XLM RoBERTa. The selected models are originally\ntrained on multilingual large corpora. The results of the proposed model are\nbased on performance metrics, F1-score, accuracy, precision, recall and MCC\nvalue.\n","authors":["Sheetal Harris","Jinshuo Liu","Hassan Jalil Hadi","Yue Cao"],"pdf_url":"https://arxiv.org/pdf/2403.14037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14009v1","updated":"2024-03-20T22:14:39Z","published":"2024-03-20T22:14:39Z","title":"A New Massive Multilingual Dataset for High-Performance Language\n  Technologies","summary":"  We present the HPLT (High Performance Language Technologies) language\nresources, a new massive multilingual dataset including both monolingual and\nbilingual corpora extracted from CommonCrawl and previously unused web crawls\nfrom the Internet Archive. We describe our methods for data acquisition,\nmanagement and processing of large corpora, which rely on open-source software\ntools and high-performance computing. Our monolingual collection focuses on\nlow- to medium-resourced languages and covers 75 languages and a total of ~5.6\ntrillion word tokens de-duplicated on the document level. Our English-centric\nparallel corpus is derived from its monolingual counterpart and covers 18\nlanguage pairs and more than 96 million aligned sentence pairs with roughly 1.4\nbillion English tokens. The HPLT language resources are one of the largest open\ntext corpora ever released, providing a great resource for language modeling\nand machine translation training. We publicly release the corpora, the\nsoftware, and the tools used in this work.\n","authors":["Ona de Gibert","Graeme Nail","Nikolay Arefyev","Marta Bañón","Jelmer van der Linde","Shaoxiong Ji","Jaume Zaragoza-Bernabeu","Mikko Aulamo","Gema Ramírez-Sánchez","Andrey Kutuzov","Sampo Pyysalo","Stephan Oepen","Jörg Tiedemann"],"pdf_url":"https://arxiv.org/pdf/2403.14009v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14006v1","updated":"2024-03-20T22:11:01Z","published":"2024-03-20T22:11:01Z","title":"On Prompt Sensitivity of ChatGPT in Affective Computing","summary":"  Recent studies have demonstrated the emerging capabilities of foundation\nmodels like ChatGPT in several fields, including affective computing. However,\naccessing these emerging capabilities is facilitated through prompt\nengineering. Despite the existence of some prompting techniques, the field is\nstill rapidly evolving and many prompting ideas still require investigation. In\nthis work, we introduce a method to evaluate and investigate the sensitivity of\nthe performance of foundation models based on different prompts or generation\nparameters. We perform our evaluation on ChatGPT within the scope of affective\ncomputing on three major problems, namely sentiment analysis, toxicity\ndetection, and sarcasm detection. First, we carry out a sensitivity analysis on\npivotal parameters in auto-regressive text generation, specifically the\ntemperature parameter $T$ and the top-$p$ parameter in Nucleus sampling,\ndictating how conservative or creative the model should be during generation.\nFurthermore, we explore the efficacy of several prompting ideas, where we\nexplore how giving different incentives or structures affect the performance.\nOur evaluation takes into consideration performance measures on the affective\ncomputing tasks, and the effectiveness of the model to follow the stated\ninstructions, hence generating easy-to-parse responses to be smoothly used in\ndownstream applications.\n","authors":["Mostafa M. Amin","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2403.14006v1.pdf","comment":"2 Tables, 1 Figure, preprint submission to ACII 2024"},{"id":"http://arxiv.org/abs/2403.14001v1","updated":"2024-03-20T21:58:32Z","published":"2024-03-20T21:58:32Z","title":"Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained\n  Sentence Embeddings","summary":"  Sentence embeddings produced by Pretrained Language Models (PLMs) have\nreceived wide attention from the NLP community due to their superior\nperformance when representing texts in numerous downstream applications.\nHowever, the high dimensionality of the sentence embeddings produced by PLMs is\nproblematic when representing large numbers of sentences in memory- or\ncompute-constrained devices. As a solution, we evaluate unsupervised\ndimensionality reduction methods to reduce the dimensionality of sentence\nembeddings produced by PLMs. Our experimental results show that simple methods\nsuch as Principal Component Analysis (PCA) can reduce the dimensionality of\nsentence embeddings by almost $50\\%$, without incurring a significant loss in\nperformance in multiple downstream tasks. Surprisingly, reducing the\ndimensionality further improves performance over the original high-dimensional\nversions for the sentence embeddings produced by some PLMs in some tasks.\n","authors":["Gaifan Zhang","Yi Zhou","Danushka Bollegala"],"pdf_url":"https://arxiv.org/pdf/2403.14001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04451v2","updated":"2024-03-20T21:34:56Z","published":"2023-10-03T19:44:37Z","title":"AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language\n  Models","summary":"  The aligned Large Language Models (LLMs) are powerful language understanding\nand decision-making tools that are created through extensive alignment with\nhuman feedback. However, these large models remain susceptible to jailbreak\nattacks, where adversaries manipulate prompts to elicit malicious outputs that\nshould not be given by aligned LLMs. Investigating jailbreak prompts can lead\nus to delve into the limitations of LLMs and further guide us to secure them.\nUnfortunately, existing jailbreak techniques suffer from either (1) scalability\nissues, where attacks heavily rely on manual crafting of prompts, or (2)\nstealthiness problems, as attacks depend on token-based algorithms to generate\nprompts that are often semantically meaningless, making them susceptible to\ndetection through basic perplexity testing. In light of these challenges, we\nintend to answer this question: Can we develop an approach that can\nautomatically generate stealthy jailbreak prompts? In this paper, we introduce\nAutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can\nautomatically generate stealthy jailbreak prompts by the carefully designed\nhierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN\nnot only automates the process while preserving semantic meaningfulness, but\nalso demonstrates superior attack strength in cross-model transferability, and\ncross-sample universality compared with the baseline. Moreover, we also compare\nAutoDAN with perplexity-based defense methods and show that AutoDAN can bypass\nthem effectively.\n","authors":["Xiaogeng Liu","Nan Xu","Muhao Chen","Chaowei Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.04451v2.pdf","comment":"Published as a conference paper at ICLR 2024. Code is available at\n  https://github.com/SheltonLiu-N/AutoDAN"},{"id":"http://arxiv.org/abs/2310.19268v2","updated":"2024-03-20T21:24:33Z","published":"2023-10-30T05:03:26Z","title":"Moral Judgments in Narratives on Reddit: Investigating Moral Sparks via\n  Social Commonsense and Linguistic Signals","summary":"  Machine ethics ensures ethical conduct in Artificial Intelligence (AI) models\nand agents. Examining real-life applications benefit learning practical ethics\nin many situations, offering valuable data to grasp the complexities of human\nethics in diverse contexts. In this paper, we examine social media platforms\nfor understanding real-life ethical scenarios and human moral judgments. We\nexamine posts from a popular Reddit subreddit (i.e., a subcommunity) called\nr/AmITheAsshole, where authors and commenters share their moral judgments on\nwho is blameworthy. We employ computational techniques to investigate the\nunderlying reasoning influencing moral judgments. We focus on excerpts-which we\nterm moral sparks-from original posts that commenters include to indicate what\nmotivates their judgments. To this end, we examine how (1) events activating\nsocial commonsense and (2) linguistic signals affect moral sparks assignment\nand their subsequent judgments. By examining over 24 672 posts and 175988\ncomments, we find that event-related negative character traits (e.g., immature\nand rude) attract attention and stimulate blame, implying a dependent\nrelationship between character traits and moral values. Specially, we focus on\ncausal graph involving events (c-events) that activate social commonsense. We\nobserve that c-events are perceived with varying levels of informativeness,\ninfluencing moral spark and judgment assignment in distinct ways. This\nobservation is reinforced by examining linguistic features describing\nsemantically similar c-events. Moreover, language influencing commenters'\ncognitive processes enhances the probability of an excerpt becoming a moral\nspark, while factual and concrete descriptions tend to inhibit this effect.\n","authors":["Ruijie Xi","Munindar P. Singh"],"pdf_url":"https://arxiv.org/pdf/2310.19268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03378v3","updated":"2024-03-20T20:57:51Z","published":"2023-09-06T21:56:24Z","title":"RoDia: A New Dataset for Romanian Dialect Identification from Speech","summary":"  We introduce RoDia, the first dataset for Romanian dialect identification\nfrom speech. The RoDia dataset includes a varied compilation of speech samples\nfrom five distinct regions of Romania, covering both urban and rural\nenvironments, totaling 2 hours of manually annotated speech data. Along with\nour dataset, we introduce a set of competitive models to be used as baselines\nfor future research. The top scoring model achieves a macro F1 score of 59.83%\nand a micro F1 score of 62.08%, indicating that the task is challenging. We\nthus believe that RoDia is a valuable resource that will stimulate research\naiming to address the challenges of Romanian dialect identification. We release\nour dataset at https://github.com/codrut2/RoDia.\n","authors":["Codrut Rotaru","Nicolae-Catalin Ristea","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2309.03378v3.pdf","comment":"Accepted at NAACL 2024"},{"id":"http://arxiv.org/abs/2403.05020v2","updated":"2024-03-20T20:44:17Z","published":"2024-03-08T03:49:17Z","title":"Is this the real life? Is this just fantasy? The Misleading Success of\n  Simulating Social Interactions With LLMs","summary":"  Recent advances in large language models (LLM) have enabled richer social\nsimulations, allowing for the study of various social phenomena with LLM-based\nagents. However, most work has used an omniscient perspective on these\nsimulations (e.g., single LLM to generate all interlocutors), which is\nfundamentally at odds with the non-omniscient, information asymmetric\ninteractions that humans have. To examine these differences, we develop an\nevaluation framework to simulate social interactions with LLMs in various\nsettings (omniscient, non-omniscient). Our experiments show that interlocutors\nsimulated omnisciently are much more successful at accomplishing social goals\ncompared to non-omniscient agents, despite the latter being the more realistic\nsetting. Furthermore, we demonstrate that learning from omniscient simulations\nimproves the apparent naturalness of interactions but scarcely enhances goal\nachievement in cooperative scenarios. Our findings indicate that addressing\ninformation asymmetry remains a fundamental challenge for LLM-based agents.\n","authors":["Xuhui Zhou","Zhe Su","Tiwalayo Eisape","Hyunwoo Kim","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2403.05020v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05342v4","updated":"2024-03-20T20:37:17Z","published":"2023-08-10T05:10:17Z","title":"Metacognitive Prompting Improves Understanding in Large Language Models","summary":"  In Large Language Models (LLMs), there have been consistent advancements in\ntask-specific performance, largely influenced by effective prompt design.\nRecent advancements in prompting have enhanced reasoning in logic-intensive\ntasks for LLMs, yet the nuanced understanding abilities of these models,\ncrucial for processing and interpreting complex information, remain\nunderexplored. In this study, we introduce Metacognitive Prompting (MP), a\nstrategy inspired by human introspective reasoning processes. Using MP, LLMs\nundergo a systematic series of structured, self-aware evaluations, drawing on\nboth their vast inherent knowledge and new insights. We conduct extensive\nexperiments on four prevalent LLMs: Llama2, PaLM2, GPT-3.5, and GPT-4, across\nten natural language understanding (NLU) datasets from GLUE, SuperGLUE, BLUE,\nand LexGLUE benchmarks. Additionally, we compare our method with\nchain-of-thought prompting and its advanced versions. The results show that\nGPT-4 consistently excels across all tasks, while other models have shown\nsignificant progress in some tasks when used in conjunction with MP.\nFurthermore, MP consistently outperforms existing prompting methods in both\ngeneral and domain-specific NLU tasks. This study underscores the potential to\namplify the understanding abilities of LLMs and highlights the benefits of\nmirroring human introspective reasoning in NLU tasks.\n","authors":["Yuqing Wang","Yun Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.05342v4.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2312.17244v2","updated":"2024-03-20T20:21:58Z","published":"2023-12-28T18:59:09Z","title":"The LLM Surgeon","summary":"  State-of-the-art language models are becoming increasingly large in an effort\nto achieve the highest performance on large corpora of available textual data.\nHowever, the sheer size of the Transformer architectures makes it difficult to\ndeploy models within computational, environmental or device-specific\nconstraints. We explore data-driven compression of existing pretrained models\nas an alternative to training smaller models from scratch. To do so, we scale\nKronecker-factored curvature approximations of the target loss landscape to\nlarge language models. In doing so, we can compute both the dynamic allocation\nof structures that can be removed as well as updates of remaining weights that\naccount for the removal. We provide a general framework for unstructured,\nsemi-structured and structured pruning and improve upon weight updates to\ncapture more correlations between weights, while remaining computationally\nefficient. Experimentally, our method can prune rows and columns from a range\nof OPT models and Llamav2-7B by 20%-30%, with a negligible loss in performance,\nand achieve state-of-the-art results in unstructured and semi-structured\npruning of large language models.\n","authors":["Tycho F. A. van der Ouderaa","Markus Nagel","Mart van Baalen","Yuki M. Asano","Tijmen Blankevoort"],"pdf_url":"https://arxiv.org/pdf/2312.17244v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14003v1","updated":"2024-03-20T22:05:18Z","published":"2024-03-20T22:05:18Z","title":"Multi-Modal Hallucination Control by Visual Information Grounding","summary":"  Generative Vision-Language Models (VLMs) are prone to generate\nplausible-sounding textual answers that, however, are not always grounded in\nthe input image. We investigate this phenomenon, usually referred to as\n\"hallucination\" and show that it stems from an excessive reliance on the\nlanguage prior. In particular, we show that as more tokens are generated, the\nreliance on the visual prompt decreases, and this behavior strongly correlates\nwith the emergence of hallucinations. To reduce hallucinations, we introduce\nMulti-Modal Mutual-Information Decoding (M3ID), a new sampling method for\nprompt amplification. M3ID amplifies the influence of the reference image over\nthe language prior, hence favoring the generation of tokens with higher mutual\ninformation with the visual prompt. M3ID can be applied to any pre-trained\nautoregressive VLM at inference time without necessitating further training and\nwith minimal computational overhead. If training is an option, we show that\nM3ID can be paired with Direct Preference Optimization (DPO) to improve the\nmodel's reliance on the prompt image without requiring any labels. Our\nempirical findings show that our algorithms maintain the fluency and linguistic\ncapabilities of pre-trained VLMs while reducing hallucinations by mitigating\nvisually ungrounded answers. Specifically, for the LLaVA 13B model, M3ID and\nM3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by\n25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as\nPOPE by 21% and 24%.\n","authors":["Alessandro Favero","Luca Zancato","Matthew Trager","Siddharth Choudhary","Pramuditha Perera","Alessandro Achille","Ashwin Swaminathan","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2403.14003v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.13808v1","updated":"2024-03-20T17:59:58Z","published":"2024-03-20T17:59:58Z","title":"On Pretraining Data Diversity for Self-Supervised Learning","summary":"  We explore the impact of training with more diverse datasets, characterized\nby the number of unique samples, on the performance of self-supervised learning\n(SSL) under a fixed computational budget. Our findings consistently demonstrate\nthat increasing pretraining data diversity enhances SSL performance, albeit\nonly when the distribution distance to the downstream data is minimal. Notably,\neven with an exceptionally large pretraining data diversity achieved through\nmethods like web crawling or diffusion-generated data, among other ways, the\ndistribution shift remains a challenge. Our experiments are comprehensive with\nseven SSL methods using large-scale datasets such as ImageNet and YFCC100M\namounting to over 200 GPU days. Code and trained models will be available at\nhttps://github.com/hammoudhasan/DiversitySSL .\n","authors":["Hasan Abed Al Kader Hammoud","Tuhin Das","Fabio Pizzati","Philip Torr","Adel Bibi","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2403.13808v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.13807v1","updated":"2024-03-20T17:59:57Z","published":"2024-03-20T17:59:57Z","title":"Editing Massive Concepts in Text-to-Image Diffusion Models","summary":"  Text-to-image diffusion models suffer from the risk of generating outdated,\ncopyrighted, incorrect, and biased content. While previous methods have\nmitigated the issues on a small scale, it is essential to handle them\nsimultaneously in larger-scale real-world scenarios. We propose a two-stage\nmethod, Editing Massive Concepts In Diffusion Models (EMCID). The first stage\nperforms memory optimization for each individual concept with dual\nself-distillation from text alignment loss and diffusion noise prediction loss.\nThe second stage conducts massive concept editing with multi-layer, closed form\nmodel editing. We further propose a comprehensive benchmark, named ImageNet\nConcept Editing Benchmark (ICEB), for evaluating massive concept editing for\nT2I models with two subtasks, free-form prompts, massive concept categories,\nand extensive evaluation metrics. Extensive experiments conducted on our\nproposed benchmark and previous benchmarks demonstrate the superior scalability\nof EMCID for editing up to 1,000 concepts, providing a practical approach for\nfast adjustment and re-deployment of T2I diffusion models in real-world\napplications.\n","authors":["Tianwei Xiong","Yue Wu","Enze Xie","Yue Wu","Zhenguo Li","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2403.13807v1.pdf","comment":"Project page: https://silentview.github.io/EMCID/ . Code:\n  https://github.com/SilentView/EMCID"},{"id":"http://arxiv.org/abs/2403.13805v1","updated":"2024-03-20T17:59:55Z","published":"2024-03-20T17:59:55Z","title":"RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition","summary":"  CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from\nnoise image-text pairs to excel at recognizing a wide array of candidates, yet\nits focus on broad associations hinders the precision in distinguishing subtle\ndifferences among fine-grained items. Conversely, Multimodal Large Language\nModels (MLLMs) excel at classifying fine-grained categories, thanks to their\nsubstantial knowledge from pre-training on web-level corpora. However, the\nperformance of MLLMs declines with an increase in category numbers, primarily\ndue to growing complexity and constraints of limited context window size. To\nsynergize the strengths of both approaches and enhance the few-shot/zero-shot\nrecognition abilities for datasets characterized by extensive and fine-grained\nvocabularies, this paper introduces RAR, a Retrieving And Ranking augmented\nmethod for MLLMs. We initially establish a multi-modal retriever based on CLIP\nto create and store explicit memory for different categories beyond the\nimmediate context window. During inference, RAR retrieves the top-k similar\nresults from the memory and uses MLLMs to rank and make the final predictions.\nOur proposed approach not only addresses the inherent limitations in\nfine-grained recognition but also preserves the model's comprehensive knowledge\nbase, significantly boosting accuracy across a range of vision-language\nrecognition tasks. Notably, our approach demonstrates a significant improvement\nin performance on 5 fine-grained visual recognition benchmarks, 11 few-shot\nimage recognition datasets, and the 2 object detection datasets under the\nzero-shot recognition setting.\n","authors":["Ziyu Liu","Zeyi Sun","Yuhang Zang","Wei Li","Pan Zhang","Xiaoyi Dong","Yuanjun Xiong","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13805v1.pdf","comment":"Project: https://github.com/Liuziyu77/RAR"},{"id":"http://arxiv.org/abs/2403.13806v1","updated":"2024-03-20T17:59:55Z","published":"2024-03-20T17:59:55Z","title":"RadSplat: Radiance Field-Informed Gaussian Splatting for Robust\n  Real-Time Rendering with 900+ FPS","summary":"  Recent advances in view synthesis and real-time rendering have achieved\nphotorealistic quality at impressive rendering speeds. While Radiance\nField-based methods achieve state-of-the-art quality in challenging scenarios\nsuch as in-the-wild captures and large-scale scenes, they often suffer from\nexcessively high compute requirements linked to volumetric rendering. Gaussian\nSplatting-based methods, on the other hand, rely on rasterization and naturally\nachieve real-time rendering but suffer from brittle optimization heuristics\nthat underperform on more challenging scenes. In this work, we present\nRadSplat, a lightweight method for robust real-time rendering of complex\nscenes. Our main contributions are threefold. First, we use radiance fields as\na prior and supervision signal for optimizing point-based scene\nrepresentations, leading to improved quality and more robust optimization.\nNext, we develop a novel pruning technique reducing the overall point count\nwhile maintaining high quality, leading to smaller and more compact scene\nrepresentations with faster inference speeds. Finally, we propose a novel\ntest-time filtering approach that further accelerates rendering and allows to\nscale to larger, house-sized scenes. We find that our method enables\nstate-of-the-art synthesis of complex captures at 900+ FPS.\n","authors":["Michael Niemeyer","Fabian Manhardt","Marie-Julie Rakotosaona","Michael Oechsle","Daniel Duckworth","Rama Gosula","Keisuke Tateno","John Bates","Dominik Kaeser","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2403.13806v1.pdf","comment":"Project page at https://m-niemeyer.github.io/radsplat/"},{"id":"http://arxiv.org/abs/2403.13804v1","updated":"2024-03-20T17:59:43Z","published":"2024-03-20T17:59:43Z","title":"Learning from Models and Data for Visual Grounding","summary":"  We introduce SynGround, a novel framework that combines data-driven learning\nand knowledge transfer from various large-scale pretrained models to enhance\nthe visual grounding capabilities of a pretrained vision-and-language model.\nThe knowledge transfer from the models initiates the generation of image\ndescriptions through an image description generator. These descriptions serve\ndual purposes: they act as prompts for synthesizing images through a\ntext-to-image generator, and as queries for synthesizing text, from which\nphrases are extracted using a large language model. Finally, we leverage an\nopen-vocabulary object detector to generate synthetic bounding boxes for the\nsynthetic images and texts. We finetune a pretrained vision-and-language model\non this dataset by optimizing a mask-attention consistency objective that\naligns region annotations with gradient-based model explanations. The resulting\nmodel improves the grounding capabilities of an off-the-shelf\nvision-and-language model. Particularly, SynGround improves the pointing game\naccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on\nRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to\n63.67%.\n","authors":["Ruozhen He","Paola Cascante-Bonilla","Ziyan Yang","Alexander C. Berg","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2403.13804v1.pdf","comment":"Project Page: https://catherine-r-he.github.io/SynGround/"},{"id":"http://arxiv.org/abs/2403.13803v1","updated":"2024-03-20T17:59:16Z","published":"2024-03-20T17:59:16Z","title":"Bounding Box Stability against Feature Dropout Reflects Detector\n  Generalization across Environments","summary":"  Bounding boxes uniquely characterize object detection, where a good detector\ngives accurate bounding boxes of categories of interest. However, in the\nreal-world where test ground truths are not provided, it is non-trivial to find\nout whether bounding boxes are accurate, thus preventing us from assessing the\ndetector generalization ability. In this work, we find under feature map\ndropout, good detectors tend to output bounding boxes whose locations do not\nchange much, while bounding boxes of poor detectors will undergo noticeable\nposition changes. We compute the box stability score (BoS score) to reflect\nthis stability. Specifically, given an image, we compute a normal set of\nbounding boxes and a second set after feature map dropout. To obtain BoS score,\nwe use bipartite matching to find the corresponding boxes between the two sets\nand compute the average Intersection over Union (IoU) across the entire test\nset. We contribute to finding that BoS score has a strong, positive correlation\nwith detection accuracy measured by mean average precision (mAP) under various\ntest environments. This relationship allows us to predict the accuracy of\ndetectors on various real-world test sets without accessing test ground truths,\nverified on canonical detection tasks such as vehicle detection and pedestrian\ndetection. Code and data are available at https://github.com/YangYangGirl/BoS.\n","authors":["Yang Yang","Wenhai Wang","Zhe Chen","Jifeng Dai","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.13803v1.pdf","comment":"ICLR 2024 spotlight"},{"id":"http://arxiv.org/abs/2403.13802v1","updated":"2024-03-20T17:59:14Z","published":"2024-03-20T17:59:14Z","title":"ZigMa: Zigzag Mamba Diffusion Model","summary":"  The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$. Code will be released at https://taohu.me/zigma/\n","authors":["Vincent Tao Hu","Stefan Andreas Baumann","Ming Gui","Olga Grebenkova","Pingchuan Ma","Johannes Fischer","Bjorn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13802v1.pdf","comment":"Project Page: https://taohu.me/zigma/"},{"id":"http://arxiv.org/abs/2312.06644v2","updated":"2024-03-20T17:58:05Z","published":"2023-12-11T18:56:37Z","title":"AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes","summary":"  Inspired by cognitive theories, we introduce AnyHome, a framework that\ntranslates any text into well-structured and textured indoor scenes at a\nhouse-scale. By prompting Large Language Models (LLMs) with designed templates,\nour approach converts provided textual narratives into amodal structured\nrepresentations. These representations guarantee consistent and realistic\nspatial layouts by directing the synthesis of a geometry mesh within defined\nconstraints. A Score Distillation Sampling process is then employed to refine\nthe geometry, followed by an egocentric inpainting process that adds lifelike\ntextures to it. AnyHome stands out with its editability, customizability,\ndiversity, and realism. The structured representations for scenes allow for\nextensive editing at varying levels of granularity. Capable of interpreting\ntexts ranging from simple labels to detailed narratives, AnyHome generates\ndetailed geometries and textures that outperform existing methods in both\nquantitative and qualitative measures.\n","authors":["Rao Fu","Zehao Wen","Zichen Liu","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2312.06644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13800v1","updated":"2024-03-20T17:57:02Z","published":"2024-03-20T17:57:02Z","title":"TimeRewind: Rewinding Time with Image-and-Events Video Diffusion","summary":"  This paper addresses the novel challenge of ``rewinding'' time from a single\ncaptured image to recover the fleeting moments missed just before the shutter\nbutton is pressed. This problem poses a significant challenge in computer\nvision and computational photography, as it requires predicting plausible\npre-capture motion from a single static frame, an inherently ill-posed task due\nto the high degree of freedom in potential pixel movements. We overcome this\nchallenge by leveraging the emerging technology of neuromorphic event cameras,\nwhich capture motion information with high temporal resolution, and integrating\nthis data with advanced image-to-video diffusion models. Our proposed framework\nintroduces an event motion adaptor conditioned on event camera data, guiding\nthe diffusion model to generate videos that are visually coherent and\nphysically grounded in the captured events. Through extensive experimentation,\nwe demonstrate the capability of our approach to synthesize high-quality videos\nthat effectively ``rewind'' time, showcasing the potential of combining event\ncamera technology with generative models. Our work opens new avenues for\nresearch at the intersection of computer vision, computational photography, and\ngenerative modeling, offering a forward-thinking solution to capturing missed\nmoments and enhancing future consumer cameras and smartphones. Please see the\nproject page at https://timerewind.github.io/ for video results and code\nrelease.\n","authors":["Jingxi Chen","Brandon Y. Feng","Haoming Cai","Mingyang Xie","Christopher Metzler","Cornelia Fermuller","Yiannis Aloimonos"],"pdf_url":"https://arxiv.org/pdf/2403.13800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13798v1","updated":"2024-03-20T17:55:21Z","published":"2024-03-20T17:55:21Z","title":"Hierarchical NeuroSymbolic Approach for Action Quality Assessment","summary":"  Action quality assessment (AQA) applies computer vision to quantitatively\nassess the performance or execution of a human action. Current AQA approaches\nare end-to-end neural models, which lack transparency and tend to be biased\nbecause they are trained on subjective human judgements as ground-truth. To\naddress these issues, we introduce a neuro-symbolic paradigm for AQA, which\nuses neural networks to abstract interpretable symbols from video data and\nmakes quality assessments by applying rules to those symbols. We take diving as\nthe case study. We found that domain experts prefer our system and find it more\ninformative than purely neural approaches to AQA in diving. Our system also\nachieves state-of-the-art action recognition and temporal segmentation, and\nautomatically generates a detailed report that breaks the dive down into its\nelements and provides objective scoring with visual evidence. As verified by a\ngroup of domain experts, this report may be used to assist judges in scoring,\nhelp train judges, and provide feedback to divers. We will open-source all of\nour annotated training data and code for ease of reproducibility.\n","authors":["Lauren Okamoto","Paritosh Parmar"],"pdf_url":"https://arxiv.org/pdf/2403.13798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13797v1","updated":"2024-03-20T17:54:58Z","published":"2024-03-20T17:54:58Z","title":"Bridge the Modality and Capacity Gaps in Vision-Language Model Selection","summary":"  Vision Language Models (VLMs) excel in zero-shot image classification by\npairing images with textual category names. The expanding variety of\nPre-Trained VLMs enhances the likelihood of identifying a suitable VLM for\nspecific tasks. Thus, a promising zero-shot image classification strategy is\nselecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely\non the text data of the target dataset without access to the dataset's images.\nIn this paper, we analyze two inherent challenges in assessing the ability of a\nVLM in this Language-Only VLM selection: the \"Modality Gap\" -- the disparity in\nVLM's embeddings across two different modalities, making text a less reliable\nsubstitute for images; and the \"Capability Gap\" -- the discrepancy between the\nVLM's overall ranking and its ranking for target dataset, hindering direct\nprediction of a model's dataset-specific performance from its general\nperformance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the\nnegative impact of these two gaps. SWAB first adopts optimal transport to\ncapture the relevance between open-source datasets and target dataset with a\ntransportation matrix. It then uses this matrix to transfer useful statistics\nof VLMs from open-source datasets to the target dataset for bridging those two\ngaps and enhancing the VLM's capacity estimation for VLM selection. Experiments\nacross various VLMs and image classification datasets validate SWAB's\neffectiveness.\n","authors":["Chao Yi","De-Chuan Zhan","Han-Jia Ye"],"pdf_url":"https://arxiv.org/pdf/2403.13797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13788v1","updated":"2024-03-20T17:51:53Z","published":"2024-03-20T17:51:53Z","title":"DepthFM: Fast Monocular Depth Estimation with Flow Matching","summary":"  Monocular depth estimation is crucial for numerous downstream vision tasks\nand applications. Current discriminative approaches to this problem are limited\ndue to blurry artifacts, while state-of-the-art generative methods suffer from\nslow sampling due to their SDE nature. Rather than starting from noise, we seek\na direct mapping from input image to depth map. We observe that this can be\neffectively framed using flow matching, since its straight trajectories through\nsolution space offer efficiency and high quality. Our study demonstrates that a\npre-trained image diffusion model can serve as an adequate prior for a flow\nmatching depth model, allowing efficient training on only synthetic data to\ngeneralize to real images. We find that an auxiliary surface normals loss\nfurther improves the depth estimates. Due to the generative nature of our\napproach, our model reliably predicts the confidence of its depth estimates. On\nstandard benchmarks of complex natural scenes, our lightweight approach\nexhibits state-of-the-art performance at favorable low computational cost\ndespite only being trained on little synthetic data.\n","authors":["Ming Gui","Johannes S. Fischer","Ulrich Prestel","Pingchuan Ma","Dmytro Kotovenko","Olga Grebenkova","Stefan Andreas Baumann","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13778v1","updated":"2024-03-20T17:41:35Z","published":"2024-03-20T17:41:35Z","title":"Certified Human Trajectory Prediction","summary":"  Trajectory prediction plays an essential role in autonomous vehicles. While\nnumerous strategies have been developed to enhance the robustness of trajectory\nprediction models, these methods are predominantly heuristic and do not offer\nguaranteed robustness against adversarial attacks and noisy observations. In\nthis work, we propose a certification approach tailored for the task of\ntrajectory prediction. To this end, we address the inherent challenges\nassociated with trajectory prediction, including unbounded outputs, and\nmutli-modality, resulting in a model that provides guaranteed robustness.\nFurthermore, we integrate a denoiser into our method to further improve the\nperformance. Through comprehensive evaluations, we demonstrate the\neffectiveness of the proposed technique across various baselines and using\nstandard trajectory prediction datasets. The code will be made available\nonline: https://s-attack.github.io/\n","authors":["Mohammadhossein Bahari","Saeed Saadatnejad","Amirhossein Asgari Farsangi","Seyed-Mohsen Moosavi-Dezfooli","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2403.13778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09368v2","updated":"2024-03-20T17:36:35Z","published":"2024-02-14T18:13:51Z","title":"Magic-Me: Identity-Specific Video Customized Diffusion","summary":"  Creating content with specified identities (ID) has attracted significant\ninterest in the field of generative models. In the field of text-to-image\ngeneration (T2I), subject-driven creation has achieved great progress with the\nidentity controlled via reference images. However, its extension to video\ngeneration is not well explored. In this work, we propose a simple yet\neffective subject identity controllable video generation framework, termed\nVideo Custom Diffusion (VCD). With a specified identity defined by a few\nimages, VCD reinforces the identity characteristics and injects frame-wise\ncorrelation at the initialization stage for stable video outputs. To achieve\nthis, we propose three novel components that are essential for high-quality\nidentity preservation and stable video generation: 1) a noise initialization\nmethod with 3D Gaussian Noise Prior for better inter-frame stability; 2) an ID\nmodule based on extended Textual Inversion trained with the cropped identity to\ndisentangle the ID information from the background 3) Face VCD and Tiled VCD\nmodules to reinforce faces and upscale the video to higher resolution while\npreserving the identity's features. We conducted extensive experiments to\nverify that VCD is able to generate stable videos with better ID over the\nbaselines. Besides, with the transferability of the encoded identity in the ID\nmodule, VCD is also working well with personalized text-to-image models\navailable publicly. The codes are available at\nhttps://github.com/Zhen-Dong/Magic-Me.\n","authors":["Ze Ma","Daquan Zhou","Chun-Hsiao Yeh","Xue-She Wang","Xiuyu Li","Huanrui Yang","Zhen Dong","Kurt Keutzer","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2402.09368v2.pdf","comment":"Project Page at https://magic-me-webpage.github.io"},{"id":"http://arxiv.org/abs/2403.11085v2","updated":"2024-03-20T17:35:15Z","published":"2024-03-17T04:36:18Z","title":"m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks","summary":"  Real-world multi-modal problems are rarely solved by a single machine\nlearning model, and often require multi-step computational plans that involve\nstitching several models. Tool-augmented LLMs hold tremendous promise for\nautomating the generation of such computational plans. However, the lack of\nstandardized benchmarks for evaluating LLMs as planners for multi-step\nmulti-modal tasks has prevented a systematic study of planner design decisions.\nShould LLMs generate a full plan in a single shot or step-by-step? Should they\ninvoke tools directly with Python code or through structured data formats like\nJSON? Does feedback improve planning? To answer these questions and more, we\nintroduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks\ninvolving 33 tools that include multi-modal models, (free) public APIs, and\nimage processing modules. For each of these task queries, we provide\nautomatically generated plans using this realistic toolset. We further provide\na high-quality subset of 1,565 task plans that are human-verified and correctly\nexecutable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies\n(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3\ntypes of feedback (parsing/verification/execution). Finally, we summarize\ntakeaways from our extensive experiments. Our dataset and code are available on\nHuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github\n(https://github.com/RAIVNLab/mnms).\n","authors":["Zixian Ma","Weikai Huang","Jieyu Zhang","Tanmay Gupta","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.11085v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13771v1","updated":"2024-03-20T17:33:02Z","published":"2024-03-20T17:33:02Z","title":"Describe-and-Dissect: Interpreting Neurons in Vision Networks with\n  Language Models","summary":"  In this paper, we propose Describe-and-Dissect (DnD), a novel method to\ndescribe the roles of hidden neurons in vision networks. DnD utilizes recent\nadvancements in multimodal deep learning to produce complex natural language\ndescriptions, without the need for labeled training data or a predefined set of\nconcepts to choose from. Additionally, DnD is training-free, meaning we don't\ntrain any new models and can easily leverage more capable general purpose\nmodels in the future. We have conducted extensive qualitative and quantitative\nanalysis to show that DnD outperforms prior work by providing higher quality\nneuron descriptions. Specifically, our method on average provides the highest\nquality labels and is more than 2 times as likely to be selected as the best\nexplanation for a neuron than the best baseline.\n","authors":["Nicholas Bai","Rahul A. Iyer","Tuomas Oikarinen","Tsui-Wei Weng"],"pdf_url":"https://arxiv.org/pdf/2403.13771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13765v1","updated":"2024-03-20T17:28:17Z","published":"2024-03-20T17:28:17Z","title":"Towards Principled Representation Learning from Videos for Reinforcement\n  Learning","summary":"  We study pre-training representations for decision-making using video data,\nwhich is abundantly available for tasks such as game agents and software\ntesting. Even though significant empirical advances have been made on this\nproblem, a theoretical understanding remains absent. We initiate the\ntheoretical investigation into principled approaches for representation\nlearning and focus on learning the latent state representations of the\nunderlying MDP using video data. We study two types of settings: one where\nthere is iid noise in the observation, and a more challenging setting where\nthere is also the presence of exogenous noise, which is non-iid noise that is\ntemporally correlated, such as the motion of people or cars in the background.\nWe study three commonly used approaches: autoencoding, temporal contrastive\nlearning, and forward modeling. We prove upper bounds for temporal contrastive\nlearning and forward modeling in the presence of only iid noise. We show that\nthese approaches can learn the latent state and use it to do efficient\ndownstream RL with polynomial sample complexity. When exogenous noise is also\npresent, we establish a lower bound result showing that the sample complexity\nof learning from video data can be exponentially worse than learning from\naction-labeled trajectory data. This partially explains why reinforcement\nlearning with video pre-training is hard. We evaluate these representational\nlearning methods in two visual domains, yielding results that are consistent\nwith our theoretical findings.\n","authors":["Dipendra Misra","Akanksha Saran","Tengyang Xie","Alex Lamb","John Langford"],"pdf_url":"https://arxiv.org/pdf/2403.13765v1.pdf","comment":"ICLR 2024 Spotlight Conference Paper"},{"id":"http://arxiv.org/abs/2312.00651v2","updated":"2024-03-20T17:28:02Z","published":"2023-12-01T15:24:38Z","title":"TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion\n  Models","summary":"  Despite remarkable achievements in video synthesis, achieving granular\ncontrol over complex dynamics, such as nuanced movement among multiple\ninteracting objects, still presents a significant hurdle for dynamic world\nmodeling, compounded by the necessity to manage appearance and disappearance,\ndrastic scale changes, and ensure consistency for instances across frames.\nThese challenges hinder the development of video generation that can faithfully\nmimic real-world complexity, limiting utility for applications requiring\nhigh-level realism and controllability, including advanced scene simulation and\ntraining of perception systems. To address that, we propose TrackDiffusion, a\nnovel video generation framework affording fine-grained trajectory-conditioned\nmotion control via diffusion models, which facilitates the precise manipulation\nof the object trajectories and interactions, overcoming the prevalent\nlimitation of scale and continuity disruptions. A pivotal component of\nTrackDiffusion is the instance enhancer, which explicitly ensures inter-frame\nconsistency of multiple objects, a critical factor overlooked in the current\nliterature. Moreover, we demonstrate that generated video sequences by our\nTrackDiffusion can be used as training data for visual perception models. To\nthe best of our knowledge, this is the first work to apply video diffusion\nmodels with tracklet conditions and demonstrate that generated frames can be\nbeneficial for improving the performance of object trackers.\n","authors":["Pengxiang Li","Kai Chen","Zhili Liu","Ruiyuan Gao","Lanqing Hong","Guo Zhou","Hua Yao","Dit-Yan Yeung","Huchuan Lu","Xu Jia"],"pdf_url":"https://arxiv.org/pdf/2312.00651v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13763v1","updated":"2024-03-20T17:26:22Z","published":"2024-03-20T17:26:22Z","title":"Practical End-to-End Optical Music Recognition for Pianoform Music","summary":"  The majority of recent progress in Optical Music Recognition (OMR) has been\nachieved with Deep Learning methods, especially models following the end-to-end\nparadigm, reading input images and producing a linear sequence of tokens.\nUnfortunately, many music scores, especially piano music, cannot be easily\nconverted to a linear sequence. This has led OMR researchers to use custom\nlinearized encodings, instead of broadly accepted structured formats for music\nnotation. Their diversity makes it difficult to compare the performance of OMR\nsystems directly. To bring recent OMR model progress closer to useful results:\n(a) We define a sequential format called Linearized MusicXML, allowing to train\nan end-to-end model directly and maintaining close cohesion and compatibility\nwith the industry-standard MusicXML format. (b) We create a dev and test set\nfor benchmarking typeset OMR with MusicXML ground truth based on the OpenScore\nLieder corpus. They contain 1,438 and 1,493 pianoform systems, each with an\nimage from IMSLP. (c) We train and fine-tune an end-to-end model to serve as a\nbaseline on the dataset and employ the TEDn metric to evaluate the model. We\nalso test our model against the recently published synthetic pianoform dataset\nGrandStaff and surpass the state-of-the-art results.\n","authors":["Jiří Mayer","Milan Straka","Jan Hajič jr.","Pavel Pecina"],"pdf_url":"https://arxiv.org/pdf/2403.13763v1.pdf","comment":"15+4 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.13761v1","updated":"2024-03-20T17:20:48Z","published":"2024-03-20T17:20:48Z","title":"HierCode: A Lightweight Hierarchical Codebook for Zero-shot Chinese Text\n  Recognition","summary":"  Text recognition, especially for complex scripts like Chinese, faces unique\nchallenges due to its intricate character structures and vast vocabulary.\nTraditional one-hot encoding methods struggle with the representation of\nhierarchical radicals, recognition of Out-Of-Vocabulary (OOV) characters, and\non-device deployment due to their computational intensity. To address these\nchallenges, we propose HierCode, a novel and lightweight codebook that exploits\nthe innate hierarchical nature of Chinese characters. HierCode employs a\nmulti-hot encoding strategy, leveraging hierarchical binary tree encoding and\nprototype learning to create distinctive, informative representations for each\ncharacter. This approach not only facilitates zero-shot recognition of OOV\ncharacters by utilizing shared radicals and structures but also excels in\nline-level recognition tasks by computing similarity with visual features, a\nnotable advantage over existing methods. Extensive experiments across diverse\nbenchmarks, including handwritten, scene, document, web, and ancient text, have\nshowcased HierCode's superiority for both conventional and zero-shot Chinese\ncharacter or text recognition, exhibiting state-of-the-art performance with\nsignificantly fewer parameters and fast inference speed.\n","authors":["Yuyi Zhang","Yuanzhi Zhu","Dezhi Peng","Peirong Zhang","Zhenhua Yang","Zhibo Yang","Cong Yao","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2403.13761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13762v1","updated":"2024-03-20T17:20:48Z","published":"2024-03-20T17:20:48Z","title":"When Cars meet Drones: Hyperbolic Federated Learning for Source-Free\n  Domain Adaptation in Adverse Weather","summary":"  In Federated Learning (FL), multiple clients collaboratively train a global\nmodel without sharing private data. In semantic segmentation, the Federated\nsource Free Domain Adaptation (FFreeDA) setting is of particular interest,\nwhere clients undergo unsupervised training after supervised pretraining at the\nserver side. While few recent works address FL for autonomous vehicles,\nintrinsic real-world challenges such as the presence of adverse weather\nconditions and the existence of different autonomous agents are still\nunexplored. To bridge this gap, we address both problems and introduce a new\nfederated semantic segmentation setting where both car and drone clients\nco-exist and collaborate. Specifically, we propose a novel approach for this\nsetting which exploits a batch-norm weather-aware strategy to dynamically adapt\nthe model to the different weather conditions, while hyperbolic space\nprototypes are used to align the heterogeneous client representations. Finally,\nwe introduce FLYAWARE, the first semantic segmentation dataset with adverse\nweather data for aerial vehicles.\n","authors":["Giulia Rizzoli","Matteo Caligiuri","Donald Shenaj","Francesco Barbato","Pietro Zanuttigh"],"pdf_url":"https://arxiv.org/pdf/2403.13762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16355v3","updated":"2024-03-20T17:13:53Z","published":"2024-01-29T17:59:19Z","title":"PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding\n  and Reasoning in Pathology","summary":"  The emergence of large multimodal models has unlocked remarkable potential in\nAI, particularly in pathology. However, the lack of specialized, high-quality\nbenchmark impeded their development and precise evaluation. To address this, we\nintroduce PathMMU, the largest and highest-quality expert-validated pathology\nbenchmark for Large Multimodal Models (LMMs). It comprises 33,428 multimodal\nmulti-choice questions and 24,067 images from various sources, each accompanied\nby an explanation for the correct answer. The construction of PathMMU harnesses\nGPT-4V's advanced capabilities, utilizing over 30,000 image-caption pairs to\nenrich captions and generate corresponding Q&As in a cascading process.\nSignificantly, to maximize PathMMU's authority, we invite seven pathologists to\nscrutinize each question under strict standards in PathMMU's validation and\ntest sets, while simultaneously setting an expert-level performance benchmark\nfor PathMMU. We conduct extensive evaluations, including zero-shot assessments\nof 14 open-sourced and 4 closed-sourced LMMs and their robustness to image\ncorruption. We also fine-tune representative LMMs to assess their adaptability\nto PathMMU. The empirical findings indicate that advanced LMMs struggle with\nthe challenging PathMMU benchmark, with the top-performing LMM, GPT-4V,\nachieving only a 49.8% zero-shot performance, significantly lower than the\n71.8% demonstrated by human pathologists. After fine-tuning, significantly\nsmaller open-sourced LMMs can outperform GPT-4V but still fall short of the\nexpertise shown by pathologists. We hope that the PathMMU will offer valuable\ninsights and foster the development of more specialized, next-generation LMMs\nfor pathology.\n","authors":["Yuxuan Sun","Hao Wu","Chenglu Zhu","Sunyi Zheng","Qizi Chen","Kai Zhang","Yunlong Zhang","Dan Wan","Xiaoxiao Lan","Mengyue Zheng","Jingxiong Li","Xinheng Lyu","Tao Lin","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.16355v3.pdf","comment":"27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2403.13756v1","updated":"2024-03-20T17:03:38Z","published":"2024-03-20T17:03:38Z","title":"Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge\n  Augmentation in Vision Language Model","summary":"  We present a knowledge augmentation strategy for assessing the diagnostic\ngroups and gait impairment from monocular gait videos. Based on a large-scale\npre-trained Vision Language Model (VLM), our model learns and improves visual,\ntextual, and numerical representations of patient gait videos, through a\ncollective learning across three distinct modalities: gait videos,\nclass-specific descriptions, and numerical gait parameters. Our specific\ncontributions are two-fold: First, we adopt a knowledge-aware prompt tuning\nstrategy to utilize the class-specific medical description in guiding the text\nprompt learning. Second, we integrate the paired gait parameters in the form of\nnumerical texts to enhance the numeracy of the textual representation. Results\ndemonstrate that our model not only significantly outperforms state-of-the-art\n(SOTA) in video-based classification tasks but also adeptly decodes the learned\nclass-specific text features into natural language descriptions using the\nvocabulary of quantitative gait parameters. The code and the model will be made\navailable at our project page.\n","authors":["Diwei Wang","Kun Yuan","Candice Muller","Frédéric Blanc","Nicolas Padoy","Hyewon Seo"],"pdf_url":"https://arxiv.org/pdf/2403.13756v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13747v1","updated":"2024-03-20T16:54:55Z","published":"2024-03-20T16:54:55Z","title":"Leveraging High-Resolution Features for Improved Deep Hashing-based\n  Image Retrieval","summary":"  Deep hashing techniques have emerged as the predominant approach for\nefficient image retrieval. Traditionally, these methods utilize pre-trained\nconvolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature\nextractors. However, the increasing complexity of datasets poses challenges for\nthese backbone architectures in capturing meaningful features essential for\neffective image retrieval. In this study, we explore the efficacy of employing\nhigh-resolution features learned through state-of-the-art techniques for image\nretrieval tasks. Specifically, we propose a novel methodology that utilizes\nHigh-Resolution Networks (HRNets) as the backbone for the deep hashing task,\ntermed High-Resolution Hashing Network (HHNet). Our approach demonstrates\nsuperior performance compared to existing methods across all tested benchmark\ndatasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance\nimprovement is more pronounced for complex datasets, which highlights the need\nto learn high-resolution features for intricate image retrieval tasks.\nFurthermore, we conduct a comprehensive analysis of different HRNet\nconfigurations and provide insights into the optimal architecture for the deep\nhashing task\n","authors":["Aymene Berriche","Mehdi Adjal Zakaria","Riyadh Baghdadi"],"pdf_url":"https://arxiv.org/pdf/2403.13747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13745v1","updated":"2024-03-20T16:53:45Z","published":"2024-03-20T16:53:45Z","title":"Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific\n  Adaptation","summary":"  Video outpainting is a challenging task, aiming at generating video content\noutside the viewport of the input video while maintaining inter-frame and\nintra-frame consistency. Existing methods fall short in either generation\nquality or flexibility. We introduce MOTIA Mastering Video Outpainting Through\nInput-Specific Adaptation, a diffusion-based pipeline that leverages both the\nintrinsic data-specific patterns of the source video and the image/video\ngenerative prior for effective outpainting. MOTIA comprises two main phases:\ninput-specific adaptation and pattern-aware outpainting. The input-specific\nadaptation phase involves conducting efficient and effective pseudo outpainting\nlearning on the single-shot source video. This process encourages the model to\nidentify and learn patterns within the source video, as well as bridging the\ngap between standard generative processes and outpainting. The subsequent\nphase, pattern-aware outpainting, is dedicated to the generalization of these\nlearned patterns to generate outpainting outcomes. Additional strategies\nincluding spatial-aware insertion and noise travel are proposed to better\nleverage the diffusion model's generative prior and the acquired video patterns\nfrom source videos. Extensive evaluations underscore MOTIA's superiority,\noutperforming existing state-of-the-art methods in widely recognized\nbenchmarks. Notably, these advancements are achieved without necessitating\nextensive, task-specific tuning.\n","authors":["Fu-Yun Wang","Xiaoshi Wu","Zhaoyang Huang","Xiaoyu Shi","Dazhong Shen","Guanglu Song","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.13745v1.pdf","comment":"Code will be available at https://github.com/G-U-N/Be-Your-Outpainter"},{"id":"http://arxiv.org/abs/2302.05666v5","updated":"2024-03-20T16:50:25Z","published":"2023-02-11T11:56:06Z","title":"Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels","summary":"  Intersection over Union (IoU) losses are surrogates that directly optimize\nthe Jaccard index. Leveraging IoU losses as part of the loss function have\ndemonstrated superior performance in semantic segmentation tasks compared to\noptimizing pixel-wise losses such as the cross-entropy loss alone. However, we\nidentify a lack of flexibility in these losses to support vital training\ntechniques like label smoothing, knowledge distillation, and semi-supervised\nlearning, mainly due to their inability to process soft labels. To address\nthis, we introduce Jaccard Metric Losses (JMLs), which are identical to the\nsoft Jaccard loss in standard settings with hard labels but are fully\ncompatible with soft labels. We apply JMLs to three prominent use cases of soft\nlabels: label smoothing, knowledge distillation and semi-supervised learning,\nand demonstrate their potential to enhance model accuracy and calibration. Our\nexperiments show consistent improvements over the cross-entropy loss across 4\nsemantic segmentation datasets (Cityscapes, PASCAL VOC, ADE20K, DeepGlobe Land)\nand 13 architectures, including classic CNNs and recent vision transformers.\nRemarkably, our straightforward approach significantly outperforms\nstate-of-the-art knowledge distillation and semi-supervised learning methods.\nThe code is available at\n\\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.\n","authors":["Zifu Wang","Xuefei Ning","Matthew B. Blaschko"],"pdf_url":"https://arxiv.org/pdf/2302.05666v5.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2311.18561v2","updated":"2024-03-20T16:27:53Z","published":"2023-11-30T13:53:50Z","title":"Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and\n  Real-time Rendering","summary":"  Modeling dynamic, large-scale urban scenes is challenging due to their highly\nintricate geometric structures and unconstrained dynamics in both space and\ntime. Prior methods often employ high-level architectural priors, separating\nstatic and dynamic elements, resulting in suboptimal capture of their\nsynergistic interactions. To address this challenge, we present a unified\nrepresentation model, called Periodic Vibration Gaussian (PVG). PVG builds upon\nthe efficient 3D Gaussian splatting technique, originally designed for static\nscene representation, by introducing periodic vibration-based temporal\ndynamics. This innovation enables PVG to elegantly and uniformly represent the\ncharacteristics of various objects and elements in dynamic urban scenes. To\nenhance temporally coherent and large scene representation learning with sparse\ntraining data, we introduce a novel temporal smoothing mechanism and a\nposition-aware adaptive control strategy respectively. Extensive experiments on\nWaymo Open Dataset and KITTI benchmarks demonstrate that PVG surpasses\nstate-of-the-art alternatives in both reconstruction and novel view synthesis\nfor both dynamic and static scenes. Notably, PVG achieves this without relying\non manually labeled object bounding boxes or expensive optical flow estimation.\nMoreover, PVG exhibits 900-fold acceleration in rendering over the best\nalternative.\n","authors":["Yurui Chen","Chun Gu","Junzhe Jiang","Xiatian Zhu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.18561v2.pdf","comment":"Project page: https://fudan-zvg.github.io/PVG/"},{"id":"http://arxiv.org/abs/2310.13805v2","updated":"2024-03-20T16:23:20Z","published":"2023-10-20T20:32:43Z","title":"Normalizing flow-based deep variational Bayesian network for seismic\n  multi-hazards and impacts estimation from InSAR imagery","summary":"  Onsite disasters like earthquakes can trigger cascading hazards and impacts,\nsuch as landslides and infrastructure damage, leading to catastrophic losses;\nthus, rapid and accurate estimates are crucial for timely and effective\npost-disaster responses. Interferometric Synthetic aperture radar (InSAR) data\nis important in providing high-resolution onsite information for rapid hazard\nestimation. Most recent methods using InSAR imagery signals predict a single\ntype of hazard and thus often suffer low accuracy due to noisy and complex\nsignals induced by co-located hazards, impacts, and irrelevant environmental\nchanges (e.g., vegetation changes, human activities). We introduce a novel\nstochastic variational inference with normalizing flows derived to jointly\napproximate posteriors of multiple unobserved hazards and impacts from noisy\nInSAR imagery.\n","authors":["Xuechun Li","Paula M. Burgi","Wei Ma","Hae Young Noh","David J. Wald","Susu Xu"],"pdf_url":"https://arxiv.org/pdf/2310.13805v2.pdf","comment":"This paper needs to be reviewed by the USGS"},{"id":"http://arxiv.org/abs/2303.17783v5","updated":"2024-03-20T16:21:33Z","published":"2023-03-31T03:14:44Z","title":"Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with\n  Wavelet Augmentation Transformer","summary":"  Unsupervised Domain Adaptation (UDA) can effectively address domain gap\nissues in real-world image Super-Resolution (SR) by accessing both the source\nand target data. Considering privacy policies or transmission restrictions of\nsource data in practical scenarios, we propose a SOurce-free Domain Adaptation\nframework for image SR (SODA-SR) to address this issue, i.e., adapt a\nsource-trained model to a target domain with only unlabeled target data.\nSODA-SR leverages the source-trained model to generate refined pseudo-labels\nfor teacher-student learning. To better utilize pseudo-labels, we propose a\nnovel wavelet-based augmentation method, named Wavelet Augmentation Transformer\n(WAT), which can be flexibly incorporated with existing networks, to implicitly\nproduce useful augmented data. WAT learns low-frequency information of varying\nlevels across diverse samples, which is aggregated efficiently via deformable\nattention. Furthermore, an uncertainty-aware self-training mechanism is\nproposed to improve the accuracy of pseudo-labels, with inaccurate predictions\nbeing rectified by uncertainty estimation. To acquire better SR results and\navoid overfitting pseudo-labels, several regularization losses are proposed to\nconstrain target LR and SR images in the frequency domain. Experiments show\nthat without accessing source data, SODA-SR outperforms state-of-the-art UDA\nmethods in both synthetic$\\rightarrow$real and real$\\rightarrow$real adaptation\nsettings, and is not constrained by specific network architectures.\n","authors":["Yuang Ai","Xiaoqiang Zhou","Huaibo Huang","Lei Zhang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2303.17783v5.pdf","comment":"11 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.13714v1","updated":"2024-03-20T16:20:54Z","published":"2024-03-20T16:20:54Z","title":"DBA-Fusion: Tightly Integrating Deep Dense Visual Bundle Adjustment with\n  Multiple Sensors for Large-Scale Localization and Mapping","summary":"  Visual simultaneous localization and mapping (VSLAM) has broad applications,\nwith state-of-the-art methods leveraging deep neural networks for better\nrobustness and applicability. However, there is a lack of research in fusing\nthese learning-based methods with multi-sensor information, which could be\nindispensable to push related applications to large-scale and complex\nscenarios. In this paper, we tightly integrate the trainable deep dense bundle\nadjustment (DBA) with multi-sensor information through a factor graph. In the\nframework, recurrent optical flow and DBA are performed among sequential\nimages. The Hessian information derived from DBA is fed into a generic factor\ngraph for multi-sensor fusion, which employs a sliding window and supports\nprobabilistic marginalization. A pipeline for visual-inertial integration is\nfirstly developed, which provides the minimum ability of metric-scale\nlocalization and mapping. Furthermore, other sensors (e.g., global navigation\nsatellite system) are integrated for driftless and geo-referencing\nfunctionality. Extensive tests are conducted on both public datasets and\nself-collected datasets. The results validate the superior localization\nperformance of our approach, which enables real-time dense mapping in\nlarge-scale environments. The code has been made open-source\n(https://github.com/GREAT-WHU/DBA-Fusion).\n","authors":["Yuxuan Zhou","Xingxing Li","Shengyu Li","Xuanbin Wang","Shaoquan Feng","Yuxuan Tan"],"pdf_url":"https://arxiv.org/pdf/2403.13714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09988v2","updated":"2024-03-20T16:19:49Z","published":"2023-12-15T18:01:47Z","title":"Towards Architecture-Agnostic Untrained Network Priors for Image\n  Reconstruction with Frequency Regularization","summary":"  Untrained networks inspired by deep image prior have shown promising\ncapabilities in recovering a high-quality image from noisy or partial\nmeasurements, without requiring training data. Their success has been widely\nattributed to the spectral bias acting as an implicit regularization induced by\nsuitable network architectures. However, applications of such network-based\npriors often entail superfluous architectural decisions, overfitting risks, and\nslow optimization, all of which hinder their practicality. In this work, we\npropose efficient, architecture-agnostic methods for a more direct frequency\ncontrol over the network priors: 1) constraining the bandwidth of the\nwhite-noise input, 2) controlling the bandwidth of the interpolation-based\nupsamplers, and 3) regularizing the Lipschitz constants of the layers. We show\nthat even with just one extra line of code, the overfitting issues in\nunderperforming architectures can be alleviated such that their performance\ngaps with the high-performing counterparts can be largely closed despite their\ndistinct configurations, mitigating the need for architecture tuning. This then\nmakes it possible to employ a more compact model to achieve similar or superior\nperformance to larger models with greater efficiency. Our regularized network\npriors compare favorably with current supervised and self-supervised methods on\nMRI reconstruction and image inpainting tasks, serving as a stronger zero-shot\nbaseline reconstructor. Our code will be made publicly available.\n","authors":["Yilin Liu","Yunkui Pang","Jiang Li","Yong Chen","Pew-Thian Yap"],"pdf_url":"https://arxiv.org/pdf/2312.09988v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18649v2","updated":"2024-03-20T16:18:09Z","published":"2023-11-30T15:57:34Z","title":"Simple Semantic-Aided Few-Shot Learning","summary":"  Learning from a limited amount of data, namely Few-Shot Learning, stands out\nas a challenging computer vision task. Several works exploit semantics and\ndesign complicated semantic fusion mechanisms to compensate for rare\nrepresentative features within restricted data. However, relying on naive\nsemantics such as class names introduces biases due to their brevity, while\nacquiring extensive semantics from external knowledge takes a huge time and\neffort. This limitation severely constrains the potential of semantics in\nfew-shot learning. In this paper, we design an automatic way called Semantic\nEvolution to generate high-quality semantics. The incorporation of high-quality\nsemantics alleviates the need for complex network structures and learning\nalgorithms used in previous works. Hence, we employ a simple two-layer network\ntermed Semantic Alignment Network to transform semantics and visual features\ninto robust class prototypes with rich discriminative features for few-shot\nclassification. The experimental results show our framework outperforms all\nprevious methods on six benchmarks, demonstrating a simple network with\nhigh-quality semantics can beat intricate multi-modal modules on few-shot\nclassification tasks. Code is available at\nhttps://github.com/zhangdoudou123/SemFew.\n","authors":["Hai Zhang","Junzhe Xu","Shanlin Jiang","Zhenan He"],"pdf_url":"https://arxiv.org/pdf/2311.18649v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2309.07915v3","updated":"2024-03-20T16:17:02Z","published":"2023-09-14T17:59:17Z","title":"MMICL: Empowering Vision-language Model with Multi-Modal In-Context\n  Learning","summary":"  Since the resurgence of deep learning, vision-language models (VLMs) enhanced\nby large language models (LLMs) have grown exponentially in popularity.\nHowever, while LLMs can utilize extensive background knowledge and task\ninformation with in-context learning, most VLMs still struggle with\nunderstanding complex multi-modal prompts with multiple images, making VLMs\nless effective in downstream vision-language tasks. In this paper, we address\nthe limitation above by 1) introducing vision-language Model with Multi-Modal\nIn-Context Learning(MMICL), a new approach to allow the VLM to deal with\nmulti-modal inputs efficiently; 2) proposing a novel context scheme to augment\nthe in-context learning ability of the VLM; 3) constructing the Multi-modal\nIn-Context Learning (MIC) dataset, designed to enhance the VLM's ability to\nunderstand complex multi-modal prompts. Our experiments confirm that MMICL\nachieves new state-of-the-art zero-shot performance on a wide range of general\nvision-language tasks, especially for complex benchmarks, including MME and\nMMBench. Our analysis demonstrates that MMICL effectively tackles the challenge\nof complex multi-modal prompt understanding and emerges the impressive ICL\nability. Furthermore, we observe that MMICL successfully alleviates language\nbias in VLMs, a common issue for VLMs that often leads to hallucination when\nfaced with extensive textual context. Our code, dataset, dataset tool, and\nmodel are available at https://github.com/PKUnlp-icler/MIC\n","authors":["Haozhe Zhao","Zefan Cai","Shuzheng Si","Xiaojian Ma","Kaikai An","Liang Chen","Zixuan Liu","Sheng Wang","Wenjuan Han","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2309.07915v3.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2312.02918v2","updated":"2024-03-20T16:12:57Z","published":"2023-12-05T17:47:11Z","title":"Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and\n  Fidelity for All-in-One Image Restoration","summary":"  Despite substantial progress, all-in-one image restoration (IR) grapples with\npersistent challenges in handling intricate real-world degradations. This paper\nintroduces MPerceiver: a novel multimodal prompt learning approach that\nharnesses Stable Diffusion (SD) priors to enhance adaptiveness,\ngeneralizability and fidelity for all-in-one image restoration. Specifically,\nwe develop a dual-branch module to master two types of SD prompts: textual for\nholistic representation and visual for multiscale detail representation. Both\nprompts are dynamically adjusted by degradation predictions from the CLIP image\nencoder, enabling adaptive responses to diverse unknown degradations. Moreover,\na plug-in detail refinement module improves restoration fidelity via direct\nencoder-to-decoder information transformation. To assess our method, MPerceiver\nis trained on 9 tasks for all-in-one IR and outperforms state-of-the-art\ntask-specific methods across most tasks. Post multitask pre-training,\nMPerceiver attains a generalized representation in low-level vision, exhibiting\nremarkable zero-shot and few-shot capabilities in unseen tasks. Extensive\nexperiments on 16 IR tasks underscore the superiority of MPerceiver in terms of\nadaptiveness, generalizability and fidelity.\n","authors":["Yuang Ai","Huaibo Huang","Xiaoqiang Zhou","Jiexiang Wang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2312.02918v2.pdf","comment":"13 pages, 8 figures, 9 tables"},{"id":"http://arxiv.org/abs/2312.04539v2","updated":"2024-03-20T16:11:22Z","published":"2023-12-07T18:55:52Z","title":"Auto-Vocabulary Semantic Segmentation","summary":"  Open-ended image understanding tasks gained significant attention from the\nresearch community, particularly with the emergence of Vision-Language Models.\nOpen-Vocabulary Segmentation (OVS) methods are capable of performing semantic\nsegmentation without relying on a fixed vocabulary, and in some cases, they\noperate without the need for training or fine-tuning. However, OVS methods\ntypically require users to specify the vocabulary based on the task or dataset\nat hand. In this paper, we introduce \\textit{Auto-Vocabulary Semantic\nSegmentation (AVS)}, advancing open-ended image understanding by eliminating\nthe necessity to predefine object categories for segmentation. Our approach,\n\\ours, presents a framework that autonomously identifies relevant class names\nusing enhanced BLIP embeddings, which are utilized for segmentation afterwards.\nGiven that open-ended object category predictions cannot be directly compared\nwith a fixed ground truth, we develop a Large Language Model-based\nAuto-Vocabulary Evaluator (LAVE) to efficiently evaluate the automatically\ngenerated class names and their corresponding segments. Our method sets new\nbenchmarks on datasets such as PASCAL VOC and Context, ADE20K, and Cityscapes\nfor AVS and showcases competitive performance to OVS methods that require\nspecified class names.\n","authors":["Osman Ülger","Maksymilian Kulicki","Yuki Asano","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2312.04539v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03320v3","updated":"2024-03-20T16:10:27Z","published":"2023-09-06T19:01:58Z","title":"CoNeS: Conditional neural fields with shift modulation for\n  multi-sequence MRI translation","summary":"  Multi-sequence magnetic resonance imaging (MRI) has found wide applications\nin both modern clinical studies and deep learning research. However, in\nclinical practice, it frequently occurs that one or more of the MRI sequences\nare missing due to different image acquisition protocols or contrast agent\ncontraindications of patients, limiting the utilization of deep learning models\ntrained on multi-sequence data. One promising approach is to leverage\ngenerative models to synthesize the missing sequences, which can serve as a\nsurrogate acquisition. State-of-the-art methods tackling this problem are based\non convolutional neural networks (CNN) which usually suffer from spectral\nbiases, resulting in poor reconstruction of high-frequency fine details. In\nthis paper, we propose Conditional Neural fields with Shift modulation (CoNeS),\na model that takes voxel coordinates as input and learns a representation of\nthe target images for multi-sequence MRI translation. The proposed model uses a\nmulti-layer perceptron (MLP) instead of a CNN as the decoder for pixel-to-pixel\nmapping. Hence, each target image is represented as a neural field that is\nconditioned on the source image via shift modulation with a learned latent\ncode. Experiments on BraTS 2018 and an in-house clinical dataset of vestibular\nschwannoma patients showed that the proposed method outperformed\nstate-of-the-art methods for multi-sequence MRI translation both visually and\nquantitatively. Moreover, we conducted spectral analysis, showing that CoNeS\nwas able to overcome the spectral bias issue common in conventional CNN models.\nTo further evaluate the usage of synthesized images in clinical downstream\ntasks, we tested a segmentation network using the synthesized images at\ninference.\n","authors":["Yunjie Chen","Marius Staring","Olaf M. Neve","Stephan R. Romeijn","Erik F. Hensen","Berit M. Verbist","Jelmer M. Wolterink","Qian Tao"],"pdf_url":"https://arxiv.org/pdf/2309.03320v3.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:004"},{"id":"http://arxiv.org/abs/2403.13703v1","updated":"2024-03-20T16:07:04Z","published":"2024-03-20T16:07:04Z","title":"Fostc3net:A Lightweight YOLOv5 Based On the Network Structure\n  Optimization","summary":"  Transmission line detection technology is crucial for automatic monitoring\nand ensuring the safety of electrical facilities. The YOLOv5 series is\ncurrently one of the most advanced and widely used methods for object\ndetection. However, it faces inherent challenges, such as high computational\nload on devices and insufficient detection accuracy. To address these concerns,\nthis paper presents an enhanced lightweight YOLOv5 technique customized for\nmobile devices, specifically intended for identifying objects associated with\ntransmission lines. The C3Ghost module is integrated into the convolutional\nnetwork of YOLOv5 to reduce floating point operations per second (FLOPs) in the\nfeature channel fusion process and improve feature expression performance. In\naddition, a FasterNet module is introduced to replace the c3 module in the\nYOLOv5 Backbone. The FasterNet module uses Partial Convolutions to process only\na portion of the input channels, improving feature extraction efficiency and\nreducing computational overhead. To address the imbalance between simple and\nchallenging samples in the dataset and the diversity of aspect ratios of\nbounding boxes, the wIoU v3 LOSS is adopted as the loss function. To validate\nthe performance of the proposed approach, Experiments are conducted on a custom\ndataset of transmission line poles. The results show that the proposed model\nachieves a 1% increase in detection accuracy, a 13% reduction in FLOPs, and a\n26% decrease in model parameters compared to the existing YOLOv5.In the\nablation experiment, it was also discovered that while the Fastnet module and\nthe CSghost module improved the precision of the original YOLOv5 baseline\nmodel, they caused a decrease in the mAP@.5-.95 metric. However, the\nimprovement of the wIoUv3 loss function significantly mitigated the decline of\nthe mAP@.5-.95 metric.\n","authors":["Danqing Ma","Shaojie Li","Bo Dang","Hengyi Zang","Xinqi Dong"],"pdf_url":"https://arxiv.org/pdf/2403.13703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13698v1","updated":"2024-03-20T16:03:01Z","published":"2024-03-20T16:03:01Z","title":"Insight Into the Collocation of Multi-Source Satellite Imagery for\n  Multi-Scale Vessel Detection","summary":"  Ship detection from satellite imagery using Deep Learning (DL) is an\nindispensable solution for maritime surveillance. However, applying DL models\ntrained on one dataset to others having differences in spatial resolution and\nradiometric features requires many adjustments. To overcome this issue, this\npaper focused on the DL models trained on datasets that consist of different\noptical images and a combination of radar and optical data. When dealing with a\nlimited number of training images, the performance of DL models via this\napproach was satisfactory. They could improve 5-20% of average precision,\ndepending on the optical images tested. Likewise, DL models trained on the\ncombined optical and radar dataset could be applied to both optical and radar\nimages. Our experiments showed that the models trained on an optical dataset\ncould be used for radar images, while those trained on a radar dataset offered\nvery poor scores when applied to optical images.\n","authors":["Tran-Vu La","Minh-Tan Pham","Marco Chini"],"pdf_url":"https://arxiv.org/pdf/2403.13698v1.pdf","comment":"5 pages, accepted to IGARSS 2024"},{"id":"http://arxiv.org/abs/2403.13690v1","updated":"2024-03-20T15:53:07Z","published":"2024-03-20T15:53:07Z","title":"MotorEase: Automated Detection of Motor Impairment Accessibility Issues\n  in Mobile App UIs","summary":"  Recent research has begun to examine the potential of automatically finding\nand fixing accessibility issues that manifest in software. However, while\nrecent work makes important progress, it has generally been skewed toward\nidentifying issues that affect users with certain disabilities, such as those\nwith visual or hearing impairments. However, there are other groups of users\nwith different types of disabilities that also need software tooling support to\nimprove their experience. As such, this paper aims to automatically identify\naccessibility issues that affect users with motor-impairments.\n  To move toward this goal, this paper introduces a novel approach, called\nMotorEase, capable of identifying accessibility issues in mobile app UIs that\nimpact motor-impaired users. Motor-impaired users often have limited ability to\ninteract with touch-based devices, and instead may make use of a switch or\nother assistive mechanism -- hence UIs must be designed to support both limited\ntouch gestures and the use of assistive devices. MotorEase adapts computer\nvision and text processing techniques to enable a semantic understanding of app\nUI screens, enabling the detection of violations related to four popular,\npreviously unexplored UI design guidelines that support motor-impaired users,\nincluding: (i) visual touch target size, (ii) expanding sections, (iii)\npersisting elements, and (iv) adjacent icon visual distance. We evaluate\nMotorEase on a newly derived benchmark, called MotorCheck, that contains 555\nmanually annotated examples of violations to the above accessibility\nguidelines, across 1599 screens collected from 70 applications via a mobile app\ntesting tool. Our experiments illustrate that MotorEase is able to identify\nviolations with an average accuracy of ~90%, and a false positive rate of less\nthan 9%, outperforming baseline techniques.\n","authors":["Arun Krishnavajjala","SM Hasan Mansur","Justin Jose","Kevin Moran"],"pdf_url":"https://arxiv.org/pdf/2403.13690v1.pdf","comment":"Accepted to ICSE 2024 Research Track, 13 pages"},{"id":"http://arxiv.org/abs/2303.16296v4","updated":"2024-03-20T15:52:49Z","published":"2023-03-28T20:35:38Z","title":"Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels","summary":"  The soft Dice loss (SDL) has taken a pivotal role in numerous automated\nsegmentation pipelines in the medical imaging community. Over the last years,\nsome reasons behind its superior functioning have been uncovered and further\noptimizations have been explored. However, there is currently no implementation\nthat supports its direct utilization in scenarios involving soft labels. Hence,\na synergy between the use of SDL and research leveraging the use of soft\nlabels, also in the context of model calibration, is still missing. In this\nwork, we introduce Dice semimetric losses (DMLs), which (i) are by design\nidentical to SDL in a standard setting with hard labels, but (ii) can be\nemployed in settings with soft labels. Our experiments on the public QUBIQ,\nLiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels\n(e.g. averaging, label smoothing, and knowledge distillation) over hard labels\n(e.g. majority voting and random selection). As a result, we obtain superior\nDice scores and model calibration, which supports the wider adoption of DMLs in\npractice. The code is available at https://github.com/zifuwanggg/JDTLosses\n","authors":["Zifu Wang","Teodora Popordanoska","Jeroen Bertels","Robin Lemmens","Matthew B. Blaschko"],"pdf_url":"https://arxiv.org/pdf/2303.16296v4.pdf","comment":"MICCAI 2023"},{"id":"http://arxiv.org/abs/2403.13684v1","updated":"2024-03-20T15:41:39Z","published":"2024-03-20T15:41:39Z","title":"SPTNet: An Efficient Alternative Framework for Generalized Category\n  Discovery with Spatial Prompt Tuning","summary":"  Generalized Category Discovery (GCD) aims to classify unlabelled images from\nboth `seen' and `unseen' classes by transferring knowledge from a set of\nlabelled `seen' class images. A key theme in existing GCD approaches is\nadapting large-scale pre-trained models for the GCD task. An alternate\nperspective, however, is to adapt the data representation itself for better\nalignment with the pre-trained model. As such, in this paper, we introduce a\ntwo-stage adaptation approach termed SPTNet, which iteratively optimizes model\nparameters (i.e., model-finetuning) and data parameters (i.e., prompt\nlearning). Furthermore, we propose a novel spatial prompt tuning method (SPT)\nwhich considers the spatial property of image data, enabling the method to\nbetter focus on object parts, which can transfer between seen and unseen\nclasses. We thoroughly evaluate our SPTNet on standard benchmarks and\ndemonstrate that our method outperforms existing GCD methods. Notably, we find\nour method achieves an average accuracy of 61.4% on the SSB, surpassing prior\nstate-of-the-art methods by approximately 10%. The improvement is particularly\nremarkable as our method yields extra parameters amounting to only 0.117% of\nthose in the backbone architecture. Project page:\nhttps://visual-ai.github.io/sptnet.\n","authors":["Hongjun Wang","Sagar Vaze","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2403.13684v1.pdf","comment":"Accepted as a conference paper at ICLR 2024; Project page:\n  https://visual-ai.github.io/sptnet"},{"id":"http://arxiv.org/abs/2403.13683v1","updated":"2024-03-20T15:41:32Z","published":"2024-03-20T15:41:32Z","title":"DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses","summary":"  Determining the relative pose of an object between two images is pivotal to\nthe success of generalizable object pose estimation. Existing approaches\ntypically approximate the continuous pose representation with a large number of\ndiscrete pose hypotheses, which incurs a computationally expensive process of\nscoring each hypothesis at test time. By contrast, we present a Deep Voxel\nMatching Network (DVMNet) that eliminates the need for pose hypotheses and\ncomputes the relative object pose in a single pass. To this end, we map the two\ninput RGB images, reference and query, to their respective voxelized 3D\nrepresentations. We then pass the resulting voxels through a pose estimation\nmodule, where the voxels are aligned and the pose is computed in an end-to-end\nfashion by solving a least-squares problem. To enhance robustness, we introduce\na weighted closest voxel algorithm capable of mitigating the impact of noisy\nvoxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaverse\ndatasets, demonstrating that our method delivers more accurate relative pose\nestimates for novel objects at a lower computational cost compared to\nstate-of-the-art methods. Our code is released at:\nhttps://github.com/sailor-z/DVMNet/.\n","authors":["Chen Zhao","Tong Zhang","Zheng Dang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2403.13683v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.13680v1","updated":"2024-03-20T15:38:53Z","published":"2024-03-20T15:38:53Z","title":"Step-Calibrated Diffusion for Biomedical Optical Image Restoration","summary":"  High-quality, high-resolution medical imaging is essential for clinical care.\nRaman-based biomedical optical imaging uses non-ionizing infrared radiation to\nevaluate human tissues in real time and is used for early cancer detection,\nbrain tumor diagnosis, and intraoperative tissue analysis. Unfortunately,\noptical imaging is vulnerable to image degradation due to laser scattering and\nabsorption, which can result in diagnostic errors and misguided treatment.\nRestoration of optical images is a challenging computer vision task because the\nsources of image degradation are multi-factorial, stochastic, and\ntissue-dependent, preventing a straightforward method to obtain paired\nlow-quality/high-quality data. Here, we present Restorative Step-Calibrated\nDiffusion (RSCD), an unpaired image restoration method that views the image\nrestoration problem as completing the finishing steps of a diffusion-based\nimage generation task. RSCD uses a step calibrator model to dynamically\ndetermine the severity of image degradation and the number of steps required to\ncomplete the reverse diffusion process for image restoration. RSCD outperforms\nother widely used unpaired image restoration methods on both image quality and\nperceptual evaluation metrics for restoring optical images. Medical imaging\nexperts consistently prefer images restored using RSCD in blinded comparison\nexperiments and report minimal to no hallucinations. Finally, we show that RSCD\nimproves performance on downstream clinical imaging tasks, including automated\nbrain tumor diagnosis and deep tissue imaging. Our code is available at\nhttps://github.com/MLNeurosurg/restorative_step-calibrated_diffusion.\n","authors":["Yiwei Lyu","Sung Jik Cha","Cheng Jiang","Asadur Chowdury","Xinhai Hou","Edward Harake","Akhil Kondepudi","Christian Freudiger","Honglak Lee","Todd C. Hollon"],"pdf_url":"https://arxiv.org/pdf/2403.13680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13678v1","updated":"2024-03-20T15:37:19Z","published":"2024-03-20T15:37:19Z","title":"AUD-TGN: Advancing Action Unit Detection with Temporal Convolution and\n  GPT-2 in Wild Audiovisual Contexts","summary":"  Leveraging the synergy of both audio data and visual data is essential for\nunderstanding human emotions and behaviors, especially in in-the-wild setting.\nTraditional methods for integrating such multimodal information often stumble,\nleading to less-than-ideal outcomes in the task of facial action unit\ndetection. To overcome these shortcomings, we propose a novel approach\nutilizing audio-visual multimodal data. This method enhances audio feature\nextraction by leveraging Mel Frequency Cepstral Coefficients (MFCC) and Log-Mel\nspectrogram features alongside a pre-trained VGGish network. Moreover, this\npaper adaptively captures fusion features across modalities by modeling the\ntemporal relationships, and ultilizes a pre-trained GPT-2 model for\nsophisticated context-aware fusion of multimodal information. Our method\nnotably improves the accuracy of AU detection by understanding the temporal and\ncontextual nuances of the data, showcasing significant advancements in the\ncomprehension of intricate scenarios. These findings underscore the potential\nof integrating temporal dynamics and contextual interpretation, paving the way\nfor future research endeavors.\n","authors":["Jun Yu","Zerui Zhang","Zhihong Wei","Gongpeng Zhao","Zhongpeng Cai","Yongqi Wang","Guochen Xie","Jichao Zhu","Wangyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.13678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13677v1","updated":"2024-03-20T15:35:36Z","published":"2024-03-20T15:35:36Z","title":"Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into\n  Vision Transformers","summary":"  Humans see low and high spatial frequency components at the same time, and\ncombine the information from both to form a visual scene. Drawing on this\nneuroscientific inspiration, we propose an altered Vision Transformer\narchitecture where patches from scaled down versions of the input image are\nadded to the input of the first Transformer Encoder layer. We name this model\nRetina Vision Transformer (RetinaViT) due to its inspiration from the human\nvisual system. Our experiments show that when trained on the ImageNet-1K\ndataset with a moderate configuration, RetinaViT achieves a 3.3% performance\nimprovement over the original ViT. We hypothesize that this improvement can be\nattributed to the inclusion of low spatial frequency components in the input,\nwhich improves the ability to capture structural features, and to select and\nforward important features to deeper layers. RetinaViT thereby opens doors to\nfurther investigations into vertical pathways and attention patterns.\n","authors":["Yuyang Shu","Michael E. Bain"],"pdf_url":"https://arxiv.org/pdf/2403.13677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06258v2","updated":"2024-03-20T15:31:28Z","published":"2024-03-10T16:56:44Z","title":"Poly Kernel Inception Network for Remote Sensing Detection","summary":"  Object detection in remote sensing images (RSIs) often suffers from several\nincreasing challenges, including the large variation in object scales and the\ndiverse-ranging context. Prior methods tried to address these challenges by\nexpanding the spatial receptive field of the backbone, either through\nlarge-kernel convolution or dilated convolution. However, the former typically\nintroduces considerable background noise, while the latter risks generating\noverly sparse feature representations. In this paper, we introduce the Poly\nKernel Inception Network (PKINet) to handle the above challenges. PKINet\nemploys multi-scale convolution kernels without dilation to extract object\nfeatures of varying scales and capture local context. In addition, a Context\nAnchor Attention (CAA) module is introduced in parallel to capture long-range\ncontextual information. These two components work jointly to advance the\nperformance of PKINet on four challenging remote sensing detection benchmarks,\nnamely DOTA-v1.0, DOTA-v1.5, HRSC2016, and DIOR-R.\n","authors":["Xinhao Cai","Qiuxia Lai","Yuwei Wang","Wenguan Wang","Zeren Sun","Yazhou Yao"],"pdf_url":"https://arxiv.org/pdf/2403.06258v2.pdf","comment":"accepted by IEEE Conference on Computer Vision and Pattern\n  Recognition, 2024"},{"id":"http://arxiv.org/abs/2308.03001v2","updated":"2024-03-20T15:29:56Z","published":"2023-08-06T03:28:08Z","title":"Weakly supervised segmentation of intracranial aneurysms using a novel\n  3D focal modulation UNet","summary":"  Accurate identification and quantification of unruptured intracranial\naneurysms (UIAs) is crucial for the risk assessment and treatment of this\ncerebrovascular disorder. Current 2D manual assessment on 3D magnetic resonance\nangiography (MRA) is suboptimal and time-consuming. In addition, one major\nissue in medical image segmentation is the need for large well-annotated data,\nwhich can be expensive to obtain. Techniques that mitigate this requirement,\nsuch as weakly supervised learning with coarse labels are highly desirable. In\nthe paper, we propose FocalSegNet, a novel 3D focal modulation UNet, to detect\nan aneurysm and offer an initial, coarse segmentation of it from time-of-flight\nMRA image patches, which is further refined with a dense conditional random\nfield (CRF) post-processing layer to produce a final segmentation map. We\ntrained and evaluated our model on a public dataset, and in terms of UIA\ndetection, our model showed a low false-positive rate of 0.21 and a high\nsensitivity of 0.80. For voxel-wise aneurysm segmentation, we achieved a Dice\nscore of 0.68 and a 95% Hausdorff distance of ~0.95 mm, demonstrating its\nstrong performance. We evaluated our algorithms against the state-of-the-art 3D\nResidual-UNet and Swin-UNETR, and illustrated the superior performance of our\nproposed FocalSegNet, highlighting the advantages of employing focal modulation\nfor this task.\n","authors":["Amirhossein Rasoulian","Arash Harirpoush","Soorena Salari","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2308.03001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13667v1","updated":"2024-03-20T15:24:57Z","published":"2024-03-20T15:24:57Z","title":"DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance","summary":"  Choreographers determine what the dances look like, while cameramen determine\nthe final presentation of dances. Recently, various methods and datasets have\nshowcased the feasibility of dance synthesis. However, camera movement\nsynthesis with music and dance remains an unsolved challenging problem due to\nthe scarcity of paired data. Thus, we present DCM, a new multi-modal 3D\ndataset, which for the first time combines camera movement with dance motion\nand music audio. This dataset encompasses 108 dance sequences (3.2 hours) of\npaired dance-camera-music data from the anime community, covering 4 music\ngenres. With this dataset, we uncover that dance camera movement is\nmultifaceted and human-centric, and possesses multiple influencing factors,\nmaking dance camera synthesis a more challenging task compared to camera or\ndance synthesis alone. To overcome these difficulties, we propose\nDanceCamera3D, a transformer-based diffusion model that incorporates a novel\nbody attention loss and a condition separation strategy. For evaluation, we\ndevise new metrics measuring camera movement quality, diversity, and dancer\nfidelity. Utilizing these metrics, we conduct extensive experiments on our DCM\ndataset, providing both quantitative and qualitative evidence showcasing the\neffectiveness of our DanceCamera3D model. Code and video demos are available at\nhttps://github.com/Carmenw1203/DanceCamera3D-Official.\n","authors":["Zixuan Wang","Jia Jia","Shikun Sun","Haozhe Wu","Rong Han","Zhenyu Li","Di Tang","Jiaqing Zhou","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13667v1.pdf","comment":"Accept to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11868v2","updated":"2024-03-20T15:22:12Z","published":"2024-03-18T15:22:09Z","title":"View-Consistent 3D Editing with Gaussian Splatting","summary":"  The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,\noffering efficient, high-fidelity rendering and enabling precise local\nmanipulations. Currently, diffusion-based 2D editing models are harnessed to\nmodify multi-view rendered images, which then guide the editing of 3DGS models.\nHowever, this approach faces a critical issue of multi-view inconsistency,\nwhere the guidance images exhibit significant discrepancies across views,\nleading to mode collapse and visual artifacts of 3DGS. To this end, we\nintroduce View-consistent Editing (VcEdit), a novel framework that seamlessly\nincorporates 3DGS into image editing processes, ensuring multi-view consistency\nin edited guidance images and effectively mitigating mode collapse issues.\nVcEdit employs two innovative consistency modules: the Cross-attention\nConsistency Module and the Editing Consistency Module, both designed to reduce\ninconsistencies in edited images. By incorporating these consistency modules\ninto an iterative pattern, VcEdit proficiently resolves the issue of multi-view\ninconsistency, facilitating high-quality 3DGS editing across a diverse range of\nscenes.\n","authors":["Yuxuan Wang","Xuanyu Yi","Zike Wu","Na Zhao","Long Chen","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13663v1","updated":"2024-03-20T15:14:22Z","published":"2024-03-20T15:14:22Z","title":"T-Pixel2Mesh: Combining Global and Local Transformer for 3D Mesh\n  Generation from a Single Image","summary":"  Pixel2Mesh (P2M) is a classical approach for reconstructing 3D shapes from a\nsingle color image through coarse-to-fine mesh deformation. Although P2M is\ncapable of generating plausible global shapes, its Graph Convolution Network\n(GCN) often produces overly smooth results, causing the loss of fine-grained\ngeometry details. Moreover, P2M generates non-credible features for occluded\nregions and struggles with the domain gap from synthetic data to real-world\nimages, which is a common challenge for single-view 3D reconstruction methods.\nTo address these challenges, we propose a novel Transformer-boosted\narchitecture, named T-Pixel2Mesh, inspired by the coarse-to-fine approach of\nP2M. Specifically, we use a global Transformer to control the holistic shape\nand a local Transformer to progressively refine the local geometry details with\ngraph-based point upsampling. To enhance real-world reconstruction, we present\nthe simple yet effective Linear Scale Search (LSS), which serves as prompt\ntuning during the input preprocessing. Our experiments on ShapeNet demonstrate\nstate-of-the-art performance, while results on real-world data show the\ngeneralization capability.\n","authors":["Shijie Zhang","Boyan Jiang","Keke He","Junwei Zhu","Ying Tai","Chengjie Wang","Yinda Zhang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2403.13663v1.pdf","comment":"Received by ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.13660v1","updated":"2024-03-20T15:08:57Z","published":"2024-03-20T15:08:57Z","title":"ProMamba: Prompt-Mamba for polyp segmentation","summary":"  Detecting polyps through colonoscopy is an important task in medical image\nsegmentation, which provides significant assistance and reference value for\nclinical surgery. However, accurate segmentation of polyps is a challenging\ntask due to two main reasons. Firstly, polyps exhibit various shapes and\ncolors. Secondly, the boundaries between polyps and their normal surroundings\nare often unclear. Additionally, significant differences between different\ndatasets lead to limited generalization capabilities of existing methods. To\naddress these issues, we propose a segmentation model based on Prompt-Mamba,\nwhich incorporates the latest Vision-Mamba and prompt technologies. Compared to\nprevious models trained on the same dataset, our model not only maintains high\nsegmentation accuracy on the validation part of the same dataset but also\ndemonstrates superior accuracy on unseen datasets, exhibiting excellent\ngeneralization capabilities. Notably, we are the first to apply the\nVision-Mamba architecture to polyp segmentation and the first to utilize prompt\ntechnology in a polyp segmentation model. Our model efficiently accomplishes\nsegmentation tasks, surpassing previous state-of-the-art methods by an average\nof 5% across six datasets. Furthermore, we have developed multiple versions of\nour model with scaled parameter counts, achieving better performance than\nprevious models even with fewer parameters. Our code and trained weights will\nbe released soon.\n","authors":["Jianhao Xie","Ruofan Liao","Ziang Zhang","Sida Yi","Yuesheng Zhu","Guibo Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13660v1.pdf","comment":"10 pages, 2 figures,3 tabels"},{"id":"http://arxiv.org/abs/2403.13659v1","updated":"2024-03-20T15:08:43Z","published":"2024-03-20T15:08:43Z","title":"Recursive Cross-Modal Attention for Multimodal Fusion in Dimensional\n  Emotion Recognition","summary":"  Multi-modal emotion recognition has recently gained a lot of attention since\nit can leverage diverse and complementary relationships over multiple\nmodalities, such as audio, visual, and text. Most state-of-the-art methods for\nmultimodal fusion rely on recurrent networks or conventional attention\nmechanisms that do not effectively leverage the complementary nature of the\nmodalities. In this paper, we focus on dimensional emotion recognition based on\nthe fusion of facial, vocal, and text modalities extracted from videos.\nSpecifically, we propose a recursive cross-modal attention (RCMA) to\neffectively capture the complementary relationships across the modalities in a\nrecursive fashion. The proposed model is able to effectively capture the\ninter-modal relationships by computing the cross-attention weights across the\nindividual modalities and the joint representation of the other two modalities.\nTo further improve the inter-modal relationships, the obtained attended\nfeatures of the individual modalities are again fed as input to the cross-modal\nattention to refine the feature representations of the individual modalities.\nIn addition to that, we have used Temporal convolution networks (TCNs) to\ncapture the temporal modeling (intra-modal relationships) of the individual\nmodalities. By deploying the TCNs as well cross-modal attention in a recursive\nfashion, we are able to effectively capture both intra- and inter-modal\nrelationships across the audio, visual, and text modalities. Experimental\nresults on validation-set videos from the AffWild2 dataset indicate that our\nproposed fusion model is able to achieve significant improvement over the\nbaseline for the sixth challenge of Affective Behavior Analysis in-the-Wild\n2024 (ABAW6) competition.\n","authors":["R. Gnana Praveen","Jahangir Alam"],"pdf_url":"https://arxiv.org/pdf/2403.13659v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2209.09068;\n  text overlap with arXiv:2203.14779 by other authors"},{"id":"http://arxiv.org/abs/2403.13658v1","updated":"2024-03-20T15:06:49Z","published":"2024-03-20T15:06:49Z","title":"Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics\n  Instability Detection","summary":"  Recent advancements in non-invasive detection of cardiac hemodynamic\ninstability (CHDI) primarily focus on applying machine learning techniques to a\nsingle data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite\ntheir potential, these approaches often fall short especially when the size of\nlabeled patient data is limited, a common challenge in the medical domain.\nFurthermore, only a few studies have explored multimodal methods to study CHDI,\nwhich mostly rely on costly modalities such as cardiac MRI and echocardiogram.\nIn response to these limitations, we propose a novel multimodal variational\nautoencoder ($\\text{CardioVAE}_\\text{X,G}$) to integrate low-cost chest X-ray\n(CXR) and electrocardiogram (ECG) modalities with pre-training on a large\nunlabeled dataset. Specifically, $\\text{CardioVAE}_\\text{X,G}$ introduces a\nnovel tri-stream pre-training strategy to learn both shared and\nmodality-specific features, thus enabling fine-tuning with both unimodal and\nmultimodal datasets. We pre-train $\\text{CardioVAE}_\\text{X,G}$ on a large,\nunlabeled dataset of $50,982$ subjects from a subset of MIMIC database and then\nfine-tune the pre-trained model on a labeled dataset of $795$ subjects from the\nASPIRE registry. Comprehensive evaluations against existing methods show that\n$\\text{CardioVAE}_\\text{X,G}$ offers promising performance (AUROC $=0.79$ and\nAccuracy $=0.77$), representing a significant step forward in non-invasive\nprediction of CHDI. Our model also excels in producing fine interpretations of\npredictions directly associated with clinical features, thereby supporting\nclinical decision-making.\n","authors":["Mohammod N. I. Suvon","Prasun C. Tripathi","Wenrui Fan","Shuo Zhou","Xianyuan Liu","Samer Alabed","Venet Osmani","Andrew J. Swift","Chen Chen","Haiping Lu"],"pdf_url":"https://arxiv.org/pdf/2403.13658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13653v1","updated":"2024-03-20T14:58:40Z","published":"2024-03-20T14:58:40Z","title":"Learning User Embeddings from Human Gaze for Personalised Saliency\n  Prediction","summary":"  Reusable embeddings of user behaviour have shown significant performance\nimprovements for the personalised saliency prediction task. However, prior\nworks require explicit user characteristics and preferences as input, which are\noften difficult to obtain. We present a novel method to extract user embeddings\nfrom pairs of natural images and corresponding saliency maps generated from a\nsmall amount of user-specific eye tracking data. At the core of our method is a\nSiamese convolutional neural encoder that learns the user embeddings by\ncontrasting the image and personal saliency map pairs of different users.\nEvaluations on two public saliency datasets show that the generated embeddings\nhave high discriminative power, are effective at refining universal saliency\nmaps to the individual users, and generalise well across users and images.\nFinally, based on our model's ability to encode individual user\ncharacteristics, our work points towards other applications that can benefit\nfrom reusable embeddings of gaze behaviour.\n","authors":["Florian Strohm","Mihai Bâce","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2403.13653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13652v1","updated":"2024-03-20T14:58:09Z","published":"2024-03-20T14:58:09Z","title":"ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer","summary":"  Deep learning models achieve high accuracy in segmentation tasks among\nothers, yet domain shift often degrades the models' performance, which can be\ncritical in real-world scenarios where no target images are available. This\npaper proposes a zero-shot domain adaptation method based on diffusion models,\ncalled ZoDi, which is two-fold by the design: zero-shot image transfer and\nmodel adaptation. First, we utilize an off-the-shelf diffusion model to\nsynthesize target-like images by transferring the domain of source images to\nthe target domain. In this we specifically try to maintain the layout and\ncontent by utilising layout-to-image diffusion models with stochastic\ninversion. Secondly, we train the model using both source images and\nsynthesized images with the original segmentation maps while maximizing the\nfeature similarity of images from the two domains to learn domain-robust\nrepresentations. Through experiments we show benefits of ZoDi in the task of\nimage segmentation over state-of-the-art methods. It is also more applicable\nthan existing CLIP-based methods because it assumes no specific backbone or\nmodels, and it enables to estimate the model's performance without target\nimages by inspecting generated images. Our implementation will be publicly\navailable.\n","authors":["Hiroki Azuma","Yusuke Matsui","Atsuto Maki"],"pdf_url":"https://arxiv.org/pdf/2403.13652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13647v1","updated":"2024-03-20T14:54:33Z","published":"2024-03-20T14:54:33Z","title":"Meta-Point Learning and Refining for Category-Agnostic Pose Estimation","summary":"  Category-agnostic pose estimation (CAPE) aims to predict keypoints for\narbitrary classes given a few support images annotated with keypoints. Existing\nmethods only rely on the features extracted at support keypoints to predict or\nrefine the keypoints on query image, but a few support feature vectors are\nlocal and inadequate for CAPE. Considering that human can quickly perceive\npotential keypoints of arbitrary objects, we propose a novel framework for CAPE\nbased on such potential keypoints (named as meta-points). Specifically, we\nmaintain learnable embeddings to capture inherent information of various\nkeypoints, which interact with image feature maps to produce meta-points\nwithout any support. The produced meta-points could serve as meaningful\npotential keypoints for CAPE. Due to the inevitable gap between inherency and\nannotation, we finally utilize the identities and details offered by support\nkeypoints to assign and refine meta-points to desired keypoints in query image.\nIn addition, we propose a progressive deformable point decoder and a slacked\nregression loss for better prediction and supervision. Our novel framework not\nonly reveals the inherency of keypoints but also outperforms existing methods\nof CAPE. Comprehensive experiments and in-depth studies on large-scale MP-100\ndataset demonstrate the effectiveness of our framework.\n","authors":["Junjie Chen","Jiebin Yan","Yuming Fang","Li Niu"],"pdf_url":"https://arxiv.org/pdf/2403.13647v1.pdf","comment":"Published in CVPR 2024"},{"id":"http://arxiv.org/abs/2403.02075v2","updated":"2024-03-20T14:52:27Z","published":"2024-03-04T14:21:51Z","title":"DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with\n  Non-linear Prediction","summary":"  In Multiple Object Tracking, objects often exhibit non-linear motion of\nacceleration and deceleration, with irregular direction changes.\nTacking-by-detection (TBD) trackers with Kalman Filter motion prediction work\nwell in pedestrian-dominant scenarios but fall short in complex situations when\nmultiple objects perform non-linear and diverse motion simultaneously. To\ntackle the complex non-linear motion, we propose a real-time diffusion-based\nMOT approach named DiffMOT. Specifically, for the motion predictor component,\nwe propose a novel Decoupled Diffusion-based Motion Predictor (D$^2$MP). It\nmodels the entire distribution of various motion presented by the data as a\nwhole. It also predicts an individual object's motion conditioning on an\nindividual's historical motion information. Furthermore, it optimizes the\ndiffusion process with much fewer sampling steps. As a MOT tracker, the DiffMOT\nis real-time at 22.7FPS, and also outperforms the state-of-the-art on\nDanceTrack and SportsMOT datasets with $62.3\\%$ and $76.2\\%$ in HOTA metrics,\nrespectively. To the best of our knowledge, DiffMOT is the first to introduce a\ndiffusion probabilistic model into the MOT to tackle non-linear motion\nprediction.\n","authors":["Weiyi Lv","Yuhang Huang","Ning Zhang","Ruei-Sung Lin","Mei Han","Dan Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.02075v2.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.13642v1","updated":"2024-03-20T14:49:52Z","published":"2024-03-20T14:49:52Z","title":"H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation","summary":"  In the field of medical image segmentation, variant models based on\nConvolutional Neural Networks (CNNs) and Visual Transformers (ViTs) as the base\nmodules have been very widely developed and applied. However, CNNs are often\nlimited in their ability to deal with long sequences of information, while the\nlow sensitivity of ViTs to local feature information and the problem of\nsecondary computational complexity limit their development. Recently, the\nemergence of state-space models (SSMs), especially 2D-selective-scan (SS2D),\nhas had an impact on the longtime dominance of traditional CNNs and ViTs as the\nfoundational modules of visual neural networks. In this paper, we extend the\nadaptability of SS2D by proposing a High-order Vision Mamba UNet (H-vmunet) for\nmedical image segmentation. Among them, the proposed High-order\n2D-selective-scan (H-SS2D) progressively reduces the introduction of redundant\ninformation during SS2D operations through higher-order interactions. In\naddition, the proposed Local-SS2D module improves the learning ability of local\nfeatures of SS2D at each order of interaction. We conducted comparison and\nablation experiments on three publicly available medical image datasets\n(ISIC2017, Spleen, and CVC-ClinicDB), and the results all demonstrate the\nstrong competitiveness of H-vmunet in medical image segmentation tasks. The\ncode is available from https://github.com/wurenkai/H-vmunet .\n","authors":["Renkai Wu","Yinghao Liu","Pengchen Liang","Qing Chang"],"pdf_url":"https://arxiv.org/pdf/2403.13642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17128v3","updated":"2024-03-20T14:49:16Z","published":"2024-02-27T01:48:19Z","title":"OSCaR: Object State Captioning and State Change Representation","summary":"  The capability of intelligent models to extrapolate and comprehend changes in\nobject states is a crucial yet demanding aspect of AI research, particularly\nthrough the lens of human interaction in real-world settings. This task\ninvolves describing complex visual environments, identifying active objects,\nand interpreting their changes as conveyed through language. Traditional\nmethods, which isolate object captioning and state change detection, offer a\nlimited view of dynamic environments. Moreover, relying on a small set of\nsymbolic words to represent changes has restricted the expressiveness of the\nlanguage. To address these challenges, in this paper, we introduce the Object\nState Captioning and State Change Representation (OSCaR) dataset and benchmark.\nOSCaR consists of 14,084 annotated video segments with nearly 1,000 unique\nobjects from various egocentric video collections. It sets a new testbed for\nevaluating multimodal large language models (MLLMs). Our experiments\ndemonstrate that while MLLMs show some skill, they lack a full understanding of\nobject state changes. The benchmark includes a fine-tuned model that, despite\ninitial capabilities, requires significant improvements in accuracy and\ngeneralization ability for effective understanding of these changes. Our code\nand dataset are available at https://github.com/nguyennm1024/OSCaR.\n","authors":["Nguyen Nguyen","Jing Bi","Ali Vosoughi","Yapeng Tian","Pooyan Fazli","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2402.17128v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2208.08270v3","updated":"2024-03-20T14:13:44Z","published":"2022-08-17T13:02:17Z","title":"On the Privacy Effect of Data Enhancement via the Lens of Memorization","summary":"  Machine learning poses severe privacy concerns as it has been shown that the\nlearned models can reveal sensitive information about their training data. Many\nworks have investigated the effect of widely adopted data augmentation and\nadversarial training techniques, termed data enhancement in the paper, on the\nprivacy leakage of machine learning models. Such privacy effects are often\nmeasured by membership inference attacks (MIAs), which aim to identify whether\na particular example belongs to the training set or not. We propose to\ninvestigate privacy from a new perspective called memorization. Through the\nlens of memorization, we find that previously deployed MIAs produce misleading\nresults as they are less likely to identify samples with higher privacy risks\nas members compared to samples with low privacy risks. To solve this problem,\nwe deploy a recent attack that can capture individual samples' memorization\ndegrees for evaluation. Through extensive experiments, we unveil several\nfindings about the connections between three essential properties of machine\nlearning models, including privacy, generalization gap, and adversarial\nrobustness. We demonstrate that the generalization gap and privacy leakage are\nless correlated than those of the previous results. Moreover, there is not\nnecessarily a trade-off between adversarial robustness and privacy as stronger\nadversarial robustness does not make the model more susceptible to privacy\nattacks.\n","authors":["Xiao Li","Qiongxiu Li","Zhanhao Hu","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2208.08270v3.pdf","comment":"Accepted by IEEE TIFS, 17 pages"},{"id":"http://arxiv.org/abs/2403.12425v2","updated":"2024-03-20T13:56:56Z","published":"2024-03-19T04:25:54Z","title":"Multimodal Fusion Method with Spatiotemporal Sequences and Relationship\n  Learning for Valence-Arousal Estimation","summary":"  This paper presents our approach for the VA (Valence-Arousal) estimation task\nin the ABAW6 competition. We devised a comprehensive model by preprocessing\nvideo frames and audio segments to extract visual and audio features. Through\nthe utilization of Temporal Convolutional Network (TCN) modules, we effectively\ncaptured the temporal and spatial correlations between these features.\nSubsequently, we employed a Transformer encoder structure to learn long-range\ndependencies, thereby enhancing the model's performance and generalization\nability. Our method leverages a multimodal data fusion approach, integrating\npre-trained audio and video backbones for feature extraction, followed by\nTCN-based spatiotemporal encoding and Transformer-based temporal information\ncapture. Experimental results demonstrate the effectiveness of our approach,\nachieving competitive performance in VA estimation on the AffWild2 dataset.\n","authors":["Jun Yu","Gongpeng Zhao","Yongqi Wang","Zhihong Wei","Yang Zheng","Zerui Zhang","Zhongpeng Cai","Guochen Xie","Jichao Zhu","Wangyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.12425v2.pdf","comment":"8 pages,3 figures"},{"id":"http://arxiv.org/abs/2403.13600v1","updated":"2024-03-20T13:48:50Z","published":"2024-03-20T13:48:50Z","title":"VL-Mamba: Exploring State Space Models for Multimodal Learning","summary":"  Multimodal large language models (MLLMs) have attracted widespread interest\nand have rich applications. However, the inherent attention mechanism in its\nTransformer structure requires quadratic complexity and results in expensive\ncomputational overhead. Therefore, in this work, we propose VL-Mamba, a\nmultimodal large language model based on state space models, which have been\nshown to have great potential for long-sequence modeling with fast inference\nand linear scaling in sequence length. Specifically, we first replace the\ntransformer-based backbone language model such as LLama or Vicuna with the\npre-trained Mamba language model. Then, we empirically explore how to\neffectively apply the 2D vision selective scan mechanism for multimodal\nlearning and the combinations of different vision encoders and variants of\npretrained Mamba language models. The extensive experiments on diverse\nmultimodal benchmarks with competitive performance show the effectiveness of\nour proposed VL-Mamba and demonstrate the great potential of applying state\nspace models for multimodal learning tasks.\n","authors":["Yanyuan Qiao","Zheng Yu","Longteng Guo","Sihan Chen","Zijia Zhao","Mingzhen Sun","Qi Wu","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2403.13600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13589v1","updated":"2024-03-20T13:37:29Z","published":"2024-03-20T13:37:29Z","title":"ReGround: Improving Textual and Spatial Grounding at No Cost","summary":"  When an image generation process is guided by both a text prompt and spatial\ncues, such as a set of bounding boxes, do these elements work in harmony, or\ndoes one dominate the other? Our analysis of a pretrained image diffusion model\nthat integrates gated self-attention into the U-Net reveals that spatial\ngrounding often outweighs textual grounding due to the sequential flow from\ngated self-attention to cross-attention. We demonstrate that such bias can be\nsignificantly mitigated without sacrificing accuracy in either grounding by\nsimply rewiring the network architecture, changing from sequential to parallel\nfor gated self-attention and cross-attention. This surprisingly simple yet\neffective solution does not require any fine-tuning of the network but\nsignificantly reduces the trade-off between the two groundings. Our experiments\ndemonstrate significant improvements from the original GLIGEN to the rewired\nversion in the trade-off between textual grounding and spatial grounding.\n","authors":["Yuseung Lee","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2403.13589v1.pdf","comment":"Project page: https://re-ground.github.io/"},{"id":"http://arxiv.org/abs/2403.13575v1","updated":"2024-03-20T13:20:05Z","published":"2024-03-20T13:20:05Z","title":"Leveraging feature communication in federated learning for remote\n  sensing image classification","summary":"  In the realm of Federated Learning (FL) applied to remote sensing image\nclassification, this study introduces and assesses several innovative\ncommunication strategies. Our exploration includes feature-centric\ncommunication, pseudo-weight amalgamation, and a combined method utilizing both\nweights and features. Experiments conducted on two public scene classification\ndatasets unveil the effectiveness of these strategies, showcasing accelerated\nconvergence, heightened privacy, and reduced network information exchange. This\nresearch provides valuable insights into the implications of feature-centric\ncommunication in FL, offering potential applications tailored for remote\nsensing scenarios.\n","authors":["Anh-Kiet Duong","Hoàng-Ân Lê","Minh-Tan Pham"],"pdf_url":"https://arxiv.org/pdf/2403.13575v1.pdf","comment":"5 pages, to appear in IGARSS 2024"},{"id":"http://arxiv.org/abs/2306.11335v4","updated":"2024-03-20T13:18:18Z","published":"2023-06-20T07:06:04Z","title":"Surfer: Progressive Reasoning with World Models for Robotic Manipulation","summary":"  Considering how to make the model accurately understand and follow natural\nlanguage instructions and perform actions consistent with world knowledge is a\nkey challenge in robot manipulation. This mainly includes human fuzzy\ninstruction reasoning and the following of physical knowledge. Therefore, the\nembodied intelligence agent must have the ability to model world knowledge from\ntraining data. However, most existing vision and language robot manipulation\nmethods mainly operate in less realistic simulator and language settings and\nlack explicit modeling of world knowledge. To bridge this gap, we introduce a\nnovel and simple robot manipulation framework, called Surfer. It is based on\nthe world model, treats robot manipulation as a state transfer of the visual\nscene, and decouples it into two parts: action and scene. Then, the\ngeneralization ability of the model on new instructions and new scenes is\nenhanced by explicit modeling of the action and scene prediction in multi-modal\ninformation. In addition to the framework, we also built a robot manipulation\nsimulator that supports full physics execution based on the MuJoCo physics\nengine. It can automatically generate demonstration training data and test\ndata, effectively reducing labor costs. To conduct a comprehensive and\nsystematic evaluation of the robot manipulation model in terms of language\nunderstanding and physical execution, we also created a robotic manipulation\nbenchmark with progressive reasoning tasks, called SeaWave. It contains 4\nlevels of progressive reasoning tasks and can provide a standardized testing\nplatform for embedded AI agents in multi-modal environments. On average, Surfer\nachieved a success rate of 54.74% on the defined four levels of manipulation\ntasks, exceeding the best baseline performance of 47.64%.\n","authors":["Pengzhen Ren","Kaidong Zhang","Hetao Zheng","Zixuan Li","Yuhang Wen","Fengda Zhu","Mas Ma","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2306.11335v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10261v2","updated":"2024-03-20T13:15:28Z","published":"2024-03-15T12:48:44Z","title":"Learning Spatiotemporal Inconsistency via Thumbnail Layout for Face\n  Deepfake Detection","summary":"  The deepfake threats to society and cybersecurity have provoked significant\npublic apprehension, driving intensified efforts within the realm of deepfake\nvideo detection. Current video-level methods are mostly based on {3D CNNs}\nresulting in high computational demands, although have achieved good\nperformance. This paper introduces an elegantly simple yet effective strategy\nnamed Thumbnail Layout (TALL), which transforms a video clip into a pre-defined\nlayout to realize the preservation of spatial and temporal dependencies. This\ntransformation process involves sequentially masking frames at the same\npositions within each frame. These frames are then resized into sub-frames and\nreorganized into the predetermined layout, forming thumbnails. TALL is\nmodel-agnostic and has remarkable simplicity, necessitating only minimal code\nmodifications. Furthermore, we introduce a graph reasoning block (GRB) and\nsemantic consistency (SC) loss to strengthen TALL, culminating in TALL++. GRB\nenhances interactions between different semantic regions to capture\nsemantic-level inconsistency clues. The semantic consistency loss imposes\nconsistency constraints on semantic features to improve model generalization\nability. Extensive experiments on intra-dataset, cross-dataset,\ndiffusion-generated image detection, and deepfake generation method recognition\nshow that TALL++ achieves results surpassing or comparable to the\nstate-of-the-art methods, demonstrating the effectiveness of our approaches for\nvarious deepfake detection problems. The code is available at\nhttps://github.com/rainy-xu/TALL4Deepfake.\n","authors":["Yuting Xu","Jian Liang","Lijun Sheng","Xiao-Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.10261v2.pdf","comment":"Accepted by IJCV"},{"id":"http://arxiv.org/abs/2302.09389v2","updated":"2024-03-20T13:11:19Z","published":"2023-02-18T17:45:11Z","title":"Vulnerability analysis of captcha using Deep learning","summary":"  Several websites improve their security and avoid dangerous Internet attacks\nby implementing CAPTCHAs (Completely Automated Public Turing test to tell\nComputers and Humans Apart), a type of verification to identify whether the\nend-user is human or a robot. The most prevalent type of CAPTCHA is text-based,\ndesigned to be easily recognized by humans while being unsolvable towards\nmachines or robots. However, as deep learning technology progresses,\ndevelopment of convolutional neural network (CNN) models that predict\ntext-based CAPTCHAs becomes easier. The purpose of this research is to\ninvestigate the flaws and vulnerabilities in the CAPTCHA generating systems in\norder to design more resilient CAPTCHAs. To achieve this, we created CapNet, a\nConvolutional Neural Network. The proposed platform can evaluate both numerical\nand alphanumerical CAPTCHAs\n","authors":["Jaskaran Singh Walia","Aryan Odugoudar"],"pdf_url":"https://arxiv.org/pdf/2302.09389v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13570v1","updated":"2024-03-20T13:09:54Z","published":"2024-03-20T13:09:54Z","title":"Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer","summary":"  In this paper, we propose a novel learning approach for feed-forward one-shot\n4D head avatar synthesis. Different from existing methods that often learn from\nreconstructing monocular videos guided by 3DMM, we employ pseudo multi-view\nvideos to learn a 4D head synthesizer in a data-driven manner, avoiding\nreliance on inaccurate 3DMM reconstruction that could be detrimental to the\nsynthesis performance. The key idea is to first learn a 3D head synthesizer\nusing synthetic multi-view images to convert monocular real videos into\nmulti-view ones, and then utilize the pseudo multi-view videos to learn a 4D\nhead synthesizer via cross-view self-reenactment. By leveraging a simple vision\ntransformer backbone with motion-aware cross-attentions, our method exhibits\nsuperior performance compared to previous methods in terms of reconstruction\nfidelity, geometry consistency, and motion control accuracy. We hope our method\noffers novel insights into integrating 3D priors with 2D supervisions for\nimproved 4D head avatar creation.\n","authors":["Yu Deng","Duomin Wang","Baoyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13570v1.pdf","comment":"Project page: https://yudeng.github.io/Portrait4D-v2/"},{"id":"http://arxiv.org/abs/2312.02696v2","updated":"2024-03-20T12:58:14Z","published":"2023-12-05T11:55:47Z","title":"Analyzing and Improving the Training Dynamics of Diffusion Models","summary":"  Diffusion models currently dominate the field of data-driven image synthesis\nwith their unparalleled scaling to large datasets. In this paper, we identify\nand rectify several causes for uneven and ineffective training in the popular\nADM diffusion model architecture, without altering its high-level structure.\nObserving uncontrolled magnitude changes and imbalances in both the network\nactivations and weights over the course of training, we redesign the network\nlayers to preserve activation, weight, and update magnitudes on expectation. We\nfind that systematic application of this philosophy eliminates the observed\ndrifts and imbalances, resulting in considerably better networks at equal\ncomputational complexity. Our modifications improve the previous record FID of\n2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic\nsampling.\n  As an independent contribution, we present a method for setting the\nexponential moving average (EMA) parameters post-hoc, i.e., after completing\nthe training run. This allows precise tuning of EMA length without the cost of\nperforming several training runs, and reveals its surprising interactions with\nnetwork architecture, training time, and guidance.\n","authors":["Tero Karras","Miika Aittala","Jaakko Lehtinen","Janne Hellsten","Timo Aila","Samuli Laine"],"pdf_url":"https://arxiv.org/pdf/2312.02696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13556v1","updated":"2024-03-20T12:51:30Z","published":"2024-03-20T12:51:30Z","title":"Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban\n  Environments","summary":"  In this work, we tackle the limitations of current LiDAR-based 3D object\ndetection systems, which are hindered by a restricted class vocabulary and the\nhigh costs associated with annotating new object classes. Our exploration of\nopen-vocabulary (OV) learning in urban environments aims to capture novel\ninstances using pre-trained vision-language models (VLMs) with multi-sensor\ndata. We design and benchmark a set of four potential solutions as baselines,\ncategorizing them into either top-down or bottom-up approaches based on their\ninput data strategies. While effective, these methods exhibit certain\nlimitations, such as missing novel objects in 3D box estimation or applying\nrigorous priors, leading to biases towards objects near the camera or of\nrectangular geometries. To overcome these limitations, we introduce a universal\n\\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the\nrecall of novel objects and propagating this detection capability to more\ndistant areas thereby progressively capturing more. In particular, we utilize a\ngreedy box seeker to search against 3D novel boxes of varying orientations and\ndepth in each generated frustum and ensure the reliability of newly identified\nboxes by cross alignment and density ranker. Additionally, the inherent bias\ntowards camera-proximal objects is alleviated by the proposed remote simulator,\nwhich randomly diversifies pseudo-labeled novel instances in the self-training\nprocess, combined with the fusion of base samples in the memory bank. Extensive\nexperiments demonstrate a 53% improvement in novel recall across diverse OV\nsettings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold\nincrease in Average Precision (AP) for novel object classes. The source code is\nmade available in the supplementary material.\n","authors":["Djamahl Etchegaray","Zi Huang","Tatsuya Harada","Yadan Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13551v1","updated":"2024-03-20T12:40:32Z","published":"2024-03-20T12:40:32Z","title":"Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute\n  Editing","summary":"  Despite recent advancements in text-to-image diffusion models facilitating\nvarious image editing techniques, complex text prompts often lead to an\noversight of some requests due to a bottleneck in processing text information.\nTo tackle this challenge, we present Ground-A-Score, a simple yet powerful\nmodel-agnostic image editing method by incorporating grounding during score\ndistillation. This approach ensures a precise reflection of intricate prompt\nrequirements in the editing outcomes, taking into account the prior knowledge\nof the object locations within the image. Moreover, the selective application\nwith a new penalty coefficient and contrastive loss helps to precisely target\nediting areas while preserving the integrity of the objects in the source\nimage. Both qualitative assessments and quantitative analyses confirm that\nGround-A-Score successfully adheres to the intricate details of extended and\nmultifaceted prompts, ensuring high-quality outcomes that respect the original\nimage attributes.\n","authors":["Hangeol Chang","Jinho Chang","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.13551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09008v2","updated":"2024-03-20T12:39:52Z","published":"2023-12-11T09:53:12Z","title":"Style Injection in Diffusion: A Training-free Approach for Adapting\n  Large-scale Diffusion Models for Style Transfer","summary":"  Despite the impressive generative capabilities of diffusion models, existing\ndiffusion model-based style transfer methods require inference-stage\noptimization (e.g. fine-tuning or textual inversion of style) which is\ntime-consuming, or fails to leverage the generative ability of large-scale\ndiffusion models. To address these issues, we introduce a novel artistic style\ntransfer method based on a pre-trained large-scale diffusion model without any\noptimization. Specifically, we manipulate the features of self-attention layers\nas the way the cross-attention mechanism works; in the generation process,\nsubstituting the key and value of content with those of style image. This\napproach provides several desirable characteristics for style transfer\nincluding 1) preservation of content by transferring similar styles into\nsimilar image patches and 2) transfer of style based on similarity of local\ntexture (e.g. edge) between content and style images. Furthermore, we introduce\nquery preservation and attention temperature scaling to mitigate the issue of\ndisruption of original content, and initial latent Adaptive Instance\nNormalization (AdaIN) to deal with the disharmonious color (failure to transfer\nthe colors of style). Our experimental results demonstrate that our proposed\nmethod surpasses state-of-the-art methods in both conventional and\ndiffusion-based style transfer baselines.\n","authors":["Jiwoo Chung","Sangeek Hyun","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2312.09008v2.pdf","comment":"Accepted to CVPR 2024. Project page:\n  https://jiwoogit.github.io/StyleID_site"},{"id":"http://arxiv.org/abs/2303.15263v4","updated":"2024-03-20T12:39:28Z","published":"2023-03-27T14:52:08Z","title":"Joint Person Identity, Gender and Age Estimation from Hand Images using\n  Deep Multi-Task Representation Learning","summary":"  In this paper, we propose a multi-task representation learning framework to\njointly estimate the identity, gender and age of individuals from their hand\nimages for the purpose of criminal investigations since the hand images are\noften the only available information in cases of serious crime such as sexual\nabuse. We investigate different up-to-date deep learning architectures and\ncompare their performance for joint estimation of identity, gender and age from\nhand images of perpetrators of serious crime. To simplify the age prediction,\nwe create age groups for the age estimation. We make extensive evaluations and\ncomparisons of both convolution-based and transformer-based deep learning\narchitectures on a publicly available 11k hands dataset. Our experimental\nanalysis shows that it is possible to efficiently estimate not only identity\nbut also other attributes such as gender and age of suspects jointly from hand\nimages for criminal investigations, which is crucial in assisting international\npolice forces in the court to identify and convict abusers.\n","authors":["Nathanael L. Baisa"],"pdf_url":"https://arxiv.org/pdf/2303.15263v4.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.04821"},{"id":"http://arxiv.org/abs/2403.13548v1","updated":"2024-03-20T12:36:41Z","published":"2024-03-20T12:36:41Z","title":"Diversity-aware Channel Pruning for StyleGAN Compression","summary":"  StyleGAN has shown remarkable performance in unconditional image generation.\nHowever, its high computational cost poses a significant challenge for\npractical applications. Although recent efforts have been made to compress\nStyleGAN while preserving its performance, existing compressed models still lag\nbehind the original model, particularly in terms of sample diversity. To\novercome this, we propose a novel channel pruning method that leverages varying\nsensitivities of channels to latent vectors, which is a key factor in sample\ndiversity. Specifically, by assessing channel importance based on their\nsensitivities to latent vector perturbations, our method enhances the diversity\nof samples in the compressed model. Since our method solely focuses on the\nchannel pruning stage, it has complementary benefits with prior training\nschemes without additional training cost. Extensive experiments demonstrate\nthat our method significantly enhances sample diversity across various\ndatasets. Moreover, in terms of FID scores, our method not only surpasses\nstate-of-the-art by a large margin but also achieves comparable scores with\nonly half training iterations.\n","authors":["Jiwoo Chung","Sangeek Hyun","Sang-Heon Shim","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2403.13548v1.pdf","comment":"Accepted to CVPR 2024. Project page:\n  https://jiwoogit.github.io/DCP-GAN_site"},{"id":"http://arxiv.org/abs/2403.13545v1","updated":"2024-03-20T12:31:13Z","published":"2024-03-20T12:31:13Z","title":"Next day fire prediction via semantic segmentation","summary":"  In this paper we present a deep learning pipeline for next day fire\nprediction. The next day fire prediction task consists in learning models that\nreceive as input the available information for an area up until a certain day,\nin order to predict the occurrence of fire for the next day. Starting from our\nprevious problem formulation as a binary classification task on instances\n(daily snapshots of each area) represented by tabular feature vectors, we\nreformulate the problem as a semantic segmentation task on images; there, each\npixel corresponds to a daily snapshot of an area, while its channels represent\nthe formerly tabular training features. We demonstrate that this problem\nformulation, built within a thorough pipeline achieves state of the art\nresults.\n","authors":["Konstantinos Alexis","Stella Girtsou","Alexis Apostolakis","Giorgos Giannopoulos","Charalampos Kontoes"],"pdf_url":"https://arxiv.org/pdf/2403.13545v1.pdf","comment":"Accepted in MACLEAN@ECML/PKDD 2023"},{"id":"http://arxiv.org/abs/2403.13537v1","updated":"2024-03-20T12:14:54Z","published":"2024-03-20T12:14:54Z","title":"What explains the success of cross-modal fine-tuning with ORCA?","summary":"  ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning,\ni.e., applying pre-trained transformer models to modalities beyond their\ntraining data. The technique consists primarily of training an embedder and\nfine-tuning the embedder and model. Despite its high performance on a variety\nof downstream tasks, we do not understand precisely how each of these\ncomponents contribute to ORCA's success. Therefore, we run a series of\nablations and find that embedder training does not help 2D tasks at all,\ncontrary to what the original paper posits. In 1D tasks, some amount of\nembedder training is necessary but more is not better. In 4 out of 6 datasets\nwe experiment with, it is model fine-tuning that makes the biggest difference.\nThrough our ablations and baselines, we contribute a better understanding of\nthe individual components of ORCA.\n","authors":["Paloma García-de-Herreros","Vagrant Gautam","Philipp Slusallek","Dietrich Klakow","Marius Mosbach"],"pdf_url":"https://arxiv.org/pdf/2403.13537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13535v1","updated":"2024-03-20T12:13:04Z","published":"2024-03-20T12:13:04Z","title":"IDAdapter: Learning Mixed Features for Tuning-Free Personalization of\n  Text-to-Image Models","summary":"  Leveraging Stable Diffusion for the generation of personalized portraits has\nemerged as a powerful and noteworthy tool, enabling users to create\nhigh-fidelity, custom character avatars based on their specific prompts.\nHowever, existing personalization methods face challenges, including test-time\nfine-tuning, the requirement of multiple input images, low preservation of\nidentity, and limited diversity in generated outcomes. To overcome these\nchallenges, we introduce IDAdapter, a tuning-free approach that enhances the\ndiversity and identity preservation in personalized image generation from a\nsingle face image. IDAdapter integrates a personalized concept into the\ngeneration process through a combination of textual and visual injections and a\nface identity loss. During the training phase, we incorporate mixed features\nfrom multiple reference images of a specific identity to enrich\nidentity-related content details, guiding the model to generate images with\nmore diverse styles, expressions, and angles compared to previous works.\nExtensive evaluations demonstrate the effectiveness of our method, achieving\nboth diversity and identity fidelity in generated images.\n","authors":["Siying Cui","Jiankang Deng","Jia Guo","Xiang An","Yongle Zhao","Xinyu Wei","Ziyong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13535v1.pdf","comment":"14 pages, 15 figures"},{"id":"http://arxiv.org/abs/2312.09031v2","updated":"2024-03-20T12:00:59Z","published":"2023-12-14T15:31:33Z","title":"iComMa: Inverting 3D Gaussian Splatting for Camera Pose Estimation via\n  Comparing and Matching","summary":"  We present a method named iComMa to address the 6D camera pose estimation\nproblem in computer vision. Conventional pose estimation methods typically rely\non the target's CAD model or necessitate specific network training tailored to\nparticular object classes. Some existing methods have achieved promising\nresults in mesh-free object and scene pose estimation by inverting the Neural\nRadiance Fields (NeRF). However, they still struggle with adverse\ninitializations such as large rotations and translations. To address this\nissue, we propose an efficient method for accurate camera pose estimation by\ninverting 3D Gaussian Splatting (3DGS). Specifically, a gradient-based\ndifferentiable framework optimizes camera pose by minimizing the residual\nbetween the query image and the rendered image, requiring no training. An\nend-to-end matching module is designed to enhance the model's robustness\nagainst adverse initializations, while minimizing pixel-level comparing loss\naids in precise pose estimation. Experimental results on synthetic and complex\nreal-world data demonstrate the effectiveness of the proposed approach in\nchallenging conditions and the accuracy of camera pose estimation.\n","authors":["Yuan Sun","Xuan Wang","Yunfan Zhang","Jie Zhang","Caigui Jiang","Yu Guo","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2312.09031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11959v2","updated":"2024-03-20T11:58:23Z","published":"2024-03-18T16:56:47Z","title":"IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video\n  Action Counting","summary":"  Video Action Counting (VAC) is crucial in analyzing sports, fitness, and\neveryday activities by quantifying repetitive actions in videos. However,\ntraditional VAC methods have overlooked the complexity of action repetitions,\nsuch as interruptions and the variability in cycle duration. Our research\naddresses the shortfall by introducing a novel approach to VAC, called\nIrregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular\nrepetition patterns in videos, which we define through two primary aspects:\nInter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle\nConsistency ensures homogeneity in the spatial-temporal representations of\ncycle segments, signifying action uniformity within cycles. Cycle-interval\ninconsistency highlights the importance of distinguishing between cycle\nsegments and intervals based on their inherent content differences. To\nencapsulate these principles, we propose a new methodology that includes\nconsistency and inconsistency modules, supported by a unique pull-push loss\n(P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence\namong cycle segment features and a push loss to clearly distinguish features of\ncycle segments from interval segments. Empirical evaluations conducted on the\nRepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in\nVAC task performance. Furthermore, the model demonstrates exceptional\nadaptability and generalization across various video contents, outperforming\nexisting models on two additional datasets, UCFRep and Countix, without the\nneed for dataset-specific optimization. These results confirm the efficacy of\nour approach in addressing irregular repetitions in videos and pave the way for\nfurther advancements in video analysis and understanding.\n","authors":["Hang Wang","Zhi-Qi Cheng","Youtian Du","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11959v2.pdf","comment":"Source code: https://github.com/hwang-cs-ime/IVAC-P2L"},{"id":"http://arxiv.org/abs/2403.13524v1","updated":"2024-03-20T11:51:04Z","published":"2024-03-20T11:51:04Z","title":"Compress3D: a Compressed Latent Space for 3D Generation from a Single\n  Image","summary":"  3D generation has witnessed significant advancements, yet efficiently\nproducing high-quality 3D assets from a single image remains challenging. In\nthis paper, we present a triplane autoencoder, which encodes 3D models into a\ncompact triplane latent space to effectively compress both the 3D geometry and\ntexture information. Within the autoencoder framework, we introduce a 3D-aware\ncross-attention mechanism, which utilizes low-resolution latent representations\nto query features from a high-resolution 3D feature volume, thereby enhancing\nthe representation capacity of the latent space. Subsequently, we train a\ndiffusion model on this refined latent space. In contrast to solely relying on\nimage embedding for 3D generation, our proposed method advocates for the\nsimultaneous utilization of both image embedding and shape embedding as\nconditions. Specifically, the shape embedding is estimated via a diffusion\nprior model conditioned on the image embedding. Through comprehensive\nexperiments, we demonstrate that our method outperforms state-of-the-art\nalgorithms, achieving superior performance while requiring less training data\nand time. Our approach enables the generation of high-quality 3D assets in\nmerely 7 seconds on a single A100 GPU.\n","authors":["Bowen Zhang","Tianyu Yang","Yu Li","Lei Zhang","Xi Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.13524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13522v1","updated":"2024-03-20T11:48:10Z","published":"2024-03-20T11:48:10Z","title":"REAL: Representation Enhanced Analytic Learning for Exemplar-free\n  Class-incremental Learning","summary":"  Exemplar-free class-incremental learning (EFCIL) aims to mitigate\ncatastrophic forgetting in class-incremental learning without available\nhistorical data. Compared with its counterpart (replay-based CIL) that stores\nhistorical samples, the EFCIL suffers more from forgetting issues under the\nexemplar-free constraint. In this paper, inspired by the recently developed\nanalytic learning (AL) based CIL, we propose a representation enhanced analytic\nlearning (REAL) for EFCIL. The REAL constructs a dual-stream base pretraining\n(DS-BPT) and a representation enhancing distillation (RED) process to enhance\nthe representation of the extractor. The DS-BPT pretrains model in streams of\nboth supervised learning and self-supervised contrastive learning (SSCL) for\nbase knowledge extraction. The RED process distills the supervised knowledge to\nthe SSCL pretrained backbone and facilitates a subsequent AL-basd CIL that\nconverts the CIL to a recursive least-square problem. Our method addresses the\nissue of insufficient discriminability in representations of unseen data caused\nby a frozen backbone in the existing AL-based CIL. Empirical results on various\ndatasets including CIFAR-100, ImageNet-100 and ImageNet-1k, demonstrate that\nour REAL outperforms the state-of-the-arts in EFCIL, and achieves comparable or\neven more superior performance compared with the replay-based methods.\n","authors":["Run He","Huiping Zhuang","Di Fang","Yizhu Chen","Kai Tong","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.13522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13518v1","updated":"2024-03-20T11:38:30Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate motion sequences from given textual\ndescriptions, where a model should explore the interactions between natural\nlanguage instructions and human body movements. While most existing works are\nconfined to coarse-grained motion descriptions (e.g., \"A man squats.\"),\nfine-grained ones specifying movements of relevant body parts are barely\nexplored. Models trained with coarse texts may not be able to learn mappings\nfrom fine-grained motion-related words to motion primitives, resulting in the\nfailure in generating motions from unseen descriptions. In this paper, we build\na large-scale language-motion dataset with fine-grained textual descriptions,\nFineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, which makes full use of\nfine-grained textual information. Our experiments show that FineMotionDiffuse\ntrained on FineHumanML3D acquires good results in quantitative evaluation. We\nalso find this model can better generate spatially/chronologically composite\nmotions by learning the implicit mappings from simple descriptions to the\ncorresponding basic motions.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13513v1","updated":"2024-03-20T11:27:20Z","published":"2024-03-20T11:27:20Z","title":"What if...?: Counterfactual Inception to Mitigate Hallucination Effects\n  in Large Multimodal Models","summary":"  This paper presents a way of enhancing the reliability of Large Multimodal\nModels (LMMs) in addressing hallucination effects, where models generate\nincorrect or unrelated responses. Without additional instruction tuning\nparadigm, we introduce Counterfactual Inception, a novel method that implants\ncounterfactual thoughts into LMMs using carefully chosen, misaligned\ncounterfactual keywords. This method is grounded in the concept of\ncounterfactual thinking, a cognitive process where humans consider alternative\nrealities and outcomes. By applying this human-like reasoning mechanism to\nLMMs, we aim to reduce hallucination effects and improve the models'\ntrustworthiness. We also propose Dual-modality Verification Process (DVP), a\nrigorous framework for selecting optimal counterfactual keywords to trigger\ncounterfactual thinking into LMMs, concurrently considering visual and\nlinguistic context. Our extensive experiments across various LMMs, including\nboth open-source and proprietary models, corroborate that our method\nsignificantly mitigates hallucination phenomena across different datasets.\n","authors":["Junho Kim","Yeon Ju Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2403.13513v1.pdf","comment":"under review, code available:\n  https://github.com/IVY-LVLM/Counterfactual-Inception"},{"id":"http://arxiv.org/abs/2403.13512v1","updated":"2024-03-20T11:21:22Z","published":"2024-03-20T11:21:22Z","title":"Scale Decoupled Distillation","summary":"  Logit knowledge distillation attracts increasing attention due to its\npracticality in recent studies. However, it often suffers inferior performance\ncompared to the feature knowledge distillation. In this paper, we argue that\nexisting logit-based methods may be sub-optimal since they only leverage the\nglobal logit output that couples multiple semantic knowledge. This may transfer\nambiguous knowledge to the student and mislead its learning. To this end, we\npropose a simple but effective method, i.e., Scale Decoupled Distillation\n(SDD), for logit knowledge distillation. SDD decouples the global logit output\ninto multiple local logit outputs and establishes distillation pipelines for\nthem. This helps the student to mine and inherit fine-grained and unambiguous\nlogit knowledge. Moreover, the decoupled knowledge can be further divided into\nconsistent and complementary logit knowledge that transfers the semantic\ninformation and sample ambiguity, respectively. By increasing the weight of\ncomplementary parts, SDD can guide the student to focus more on ambiguous\nsamples, improving its discrimination ability. Extensive experiments on several\nbenchmark datasets demonstrate the effectiveness of SDD for wide\nteacher-student pairs, especially in the fine-grained classification task. Code\nis available at: https://github.com/shicaiwei123/SDD-CVPR2024\n","authors":["Shicai Wei Chunbo Luo Yang Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13512v1.pdf","comment":"Accepted to CVPR2024 10 pages 6figure"},{"id":"http://arxiv.org/abs/2403.13509v1","updated":"2024-03-20T11:12:57Z","published":"2024-03-20T11:12:57Z","title":"High-confidence pseudo-labels for domain adaptation in COVID-19\n  detection","summary":"  This paper outlines our submission for the 4th COV19D competition as part of\nthe `Domain adaptation, Explainability, Fairness in AI for Medical Image\nAnalysis' (DEF-AI-MIA) workshop at the Computer Vision and Pattern Recognition\nConference (CVPR). The competition consists of two challenges. The first is to\ntrain a classifier to detect the presence of COVID-19 from over one thousand CT\nscans from the COV19-CT-DB database. The second challenge is to perform domain\nadaptation by taking the dataset from Challenge 1 and adding a small number of\nscans (some annotated and other not) for a different distribution. We\npreprocessed the CT scans to segment the lungs, and output volumes with the\nlungs individually and together. We then trained 3D ResNet and Swin Transformer\nmodels on these inputs. We annotated the unlabeled CT scans using an ensemble\nof these models and chose the high-confidence predictions as pseudo-labels for\nfine-tuning. This resulted in a best cross-validation mean F1 score of 93.39\\%\nfor Challenge 1 and a mean F1 score of 92.15 for Challenge 2.\n","authors":["Robert Turnbull","Simon Mutch"],"pdf_url":"https://arxiv.org/pdf/2403.13509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13507v1","updated":"2024-03-20T11:05:07Z","published":"2024-03-20T11:05:07Z","title":"FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based\n  LLMs","summary":"  Despite the remarkable performance of video-based large language models\n(LLMs), their adversarial threat remains unexplored. To fill this gap, we\npropose the first adversarial attack tailored for video-based LLMs by crafting\nflow-based multi-modal adversarial perturbations on a small fraction of frames\nwithin a video, dubbed FMM-Attack. Extensive experiments show that our attack\ncan effectively induce video-based LLMs to generate incorrect answers when\nvideos are added with imperceptible adversarial perturbations. Intriguingly,\nour FMM-Attack can also induce garbling in the model output, prompting\nvideo-based LLMs to hallucinate. Overall, our observations inspire a further\nunderstanding of multi-modal robustness and safety-related feature alignment\nacross different modalities, which is of great importance for various large\nmulti-modal models. Our code is available at\nhttps://github.com/THU-Kingmin/FMM-Attack.\n","authors":["Jinmin Li","Kuofeng Gao","Yang Bai","Jingyun Zhang","Shu-tao Xia","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13501v1","updated":"2024-03-20T10:58:58Z","published":"2024-03-20T10:58:58Z","title":"VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis","summary":"  Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.\n","authors":["Yumeng Li","William Beluch","Margret Keuper","Dan Zhang","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2403.13501v1.pdf","comment":"Project page: https://yumengli007.github.io/VSTAR"},{"id":"http://arxiv.org/abs/2403.13499v1","updated":"2024-03-20T10:57:17Z","published":"2024-03-20T10:57:17Z","title":"Improved Baselines for Data-efficient Perceptual Augmentation of LLMs","summary":"  The abilities of large language models (LLMs) have recently progressed to\nunprecedented levels, paving the way to novel applications in a wide variety of\nareas. In computer vision, LLMs can be used to prime vision-language tasks such\nimage captioning and visual question answering when coupled with pre-trained\nvision backbones. While different approaches have been explored to interface\nLLMs with ``perceptual backbones'' that process, e.g., visual or audio data,\nthey are often explored for different tasks, different datasets, and using\ndifferent perceptual backbones and language models, hindering direct comparison\nof the interfacing mechanisms. To remedy this lack of comparability between\nmethods, we present an extensive experimental evaluation of different\ninterfacing mechanisms, across multiple tasks (including image, video, and\naudio captioning as well as visual question answering), datasets and backbones,\npaying special attention to low-data settings. We find improved performance\nusing existing mechanisms over state-of-the-art results, and identify a new\ninterfacing mechanism that yields (near) optimal results across different\ntasks, while obtaining a 4x reduction in training time.\n","authors":["Théophane Vallaeys","Mustafa Shukor","Matthieu Cord","Jakob Verbeek"],"pdf_url":"https://arxiv.org/pdf/2403.13499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13480v1","updated":"2024-03-20T10:34:40Z","published":"2024-03-20T10:34:40Z","title":"A Unified Optimal Transport Framework for Cross-Modal Retrieval with\n  Noisy Labels","summary":"  Cross-modal retrieval (CMR) aims to establish interaction between different\nmodalities, among which supervised CMR is emerging due to its flexibility in\nlearning semantic category discrimination. Despite the remarkable performance\nof previous supervised CMR methods, much of their success can be attributed to\nthe well-annotated data. However, even for unimodal data, precise annotation is\nexpensive and time-consuming, and it becomes more challenging with the\nmultimodal scenario. In practice, massive multimodal data are collected from\nthe Internet with coarse annotation, which inevitably introduces noisy labels.\nTraining with such misleading labels would bring two key challenges --\nenforcing the multimodal samples to \\emph{align incorrect semantics} and\n\\emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To\ntackle these challenges, this work proposes UOT-RCL, a Unified framework based\non Optimal Transport (OT) for Robust Cross-modal Retrieval. First, we propose a\nsemantic alignment based on partial OT to progressively correct the noisy\nlabels, where a novel cross-modal consistent cost function is designed to blend\ndifferent modalities and provide precise transport cost. Second, to narrow the\ndiscrepancy in multi-modal data, an OT-based relation alignment is proposed to\ninfer the semantic-level cross-modal matching. Both of these two components\nleverage the inherent correlation among multi-modal data to facilitate\neffective cost function. The experiments on three widely-used cross-modal\nretrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art\napproaches and significantly improves the robustness against noisy labels.\n","authors":["Haochen Han","Minnan Luo","Huan Liu","Fang Nan"],"pdf_url":"https://arxiv.org/pdf/2403.13480v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.13479v1","updated":"2024-03-20T10:33:10Z","published":"2024-03-20T10:33:10Z","title":"Deepfake Detection without Deepfakes: Generalization via Synthetic\n  Frequency Patterns Injection","summary":"  Deepfake detectors are typically trained on large sets of pristine and\ngenerated images, resulting in limited generalization capacity; they excel at\nidentifying deepfakes created through methods encountered during training but\nstruggle with those generated by unknown techniques. This paper introduces a\nlearning approach aimed at significantly enhancing the generalization\ncapabilities of deepfake detectors. Our method takes inspiration from the\nunique \"fingerprints\" that image generation processes consistently introduce\ninto the frequency domain. These fingerprints manifest as structured and\ndistinctly recognizable frequency patterns. We propose to train detectors using\nonly pristine images injecting in part of them crafted frequency patterns,\nsimulating the effects of various deepfake generation techniques without being\nspecific to any. These synthetic patterns are based on generic shapes, grids,\nor auras. We evaluated our approach using diverse architectures across 25\ndifferent generation methods. The models trained with our approach were able to\nperform state-of-the-art deepfake detection, demonstrating also superior\ngeneralization capabilities in comparison with previous methods. Indeed, they\nare untied to any specific generation technique and can effectively identify\ndeepfakes regardless of how they were made.\n","authors":["Davide Alessandro Coccomini","Roberto Caldelli","Claudio Gennaro","Giuseppe Fiameni","Giuseppe Amato","Fabrizio Falchi"],"pdf_url":"https://arxiv.org/pdf/2403.13479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11427v2","updated":"2024-03-20T10:23:49Z","published":"2023-02-22T15:07:29Z","title":"Enhanced Face Authentication With Separate Loss Functions","summary":"  The overall objective of the main project is to propose and develop a system\nof facial authentication in unlocking phones or applications in phones using\nfacial recognition. The system will include four separate architectures: face\ndetection, face recognition, face spoofing, and classification of closed eyes.\nIn which, we consider the problem of face recognition to be the most important,\ndetermining the true identity of the person standing in front of the screen\nwith absolute accuracy is what facial recognition systems need to achieve.\nAlong with the development of the face recognition problem, the problem of the\nanti-fake face is also gradually becoming popular and equally important. Our\ngoal is to propose and develop two loss functions: LMCot and Double Loss. Then\napply them to the face authentication process.\n","authors":["Anh-Kiet Duong","Hoang-Lan Nguyen","Toan-Thinh Truong"],"pdf_url":"https://arxiv.org/pdf/2302.11427v2.pdf","comment":"in Vietnamese language"},{"id":"http://arxiv.org/abs/2403.13470v1","updated":"2024-03-20T10:19:05Z","published":"2024-03-20T10:19:05Z","title":"Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion","summary":"  Computer vision techniques play a central role in the perception stack of\nautonomous vehicles. Such methods are employed to perceive the vehicle\nsurroundings given sensor data. 3D LiDAR sensors are commonly used to collect\nsparse 3D point clouds from the scene. However, compared to human perception,\nsuch systems struggle to deduce the unseen parts of the scene given those\nsparse point clouds. In this matter, the scene completion task aims at\npredicting the gaps in the LiDAR measurements to achieve a more complete scene\nrepresentation. Given the promising results of recent diffusion models as\ngenerative models for images, we propose extending them to achieve scene\ncompletion from a single 3D LiDAR scan. Previous works used diffusion models\nover range images extracted from LiDAR data, directly applying image-based\ndiffusion methods. Distinctly, we propose to directly operate on the points,\nreformulating the noising and denoising diffusion process such that it can\nefficiently work at scene scale. Together with our approach, we propose a\nregularization loss to stabilize the noise predicted during the denoising\nprocess. Our experimental evaluation shows that our method can complete the\nscene given a single LiDAR scan as input, producing a scene with more details\ncompared to state-of-the-art scene completion methods. We believe that our\nproposed diffusion process formulation can support further research in\ndiffusion models applied to scene-scale point cloud data.\n","authors":["Lucas Nunes","Rodrigo Marcuzzi","Benedikt Mersch","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2403.13470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13469v1","updated":"2024-03-20T10:18:20Z","published":"2024-03-20T10:18:20Z","title":"Progressive trajectory matching for medical dataset distillation","summary":"  It is essential but challenging to share medical image datasets due to\nprivacy issues, which prohibit building foundation models and knowledge\ntransfer. In this paper, we propose a novel dataset distillation method to\ncondense the original medical image datasets into a synthetic one that\npreserves useful information for building an analysis model without accessing\nthe original datasets. Existing methods tackle only natural images by randomly\nmatching parts of the training trajectories of the model parameters trained by\nthe whole real datasets. However, through extensive experiments on medical\nimage datasets, the training process is extremely unstable and achieves\ninferior distillation results. To solve these barriers, we propose to design a\nnovel progressive trajectory matching strategy to improve the training\nstability for medical image dataset distillation. Additionally, it is observed\nthat improved stability prevents the synthetic dataset diversity and final\nperformance improvements. Therefore, we propose a dynamic overlap mitigation\nmodule that improves the synthetic dataset diversity by dynamically eliminating\nthe overlap across different images and retraining parts of the synthetic\nimages for better convergence. Finally, we propose a new medical image dataset\ndistillation benchmark of various modalities and configurations to promote fair\nevaluations. It is validated that our proposed method achieves 8.33%\nimprovement over previous state-of-the-art methods on average, and 11.7%\nimprovement when ipc=2 (i.e., image per class is 2). Codes and benchmarks will\nbe released.\n","authors":["Zhen Yu","Yang Liu","Qingchao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.13469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13467v1","updated":"2024-03-20T10:17:39Z","published":"2024-03-20T10:17:39Z","title":"CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language\n  Models","summary":"  This paper introduces CLIPSwarm, a new algorithm designed to automate the\nmodeling of swarm drone formations based on natural language. The algorithm\nbegins by enriching a provided word, to compose a text prompt that serves as\ninput to an iterative approach to find the formation that best matches the\nprovided word. The algorithm iteratively refines formations of robots to align\nwith the textual description, employing different steps for \"exploration\" and\n\"exploitation\". Our framework is currently evaluated on simple formation\ntargets, limited to contour shapes. A formation is visually represented through\nalpha-shape contours and the most representative color is automatically found\nfor the input word. To measure the similarity between the description and the\nvisual representation of the formation, we use CLIP [1], encoding text and\nimages into vectors and assessing their similarity. Subsequently, the algorithm\nrearranges the formation to visually represent the word more effectively,\nwithin the given constraints of available drones. Control actions are then\nassigned to the drones, ensuring robotic behavior and collision-free movement.\nExperimental results demonstrate the system's efficacy in accurately modeling\nrobot formations from natural language descriptions. The algorithm's\nversatility is showcased through the execution of drone shows in photorealistic\nsimulation with varying shapes. We refer the reader to the supplementary video\nfor a visual reference of the results.\n","authors":["Pablo Pueyo","Eduardo Montijano","Ana C. Murillo","Mac Schwager"],"pdf_url":"https://arxiv.org/pdf/2403.13467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13466v1","updated":"2024-03-20T10:16:40Z","published":"2024-03-20T10:16:40Z","title":"An AI-Assisted Skincare Routine Recommendation System in XR","summary":"  In recent years, there has been an increasing interest in the use of\nartificial intelligence (AI) and extended reality (XR) in the beauty industry.\nIn this paper, we present an AI-assisted skin care recommendation system\nintegrated into an XR platform. The system uses a convolutional neural network\n(CNN) to analyse an individual's skin type and recommend personalised skin care\nproducts in an immersive and interactive manner. Our methodology involves\ncollecting data from individuals through a questionnaire and conducting skin\nanalysis using a provided facial image in an immersive environment. This data\nis then used to train the CNN model, which recognises the skin type and\nexisting issues and allows the recommendation engine to suggest personalised\nskin care products. We evaluate our system in terms of the accuracy of the CNN\nmodel, which achieves an average score of 93% in correctly classifying existing\nskin issues. Being integrated into an XR system, this approach has the\npotential to significantly enhance the beauty industry by providing immersive\nand engaging experiences to users, leading to more efficient and consistent\nskincare routines.\n","authors":["Gowravi Malalur Rajegowda","Yannis Spyridis","Barbara Villarini","Vasileios Argyriou"],"pdf_url":"https://arxiv.org/pdf/2403.13466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09380v2","updated":"2024-03-20T10:09:42Z","published":"2024-03-14T13:31:56Z","title":"Impact of Synthetic Images on Morphing Attack Detection Using a Siamese\n  Network","summary":"  This paper evaluated the impact of synthetic images on Morphing Attack\nDetection (MAD) using a Siamese network with a semi-hard-loss function. Intra\nand cross-dataset evaluations were performed to measure synthetic image\ngeneralisation capabilities using a cross-dataset for evaluation. Three\ndifferent pre-trained networks were used as feature extractors from traditional\nMobileNetV2, MobileNetV3 and EfficientNetB0. Our results show that MAD trained\non EfficientNetB0 from FERET, FRGCv2, and FRLL can reach a lower error rate in\ncomparison with SOTA. Conversely, worse performances were reached when the\nsystem was trained only with synthetic images. A mixed approach (synthetic +\ndigital) database may help to improve MAD and reduce the error rate. This fact\nshows that we still need to keep going with our efforts to include synthetic\nimages in the training process.\n","authors":["Juan Tapia","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2403.09380v2.pdf","comment":"Arxiv version of CIARP2023 - fixed typo errors"},{"id":"http://arxiv.org/abs/2311.13261v2","updated":"2024-03-20T10:06:09Z","published":"2023-11-22T09:25:08Z","title":"Immunohistochemistry guided segmentation of benign epithelial cells, in\n  situ lesions, and invasive epithelial cells in breast cancer slides","summary":"  Digital pathology enables automatic analysis of histopathological sections\nusing artificial intelligence (AI). Automatic evaluation could improve\ndiagnostic efficiency and help find associations between morphological features\nand clinical outcome. For development of such prediction models, identifying\ninvasive epithelial cells, and separating these from benign epithelial cells\nand in situ lesions would be the first step. In this study, we aimed to develop\nan AI model for segmentation of epithelial cells in sections from breast\ncancer. We generated epithelial ground truth masks by restaining hematoxylin\nand eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists'\nannotations. HE/CK image pairs were used to train a convolutional neural\nnetwork, and data augmentation was used to make the model more robust. Tissue\nmicroarrays (TMAs) from 839 patients, and whole slide images from two patients\nwere used for training and evaluation of the models. The sections were derived\nfrom four cohorts of breast cancer patients. TMAs from 21 patients from a fifth\ncohort was used as a second test set. In quantitative evaluation, a mean Dice\nscore of 0.70, 0.79, and 0.75 for invasive epithelial cells, benign epithelial\ncells, and in situ lesions, respectively, were achieved. In qualitative scoring\n(0-5) by pathologists, results were best for all epithelium and invasive\nepithelium, with scores of 4.7 and 4.4. Scores for benign epithelium and in\nsitu lesions were 3.7 and 2.0. The proposed model segmented epithelial cells in\nHE stained breast cancer slides well, but further work is needed for accurate\ndivision between the classes. Immunohistochemistry, together with pathologists'\nannotations, enabled the creation of accurate ground truths. The model is made\nfreely available in FastPathology and the code is available at\nhttps://github.com/AICAN-Research/breast-epithelium-segmentation\n","authors":["Maren Høibø","André Pedersen","Vibeke Grotnes Dale","Sissel Marie Berget","Borgny Ytterhus","Cecilia Lindskog","Elisabeth Wik","Lars A. Akslen","Ingerid Reinertsen","Erik Smistad","Marit Valla"],"pdf_url":"https://arxiv.org/pdf/2311.13261v2.pdf","comment":"19 pages, 6 figures. Submitted to a scientific journal"},{"id":"http://arxiv.org/abs/2403.06225v2","updated":"2024-03-20T10:05:02Z","published":"2024-03-10T14:11:25Z","title":"MoST: Motion Style Transformer between Diverse Action Contents","summary":"  While existing motion style transfer methods are effective between two\nmotions with identical content, their performance significantly diminishes when\ntransferring style between motions with different contents. This challenge lies\nin the lack of clear separation between content and style of a motion. To\ntackle this challenge, we propose a novel motion style transformer that\neffectively disentangles style from content and generates a plausible motion\nwith transferred style from a source motion. Our distinctive approach to\nachieving the goal of disentanglement is twofold: (1) a new architecture for\nmotion style transformer with `part-attentive style modulator across body\nparts' and `Siamese encoders that encode style and content features\nseparately'; (2) style disentanglement loss. Our method outperforms existing\nmethods and demonstrates exceptionally high quality, particularly in motion\npairs with different contents, without the need for heuristic post-processing.\nCodes are available at https://github.com/Boeun-Kim/MoST.\n","authors":["Boeun Kim","Jungho Kim","Hyung Jin Chang","Jin Young Choi"],"pdf_url":"https://arxiv.org/pdf/2403.06225v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2305.11488v2","updated":"2024-03-20T09:44:50Z","published":"2023-05-19T07:39:17Z","title":"AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning","summary":"  Continual learning aims to enable a model to incrementally learn knowledge\nfrom sequentially arrived data. Previous works adopt the conventional\nclassification architecture, which consists of a feature extractor and a\nclassifier. The feature extractor is shared across sequentially arrived tasks\nor classes, but one specific group of weights of the classifier corresponding\nto one new class should be incrementally expanded. Consequently, the parameters\nof a continual learner gradually increase. Moreover, as the classifier contains\nall historical arrived classes, a certain size of the memory is usually\nrequired to store rehearsal data to mitigate classifier bias and catastrophic\nforgetting. In this paper, we propose a non-incremental learner, named\nAttriCLIP, to incrementally extract knowledge of new classes or tasks.\nSpecifically, AttriCLIP is built upon the pre-trained visual-language model\nCLIP. Its image encoder and text encoder are fixed to extract features from\nboth images and text. Text consists of a category name and a fixed number of\nlearnable parameters which are selected from our designed attribute word bank\nand serve as attributes. As we compute the visual and textual similarity for\nclassification, AttriCLIP is a non-incremental learner. The attribute prompts,\nwhich encode the common knowledge useful for classification, can effectively\nmitigate the catastrophic forgetting and avoid constructing a replay memory. We\nevaluate our AttriCLIP and compare it with CLIP-based and previous\nstate-of-the-art continual learning methods in realistic settings with\ndomain-shift and long-sequence learning. The results show that our method\nperforms favorably against previous state-of-the-arts. The implementation code\ncan be available at https://github.com/bhrqw/AttriCLIP.\n","authors":["Runqi Wang","Xiaoyue Duan","Guoliang Kang","Jianzhuang Liu","Shaohui Lin","Songcen Xu","Jinhu Lv","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.11488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13447v1","updated":"2024-03-20T09:42:43Z","published":"2024-03-20T09:42:43Z","title":"HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal\n  Large Language Models","summary":"  Recent advancements indicate that scaling up Multimodal Large Language Models\n(MLLMs) effectively enhances performance on downstream multimodal tasks. The\nprevailing MLLM paradigm, \\emph{e.g.}, LLaVA, transforms visual features into\ntext-like tokens using a \\emph{static} vision-language mapper, thereby enabling\n\\emph{static} LLMs to develop the capability to comprehend visual information\nthrough visual instruction tuning. Although promising, the \\emph{static} tuning\nstrategy~\\footnote{The static tuning refers to the trained model with static\nparameters.} that shares the same parameters may constrain performance across\ndifferent downstream multimodal tasks. In light of this, we introduce\nHyperLLaVA, which involves adaptive tuning of the projector and LLM parameters,\nin conjunction with a dynamic visual expert and language expert, respectively.\nThese experts are derived from HyperNetworks, which generates adaptive\nparameter shifts through visual and language guidance, enabling dynamic\nprojector and LLM modeling in two-stage training.\n  Our experiments demonstrate that our solution significantly surpasses LLaVA\non existing MLLM benchmarks, including MME, MMBench, SEED-Bench, and\nLLaVA-Bench. ~\\footnote{Our project is available on the link\nhttps://github.com/DCDmllm/HyperLLaVA}.\n","authors":["Wenqiao Zhang","Tianwei Lin","Jiang Liu","Fangxun Shu","Haoyuan Li","Lei Zhang","He Wanggui","Hao Zhou","Zheqi Lv","Hao Jiang","Juncheng Li","Siliang Tang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.13447v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.13747v1","updated":"2024-03-20T16:54:55Z","published":"2024-03-20T16:54:55Z","title":"Leveraging High-Resolution Features for Improved Deep Hashing-based\n  Image Retrieval","summary":"  Deep hashing techniques have emerged as the predominant approach for\nefficient image retrieval. Traditionally, these methods utilize pre-trained\nconvolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature\nextractors. However, the increasing complexity of datasets poses challenges for\nthese backbone architectures in capturing meaningful features essential for\neffective image retrieval. In this study, we explore the efficacy of employing\nhigh-resolution features learned through state-of-the-art techniques for image\nretrieval tasks. Specifically, we propose a novel methodology that utilizes\nHigh-Resolution Networks (HRNets) as the backbone for the deep hashing task,\ntermed High-Resolution Hashing Network (HHNet). Our approach demonstrates\nsuperior performance compared to existing methods across all tested benchmark\ndatasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance\nimprovement is more pronounced for complex datasets, which highlights the need\nto learn high-resolution features for intricate image retrieval tasks.\nFurthermore, we conduct a comprehensive analysis of different HRNet\nconfigurations and provide insights into the optimal architecture for the deep\nhashing task\n","authors":["Aymene Berriche","Mehdi Adjal Zakaria","Riyadh Baghdadi"],"pdf_url":"https://arxiv.org/pdf/2403.13747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13597v1","updated":"2024-03-20T13:44:30Z","published":"2024-03-20T13:44:30Z","title":"No more optimization rules: LLM-enabled policy-based multi-modal query\n  optimizer (version 1)","summary":"  Large language model (LLM) has marked a pivotal moment in the field of\nmachine learning and deep learning. Recently its capability for query planning\nhas been investigated, including both single-modal and multi-modal queries.\nHowever, there is no work on the query optimization capability of LLM. As a\ncritical (or could even be the most important) step that significantly impacts\nthe execution performance of the query plan, such analysis and attempts should\nnot be missed. From another aspect, existing query optimizers are usually\nrule-based or rule-based + cost-based, i.e., they are dependent on manually\ncreated rules to complete the query plan rewrite/transformation. Given the fact\nthat modern optimizers include hundreds to thousands of rules, designing a\nmulti-modal query optimizer following a similar way is significantly\ntime-consuming since we will have to enumerate as many multi-modal optimization\nrules as possible, which has not been well addressed today. In this paper, we\ninvestigate the query optimization ability of LLM and use LLM to design LaPuda,\na novel LLM and Policy based multi-modal query optimizer. Instead of\nenumerating specific and detailed rules, LaPuda only needs a few abstract\npolicies to guide LLM in the optimization, by which much time and human effort\nare saved. Furthermore, to prevent LLM from making mistakes or negative\noptimization, we borrow the idea of gradient descent and propose a guided cost\ndescent (GCD) algorithm to perform the optimization, such that the optimization\ncan be kept in the correct direction. In our evaluation, our methods\nconsistently outperform the baselines in most cases. For example, the optimized\nplans generated by our methods result in 1~3x higher execution speed than those\nby the baselines.\n","authors":["Yifan Wang","Haodi Ma","Daisy Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13597v1.pdf","comment":"Yifan and Haodi contribute equally to the work"},{"id":"http://arxiv.org/abs/2403.13574v1","updated":"2024-03-20T13:14:29Z","published":"2024-03-20T13:14:29Z","title":"A Large Language Model Enhanced Sequential Recommender for Joint Video\n  and Comment Recommendation","summary":"  In online video platforms, reading or writing comments on interesting videos\nhas become an essential part of the video watching experience. However,\nexisting video recommender systems mainly model users' interaction behaviors\nwith videos, lacking consideration of comments in user behavior modeling. In\nthis paper, we propose a novel recommendation approach called LSVCR by\nleveraging user interaction histories with both videos and comments, so as to\njointly conduct personalized video and comment recommendation. Specifically,\nour approach consists of two key components, namely sequential recommendation\n(SR) model and supplemental large language model (LLM) recommender. The SR\nmodel serves as the primary recommendation backbone (retained in deployment) of\nour approach, allowing for efficient user preference modeling. Meanwhile, we\nleverage the LLM recommender as a supplemental component (discarded in\ndeployment) to better capture underlying user preferences from heterogeneous\ninteraction behaviors. In order to integrate the merits of the SR model and the\nsupplemental LLM recommender, we design a twostage training paradigm. The first\nstage is personalized preference alignment, which aims to align the preference\nrepresentations from both components, thereby enhancing the semantics of the SR\nmodel. The second stage is recommendation-oriented fine-tuning, in which the\nalignment-enhanced SR model is fine-tuned according to specific objectives.\nExtensive experiments in both video and comment recommendation tasks\ndemonstrate the effectiveness of LSVCR. Additionally, online A/B testing on the\nKuaiShou platform verifies the actual benefits brought by our approach. In\nparticular, we achieve a significant overall gain of 4.13% in comment watch\ntime.\n","authors":["Bowen Zheng","Zihan Lin","Enze Liu","Chen Yang","Enyang Bai","Cheng Ling","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.13574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17649v2","updated":"2024-03-20T12:24:15Z","published":"2023-12-29T15:33:14Z","title":"Investigating the Effects of Sparse Attention on Cross-Encoders","summary":"  Cross-encoders are effective passage and document re-rankers but less\nefficient than other neural or classic retrieval models. A few previous studies\nhave applied windowed self-attention to make cross-encoders more efficient.\nHowever, these studies did not investigate the potential and limits of\ndifferent attention patterns or window sizes. We close this gap and\nsystematically analyze how token interactions can be reduced without harming\nthe re-ranking effectiveness. Experimenting with asymmetric attention and\ndifferent window sizes, we find that the query tokens do not need to attend to\nthe passage or document tokens for effective re-ranking and that very small\nwindow sizes suffice. In our experiments, even windows of 4 tokens still yield\neffectiveness on par with previous cross-encoders while reducing the memory\nrequirements by at least 22% / 59% and being 1% / 43% faster at inference time\nfor passages / documents.\n","authors":["Ferdinand Schlatt","Maik Fröbe","Matthias Hagen"],"pdf_url":"https://arxiv.org/pdf/2312.17649v2.pdf","comment":"Accepted at ECIR'24"},{"id":"http://arxiv.org/abs/2310.04268v3","updated":"2024-03-20T11:46:41Z","published":"2023-10-06T14:11:58Z","title":"WaZI: A Learned and Workload-aware Z-Index","summary":"  Learned indexes fit machine learning (ML) models to the data and use them to\nmake query operations more time and space-efficient. Recent works propose using\nlearned spatial indexes to improve spatial query performance by optimizing the\nstorage layout or internal search structures according to the data\ndistribution. However, only a few learned indexes exploit the query workload\ndistribution to enhance their performance. In addition, building and updating\nlearned spatial indexes are often costly on large datasets due to the\ninefficiency of (re)training ML models. In this paper, we present WaZI, a\nlearned and workload-aware variant of the Z-index, which jointly optimizes the\nstorage layout and search structures, as a viable solution for the above\nchallenges of spatial indexing. Specifically, we first formulate a cost\nfunction to measure the performance of a Z-index on a dataset for a range-query\nworkload. Then, we optimize the Z-index structure by minimizing the cost\nfunction through adaptive partitioning and ordering for index construction.\nMoreover, we design a novel page-skipping mechanism to improve the query\nperformance of WaZI by reducing access to irrelevant data pages. Our extensive\nexperiments show that the WaZI index improves range query time by 40% on\naverage over the baselines while always performing better or comparably to\nstate-of-the-art spatial indexes. Additionally, it also maintains good point\nquery performance. Generally, WaZI provides favorable tradeoffs among query\nlatency, construction time, and index size.\n","authors":["Sachith Pai","Michael Mathioudakis","Yanhao Wang"],"pdf_url":"https://arxiv.org/pdf/2310.04268v3.pdf","comment":"Camera-ready version accepted to EDBT 2024"},{"id":"http://arxiv.org/abs/2403.13480v1","updated":"2024-03-20T10:34:40Z","published":"2024-03-20T10:34:40Z","title":"A Unified Optimal Transport Framework for Cross-Modal Retrieval with\n  Noisy Labels","summary":"  Cross-modal retrieval (CMR) aims to establish interaction between different\nmodalities, among which supervised CMR is emerging due to its flexibility in\nlearning semantic category discrimination. Despite the remarkable performance\nof previous supervised CMR methods, much of their success can be attributed to\nthe well-annotated data. However, even for unimodal data, precise annotation is\nexpensive and time-consuming, and it becomes more challenging with the\nmultimodal scenario. In practice, massive multimodal data are collected from\nthe Internet with coarse annotation, which inevitably introduces noisy labels.\nTraining with such misleading labels would bring two key challenges --\nenforcing the multimodal samples to \\emph{align incorrect semantics} and\n\\emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To\ntackle these challenges, this work proposes UOT-RCL, a Unified framework based\non Optimal Transport (OT) for Robust Cross-modal Retrieval. First, we propose a\nsemantic alignment based on partial OT to progressively correct the noisy\nlabels, where a novel cross-modal consistent cost function is designed to blend\ndifferent modalities and provide precise transport cost. Second, to narrow the\ndiscrepancy in multi-modal data, an OT-based relation alignment is proposed to\ninfer the semantic-level cross-modal matching. Both of these two components\nleverage the inherent correlation among multi-modal data to facilitate\neffective cost function. The experiments on three widely-used cross-modal\nretrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art\napproaches and significantly improves the robustness against noisy labels.\n","authors":["Haochen Han","Minnan Luo","Huan Liu","Fang Nan"],"pdf_url":"https://arxiv.org/pdf/2403.13480v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.13468v1","updated":"2024-03-20T10:18:05Z","published":"2024-03-20T10:18:05Z","title":"DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using\n  Mixture-of-Experts","summary":"  Open-domain question answering requires retrieval systems able to cope with\nthe diverse and varied nature of questions, providing accurate answers across a\nbroad spectrum of query types and topics. To deal with such topic heterogeneity\nthrough a unique model, we propose DESIRE-ME, a neural information retrieval\nmodel that leverages the Mixture-of-Experts framework to combine multiple\nspecialized neural models. We rely on Wikipedia data to train an effective\nneural gating mechanism that classifies the incoming query and that weighs the\npredictions of the different domain-specific experts correspondingly. This\nallows DESIRE-ME to specialize adaptively in multiple domains. Through\nextensive experiments on publicly available datasets, we show that our proposal\ncan effectively generalize domain-enhanced neural models. DESIRE-ME excels in\nhandling open-domain questions adaptively, boosting by up to 12% in NDCG@10 and\n22% in P@1, the underlying state-of-the-art dense retrieval model.\n","authors":["Pranav Kasela","Gabriella Pasi","Raffaele Perego","Nicola Tonellotto"],"pdf_url":"https://arxiv.org/pdf/2403.13468v1.pdf","comment":"Accepted at the 46th European Conference on Information Retrieval\n  (ECIR 2024)"},{"id":"http://arxiv.org/abs/2403.13344v1","updated":"2024-03-20T07:05:19Z","published":"2024-03-20T07:05:19Z","title":"USE: Dynamic User Modeling with Stateful Sequence Models","summary":"  User embeddings play a crucial role in user engagement forecasting and\npersonalized services. Recent advances in sequence modeling have sparked\ninterest in learning user embeddings from behavioral data. Yet behavior-based\nuser embedding learning faces the unique challenge of dynamic user modeling. As\nusers continuously interact with the apps, user embeddings should be\nperiodically updated to account for users' recent and long-term behavior\npatterns. Existing methods highly rely on stateless sequence models that lack\nmemory of historical behavior. They have to either discard historical data and\nuse only the most recent data or reprocess the old and new data jointly. Both\ncases incur substantial computational overhead. To address this limitation, we\nintroduce User Stateful Embedding (USE). USE generates user embeddings and\nreflects users' evolving behaviors without the need for exhaustive reprocessing\nby storing previous model states and revisiting them in the future.\nFurthermore, we introduce a novel training objective named future W-behavior\nprediction to transcend the limitations of next-token prediction by forecasting\na broader horizon of upcoming user behaviors. By combining it with the Same\nUser Prediction, a contrastive learning-based objective that predicts whether\ndifferent segments of behavior sequences belong to the same user, we further\nimprove the embeddings' distinctiveness and representativeness. We conducted\nexperiments on 8 downstream tasks using Snapchat users' behavioral logs in both\nstatic (i.e., fixed user behavior sequences) and dynamic (i.e., periodically\nupdated user behavior sequences) settings. We demonstrate USE's superior\nperformance over established baselines. The results underscore USE's\neffectiveness and efficiency in integrating historical and recent user behavior\nsequences into user embeddings in dynamic user modeling.\n","authors":["Zhihan Zhou","Qixiang Fang","Leonardo Neves","Francesco Barbieri","Yozen Liu","Han Liu","Maarten W. Bos","Ron Dotsch"],"pdf_url":"https://arxiv.org/pdf/2403.13344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13325v1","updated":"2024-03-20T06:09:30Z","published":"2024-03-20T06:09:30Z","title":"Harnessing Large Language Models for Text-Rich Sequential Recommendation","summary":"  Recent advances in Large Language Models (LLMs) have been changing the\nparadigm of Recommender Systems (RS). However, when items in the recommendation\nscenarios contain rich textual information, such as product descriptions in\nonline shopping or news headlines on social media, LLMs require longer texts to\ncomprehensively depict the historical user behavior sequence. This poses\nsignificant challenges to LLM-based recommenders, such as over-length\nlimitations, extensive time and space overheads, and suboptimal model\nperformance. To this end, in this paper, we design a novel framework for\nharnessing Large Language Models for Text-Rich Sequential Recommendation\n(LLM-TRSR). Specifically, we first propose to segment the user historical\nbehaviors and subsequently employ an LLM-based summarizer for summarizing these\nuser behavior blocks. Particularly, drawing inspiration from the successful\napplication of Convolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN) models in user modeling, we introduce two unique summarization\ntechniques in this paper, respectively hierarchical summarization and recurrent\nsummarization. Then, we construct a prompt text encompassing the user\npreference summary, recent user interactions, and candidate item information\ninto an LLM-based recommender, which is subsequently fine-tuned using\nSupervised Fine-Tuning (SFT) techniques to yield our final recommendation\nmodel. We also use Low-Rank Adaptation (LoRA) for Parameter-Efficient\nFine-Tuning (PEFT). We conduct experiments on two public datasets, and the\nresults clearly demonstrate the effectiveness of our approach.\n","authors":["Zhi Zheng","Wenshuo Chao","Zhaopeng Qiu","Hengshu Zhu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.13325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13317v1","updated":"2024-03-20T05:38:50Z","published":"2024-03-20T05:38:50Z","title":"Flickr30K-CFQ: A Compact and Fragmented Query Dataset for Text-image\n  Retrieval","summary":"  With the explosive growth of multi-modal information on the Internet,\nunimodal search cannot satisfy the requirement of Internet applications.\nText-image retrieval research is needed to realize high-quality and efficient\nretrieval between different modalities. Existing text-image retrieval research\nis mostly based on general vision-language datasets (e.g. MS-COCO, Flickr30K),\nin which the query utterance is rigid and unnatural (i.e. verbosity and\nformality). To overcome the shortcoming, we construct a new Compact and\nFragmented Query challenge dataset (named Flickr30K-CFQ) to model text-image\nretrieval task considering multiple query content and style, including compact\nand fine-grained entity-relation corpus. We propose a novel query-enhanced\ntext-image retrieval method using prompt engineering based on LLM. Experiments\nshow that our proposed Flickr30-CFQ reveals the insufficiency of existing\nvision-language datasets in realistic text-image tasks. Our LLM-based\nQuery-enhanced method applied on different existing text-image retrieval models\nimproves query understanding performance both on public dataset and our\nchallenge set Flickr30-CFQ with over 0.9% and 2.4% respectively. Our project\ncan be available anonymously in https://sites.google.com/view/Flickr30K-cfq.\n","authors":["Haoyu Liu","Yaoxian Song","Xuwu Wang","Zhu Xiangru","Zhixu Li","Wei Song","Tiefeng Li"],"pdf_url":"https://arxiv.org/pdf/2403.13317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13310v1","updated":"2024-03-20T05:23:09Z","published":"2024-03-20T05:23:09Z","title":"A Semantic Search Engine for Mathlib4","summary":"  The interactive theorem prover, Lean, enables the verification of formal\nmathematical proofs and is backed by an expanding community. Central to this\necosystem is its mathematical library, mathlib4, which lays the groundwork for\nthe formalization of an expanding range of mathematical theories. However,\nsearching for theorems in mathlib4 can be challenging. To successfully search\nin mathlib4, users often need to be familiar with its naming conventions or\ndocumentation strings. Therefore, creating a semantic search engine that can be\nused easily by individuals with varying familiarity with mathlib4 is very\nimportant. In this paper, we present a semantic search engine for mathlib4 that\naccepts informal queries and finds the relevant theorems. We also establish a\nbenchmark for assessing the performance of various search engines for mathlib4.\n","authors":["Guoxiong Gao","Haocheng Ju","Jiedong Jiang","Zihan Qin","Bin Dong"],"pdf_url":"https://arxiv.org/pdf/2403.13310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12660v2","updated":"2024-03-20T05:10:22Z","published":"2024-03-19T11:49:35Z","title":"ERASE: Benchmarking Feature Selection Methods for Deep Recommender\n  Systems","summary":"  Deep Recommender Systems (DRS) are increasingly dependent on a large number\nof feature fields for more precise recommendations. Effective feature selection\nmethods are consequently becoming critical for further enhancing the accuracy\nand optimizing storage efficiencies to align with the deployment demands. This\nresearch area, particularly in the context of DRS, is nascent and faces three\ncore challenges. Firstly, variant experimental setups across research papers\noften yield unfair comparisons, obscuring practical insights. Secondly, the\nexisting literature's lack of detailed analysis on selection attributes, based\non large-scale datasets and a thorough comparison among selection techniques\nand DRS backbones, restricts the generalizability of findings and impedes\ndeployment on DRS. Lastly, research often focuses on comparing the peak\nperformance achievable by feature selection methods, an approach that is\ntypically computationally infeasible for identifying the optimal\nhyperparameters and overlooks evaluating the robustness and stability of these\nmethods. To bridge these gaps, this paper presents ERASE, a comprehensive\nbEnchmaRk for feAture SElection for DRS. ERASE comprises a thorough evaluation\nof eleven feature selection methods, covering both traditional and deep\nlearning approaches, across four public datasets, private industrial datasets,\nand a real-world commercial platform, achieving significant enhancement. Our\ncode is available online for ease of reproduction.\n","authors":["Pengyue Jia","Yejing Wang","Zhaocheng Du","Xiangyu Zhao","Yichao Wang","Bo Chen","Wanyu Wang","Huifeng Guo","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2403.12660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07838v2","updated":"2024-03-20T05:04:06Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v2.pdf","comment":"Accepted by NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.13291v1","updated":"2024-03-20T04:10:37Z","published":"2024-03-20T04:10:37Z","title":"An Analysis on Matching Mechanisms and Token Pruning for\n  Late-interaction Models","summary":"  With the development of pre-trained language models, the dense retrieval\nmodels have become promising alternatives to the traditional retrieval models\nthat rely on exact match and sparse bag-of-words representations. Different\nfrom most dense retrieval models using a bi-encoder to encode each query or\ndocument into a dense vector, the recently proposed late-interaction\nmulti-vector models (i.e., ColBERT and COIL) achieve state-of-the-art retrieval\neffectiveness by using all token embeddings to represent documents and queries\nand modeling their relevance with a sum-of-max operation. However, these\nfine-grained representations may cause unacceptable storage overhead for\npractical search systems. In this study, we systematically analyze the matching\nmechanism of these late-interaction models and show that the sum-of-max\noperation heavily relies on the co-occurrence signals and some important words\nin the document. Based on these findings, we then propose several simple\ndocument pruning methods to reduce the storage overhead and compare the\neffectiveness of different pruning methods on different late-interaction\nmodels. We also leverage query pruning methods to further reduce the retrieval\nlatency. We conduct extensive experiments on both in-domain and out-domain\ndatasets and show that some of the used pruning methods can significantly\nimprove the efficiency of these late-interaction models without substantially\nhurting their retrieval effectiveness.\n","authors":["Qi Liu","Gang Guo","Jiaxin Mao","Zhicheng Dou","Ji-Rong Wen","Hao Jiang","Xinyu Zhang","Zhao Cao"],"pdf_url":"https://arxiv.org/pdf/2403.13291v1.pdf","comment":"Accepted by ACM Transactions on Information Systems"},{"id":"http://arxiv.org/abs/2403.12384v2","updated":"2024-03-20T02:50:36Z","published":"2024-03-19T02:49:32Z","title":"An Aligning and Training Framework for Multimodal Recommendations","summary":"  With the development of multimedia applications, multimodal recommendations\nare playing an essential role, as they can leverage rich contexts beyond user\ninteractions. Existing methods mainly regard multimodal information as an\nauxiliary, using them to help learn ID features; however, there exist semantic\ngaps among multimodal content features and ID features, for which directly\nusing multimodal information as an auxiliary would lead to misalignment in\nrepresentations of users and items. In this paper, we first systematically\ninvestigate the misalignment issue in multimodal recommendations, and propose a\nsolution named AlignRec. In AlignRec, the recommendation objective is\ndecomposed into three alignments, namely alignment within contents, alignment\nbetween content and categorical ID, and alignment between users and items. Each\nalignment is characterized by a specific objective function and is integrated\ninto our multimodal recommendation framework. To effectively train our\nAlignRec, we propose starting from pre-training the first alignment to obtain\nunified multimodal features and subsequently training the following two\nalignments together with these features as input. As it is essential to analyze\nwhether each multimodal feature helps in training, we design three new classes\nof metrics to evaluate intermediate performance. Our extensive experiments on\nthree real-world datasets consistently verify the superiority of AlignRec\ncompared to nine baselines. We also find that the multimodal features generated\nby AlignRec are better than currently used ones, which are to be open-sourced.\n","authors":["Yifan Liu","Kangning Zhang","Xiangyuan Ren","Yanhua Huang","Jiarui Jin","Yingjie Qin","Ruilong Su","Ruiwen Xu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12384v2.pdf","comment":"11 pages, add some necessary explanations, revise typos"},{"id":"http://arxiv.org/abs/2306.02371v3","updated":"2024-03-20T02:22:32Z","published":"2023-06-04T15:02:11Z","title":"I^3 Retriever: Incorporating Implicit Interaction in Pre-trained\n  Language Models for Passage Retrieval","summary":"  Passage retrieval is a fundamental task in many information systems, such as\nweb search and question answering, where both efficiency and effectiveness are\ncritical concerns. In recent years, neural retrievers based on pre-trained\nlanguage models (PLM), such as dual-encoders, have achieved huge success. Yet,\nstudies have found that the performance of dual-encoders are often limited due\nto the neglecting of the interaction information between queries and candidate\npassages. Therefore, various interaction paradigms have been proposed to\nimprove the performance of vanilla dual-encoders. Particularly, recent\nstate-of-the-art methods often introduce late-interaction during the model\ninference process. However, such late-interaction based methods usually bring\nextensive computation and storage cost on large corpus. Despite their\neffectiveness, the concern of efficiency and space footprint is still an\nimportant factor that limits the application of interaction-based neural\nretrieval models. To tackle this issue, we incorporate implicit interaction\ninto dual-encoders, and propose I^3 retriever. In particular, our implicit\ninteraction paradigm leverages generated pseudo-queries to simulate\nquery-passage interaction, which jointly optimizes with query and passage\nencoders in an end-to-end manner. It can be fully pre-computed and cached, and\nits inference process only involves simple dot product operation of the query\nvector and passage vector, which makes it as efficient as the vanilla dual\nencoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep\nLearning Datasets, demonstrating the I^3 retriever's superiority in terms of\nboth effectiveness and efficiency. Moreover, the proposed implicit interaction\nis compatible with special pre-training and knowledge distillation for passage\nretrieval, which brings a new state-of-the-art performance.\n","authors":["Qian Dong","Yiding Liu","Qingyao Ai","Haitao Li","Shuaiqiang Wang","Yiqun Liu","Dawei Yin","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2306.02371v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.13242v1","updated":"2024-03-20T02:13:05Z","published":"2024-03-20T02:13:05Z","title":"Improving Legal Case Retrieval with Brain Signals","summary":"  The tasks of legal case retrieval have received growing attention from the IR\ncommunity in the last decade. Relevance feedback techniques with implicit user\nfeedback (e.g., clicks) have been demonstrated to be effective in traditional\nsearch tasks (e.g., Web search). In legal case retrieval, however, collecting\nrelevance feedback faces a couple of challenges that are difficult to resolve\nunder existing feedback paradigms. First, legal case retrieval is a complex\ntask as users often need to understand the relationship between legal cases in\ndetail to correctly judge their relevance. Traditional feedback signal such as\nclicks is too coarse to use as they do not reflect any fine-grained relevance\ninformation. Second, legal case documents are usually long, users often need\neven tens of minutes to read and understand them. Simple behavior signal such\nas clicks and eye-tracking fixations can hardly be useful when users almost\nclick and examine every part of the document. In this paper, we explore the\npossibility of solving the feedback problem in legal case retrieval with brain\nsignal. Recent advances in brain signal processing have shown that human\nemotional can be collected in fine grains through Brain-Machine Interfaces\n(BMI) without interrupting the users in their tasks. Therefore, we propose a\nframework for legal case retrieval that uses EEG signal to optimize retrieval\nresults. We collected and create a legal case retrieval dataset with users EEG\nsignal and propose several methods to extract effective EEG features for\nrelevance feedback. Our proposed features achieve a 71% accuracy for feedback\nprediction with an SVM-RFE model, and our proposed ranking method that takes\ninto account the diverse needs of users can significantly improve user\nsatisfaction for legal case retrieval. Experiment results show that re-ranked\nresult list make user more satisfied.\n","authors":["Ruizhe Zhang","Qingyao Ai","Ziyi Ye","Yueyue Wu","Xiaohui Xie","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.13242v1.pdf","comment":"11pages, 8 figures"},{"id":"http://arxiv.org/abs/2308.14296v3","updated":"2024-03-20T18:13:10Z","published":"2023-08-28T04:31:04Z","title":"RecMind: Large Language Model Powered Agent For Recommendation","summary":"  While the recommendation system (RS) has advanced significantly through deep\nlearning, current RS approaches usually train and fine-tune models on\ntask-specific datasets, limiting their generalizability to new recommendation\ntasks and their ability to leverage external knowledge due to model scale and\ndata size constraints. Thus, we designed an LLM-powered autonomous recommender\nagent, RecMind, which is capable of leveraging external knowledge, utilizing\ntools with careful planning to provide zero-shot personalized recommendations.\nWe propose a Self-Inspiring algorithm to improve the planning ability. At each\nintermediate step, the LLM self-inspires to consider all previously explored\nstates to plan for the next step. This mechanism greatly improves the model's\nability to comprehend and utilize historical information in planning for\nrecommendation. We evaluate RecMind's performance in various recommendation\nscenarios. Our experiment shows that RecMind outperforms existing zero/few-shot\nLLM-based recommendation baseline methods in various tasks and achieves\ncomparable performance to a fully trained recommendation model P5.\n","authors":["Yancheng Wang","Ziyan Jiang","Zheng Chen","Fan Yang","Yingxue Zhou","Eunah Cho","Xing Fan","Xiaojiang Huang","Yanbin Lu","Yingzhen Yang"],"pdf_url":"https://arxiv.org/pdf/2308.14296v3.pdf","comment":"Accepted by NAACL 2024 (Findings)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.13808v1","updated":"2024-03-20T17:59:58Z","published":"2024-03-20T17:59:58Z","title":"On Pretraining Data Diversity for Self-Supervised Learning","summary":"  We explore the impact of training with more diverse datasets, characterized\nby the number of unique samples, on the performance of self-supervised learning\n(SSL) under a fixed computational budget. Our findings consistently demonstrate\nthat increasing pretraining data diversity enhances SSL performance, albeit\nonly when the distribution distance to the downstream data is minimal. Notably,\neven with an exceptionally large pretraining data diversity achieved through\nmethods like web crawling or diffusion-generated data, among other ways, the\ndistribution shift remains a challenge. Our experiments are comprehensive with\nseven SSL methods using large-scale datasets such as ImageNet and YFCC100M\namounting to over 200 GPU days. Code and trained models will be available at\nhttps://github.com/hammoudhasan/DiversitySSL .\n","authors":["Hasan Abed Al Kader Hammoud","Tuhin Das","Fabio Pizzati","Philip Torr","Adel Bibi","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2403.13808v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.13807v1","updated":"2024-03-20T17:59:57Z","published":"2024-03-20T17:59:57Z","title":"Editing Massive Concepts in Text-to-Image Diffusion Models","summary":"  Text-to-image diffusion models suffer from the risk of generating outdated,\ncopyrighted, incorrect, and biased content. While previous methods have\nmitigated the issues on a small scale, it is essential to handle them\nsimultaneously in larger-scale real-world scenarios. We propose a two-stage\nmethod, Editing Massive Concepts In Diffusion Models (EMCID). The first stage\nperforms memory optimization for each individual concept with dual\nself-distillation from text alignment loss and diffusion noise prediction loss.\nThe second stage conducts massive concept editing with multi-layer, closed form\nmodel editing. We further propose a comprehensive benchmark, named ImageNet\nConcept Editing Benchmark (ICEB), for evaluating massive concept editing for\nT2I models with two subtasks, free-form prompts, massive concept categories,\nand extensive evaluation metrics. Extensive experiments conducted on our\nproposed benchmark and previous benchmarks demonstrate the superior scalability\nof EMCID for editing up to 1,000 concepts, providing a practical approach for\nfast adjustment and re-deployment of T2I diffusion models in real-world\napplications.\n","authors":["Tianwei Xiong","Yue Wu","Enze Xie","Yue Wu","Zhenguo Li","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2403.13807v1.pdf","comment":"Project page: https://silentview.github.io/EMCID/ . Code:\n  https://github.com/SilentView/EMCID"},{"id":"http://arxiv.org/abs/2403.13805v1","updated":"2024-03-20T17:59:55Z","published":"2024-03-20T17:59:55Z","title":"RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition","summary":"  CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from\nnoise image-text pairs to excel at recognizing a wide array of candidates, yet\nits focus on broad associations hinders the precision in distinguishing subtle\ndifferences among fine-grained items. Conversely, Multimodal Large Language\nModels (MLLMs) excel at classifying fine-grained categories, thanks to their\nsubstantial knowledge from pre-training on web-level corpora. However, the\nperformance of MLLMs declines with an increase in category numbers, primarily\ndue to growing complexity and constraints of limited context window size. To\nsynergize the strengths of both approaches and enhance the few-shot/zero-shot\nrecognition abilities for datasets characterized by extensive and fine-grained\nvocabularies, this paper introduces RAR, a Retrieving And Ranking augmented\nmethod for MLLMs. We initially establish a multi-modal retriever based on CLIP\nto create and store explicit memory for different categories beyond the\nimmediate context window. During inference, RAR retrieves the top-k similar\nresults from the memory and uses MLLMs to rank and make the final predictions.\nOur proposed approach not only addresses the inherent limitations in\nfine-grained recognition but also preserves the model's comprehensive knowledge\nbase, significantly boosting accuracy across a range of vision-language\nrecognition tasks. Notably, our approach demonstrates a significant improvement\nin performance on 5 fine-grained visual recognition benchmarks, 11 few-shot\nimage recognition datasets, and the 2 object detection datasets under the\nzero-shot recognition setting.\n","authors":["Ziyu Liu","Zeyi Sun","Yuhang Zang","Wei Li","Pan Zhang","Xiaoyi Dong","Yuanjun Xiong","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13805v1.pdf","comment":"Project: https://github.com/Liuziyu77/RAR"},{"id":"http://arxiv.org/abs/2403.13804v1","updated":"2024-03-20T17:59:43Z","published":"2024-03-20T17:59:43Z","title":"Learning from Models and Data for Visual Grounding","summary":"  We introduce SynGround, a novel framework that combines data-driven learning\nand knowledge transfer from various large-scale pretrained models to enhance\nthe visual grounding capabilities of a pretrained vision-and-language model.\nThe knowledge transfer from the models initiates the generation of image\ndescriptions through an image description generator. These descriptions serve\ndual purposes: they act as prompts for synthesizing images through a\ntext-to-image generator, and as queries for synthesizing text, from which\nphrases are extracted using a large language model. Finally, we leverage an\nopen-vocabulary object detector to generate synthetic bounding boxes for the\nsynthetic images and texts. We finetune a pretrained vision-and-language model\non this dataset by optimizing a mask-attention consistency objective that\naligns region annotations with gradient-based model explanations. The resulting\nmodel improves the grounding capabilities of an off-the-shelf\nvision-and-language model. Particularly, SynGround improves the pointing game\naccuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on\nRefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to\n63.67%.\n","authors":["Ruozhen He","Paola Cascante-Bonilla","Ziyan Yang","Alexander C. Berg","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2403.13804v1.pdf","comment":"Project Page: https://catherine-r-he.github.io/SynGround/"},{"id":"http://arxiv.org/abs/2403.13802v1","updated":"2024-03-20T17:59:14Z","published":"2024-03-20T17:59:14Z","title":"ZigMa: Zigzag Mamba Diffusion Model","summary":"  The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$. Code will be released at https://taohu.me/zigma/\n","authors":["Vincent Tao Hu","Stefan Andreas Baumann","Ming Gui","Olga Grebenkova","Pingchuan Ma","Johannes Fischer","Bjorn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13802v1.pdf","comment":"Project Page: https://taohu.me/zigma/"},{"id":"http://arxiv.org/abs/2310.07923v4","updated":"2024-03-20T17:55:48Z","published":"2023-10-11T22:35:18Z","title":"The Expressive Power of Transformers with Chain of Thought","summary":"  Recent theoretical work has identified surprisingly simple reasoning\nproblems, such as checking if two nodes in a graph are connected or simulating\nfinite-state machines, that are provably unsolvable by standard transformers\nthat answer immediately after reading their input. However, in practice,\ntransformers' reasoning can be improved by allowing them to use a \"chain of\nthought\" or \"scratchpad\", i.e., generate and condition on a sequence of\nintermediate tokens before answering. Motivated by this, we ask: Does such\nintermediate generation fundamentally extend the computational power of a\ndecoder-only transformer? We show that the answer is yes, but the amount of\nincrease depends crucially on the amount of intermediate generation. For\ninstance, we find that transformer decoders with a logarithmic number of\ndecoding steps (w.r.t. the input length) push the limits of standard\ntransformers only slightly, while a linear number of decoding steps, assuming a\nslight generalization to standard pre-norm, adds a clear new ability (under\nstandard complexity conjectures): recognizing all regular languages. Our\nresults also imply that linear steps keep transformer decoders within\ncontext-sensitive languages, and polynomial steps with generalized pre-norm\nmake them recognize exactly the class of polynomial-time solvable problems --\nthe first exact characterization of a type of transformers in terms of standard\ncomplexity classes. Together, our results provide a nuanced framework for\nunderstanding how the length of a transformer's chain of thought or scratchpad\nimpacts its reasoning power.\n","authors":["William Merrill","Ashish Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2310.07923v4.pdf","comment":"9-page preprint. Updated March 20 after ICLR acceptance"},{"id":"http://arxiv.org/abs/2403.13798v1","updated":"2024-03-20T17:55:21Z","published":"2024-03-20T17:55:21Z","title":"Hierarchical NeuroSymbolic Approach for Action Quality Assessment","summary":"  Action quality assessment (AQA) applies computer vision to quantitatively\nassess the performance or execution of a human action. Current AQA approaches\nare end-to-end neural models, which lack transparency and tend to be biased\nbecause they are trained on subjective human judgements as ground-truth. To\naddress these issues, we introduce a neuro-symbolic paradigm for AQA, which\nuses neural networks to abstract interpretable symbols from video data and\nmakes quality assessments by applying rules to those symbols. We take diving as\nthe case study. We found that domain experts prefer our system and find it more\ninformative than purely neural approaches to AQA in diving. Our system also\nachieves state-of-the-art action recognition and temporal segmentation, and\nautomatically generates a detailed report that breaks the dive down into its\nelements and provides objective scoring with visual evidence. As verified by a\ngroup of domain experts, this report may be used to assist judges in scoring,\nhelp train judges, and provide feedback to divers. We will open-source all of\nour annotated training data and code for ease of reproducibility.\n","authors":["Lauren Okamoto","Paritosh Parmar"],"pdf_url":"https://arxiv.org/pdf/2403.13798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13797v1","updated":"2024-03-20T17:54:58Z","published":"2024-03-20T17:54:58Z","title":"Bridge the Modality and Capacity Gaps in Vision-Language Model Selection","summary":"  Vision Language Models (VLMs) excel in zero-shot image classification by\npairing images with textual category names. The expanding variety of\nPre-Trained VLMs enhances the likelihood of identifying a suitable VLM for\nspecific tasks. Thus, a promising zero-shot image classification strategy is\nselecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely\non the text data of the target dataset without access to the dataset's images.\nIn this paper, we analyze two inherent challenges in assessing the ability of a\nVLM in this Language-Only VLM selection: the \"Modality Gap\" -- the disparity in\nVLM's embeddings across two different modalities, making text a less reliable\nsubstitute for images; and the \"Capability Gap\" -- the discrepancy between the\nVLM's overall ranking and its ranking for target dataset, hindering direct\nprediction of a model's dataset-specific performance from its general\nperformance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the\nnegative impact of these two gaps. SWAB first adopts optimal transport to\ncapture the relevance between open-source datasets and target dataset with a\ntransportation matrix. It then uses this matrix to transfer useful statistics\nof VLMs from open-source datasets to the target dataset for bridging those two\ngaps and enhancing the VLM's capacity estimation for VLM selection. Experiments\nacross various VLMs and image classification datasets validate SWAB's\neffectiveness.\n","authors":["Chao Yi","De-Chuan Zhan","Han-Jia Ye"],"pdf_url":"https://arxiv.org/pdf/2403.13797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13793v1","updated":"2024-03-20T17:54:26Z","published":"2024-03-20T17:54:26Z","title":"Evaluating Frontier Models for Dangerous Capabilities","summary":"  To understand the risks posed by a new AI system, we must understand what it\ncan and cannot do. Building on prior work, we introduce a programme of new\n\"dangerous capability\" evaluations and pilot them on Gemini 1.0 models. Our\nevaluations cover four areas: (1) persuasion and deception; (2) cyber-security;\n(3) self-proliferation; and (4) self-reasoning. We do not find evidence of\nstrong dangerous capabilities in the models we evaluated, but we flag early\nwarning signs. Our goal is to help advance a rigorous science of dangerous\ncapability evaluation, in preparation for future models.\n","authors":["Mary Phuong","Matthew Aitchison","Elliot Catt","Sarah Cogan","Alexandre Kaskasoli","Victoria Krakovna","David Lindner","Matthew Rahtz","Yannis Assael","Sarah Hodkinson","Heidi Howard","Tom Lieberum","Ramana Kumar","Maria Abi Raad","Albert Webson","Lewis Ho","Sharon Lin","Sebastian Farquhar","Marcus Hutter","Gregoire Deletang","Anian Ruoss","Seliem El-Sayed","Sasha Brown","Anca Dragan","Rohin Shah","Allan Dafoe","Toby Shevlane"],"pdf_url":"https://arxiv.org/pdf/2403.13793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13787v1","updated":"2024-03-20T17:49:54Z","published":"2024-03-20T17:49:54Z","title":"RewardBench: Evaluating Reward Models for Language Modeling","summary":"  Reward models (RMs) are at the crux of successful RLHF to align pretrained\nmodels to human preferences, yet there has been relatively little study that\nfocuses on evaluation of those reward models. Evaluating reward models presents\nan opportunity to understand the opaque technologies used for alignment of\nlanguage models and which values are embedded in them. To date, very few\ndescriptors of capabilities, training methods, or open-source reward models\nexist. In this paper, we present RewardBench, a benchmark dataset and code-base\nfor evaluation, to enhance scientific understanding of reward models. The\nRewardBench dataset is a collection of prompt-win-lose trios spanning chat,\nreasoning, and safety, to benchmark how reward models perform on challenging,\nstructured and out-of-distribution queries. We created specific comparison\ndatasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect\nfacts) why one answer should be preferred to another. On the RewardBench\nleaderboard, we evaluate reward models trained with a variety of methods, such\nas the direct MLE training of classifiers and the implicit reward modeling of\nDirect Preference Optimization (DPO), and on a spectrum of datasets. We present\nmany findings on propensity for refusals, reasoning limitations, and\ninstruction following shortcomings of various reward models towards a better\nunderstanding of the RLHF process.\n","authors":["Nathan Lambert","Valentina Pyatkin","Jacob Morrison","LJ Miranda","Bill Yuchen Lin","Khyathi Chandu","Nouha Dziri","Sachin Kumar","Tom Zick","Yejin Choi","Noah A. Smith","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2403.13787v1.pdf","comment":"40 pages, 19 figures, 12 tables"},{"id":"http://arxiv.org/abs/2403.13785v1","updated":"2024-03-20T17:47:25Z","published":"2024-03-20T17:47:25Z","title":"Towards an extension of Fault Trees in the Predictive Maintenance\n  Scenario","summary":"  One of the most appreciated features of Fault Trees (FTs) is their\nsimplicity, making them fit into industrial processes. As such processes evolve\nin time, considering new aspects of large modern systems, modelling techniques\nbased on FTs have adapted to these needs. This paper proposes an extension of\nFTs to take into account the problem of Predictive Maintenance, one of the\nchallenges of the modern dependability field of study. The paper sketches the\nPredictive Fault Tree language and proposes some use cases to support their\nmodelling and analysis in concrete industrial settings.\n","authors":["Roberta De Fazio","Stefano Marrone","Laura Verde","Vincenzo Reccia","Paolo Valletta"],"pdf_url":"https://arxiv.org/pdf/2403.13785v1.pdf","comment":"S. Bernardi, T. Zoppi (Editors), Fast Abstracts and Student Forum\n  Proceedings - EDCC 2024 - 19th European Dependable Computing Conference,\n  Leuven, Belgium, 8-11 April 2024"},{"id":"http://arxiv.org/abs/2403.13784v1","updated":"2024-03-20T17:47:08Z","published":"2024-03-20T17:47:08Z","title":"The Model Openness Framework: Promoting Completeness and Openness for\n  Reproducibility, Transparency and Usability in AI","summary":"  Generative AI (GAI) offers unprecedented possibilities but its\ncommercialization has raised concerns about transparency, reproducibility,\nbias, and safety. Many \"open-source\" GAI models lack the necessary components\nfor full understanding and reproduction, and some use restrictive licenses, a\npractice known as \"openwashing.\" We propose the Model Openness Framework (MOF),\na ranked classification system that rates machine learning models based on\ntheir completeness and openness, following principles of open science, open\nsource, open data, and open access. The MOF requires specific components of the\nmodel development lifecycle to be included and released under appropriate open\nlicenses. This framework aims to prevent misrepresentation of models claiming\nto be open, guide researchers and developers in providing all model components\nunder permissive licenses, and help companies, academia, and hobbyists identify\nmodels that can be safely adopted without restrictions. Wide adoption of the\nMOF will foster a more open AI ecosystem, accelerating research, innovation,\nand adoption.\n","authors":["Matt White","Ibrahim Haddad","Cailean Osborne"," Xiao-Yang"," Liu","Ahmed Abdelmonsef","Sachin Varghese"],"pdf_url":"https://arxiv.org/pdf/2403.13784v1.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2403.13781v1","updated":"2024-03-20T17:43:58Z","published":"2024-03-20T17:43:58Z","title":"Sparse Implementation of Versatile Graph-Informed Layers","summary":"  Graph Neural Networks (GNNs) have emerged as effective tools for learning\ntasks on graph-structured data. Recently, Graph-Informed (GI) layers were\nintroduced to address regression tasks on graph nodes, extending their\napplicability beyond classic GNNs. However, existing implementations of GI\nlayers lack efficiency due to dense memory allocation. This paper presents a\nsparse implementation of GI layers, leveraging the sparsity of adjacency\nmatrices to reduce memory usage significantly. Additionally, a versatile\ngeneral form of GI layers is introduced, enabling their application to subsets\nof graph nodes. The proposed sparse implementation improves the concrete\ncomputational efficiency and scalability of the GI layers, permitting to build\ndeeper Graph-Informed Neural Networks (GINNs) and facilitating their\nscalability to larger graphs.\n","authors":["Francesco Della Santa"],"pdf_url":"https://arxiv.org/pdf/2403.13781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06192v5","updated":"2024-03-20T17:36:07Z","published":"2023-06-09T18:45:15Z","title":"Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy\n  Learning for Robotic Navigation","summary":"  Trajectory length stands as a crucial hyperparameter within reinforcement\nlearning (RL) algorithms, significantly contributing to the sample inefficiency\nin robotics applications. Motivated by the pivotal role trajectory length plays\nin the training process, we introduce Ada-NAV, a novel adaptive trajectory\nlength scheme designed to enhance the training sample efficiency of RL\nalgorithms in robotic navigation tasks. Unlike traditional approaches that\ntreat trajectory length as a fixed hyperparameter, we propose to dynamically\nadjust it based on the entropy of the underlying navigation policy.\nInterestingly, Ada-NAV can be applied to both existing on-policy and off-policy\nRL methods, which we demonstrate by empirically validating its efficacy on\nthree popular RL methods: REINFORCE, Proximal Policy Optimization (PPO), and\nSoft Actor-Critic (SAC). We demonstrate through simulated and real-world\nrobotic experiments that Ada-NAV outperforms conventional methods that employ\nconstant or randomly sampled trajectory lengths. Specifically, for a fixed\nsample budget, Ada-NAV achieves an 18\\% increase in navigation success rate, a\n20-38\\% reduction in navigation path length, and a 9.32\\% decrease in elevation\ncosts. Furthermore, we showcase the versatility of Ada-NAV by integrating it\nwith the Clearpath Husky robot, illustrating its applicability in complex\noutdoor environments.\n","authors":["Bhrij Patel","Kasun Weerakoon","Wesley A. Suttle","Alec Koppel","Brian M. Sadler","Tianyi Zhou","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2306.06192v5.pdf","comment":"11 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.13771v1","updated":"2024-03-20T17:33:02Z","published":"2024-03-20T17:33:02Z","title":"Describe-and-Dissect: Interpreting Neurons in Vision Networks with\n  Language Models","summary":"  In this paper, we propose Describe-and-Dissect (DnD), a novel method to\ndescribe the roles of hidden neurons in vision networks. DnD utilizes recent\nadvancements in multimodal deep learning to produce complex natural language\ndescriptions, without the need for labeled training data or a predefined set of\nconcepts to choose from. Additionally, DnD is training-free, meaning we don't\ntrain any new models and can easily leverage more capable general purpose\nmodels in the future. We have conducted extensive qualitative and quantitative\nanalysis to show that DnD outperforms prior work by providing higher quality\nneuron descriptions. Specifically, our method on average provides the highest\nquality labels and is more than 2 times as likely to be selected as the best\nexplanation for a neuron than the best baseline.\n","authors":["Nicholas Bai","Rahul A. Iyer","Tuomas Oikarinen","Tsui-Wei Weng"],"pdf_url":"https://arxiv.org/pdf/2403.13771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13765v1","updated":"2024-03-20T17:28:17Z","published":"2024-03-20T17:28:17Z","title":"Towards Principled Representation Learning from Videos for Reinforcement\n  Learning","summary":"  We study pre-training representations for decision-making using video data,\nwhich is abundantly available for tasks such as game agents and software\ntesting. Even though significant empirical advances have been made on this\nproblem, a theoretical understanding remains absent. We initiate the\ntheoretical investigation into principled approaches for representation\nlearning and focus on learning the latent state representations of the\nunderlying MDP using video data. We study two types of settings: one where\nthere is iid noise in the observation, and a more challenging setting where\nthere is also the presence of exogenous noise, which is non-iid noise that is\ntemporally correlated, such as the motion of people or cars in the background.\nWe study three commonly used approaches: autoencoding, temporal contrastive\nlearning, and forward modeling. We prove upper bounds for temporal contrastive\nlearning and forward modeling in the presence of only iid noise. We show that\nthese approaches can learn the latent state and use it to do efficient\ndownstream RL with polynomial sample complexity. When exogenous noise is also\npresent, we establish a lower bound result showing that the sample complexity\nof learning from video data can be exponentially worse than learning from\naction-labeled trajectory data. This partially explains why reinforcement\nlearning with video pre-training is hard. We evaluate these representational\nlearning methods in two visual domains, yielding results that are consistent\nwith our theoretical findings.\n","authors":["Dipendra Misra","Akanksha Saran","Tengyang Xie","Alex Lamb","John Langford"],"pdf_url":"https://arxiv.org/pdf/2403.13765v1.pdf","comment":"ICLR 2024 Spotlight Conference Paper"},{"id":"http://arxiv.org/abs/2305.17282v5","updated":"2024-03-20T17:25:52Z","published":"2023-05-26T22:01:47Z","title":"Universal consistency of the $k$-NN rule in metric spaces and Nagata\n  dimension. II","summary":"  We continue to investigate the $k$ nearest neighbour ($k$-NN) learning rule\nin complete separable metric spaces. Thanks to the results of C\\'erou and\nGuyader (2006) and Preiss (1983), this rule is known to be universally\nconsistent in every such metric space that is sigma-finite dimensional in the\nsense of Nagata. Here we show that the rule is strongly universally consistent\nin such spaces in the absence of ties. Under the tie-breaking strategy applied\nby Devroye, Gy\\\"{o}rfi, Krzy\\.{z}ak, and Lugosi (1994) in the Euclidean\nsetting, we manage to show the strong universal consistency in non-Archimedian\nmetric spaces (that is, those of Nagata dimension zero). Combining the theorem\nof C\\'erou and Guyader with results of Assouad and Quentin de Gromard (2006),\none deduces that the $k$-NN rule is universally consistent in metric spaces\nhaving finite dimension in the sense of de Groot. In particular, the $k$-NN\nrule is universally consistent in the Heisenberg group which is not\nsigma-finite dimensional in the sense of Nagata as follows from an example\nindependently constructed by Kor\\'anyi and Reimann (1995) and Sawyer and\nWheeden (1992).\n","authors":["Sushma Kumari","Vladimir G. Pestov"],"pdf_url":"https://arxiv.org/pdf/2305.17282v5.pdf","comment":"Latex 2e, 27 pages, 1 figure. Minor revisions to conform with the\n  last set of journal page proofs: two typos corrected, the bibliography\n  rearranged in the order of citations (the ESAIM:PS home style), and two\n  articles that were no longer cited removed"},{"id":"http://arxiv.org/abs/2305.14456v4","updated":"2024-03-20T17:16:37Z","published":"2023-05-23T18:27:51Z","title":"Having Beer after Prayer? Measuring Cultural Bias in Large Language\n  Models","summary":"  As the reach of large language models (LMs) expands globally, their ability\nto cater to diverse cultural contexts becomes crucial. Despite advancements in\nmultilingual capabilities, models are not designed with appropriate cultural\nnuances. In this paper, we show that multilingual and Arabic monolingual LMs\nexhibit bias towards entities associated with Western culture. We introduce\nCAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities\nspanning eight types that contrast Arab and Western cultures. CAMeL provides a\nfoundation for measuring cultural biases in LMs through both extrinsic and\nintrinsic evaluations. Using CAMeL, we examine the cross-cultural performance\nin Arabic of 16 different LMs on tasks such as story generation, NER, and\nsentiment analysis, where we find concerning cases of stereotyping and cultural\nunfairness. We further test their text-infilling performance, revealing the\nincapability of appropriate adaptation to Arab cultural contexts. Finally, we\nanalyze 6 Arabic pre-training corpora and find that commonly used sources such\nas Wikipedia may not be best suited to build culturally aware LMs, if used as\nthey are without adjustment. We will make CAMeL publicly available at:\nhttps://github.com/tareknaous/camel\n","authors":["Tarek Naous","Michael J. Ryan","Alan Ritter","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2305.14456v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2002.10121v4","updated":"2024-03-20T17:15:32Z","published":"2020-02-24T08:59:34Z","title":"The Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed\n  Bandit with Many Arms","summary":"  We investigate a Bayesian $k$-armed bandit problem in the \\emph{many-armed}\nregime, where $k \\geq \\sqrt{T}$ and $T$ represents the time horizon. Initially,\nand aligned with recent literature on many-armed bandit problems, we observe\nthat subsampling plays a key role in designing optimal algorithms; the\nconventional UCB algorithm is sub-optimal, whereas a subsampled UCB (SS-UCB),\nwhich selects $\\Theta(\\sqrt{T})$ arms for execution under the UCB framework,\nachieves rate-optimality. However, despite SS-UCB's theoretical promise of\noptimal regret, it empirically underperforms compared to a greedy algorithm\nthat consistently chooses the empirically best arm. This observation extends to\ncontextual settings through simulations with real-world data. Our findings\nsuggest a new form of \\emph{free exploration} beneficial to greedy algorithms\nin the many-armed context, fundamentally linked to a tail event concerning the\nprior distribution of arm rewards. This finding diverges from the notion of\nfree exploration, which relates to covariate variation, as recently discussed\nin contextual bandit literature. Expanding upon these insights, we establish\nthat the subsampled greedy approach not only achieves rate-optimality for\nBernoulli bandits within the many-armed regime but also attains sublinear\nregret across broader distributions. Collectively, our research indicates that\nin the many-armed regime, practitioners might find greater value in adopting\ngreedy algorithms.\n","authors":["Mohsen Bayati","Nima Hamidi","Ramesh Johari","Khashayar Khosravi"],"pdf_url":"https://arxiv.org/pdf/2002.10121v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13749v1","updated":"2024-03-20T16:58:28Z","published":"2024-03-20T16:58:28Z","title":"Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph\n  Representational Learning","summary":"  We introduce $r$-loopy Weisfeiler-Leman ($r$-$\\ell{}$WL), a novel hierarchy\nof graph isomorphism tests and a corresponding GNN framework, $r$-$\\ell{}$MPNN,\nthat can count cycles up to length $r + 2$. Most notably, we show that\n$r$-$\\ell{}$WL can count homomorphisms of cactus graphs. This strictly extends\nclassical 1-WL, which can only count homomorphisms of trees and, in fact, is\nincomparable to $k$-WL for any fixed $k$. We empirically validate the\nexpressive and counting power of the proposed $r$-$\\ell{}$MPNN on several\nsynthetic datasets and present state-of-the-art predictive performance on\nvarious real-world datasets. The code is available at\nhttps://github.com/RPaolino/loopy\n","authors":["Raffaele Paolino","Sohir Maskey","Pascal Welke","Gitta Kutyniok"],"pdf_url":"https://arxiv.org/pdf/2403.13749v1.pdf","comment":"Accepted at ICLR 2024 Workshop on Bridging the Gap Between Practice\n  and Theory in Deep Learning"},{"id":"http://arxiv.org/abs/2403.13748v1","updated":"2024-03-20T16:56:08Z","published":"2024-03-20T16:56:08Z","title":"An Ordering of Divergences for Variational Inference with Factorized\n  Gaussian Approximations","summary":"  Given an intractable distribution $p$, the problem of variational inference\n(VI) is to compute the best approximation $q$ from some more tractable family\n$\\mathcal{Q}$. Most commonly the approximation is found by minimizing a\nKullback-Leibler (KL) divergence. However, there exist other valid choices of\ndivergences, and when $\\mathcal{Q}$ does not contain~$p$, each divergence\nchampions a different solution. We analyze how the choice of divergence affects\nthe outcome of VI when a Gaussian with a dense covariance matrix is\napproximated by a Gaussian with a diagonal covariance matrix. In this setting\nwe show that different divergences can be \\textit{ordered} by the amount that\ntheir variational approximations misestimate various measures of uncertainty,\nsuch as the variance, precision, and entropy. We also derive an impossibility\ntheorem showing that no two of these measures can be simultaneously matched by\na factorized approximation; hence, the choice of divergence informs which\nmeasure, if any, is correctly estimated. Our analysis covers the KL divergence,\nthe R\\'enyi divergences, and a score-based divergence that compares $\\nabla\\log\np$ and $\\nabla\\log q$. We empirically evaluate whether these orderings hold\nwhen VI is used to approximate non-Gaussian distributions.\n","authors":["Charles C. Margossian","Loucas Pillaud-Vivien","Lawrence K. Saul"],"pdf_url":"https://arxiv.org/pdf/2403.13748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05666v5","updated":"2024-03-20T16:50:25Z","published":"2023-02-11T11:56:06Z","title":"Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels","summary":"  Intersection over Union (IoU) losses are surrogates that directly optimize\nthe Jaccard index. Leveraging IoU losses as part of the loss function have\ndemonstrated superior performance in semantic segmentation tasks compared to\noptimizing pixel-wise losses such as the cross-entropy loss alone. However, we\nidentify a lack of flexibility in these losses to support vital training\ntechniques like label smoothing, knowledge distillation, and semi-supervised\nlearning, mainly due to their inability to process soft labels. To address\nthis, we introduce Jaccard Metric Losses (JMLs), which are identical to the\nsoft Jaccard loss in standard settings with hard labels but are fully\ncompatible with soft labels. We apply JMLs to three prominent use cases of soft\nlabels: label smoothing, knowledge distillation and semi-supervised learning,\nand demonstrate their potential to enhance model accuracy and calibration. Our\nexperiments show consistent improvements over the cross-entropy loss across 4\nsemantic segmentation datasets (Cityscapes, PASCAL VOC, ADE20K, DeepGlobe Land)\nand 13 architectures, including classic CNNs and recent vision transformers.\nRemarkably, our straightforward approach significantly outperforms\nstate-of-the-art knowledge distillation and semi-supervised learning methods.\nThe code is available at\n\\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.\n","authors":["Zifu Wang","Xuefei Ning","Matthew B. Blaschko"],"pdf_url":"https://arxiv.org/pdf/2302.05666v5.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2403.13740v1","updated":"2024-03-20T16:47:28Z","published":"2024-03-20T16:47:28Z","title":"Uncertainty-Aware Explanations Through Probabilistic Self-Explainable\n  Neural Networks","summary":"  The lack of transparency of Deep Neural Networks continues to be a limitation\nthat severely undermines their reliability and usage in high-stakes\napplications. Promising approaches to overcome such limitations are\nPrototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions\nrely on the similarity between the input at hand and a set of prototypical\nrepresentations of the output classes, offering therefore a deep, yet\ntransparent-by-design, architecture. So far, such models have been designed by\nconsidering pointwise estimates for the prototypes, which remain fixed after\nthe learning phase of the model. In this paper, we introduce a probabilistic\nreformulation of PSENNs, called Prob-PSENN, which replaces point estimates for\nthe prototypes with probability distributions over their values. This provides\nnot only a more flexible framework for an end-to-end learning of prototypes,\nbut can also capture the explanatory uncertainty of the model, which is a\nmissing feature in previous approaches. In addition, since the prototypes\ndetermine both the explanation and the prediction, Prob-PSENNs allow us to\ndetect when the model is making uninformed or uncertain predictions, and to\nobtain valid explanations for them. Our experiments demonstrate that\nProb-PSENNs provide more meaningful and robust explanations than their\nnon-probabilistic counterparts, thus enhancing the explainability and\nreliability of the models.\n","authors":["Jon Vadillo","Roberto Santana","Jose A. Lozano","Marta Kwiatkowska"],"pdf_url":"https://arxiv.org/pdf/2403.13740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.14961v3","updated":"2024-03-20T16:45:00Z","published":"2021-10-28T09:03:37Z","title":"Roto-translated Local Coordinate Frames For Interacting Dynamical\n  Systems","summary":"  Modelling interactions is critical in learning complex dynamical systems,\nnamely systems of interacting objects with highly non-linear and time-dependent\nbehaviour. A large class of such systems can be formalized as\n$\\textit{geometric graphs}$, $\\textit{i.e.}$, graphs with nodes positioned in\nthe Euclidean space given an $\\textit{arbitrarily}$ chosen global coordinate\nsystem, for instance vehicles in a traffic scene. Notwithstanding the arbitrary\nglobal coordinate system, the governing dynamics of the respective dynamical\nsystems are invariant to rotations and translations, also known as\n$\\textit{Galilean invariance}$. As ignoring these invariances leads to worse\ngeneralization, in this work we propose local coordinate frames per node-object\nto induce roto-translation invariance to the geometric graph of the interacting\ndynamical system. Further, the local coordinate frames allow for a natural\ndefinition of anisotropic filtering in graph neural networks. Experiments in\ntraffic scenes, 3D motion capture, and colliding particles demonstrate that the\nproposed approach comfortably outperforms the recent state-of-the-art.\n","authors":["Miltiadis Kofinas","Naveen Shankar Nagaraja","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2110.14961v3.pdf","comment":"In NeurIPS 2021. Source code: https://github.com/mkofinas/locs"},{"id":"http://arxiv.org/abs/2403.13729v1","updated":"2024-03-20T16:39:17Z","published":"2024-03-20T16:39:17Z","title":"Reinforcement Learning for Online Testing of Autonomous Driving Systems:\n  a Replication and Extension Study","summary":"  In a recent study, Reinforcement Learning (RL) used in combination with\nmany-objective search, has been shown to outperform alternative techniques\n(random search and many-objective search) for online testing of Deep Neural\nNetwork-enabled systems. The empirical evaluation of these techniques was\nconducted on a state-of-the-art Autonomous Driving System (ADS). This work is a\nreplication and extension of that empirical study. Our replication shows that\nRL does not outperform pure random test generation in a comparison conducted\nunder the same settings of the original study, but with no confounding factor\ncoming from the way collisions are measured. Our extension aims at eliminating\nsome of the possible reasons for the poor performance of RL observed in our\nreplication: (1) the presence of reward components providing contrasting or\nuseless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)\nwhich requires discretization of an intrinsically continuous state space.\nResults show that our new RL agent is able to converge to an effective policy\nthat outperforms random testing. Results also highlight other possible\nimprovements, which open to further investigations on how to best leverage RL\nfor online ADS testing.\n","authors":["Luca Giamattei","Matteo Biagiola","Roberto Pietrantuono","Stefano Russo","Paolo Tonella"],"pdf_url":"https://arxiv.org/pdf/2403.13729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13728v1","updated":"2024-03-20T16:38:26Z","published":"2024-03-20T16:38:26Z","title":"M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via\n  Multiplier Induced Loss Landscape Scheduling","summary":"  When a neural network parameterized loss function consists of many terms, the\ncombinatorial choice of weight multipliers during the optimization process\nforms a challenging problem. To address this, we proposed a probabilistic\ngraphical model (PGM) for the joint model parameter and multiplier evolution\nprocess, with a hypervolume based likelihood that promotes multi-objective\ndescent of each loss term. The corresponding parameter and multiplier\nestimation as a sequential decision process is then cast into an optimal\ncontrol problem, where the multi-objective descent goal is dispatched\nhierarchically into a series of constraint optimization sub-problems. The\nsub-problem constraint automatically adapts itself according to Pareto\ndominance and serves as the setpoint for the low level multiplier controller to\nschedule loss landscapes via output feedback of each loss term. Our method is\nmultiplier-free and operates at the timescale of epochs, thus saves tremendous\ncomputational resources compared to full training cycle multiplier tuning. We\napplied it to domain invariant variational auto-encoding with 6 loss terms on\nthe PACS domain generalization task, and observed robust performance across a\nrange of controller hyperparameters, as well as different multiplier initial\nconditions, outperforming other multiplier scheduling methods. We offered\nmodular implementation of our method, admitting custom definition of many loss\nterms for applying our multi-objective hierarchical output feedback training\nscheme to other deep learning fields.\n","authors":["Xudong Sun","Nutan Chen","Alexej Gossmann","Yu Xing","Carla Feistner","Emilio Dorigatt","Felix Drost","Daniele Scarcella","Lisa Beer","Carsten Marr"],"pdf_url":"https://arxiv.org/pdf/2403.13728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19212v3","updated":"2024-03-20T16:34:27Z","published":"2024-02-29T14:41:31Z","title":"Deep Reinforcement Learning: A Convex Optimization Approach","summary":"  In this paper, we consider reinforcement learning of nonlinear systems with\ncontinuous state and action spaces. We present an episodic learning algorithm,\nwhere we for each episode use convex optimization to find a two-layer neural\nnetwork approximation of the optimal $Q$-function. The convex optimization\napproach guarantees that the weights calculated at each episode are optimal,\nwith respect to the given sampled states and actions of the current episode.\nFor stable nonlinear systems, we show that the algorithm converges and that the\nconverging parameters of the trained neural network can be made arbitrarily\nclose to the optimal neural network parameters. In particular, if the\nregularization parameter is $\\rho$ and the time horizon is $T$, then the\nparameters of the trained neural network converge to $w$, where the distance\nbetween $w$ from the optimal parameters $w^\\star$ is bounded by\n$\\mathcal{O}(\\rho T^{-1})$. That is, when the number of episodes goes to\ninfinity, there exists a constant $C$ such that \\[\\|w-w^\\star\\| \\le\nC\\cdot\\frac{\\rho}{T}.\\] In particular, our algorithm converges arbitrarily\nclose to the optimal neural network parameters as the time horizon increases or\nas the regularization parameter decreases.\n","authors":["Ather Gattami"],"pdf_url":"https://arxiv.org/pdf/2402.19212v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13724v1","updated":"2024-03-20T16:33:06Z","published":"2024-03-20T16:33:06Z","title":"Probabilistic Forecasting with Stochastic Interpolants and Föllmer\n  Processes","summary":"  We propose a framework for probabilistic forecasting of dynamical systems\nbased on generative modeling. Given observations of the system state over time,\nwe formulate the forecasting problem as sampling from the conditional\ndistribution of the future system state given its current state. To this end,\nwe leverage the framework of stochastic interpolants, which facilitates the\nconstruction of a generative model between an arbitrary base distribution and\nthe target. We design a fictitious, non-physical stochastic dynamics that takes\nas initial condition the current system state and produces as output a sample\nfrom the target conditional distribution in finite time and without bias. This\nprocess therefore maps a point mass centered at the current state onto a\nprobabilistic ensemble of forecasts. We prove that the drift coefficient\nentering the stochastic differential equation (SDE) achieving this task is\nnon-singular, and that it can be learned efficiently by square loss regression\nover the time-series data. We show that the drift and the diffusion\ncoefficients of this SDE can be adjusted after training, and that a specific\nchoice that minimizes the impact of the estimation error gives a F\\\"ollmer\nprocess. We highlight the utility of our approach on several complex,\nhigh-dimensional forecasting problems, including stochastically forced\nNavier-Stokes and video prediction on the KTH and CLEVRER datasets.\n","authors":["Yifan Chen","Mark Goldstein","Mengjian Hua","Michael S. Albergo","Nicholas M. Boffi","Eric Vanden-Eijnden"],"pdf_url":"https://arxiv.org/pdf/2403.13724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18430v3","updated":"2024-03-20T16:32:42Z","published":"2023-10-27T19:02:22Z","title":"MCRAGE: Synthetic Healthcare Data for Fairness","summary":"  In the field of healthcare, electronic health records (EHR) serve as crucial\ntraining data for developing machine learning models for diagnosis, treatment,\nand the management of healthcare resources. However, medical datasets are often\nimbalanced in terms of sensitive attributes such as race/ethnicity, gender, and\nage. Machine learning models trained on class-imbalanced EHR datasets perform\nsignificantly worse in deployment for individuals of the minority classes\ncompared to those from majority classes, which may lead to inequitable\nhealthcare outcomes for minority groups. To address this challenge, we propose\nMinority Class Rebalancing through Augmentation by Generative modeling\n(MCRAGE), a novel approach to augment imbalanced datasets using samples\ngenerated by a deep generative model. The MCRAGE process involves training a\nConditional Denoising Diffusion Probabilistic Model (CDDPM) capable of\ngenerating high-quality synthetic EHR samples from underrepresented classes. We\nuse this synthetic data to augment the existing imbalanced dataset, resulting\nin a more balanced distribution across all classes, which can be used to train\nless biased downstream models. We measure the performance of MCRAGE versus\nalternative approaches using Accuracy, F1 score and AUROC of these downstream\nmodels. We provide theoretical justification for our method in terms of recent\nconvergence results for DDPMs.\n","authors":["Keira Behal","Jiayi Chen","Caleb Fikes","Sophia Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.18430v3.pdf","comment":"Keywords: synthetic electronic health records, conditional denoising\n  diffusion probabilistic model, healthcare AI, tabular data, fairness,\n  synthetic data. This paper is the result of work completed at the 2023 Emory\n  University Department of Mathematics REU/RET program under the direction of\n  Project Advisor Dr. Xi Yuanzhe. This work is sponsored by NSF DMS 2051019"},{"id":"http://arxiv.org/abs/2310.13805v2","updated":"2024-03-20T16:23:20Z","published":"2023-10-20T20:32:43Z","title":"Normalizing flow-based deep variational Bayesian network for seismic\n  multi-hazards and impacts estimation from InSAR imagery","summary":"  Onsite disasters like earthquakes can trigger cascading hazards and impacts,\nsuch as landslides and infrastructure damage, leading to catastrophic losses;\nthus, rapid and accurate estimates are crucial for timely and effective\npost-disaster responses. Interferometric Synthetic aperture radar (InSAR) data\nis important in providing high-resolution onsite information for rapid hazard\nestimation. Most recent methods using InSAR imagery signals predict a single\ntype of hazard and thus often suffer low accuracy due to noisy and complex\nsignals induced by co-located hazards, impacts, and irrelevant environmental\nchanges (e.g., vegetation changes, human activities). We introduce a novel\nstochastic variational inference with normalizing flows derived to jointly\napproximate posteriors of multiple unobserved hazards and impacts from noisy\nInSAR imagery.\n","authors":["Xuechun Li","Paula M. Burgi","Wei Ma","Hae Young Noh","David J. Wald","Susu Xu"],"pdf_url":"https://arxiv.org/pdf/2310.13805v2.pdf","comment":"This paper needs to be reviewed by the USGS"},{"id":"http://arxiv.org/abs/2403.12143v2","updated":"2024-03-20T16:12:12Z","published":"2024-03-18T18:01:01Z","title":"Graph Neural Networks for Learning Equivariant Representations of Neural\n  Networks","summary":"  Neural networks that process the parameters of other neural networks find\napplications in domains as diverse as classifying implicit neural\nrepresentations, generating neural network weights, and predicting\ngeneralization errors. However, existing approaches either overlook the\ninherent permutation symmetry in the neural network or rely on intricate\nweight-sharing patterns to achieve equivariance, while ignoring the impact of\nthe network architecture itself. In this work, we propose to represent neural\nnetworks as computational graphs of parameters, which allows us to harness\npowerful graph neural networks and transformers that preserve permutation\nsymmetry. Consequently, our approach enables a single model to encode neural\ncomputational graphs with diverse architectures. We showcase the effectiveness\nof our method on a wide range of tasks, including classification and editing of\nimplicit neural representations, predicting generalization performance, and\nlearning to optimize, while consistently outperforming state-of-the-art\nmethods. The source code is open-sourced at\nhttps://github.com/mkofinas/neural-graphs.\n","authors":["Miltiadis Kofinas","Boris Knyazev","Yan Zhang","Yunlu Chen","Gertjan J. Burghouts","Efstratios Gavves","Cees G. M. Snoek","David W. Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12143v2.pdf","comment":"In ICLR 2024. Source code: https://github.com/mkofinas/neural-graphs"},{"id":"http://arxiv.org/abs/2403.13704v1","updated":"2024-03-20T16:08:27Z","published":"2024-03-20T16:08:27Z","title":"Improving the Adaptive Moment Estimation (ADAM) stochastic optimizer\n  through an Implicit-Explicit (IMEX) time-stepping approach","summary":"  The Adam optimizer, often used in Machine Learning for neural network\ntraining, corresponds to an underlying ordinary differential equation (ODE) in\nthe limit of very small learning rates. This work shows that the classical Adam\nalgorithm is a first order implicit-explicit (IMEX) Euler discretization of the\nunderlying ODE. Employing the time discretization point of view, we propose new\nextensions of the Adam scheme obtained by using higher order IMEX methods to\nsolve the ODE. Based on this approach, we derive a new optimization algorithm\nfor neural network training that performs better than classical Adam on several\nregression and classification problems.\n","authors":["Abhinab Bhattacharjee","Andrey A. Popov","Arash Sarshar","Adrian Sandu"],"pdf_url":"https://arxiv.org/pdf/2403.13704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13701v1","updated":"2024-03-20T16:06:01Z","published":"2024-03-20T16:06:01Z","title":"What Matters for Active Texture Recognition With Vision-Based Tactile\n  Sensors","summary":"  This paper explores active sensing strategies that employ vision-based\ntactile sensors for robotic perception and classification of fabric textures.\nWe formalize the active sampling problem in the context of tactile fabric\nrecognition and provide an implementation of information-theoretic exploration\nstrategies based on minimizing predictive entropy and variance of probabilistic\nmodels. Through ablation studies and human experiments, we investigate which\ncomponents are crucial for quick and reliable texture recognition. Along with\nthe active sampling strategies, we evaluate neural network architectures,\nrepresentations of uncertainty, influence of data augmentation, and dataset\nvariability. By evaluating our method on a previously published Active Clothing\nPerception Dataset and on a real robotic system, we establish that the choice\nof the active exploration strategy has only a minor influence on the\nrecognition accuracy, whereas data augmentation and dropout rate play a\nsignificantly larger role. In a comparison study, while humans achieve 66.9%\nrecognition accuracy, our best approach reaches 90.0% in under 5 touches,\nhighlighting that vision-based tactile sensors are highly effective for fabric\ntexture recognition.\n","authors":["Alina Böhm","Tim Schneider","Boris Belousov","Alap Kshirsagar","Lisa Lin","Katja Doerschner","Knut Drewing","Constantin A. Rothkopf","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2403.13701v1.pdf","comment":"7 pages, 9 figures, accepted at 2024 IEEE International Conference on\n  Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2310.20679v2","updated":"2024-03-20T16:05:03Z","published":"2023-10-31T17:45:39Z","title":"Latent Field Discovery In Interacting Dynamical Systems With Neural\n  Fields","summary":"  Systems of interacting objects often evolve under the influence of field\neffects that govern their dynamics, yet previous works have abstracted away\nfrom such effects, and assume that systems evolve in a vacuum. In this work, we\nfocus on discovering these fields, and infer them from the observed dynamics\nalone, without directly observing them. We theorize the presence of latent\nforce fields, and propose neural fields to learn them. Since the observed\ndynamics constitute the net effect of local object interactions and global\nfield effects, recently popularized equivariant networks are inapplicable, as\nthey fail to capture global information. To address this, we propose to\ndisentangle local object interactions -- which are $\\mathrm{SE}(n)$ equivariant\nand depend on relative states -- from external global field effects -- which\ndepend on absolute states. We model interactions with equivariant graph\nnetworks, and combine them with neural fields in a novel graph network that\nintegrates field forces. Our experiments show that we can accurately discover\nthe underlying fields in charged particles settings, traffic scenes, and\ngravitational n-body problems, and effectively use them to learn the system and\nforecast future trajectories.\n","authors":["Miltiadis Kofinas","Erik J. Bekkers","Naveen Shankar Nagaraja","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2310.20679v2.pdf","comment":"In NeurIPS 2023. Source code: https://github.com/mkofinas/aether"},{"id":"http://arxiv.org/abs/2403.13695v1","updated":"2024-03-20T15:57:44Z","published":"2024-03-20T15:57:44Z","title":"Loss Regularizing Robotic Terrain Classification","summary":"  Locomotion mechanics of legged robots are suitable when pacing through\ndifficult terrains. Recognising terrains for such robots are important to fully\nyoke the versatility of their movements. Consequently, robotic terrain\nclassification becomes significant to classify terrains in real time with high\naccuracy. The conventional classifiers suffer from overfitting problem, low\naccuracy problem, high variance problem, and not suitable for live dataset. On\nthe other hand, classifying a growing dataset is difficult for convolution\nbased terrain classification. Supervised recurrent models are also not\npractical for this classification. Further, the existing recurrent\narchitectures are still evolving to improve accuracy of terrain classification\nbased on live variable-length sensory data collected from legged robots. This\npaper proposes a new semi-supervised method for terrain classification of\nlegged robots, avoiding preprocessing of long variable-length dataset. The\nproposed method has a stacked Long Short-Term Memory architecture, including a\nnew loss regularization. The proposed method solves the existing problems and\nimproves accuracy. Comparison with the existing architectures show the\nimprovements.\n","authors":["Shakti Deo Kumar","Sudhanshu Tripathi","Krishna Ujjwal","Sarvada Sakshi Jha","Suddhasil De"],"pdf_url":"https://arxiv.org/pdf/2403.13695v1.pdf","comment":"Preliminary draft of the work published in IEEE conference 2023"},{"id":"http://arxiv.org/abs/2303.16296v4","updated":"2024-03-20T15:52:49Z","published":"2023-03-28T20:35:38Z","title":"Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels","summary":"  The soft Dice loss (SDL) has taken a pivotal role in numerous automated\nsegmentation pipelines in the medical imaging community. Over the last years,\nsome reasons behind its superior functioning have been uncovered and further\noptimizations have been explored. However, there is currently no implementation\nthat supports its direct utilization in scenarios involving soft labels. Hence,\na synergy between the use of SDL and research leveraging the use of soft\nlabels, also in the context of model calibration, is still missing. In this\nwork, we introduce Dice semimetric losses (DMLs), which (i) are by design\nidentical to SDL in a standard setting with hard labels, but (ii) can be\nemployed in settings with soft labels. Our experiments on the public QUBIQ,\nLiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels\n(e.g. averaging, label smoothing, and knowledge distillation) over hard labels\n(e.g. majority voting and random selection). As a result, we obtain superior\nDice scores and model calibration, which supports the wider adoption of DMLs in\npractice. The code is available at https://github.com/zifuwanggg/JDTLosses\n","authors":["Zifu Wang","Teodora Popordanoska","Jeroen Bertels","Robin Lemmens","Matthew B. Blaschko"],"pdf_url":"https://arxiv.org/pdf/2303.16296v4.pdf","comment":"MICCAI 2023"},{"id":"http://arxiv.org/abs/2305.09098v2","updated":"2024-03-20T15:41:07Z","published":"2023-05-16T01:51:22Z","title":"Weight-Inherited Distillation for Task-Agnostic BERT Compression","summary":"  Knowledge Distillation (KD) is a predominant approach for BERT compression.\nPrevious KD-based methods focus on designing extra alignment losses for the\nstudent model to mimic the behavior of the teacher model. These methods\ntransfer the knowledge in an indirect way. In this paper, we propose a novel\nWeight-Inherited Distillation (WID), which directly transfers knowledge from\nthe teacher. WID does not require any additional alignment loss and trains a\ncompact student by inheriting the weights, showing a new perspective of\nknowledge distillation. Specifically, we design the row compactors and column\ncompactors as mappings and then compress the weights via structural\nre-parameterization. Experimental results on the GLUE and SQuAD benchmarks show\nthat WID outperforms previous state-of-the-art KD-based baselines. Further\nanalysis indicates that WID can also learn the attention patterns from the\nteacher model without any alignment loss on attention distributions. The code\nis available at https://github.com/wutaiqiang/WID-NAACL2024.\n","authors":["Taiqiang Wu","Cheng Hou","Shanshan Lao","Jiayi Li","Ngai Wong","Zhe Zhao","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2305.09098v2.pdf","comment":"9 pages, 4 figures, NAACL2024 findings"},{"id":"http://arxiv.org/abs/2403.13681v1","updated":"2024-03-20T15:39:54Z","published":"2024-03-20T15:39:54Z","title":"PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned\n  Language Model for Indian Legal Case Documents","summary":"  In this paper, we present PARAMANU-AYN, a language model based exclusively on\ncase documents of the Supreme Court of India, the Constitution of India, and\nthe Indian Penal Code. The novel Auto Regressive (AR) decoder based model is\npretrained from scratch at a context size of 8192. We evaluated our pretrained\nlegal model on perplexity metrics. We also instruction-tuned our pretrained\nmodel on a set of 10,763 instructions covering various legal tasks such as\nlegal reasoning, judgement explanation, legal clause generation, legal\ndrafting, legal contract drafting, case summarization, constitutional\nquestion-answering, etc. We also evaluated the responses of prompts for\ninstruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness,\nand legal reasoning metrics in a scale of 10. Our model can be run on CPU and\nachieved 42.46 tokens/sec CPU inference speed. We found that our models,\ndespite not being pretrained on legal books, various legal contracts, and legal\ndocuments, were able to learn the domain knowledge required for drafting\nvarious legal contracts and legal clauses, and generalize to draft legal\ncontracts and legal clauses with limited instruction tuning. Hence, we conclude\nthat for a strong domain-specialized generative language model (such as legal),\nvery large amounts of data are not required to develop models from scratch. We\nbelieve that this work is the first attempt to make a dedicated generative\nlegal language model from scratch for Indian Supreme Court jurisdiction or in\nlegal NLP overall. We plan to release our Paramanu-Ayn model at\nhttps://www.bharatgpts.com.\n","authors":["Mitodru Niyogi","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2403.13681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05667v2","updated":"2024-03-20T15:33:23Z","published":"2024-02-08T13:38:23Z","title":"S$Ω$I: Score-based O-INFORMATION Estimation","summary":"  The analysis of scientific data and complex multivariate systems requires\ninformation quantities that capture relationships among multiple random\nvariables. Recently, new information-theoretic measures have been developed to\novercome the shortcomings of classical ones, such as mutual information, that\nare restricted to considering pairwise interactions. Among them, the concept of\ninformation synergy and redundancy is crucial for understanding the high-order\ndependencies between variables. One of the most prominent and versatile\nmeasures based on this concept is O-information, which provides a clear and\nscalable way to quantify the synergy-redundancy balance in multivariate\nsystems. However, its practical application is limited to simplified cases. In\nthis work, we introduce S$\\Omega$I, which allows for the first time to compute\nO-information without restrictive assumptions about the system. Our experiments\nvalidate our approach on synthetic data, and demonstrate the effectiveness of\nS$\\Omega$I in the context of a real-world use case.\n","authors":["Mustapha Bounoua","Giulio Franzese","Pietro Michiardi"],"pdf_url":"https://arxiv.org/pdf/2402.05667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10026v2","updated":"2024-03-20T15:30:31Z","published":"2023-11-16T17:14:26Z","title":"Guaranteeing Control Requirements via Reward Shaping in Reinforcement\n  Learning","summary":"  In addressing control problems such as regulation and tracking through\nreinforcement learning, it is often required to guarantee that the acquired\npolicy meets essential performance and stability criteria such as a desired\nsettling time and steady-state error prior to deployment. Motivated by this\nnecessity, we present a set of results and a systematic reward shaping\nprocedure that (i) ensures the optimal policy generates trajectories that align\nwith specified control requirements and (ii) allows to assess whether any given\npolicy satisfies them. We validate our approach through comprehensive numerical\nexperiments conducted in two representative environments from OpenAI Gym: the\nInverted Pendulum swing-up problem and the Lunar Lander. Utilizing both tabular\nand deep reinforcement learning methods, our experiments consistently affirm\nthe efficacy of our proposed framework, highlighting its effectiveness in\nensuring policy adherence to the prescribed control requirements.\n","authors":["Francesco De Lellis","Marco Coraggio","Giovanni Russo","Mirco Musolesi","Mario di Bernardo"],"pdf_url":"https://arxiv.org/pdf/2311.10026v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13672v1","updated":"2024-03-20T15:29:59Z","published":"2024-03-20T15:29:59Z","title":"Machine Learning Optimized Approach for Parameter Selection in MESHFREE\n  Simulations","summary":"  Meshfree simulation methods are emerging as compelling alternatives to\nconventional mesh-based approaches, particularly in the fields of Computational\nFluid Dynamics (CFD) and continuum mechanics. In this publication, we provide a\ncomprehensive overview of our research combining Machine Learning (ML) and\nFraunhofer's MESHFREE software (www.meshfree.eu), a powerful tool utilizing a\nnumerical point cloud in a Generalized Finite Difference Method (GFDM). This\ntool enables the effective handling of complex flow domains, moving geometries,\nand free surfaces, while allowing users to finely tune local refinement and\nquality parameters for an optimal balance between computation time and results\naccuracy. However, manually determining the optimal parameter combination poses\nchallenges, especially for less experienced users. We introduce a novel\nML-optimized approach, using active learning, regression trees, and\nvisualization on MESHFREE simulation data, demonstrating the impact of input\ncombinations on results quality and computation time. This research contributes\nvaluable insights into parameter optimization in meshfree simulations,\nenhancing accessibility and usability for a broader user base in scientific and\nengineering applications.\n","authors":["Paulami Banerjee","Mohan Padmanabha","Chaitanya Sanghavi","Isabel Michel","Simone Gramsch"],"pdf_url":"https://arxiv.org/pdf/2403.13672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12166v2","updated":"2024-03-20T15:27:44Z","published":"2024-03-18T18:30:22Z","title":"The Power of Few: Accelerating and Enhancing Data Reweighting with\n  Coreset Selection","summary":"  As machine learning tasks continue to evolve, the trend has been to gather\nlarger datasets and train increasingly larger models. While this has led to\nadvancements in accuracy, it has also escalated computational costs to\nunsustainable levels. Addressing this, our work aims to strike a delicate\nbalance between computational efficiency and model accuracy, a persisting\nchallenge in the field. We introduce a novel method that employs core subset\nselection for reweighting, effectively optimizing both computational time and\nmodel performance. By focusing on a strategically selected coreset, our\napproach offers a robust representation, as it efficiently minimizes the\ninfluence of outliers. The re-calibrated weights are then mapped back to and\npropagated across the entire dataset. Our experimental results substantiate the\neffectiveness of this approach, underscoring its potential as a scalable and\nprecise solution for model training.\n","authors":["Mohammad Jafari","Yimeng Zhang","Yihua Zhang","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12166v2.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.10705v4","updated":"2024-03-20T15:26:55Z","published":"2023-10-16T14:46:45Z","title":"Observational and Experimental Insights into Machine Learning-Based\n  Defect Classification in Wafers","summary":"  This survey paper offers a comprehensive review of methodologies utilizing\nmachine learning (ML) classification techniques for identifying wafer defects\nin semiconductor manufacturing. Despite the growing body of research\ndemonstrating the effectiveness of ML in wafer defect identification, there is\na noticeable absence of comprehensive reviews on this subject. This survey\nattempts to fill this void by amalgamating available literature and providing\nan in-depth analysis of the advantages, limitations, and potential applications\nof various ML classification algorithms in the realm of wafer defect detection.\nAn innovative taxonomy of methodologies that we present provides a detailed\nclassification of algorithms into more refined categories and techniques. This\ntaxonomy follows a three-tier structure, starting from broad methodology\ncategories and ending with specific techniques. It aids researchers in\ncomprehending the complex relationships between different algorithms and their\ntechniques. We employ a rigorous Observational and experimental evaluation to\nrank these varying techniques. For the Observational evaluation, we assess\ntechniques based on a set of four criteria. The experimental evaluation ranks\nthe algorithms employing the same techniques, sub-categories, and categories.\nAlso the paper illuminates the future prospects of ML classification techniques\nfor wafer defect identification, underscoring potential advancements and\nopportunities for further research in this field\n","authors":["Kamal Taha"],"pdf_url":"https://arxiv.org/pdf/2310.10705v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00477v2","updated":"2024-03-20T15:25:02Z","published":"2023-12-01T10:18:50Z","title":"Interpretable Meta-Learning of Physical Systems","summary":"  Machine learning methods can be a valuable aid in the scientific process, but\nthey need to face challenging settings where data come from inhomogeneous\nexperimental conditions. Recent meta-learning methods have made significant\nprogress in multi-task learning, but they rely on black-box neural networks,\nresulting in high computational costs and limited interpretability. Leveraging\nthe structure of the learning problem, we argue that multi-environment\ngeneralization can be achieved using a simpler learning model, with an affine\nstructure with respect to the learning task. Crucially, we prove that this\narchitecture can identify the physical parameters of the system, enabling\ninterpreable learning. We demonstrate the competitive generalization\nperformance and the low computational cost of our method by comparing it to\nstate-of-the-art algorithms on physical systems, ranging from toy models to\ncomplex, non-analytical systems. The interpretability of our method is\nillustrated with original applications to physical-parameter-induced adaptation\nand to adaptive control.\n","authors":["Matthieu Blanke","Marc Lelarge"],"pdf_url":"https://arxiv.org/pdf/2312.00477v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00618v2","updated":"2024-03-20T15:17:43Z","published":"2023-07-02T17:18:17Z","title":"Bounce: Reliable High-Dimensional Bayesian Optimization for\n  Combinatorial and Mixed Spaces","summary":"  Impactful applications such as materials discovery, hardware design, neural\narchitecture search, or portfolio optimization require optimizing\nhigh-dimensional black-box functions with mixed and combinatorial input spaces.\nWhile Bayesian optimization has recently made significant progress in solving\nsuch problems, an in-depth analysis reveals that the current state-of-the-art\nmethods are not reliable. Their performances degrade substantially when the\nunknown optima of the function do not have a certain structure. To fill the\nneed for a reliable algorithm for combinatorial and mixed spaces, this paper\nproposes Bounce that relies on a novel map of various variable types into\nnested embeddings of increasing dimensionality. Comprehensive experiments show\nthat Bounce reliably achieves and often even improves upon state-of-the-art\nperformance on a variety of high-dimensional problems.\n","authors":["Leonard Papenmeier","Luigi Nardi","Matthias Poloczek"],"pdf_url":"https://arxiv.org/pdf/2307.00618v2.pdf","comment":"30 pages, 22 figures"},{"id":"http://arxiv.org/abs/2305.11288v2","updated":"2024-03-20T15:10:09Z","published":"2023-05-18T20:12:22Z","title":"Riemannian Multinomial Logistics Regression for SPD Neural Networks","summary":"  Deep neural networks for learning Symmetric Positive Definite (SPD) matrices\nare gaining increasing attention in machine learning. Despite the significant\nprogress, most existing SPD networks use traditional Euclidean classifiers on\nan approximated space rather than intrinsic classifiers that accurately capture\nthe geometry of SPD manifolds. Inspired by Hyperbolic Neural Networks (HNNs),\nwe propose Riemannian Multinomial Logistics Regression (RMLR) for the\nclassification layers in SPD networks. We introduce a unified framework for\nbuilding Riemannian classifiers under the metrics pulled back from the\nEuclidean space, and showcase our framework under the parameterized\nLog-Euclidean Metric (LEM) and Log-Cholesky Metric (LCM). Besides, our\nframework offers a novel intrinsic explanation for the most popular LogEig\nclassifier in existing SPD networks. The effectiveness of our method is\ndemonstrated in three applications: radar recognition, human action\nrecognition, and electroencephalography (EEG) classification. The code is\navailable at https://github.com/GitZH-Chen/SPDMLR.git.\n","authors":["Ziheng Chen","Yue Song","Gaowen Liu","Ramana Rao Kompella","Xiaojun Wu","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2305.11288v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2402.03818v2","updated":"2024-03-20T15:08:27Z","published":"2024-02-06T09:07:26Z","title":"Asymptotic generalization error of a single-layer graph convolutional\n  network","summary":"  While graph convolutional networks show great practical promises, the\ntheoretical understanding of their generalization properties as a function of\nthe number of samples is still in its infancy compared to the more broadly\nstudied case of supervised fully connected neural networks. In this article, we\npredict the performances of a single-layer graph convolutional network (GCN)\ntrained on data produced by attributed stochastic block models (SBMs) in the\nhigh-dimensional limit. Previously, only ridge regression on contextual-SBM\n(CSBM) has been considered in Shi et al. 2022; we generalize the analysis to\narbitrary convex loss and regularization for the CSBM and add the analysis for\nanother data model, the neural-prior SBM. We also study the high\nsignal-to-noise ratio limit, detail the convergence rates of the GCN and show\nthat, while consistent, it does not reach the Bayes-optimal rate for any of the\nconsidered cases.\n","authors":["O. Duranthon","L. Zdeborová"],"pdf_url":"https://arxiv.org/pdf/2402.03818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13658v1","updated":"2024-03-20T15:06:49Z","published":"2024-03-20T15:06:49Z","title":"Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics\n  Instability Detection","summary":"  Recent advancements in non-invasive detection of cardiac hemodynamic\ninstability (CHDI) primarily focus on applying machine learning techniques to a\nsingle data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite\ntheir potential, these approaches often fall short especially when the size of\nlabeled patient data is limited, a common challenge in the medical domain.\nFurthermore, only a few studies have explored multimodal methods to study CHDI,\nwhich mostly rely on costly modalities such as cardiac MRI and echocardiogram.\nIn response to these limitations, we propose a novel multimodal variational\nautoencoder ($\\text{CardioVAE}_\\text{X,G}$) to integrate low-cost chest X-ray\n(CXR) and electrocardiogram (ECG) modalities with pre-training on a large\nunlabeled dataset. Specifically, $\\text{CardioVAE}_\\text{X,G}$ introduces a\nnovel tri-stream pre-training strategy to learn both shared and\nmodality-specific features, thus enabling fine-tuning with both unimodal and\nmultimodal datasets. We pre-train $\\text{CardioVAE}_\\text{X,G}$ on a large,\nunlabeled dataset of $50,982$ subjects from a subset of MIMIC database and then\nfine-tune the pre-trained model on a labeled dataset of $795$ subjects from the\nASPIRE registry. Comprehensive evaluations against existing methods show that\n$\\text{CardioVAE}_\\text{X,G}$ offers promising performance (AUROC $=0.79$ and\nAccuracy $=0.77$), representing a significant step forward in non-invasive\nprediction of CHDI. Our model also excels in producing fine interpretations of\npredictions directly associated with clinical features, thereby supporting\nclinical decision-making.\n","authors":["Mohammod N. I. Suvon","Prasun C. Tripathi","Wenrui Fan","Shuo Zhou","Xianyuan Liu","Samer Alabed","Venet Osmani","Andrew J. Swift","Chen Chen","Haiping Lu"],"pdf_url":"https://arxiv.org/pdf/2403.13658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07999v3","updated":"2024-03-20T14:30:41Z","published":"2024-02-12T19:04:32Z","title":"NetInfoF Framework: Measuring and Exploiting Network Usable Information","summary":"  Given a node-attributed graph, and a graph task (link prediction or node\nclassification), can we tell if a graph neural network (GNN) will perform well?\nMore specifically, do the graph structure and the node features carry enough\nusable information for the task? Our goals are (1) to develop a fast tool to\nmeasure how much information is in the graph structure and in the node\nfeatures, and (2) to exploit the information to solve the task, if there is\nenough. We propose NetInfoF, a framework including NetInfoF_Probe and\nNetInfoF_Act, for the measurement and the exploitation of network usable\ninformation (NUI), respectively. Given a graph data, NetInfoF_Probe measures\nNUI without any model training, and NetInfoF_Act solves link prediction and\nnode classification, while two modules share the same backbone. In summary,\nNetInfoF has following notable advantages: (a) General, handling both link\nprediction and node classification; (b) Principled, with theoretical guarantee\nand closed-form solution; (c) Effective, thanks to the proposed adjustment to\nnode similarity; (d) Scalable, scaling linearly with the input size. In our\ncarefully designed synthetic datasets, NetInfoF correctly identifies the ground\ntruth of NUI and is the only method being robust to all graph scenarios.\nApplied on real-world datasets, NetInfoF wins in 11 out of 12 times on link\nprediction compared to general GNN baselines.\n","authors":["Meng-Chieh Lee","Haiyang Yu","Jian Zhang","Vassilis N. Ioannidis","Xiang Song","Soji Adeshina","Da Zheng","Christos Faloutsos"],"pdf_url":"https://arxiv.org/pdf/2402.07999v3.pdf","comment":"Accepted to ICLR 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2310.00117v3","updated":"2024-03-20T14:26:12Z","published":"2023-09-29T20:11:15Z","title":"ABScribe: Rapid Exploration & Organization of Multiple Writing\n  Variations in Human-AI Co-Writing Tasks using Large Language Models","summary":"  Exploring alternative ideas by rewriting text is integral to the writing\nprocess. State-of-the-art Large Language Models (LLMs) can simplify writing\nvariation generation. However, current interfaces pose challenges for\nsimultaneous consideration of multiple variations: creating new variations\nwithout overwriting text can be difficult, and pasting them sequentially can\nclutter documents, increasing workload and disrupting writers' flow. To tackle\nthis, we present ABScribe, an interface that supports rapid, yet visually\nstructured, exploration and organization of writing variations in human-AI\nco-writing tasks. With ABScribe, users can swiftly modify variations using LLM\nprompts, which are auto-converted into reusable buttons. Variations are stored\nadjacently within text fields for rapid in-place comparisons using mouse-over\ninteractions on a popup toolbar. Our user study with 12 writers shows that\nABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances\nuser perceptions of the revision process (d = 2.41, p < 0.001) compared to a\npopular baseline workflow, and provides insights into how writers explore\nvariations using LLMs.\n","authors":["Mohi Reza","Nathan Laundry","Ilya Musabirov","Peter Dushniku","Zhi Yuan \"Michael\" Yu","Kashish Mittal","Tovi Grossman","Michael Liut","Anastasia Kuzminykh","Joseph Jay Williams"],"pdf_url":"https://arxiv.org/pdf/2310.00117v3.pdf","comment":"CHI 2024"},{"id":"http://arxiv.org/abs/2403.13627v1","updated":"2024-03-20T14:23:17Z","published":"2024-03-20T14:23:17Z","title":"Efficient exploration of high-Tc superconductors by a gradient-based\n  composition design","summary":"  We propose a material design method via gradient-based optimization on\ncompositions, overcoming the limitations of traditional methods: exhaustive\ndatabase searches and conditional generation models. It optimizes inputs via\nbackpropagation, aligning the model's output closely with the target property\nand facilitating the discovery of unlisted materials and precise property\ndetermination. Our method is also capable of adaptive optimization under new\nconditions without retraining. Applying to exploring high-Tc superconductors,\nwe identified potential compositions beyond existing databases and discovered\nnew hydrogen superconductors via conditional optimization. This method is\nversatile and significantly advances material design by enabling efficient,\nextensive searches and adaptability to new constraints.\n","authors":["Akihiro Fujii","Koji Shimizu","Satoshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2403.13627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13625v1","updated":"2024-03-20T14:22:19Z","published":"2024-03-20T14:22:19Z","title":"Enhancing Law Enforcement Training: A Gamified Approach to Detecting\n  Terrorism Financing","summary":"  Tools for fighting cyber-criminal activities using new technologies are\npromoted and deployed every day. However, too often, they are unnecessarily\ncomplex and hard to use, requiring deep domain and technical knowledge. These\ncharacteristics often limit the engagement of law enforcement and end-users in\nthese technologies that, despite their potential, remain misunderstood. For\nthis reason, in this study, we describe our experience in combining learning\nand training methods and the potential benefits of gamification to enhance\ntechnology transfer and increase adult learning. In fact, in this case,\nparticipants are experienced practitioners in professions/industries that are\nexposed to terrorism financing (such as Law Enforcement Officers, Financial\nInvestigation Officers, private investigators, etc.) We define training\nactivities on different levels for increasing the exchange of information about\nnew trends and criminal modus operandi among and within law enforcement\nagencies, intensifying cross-border cooperation and supporting efforts to\ncombat and prevent terrorism funding activities. On the other hand, a game\n(hackathon) is designed to address realistic challenges related to the dark\nnet, crypto assets, new payment systems and dark web marketplaces that could be\nused for terrorist activities. The entire methodology was evaluated using\nquizzes, contest results, and engagement metrics. In particular, training\nevents show about 60% of participants complete the 11-week training course,\nwhile the Hackathon results, gathered in two pilot studies (Madrid and The\nHague), show increasing expertise among the participants (progression in the\nachieved points on average). At the same time, more than 70% of participants\npositively evaluate the use of the gamification approach, and more than 85% of\nthem consider the implemented Use Cases suitable for their investigations.\n","authors":["Francesco Zola","Lander Segurola","Erin King","Martin Mullins","Raul Orduna"],"pdf_url":"https://arxiv.org/pdf/2403.13625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.08270v3","updated":"2024-03-20T14:13:44Z","published":"2022-08-17T13:02:17Z","title":"On the Privacy Effect of Data Enhancement via the Lens of Memorization","summary":"  Machine learning poses severe privacy concerns as it has been shown that the\nlearned models can reveal sensitive information about their training data. Many\nworks have investigated the effect of widely adopted data augmentation and\nadversarial training techniques, termed data enhancement in the paper, on the\nprivacy leakage of machine learning models. Such privacy effects are often\nmeasured by membership inference attacks (MIAs), which aim to identify whether\na particular example belongs to the training set or not. We propose to\ninvestigate privacy from a new perspective called memorization. Through the\nlens of memorization, we find that previously deployed MIAs produce misleading\nresults as they are less likely to identify samples with higher privacy risks\nas members compared to samples with low privacy risks. To solve this problem,\nwe deploy a recent attack that can capture individual samples' memorization\ndegrees for evaluation. Through extensive experiments, we unveil several\nfindings about the connections between three essential properties of machine\nlearning models, including privacy, generalization gap, and adversarial\nrobustness. We demonstrate that the generalization gap and privacy leakage are\nless correlated than those of the previous results. Moreover, there is not\nnecessarily a trade-off between adversarial robustness and privacy as stronger\nadversarial robustness does not make the model more susceptible to privacy\nattacks.\n","authors":["Xiao Li","Qiongxiu Li","Zhanhao Hu","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2208.08270v3.pdf","comment":"Accepted by IEEE TIFS, 17 pages"},{"id":"http://arxiv.org/abs/2403.13612v1","updated":"2024-03-20T14:03:57Z","published":"2024-03-20T14:03:57Z","title":"Does Differentially Private Synthetic Data Lead to Synthetic\n  Discoveries?","summary":"  Background: Synthetic data has been proposed as a solution for sharing\nanonymized versions of sensitive biomedical datasets. Ideally, synthetic data\nshould preserve the structure and statistical properties of the original data,\nwhile protecting the privacy of the individual subjects. Differential privacy\n(DP) is currently considered the gold standard approach for balancing this\ntrade-off.\n  Objectives: The aim of this study is to evaluate the Mann-Whitney U test on\nDP-synthetic biomedical data in terms of Type I and Type II errors, in order to\nestablish whether statistical hypothesis testing performed on privacy\npreserving synthetic data is likely to lead to loss of test's validity or\ndecreased power.\n  Methods: We evaluate the Mann-Whitney U test on DP-synthetic data generated\nfrom real-world data, including a prostate cancer dataset (n=500) and a\ncardiovascular dataset (n=70 000), as well as on data drawn from two Gaussian\ndistributions. Five different DP-synthetic data generation methods are\nevaluated, including two basic DP histogram release methods and MWEM,\nPrivate-PGM, and DP GAN algorithms.\n  Conclusion: Most of the tested DP-synthetic data generation methods showed\ninflated Type I error, especially at privacy budget levels of $\\epsilon\\leq 1$.\nThis result calls for caution when releasing and analyzing DP-synthetic data:\nlow p-values may be obtained in statistical tests simply as a byproduct of the\nnoise added to protect privacy. A DP smoothed histogram-based synthetic data\ngeneration method was shown to produce valid Type I error for all privacy\nlevels tested but required a large original dataset size and a modest privacy\nbudget ($\\epsilon\\geq 5$) in order to have reasonable Type II error levels.\n","authors":["Ileana Montoya Perez","Parisa Movahedi","Valtteri Nieminen","Antti Airola","Tapio Pahikkala"],"pdf_url":"https://arxiv.org/pdf/2403.13612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17885v2","updated":"2024-03-20T13:36:23Z","published":"2023-11-29T18:32:37Z","title":"Are Ensembles Getting Better all the Time?","summary":"  Ensemble methods combine the predictions of several base models. We study\nwhether or not including more models always improves their average performance.\nThis question depends on the kind of ensemble considered, as well as the\npredictive metric chosen. We focus on situations where all members of the\nensemble are a priori expected to perform as well, which is the case of several\npopular methods such as random forests or deep ensembles. In this setting, we\nshow that ensembles are getting better all the time if, and only if, the\nconsidered loss function is convex. More precisely, in that case, the average\nloss of the ensemble is a decreasing function of the number of models. When the\nloss function is nonconvex, we show a series of results that can be summarised\nas: ensembles of good models keep getting better, and ensembles of bad models\nkeep getting worse. To this end, we prove a new result on the monotonicity of\ntail probabilities that may be of independent interest. We illustrate our\nresults on a medical prediction problem (diagnosing melanomas using neural\nnets) and a \"wisdom of crowds\" experiment (guessing the ratings of upcoming\nmovies).\n","authors":["Pierre-Alexandre Mattei","Damien Garreau"],"pdf_url":"https://arxiv.org/pdf/2311.17885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13583v1","updated":"2024-03-20T13:33:55Z","published":"2024-03-20T13:33:55Z","title":"CONLINE: Complex Code Generation and Refinement with Online Searching\n  and Correctness Testing","summary":"  Large Language Models (LLMs) have revolutionized code generation ability by\nconverting natural language descriptions into executable code. However,\ngenerating complex code within real-world scenarios remains challenging due to\nintricate structures, subtle bugs, understanding of advanced data types, and\nlack of supplementary contents. To address these challenges, we introduce the\nCONLINE framework, which enhances code generation by incorporating planned\nonline searches for information retrieval and automated correctness testing for\niterative refinement. CONLINE also serializes the complex inputs and outputs to\nimprove comprehension and generate test case to ensure the framework's\nadaptability for real-world applications. CONLINE is validated through rigorous\nexperiments on the DS-1000 and ClassEval datasets. It shows that CONLINE\nsubstantially improves the quality of complex code generation, highlighting its\npotential to enhance the practicality and reliability of LLMs in generating\nintricate code.\n","authors":["Xinyi He","Jiaru Zou","Yun Lin","Mengyu Zhou","Shi Han","Zejian Yuan","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.13583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13578v1","updated":"2024-03-20T13:24:41Z","published":"2024-03-20T13:24:41Z","title":"Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for\n  Counselor Reflection Generation","summary":"  In this paper, we study the problem of multi-reward reinforcement learning to\njointly optimize for multiple text qualities for natural language generation.\nWe focus on the task of counselor reflection generation, where we optimize the\ngenerators to simultaneously improve the fluency, coherence, and reflection\nquality of generated counselor responses. We introduce two novel bandit\nmethods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining\nrewards into a single value and optimizing them simultaneously. Specifically,\nwe employ non-contextual and contextual multi-arm bandits to dynamically adjust\nmultiple reward weights during training. Through automatic and manual\nevaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt,\noutperform existing naive and bandit baselines, showcasing their potential for\nenhancing language models.\n","authors":["Do June Min","Veronica Perez-Rosas","Kenneth Resnicow","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2403.13578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11335v4","updated":"2024-03-20T13:18:18Z","published":"2023-06-20T07:06:04Z","title":"Surfer: Progressive Reasoning with World Models for Robotic Manipulation","summary":"  Considering how to make the model accurately understand and follow natural\nlanguage instructions and perform actions consistent with world knowledge is a\nkey challenge in robot manipulation. This mainly includes human fuzzy\ninstruction reasoning and the following of physical knowledge. Therefore, the\nembodied intelligence agent must have the ability to model world knowledge from\ntraining data. However, most existing vision and language robot manipulation\nmethods mainly operate in less realistic simulator and language settings and\nlack explicit modeling of world knowledge. To bridge this gap, we introduce a\nnovel and simple robot manipulation framework, called Surfer. It is based on\nthe world model, treats robot manipulation as a state transfer of the visual\nscene, and decouples it into two parts: action and scene. Then, the\ngeneralization ability of the model on new instructions and new scenes is\nenhanced by explicit modeling of the action and scene prediction in multi-modal\ninformation. In addition to the framework, we also built a robot manipulation\nsimulator that supports full physics execution based on the MuJoCo physics\nengine. It can automatically generate demonstration training data and test\ndata, effectively reducing labor costs. To conduct a comprehensive and\nsystematic evaluation of the robot manipulation model in terms of language\nunderstanding and physical execution, we also created a robotic manipulation\nbenchmark with progressive reasoning tasks, called SeaWave. It contains 4\nlevels of progressive reasoning tasks and can provide a standardized testing\nplatform for embedded AI agents in multi-modal environments. On average, Surfer\nachieved a success rate of 54.74% on the defined four levels of manipulation\ntasks, exceeding the best baseline performance of 47.64%.\n","authors":["Pengzhen Ren","Kaidong Zhang","Hetao Zheng","Zixuan Li","Yuhang Wen","Fengda Zhu","Mas Ma","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2306.11335v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09389v2","updated":"2024-03-20T13:11:19Z","published":"2023-02-18T17:45:11Z","title":"Vulnerability analysis of captcha using Deep learning","summary":"  Several websites improve their security and avoid dangerous Internet attacks\nby implementing CAPTCHAs (Completely Automated Public Turing test to tell\nComputers and Humans Apart), a type of verification to identify whether the\nend-user is human or a robot. The most prevalent type of CAPTCHA is text-based,\ndesigned to be easily recognized by humans while being unsolvable towards\nmachines or robots. However, as deep learning technology progresses,\ndevelopment of convolutional neural network (CNN) models that predict\ntext-based CAPTCHAs becomes easier. The purpose of this research is to\ninvestigate the flaws and vulnerabilities in the CAPTCHA generating systems in\norder to design more resilient CAPTCHAs. To achieve this, we created CapNet, a\nConvolutional Neural Network. The proposed platform can evaluate both numerical\nand alphanumerical CAPTCHAs\n","authors":["Jaskaran Singh Walia","Aryan Odugoudar"],"pdf_url":"https://arxiv.org/pdf/2302.09389v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13565v1","updated":"2024-03-20T12:58:46Z","published":"2024-03-20T12:58:46Z","title":"AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for\n  High-dimensional Regression","summary":"  We consider the transfer learning problem in the high dimensional setting,\nwhere the feature dimension is larger than the sample size. To learn\ntransferable information, which may vary across features or the source samples,\nwe propose an adaptive transfer learning method that can detect and aggregate\nthe feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable\nstructures. We achieve this by employing a novel fused-penalty, coupled with\nweights that can adapt according to the transferable structure. To choose the\nweight, we propose a theoretically informed, data-driven procedure, enabling\nF-AdaTrans to selectively fuse the transferable signals with the target while\nfiltering out non-transferable signals, and S-AdaTrans to obtain the optimal\ncombination of information transferred from each source sample. The\nnon-asymptotic rates are established, which recover existing near-minimax\noptimal rates in special cases. The effectiveness of the proposed method is\nvalidated using both synthetic and real data.\n","authors":["Zelin He","Ying Sun","Jingyuan Liu","Runze Li"],"pdf_url":"https://arxiv.org/pdf/2403.13565v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2312.02696v2","updated":"2024-03-20T12:58:14Z","published":"2023-12-05T11:55:47Z","title":"Analyzing and Improving the Training Dynamics of Diffusion Models","summary":"  Diffusion models currently dominate the field of data-driven image synthesis\nwith their unparalleled scaling to large datasets. In this paper, we identify\nand rectify several causes for uneven and ineffective training in the popular\nADM diffusion model architecture, without altering its high-level structure.\nObserving uncontrolled magnitude changes and imbalances in both the network\nactivations and weights over the course of training, we redesign the network\nlayers to preserve activation, weight, and update magnitudes on expectation. We\nfind that systematic application of this philosophy eliminates the observed\ndrifts and imbalances, resulting in considerably better networks at equal\ncomputational complexity. Our modifications improve the previous record FID of\n2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic\nsampling.\n  As an independent contribution, we present a method for setting the\nexponential moving average (EMA) parameters post-hoc, i.e., after completing\nthe training run. This allows precise tuning of EMA length without the cost of\nperforming several training runs, and reveals its surprising interactions with\nnetwork architecture, training time, and guidance.\n","authors":["Tero Karras","Miika Aittala","Jaakko Lehtinen","Janne Hellsten","Timo Aila","Samuli Laine"],"pdf_url":"https://arxiv.org/pdf/2312.02696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13563v1","updated":"2024-03-20T12:56:40Z","published":"2024-03-20T12:56:40Z","title":"DL2Fence: Integrating Deep Learning and Frame Fusion for Enhanced\n  Detection and Localization of Refined Denial-of-Service in Large-Scale NoCs","summary":"  This study introduces a refined Flooding Injection Rate-adjustable\nDenial-of-Service (DoS) model for Network-on-Chips (NoCs) and more importantly\npresents DL2Fence, a novel framework utilizing Deep Learning (DL) and Frame\nFusion (2F) for DoS detection and localization. Two Convolutional Neural\nNetworks models for classification and segmentation were developed to detect\nand localize DoS respectively. It achieves detection and localization\naccuracies of 95.8\\% and 91.7\\%, and precision rates of 98.5\\% and 99.3\\% in a\n16x16 mesh NoC. The framework's hardware overhead notably decreases by 76.3\\%\nwhen scaling from 8x8 to 16x16 NoCs, and it requires 42.4\\% less hardware\ncompared to state-of-the-arts. This advancement demonstrates DL2Fence's\neffectiveness in balancing outstanding detection performance in large-scale\nNoCs with extremely low hardware overhead.\n","authors":["Haoyu Wang","Basel Halak","Jianjie Ren","Ahmad Atamli"],"pdf_url":"https://arxiv.org/pdf/2403.13563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10402v2","updated":"2024-03-20T12:52:10Z","published":"2023-10-16T13:45:26Z","title":"Real-Fake: Effective Training Data Synthesis Through Distribution\n  Matching","summary":"  Synthetic training data has gained prominence in numerous learning tasks and\nscenarios, offering advantages such as dataset augmentation, generalization\nevaluation, and privacy preservation. Despite these benefits, the efficiency of\nsynthetic data generated by current methodologies remains inferior when\ntraining advanced deep models exclusively, limiting its practical utility. To\naddress this challenge, we analyze the principles underlying training data\nsynthesis for supervised learning and elucidate a principled theoretical\nframework from the distribution-matching perspective that explicates the\nmechanisms governing synthesis efficacy. Through extensive experiments, we\ndemonstrate the effectiveness of our synthetic data across diverse image\nclassification tasks, both as a replacement for and augmentation to real\ndatasets, while also benefits such as out-of-distribution generalization,\nprivacy preservation, and scalability. Specifically, we achieve 70.9% top1\nclassification accuracy on ImageNet1K when training solely with synthetic data\nequivalent to 1 X the original real data size, which increases to 76.0% when\nscaling up to 10 X synthetic data.\n","authors":["Jianhao Yuan","Jie Zhang","Shuyang Sun","Philip Torr","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.10402v2.pdf","comment":"Code released at\n  (https://github.com/BAAI-DCAI/Training-Data-Synthesis)"},{"id":"http://arxiv.org/abs/2403.13551v1","updated":"2024-03-20T12:40:32Z","published":"2024-03-20T12:40:32Z","title":"Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute\n  Editing","summary":"  Despite recent advancements in text-to-image diffusion models facilitating\nvarious image editing techniques, complex text prompts often lead to an\noversight of some requests due to a bottleneck in processing text information.\nTo tackle this challenge, we present Ground-A-Score, a simple yet powerful\nmodel-agnostic image editing method by incorporating grounding during score\ndistillation. This approach ensures a precise reflection of intricate prompt\nrequirements in the editing outcomes, taking into account the prior knowledge\nof the object locations within the image. Moreover, the selective application\nwith a new penalty coefficient and contrastive loss helps to precisely target\nediting areas while preserving the integrity of the objects in the source\nimage. Both qualitative assessments and quantitative analyses confirm that\nGround-A-Score successfully adheres to the intricate details of extended and\nmultifaceted prompts, ensuring high-quality outcomes that respect the original\nimage attributes.\n","authors":["Hangeol Chang","Jinho Chang","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.13551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13547v1","updated":"2024-03-20T12:33:51Z","published":"2024-03-20T12:33:51Z","title":"Integrating Large Language Models for Severity Classification in Traffic\n  Incident Management: A Machine Learning Approach","summary":"  This study evaluates the impact of large language models on enhancing machine\nlearning processes for managing traffic incidents. It examines the extent to\nwhich features generated by modern language models improve or match the\naccuracy of predictions when classifying the severity of incidents using\naccident reports. Multiple comparisons performed between combinations of\nlanguage models and machine learning algorithms, including Gradient Boosted\nDecision Trees, Random Forests, and Extreme Gradient Boosting. Our research\nuses both conventional and language model-derived features from texts and\nincident reports, and their combinations to perform severity classification.\nIncorporating features from language models with those directly obtained from\nincident reports has shown to improve, or at least match, the performance of\nmachine learning techniques in assigning severity levels to incidents,\nparticularly when employing Random Forests and Extreme Gradient Boosting\nmethods. This comparison was quantified using the F1-score over uniformly\nsampled data sets to obtain balanced severity classes. The primary contribution\nof this research is in the demonstration of how Large Language Models can be\nintegrated into machine learning workflows for incident management, thereby\nsimplifying feature extraction from unstructured text and enhancing or matching\nthe precision of severity predictions using conventional machine learning\npipeline. The engineering application of this research is illustrated through\nthe effective use of these language processing models to refine the modelling\nprocess for incident severity classification. This work provides significant\ninsights into the application of language processing capabilities in\ncombination with traditional data for improving machine learning pipelines in\nthe context of classifying incident severity.\n","authors":["Artur Grigorev","Khaled Saleh","Yuming Ou","Adriana-Simona Mihaita"],"pdf_url":"https://arxiv.org/pdf/2403.13547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13545v1","updated":"2024-03-20T12:31:13Z","published":"2024-03-20T12:31:13Z","title":"Next day fire prediction via semantic segmentation","summary":"  In this paper we present a deep learning pipeline for next day fire\nprediction. The next day fire prediction task consists in learning models that\nreceive as input the available information for an area up until a certain day,\nin order to predict the occurrence of fire for the next day. Starting from our\nprevious problem formulation as a binary classification task on instances\n(daily snapshots of each area) represented by tabular feature vectors, we\nreformulate the problem as a semantic segmentation task on images; there, each\npixel corresponds to a daily snapshot of an area, while its channels represent\nthe formerly tabular training features. We demonstrate that this problem\nformulation, built within a thorough pipeline achieves state of the art\nresults.\n","authors":["Konstantinos Alexis","Stella Girtsou","Alexis Apostolakis","Giorgos Giannopoulos","Charalampos Kontoes"],"pdf_url":"https://arxiv.org/pdf/2403.13545v1.pdf","comment":"Accepted in MACLEAN@ECML/PKDD 2023"},{"id":"http://arxiv.org/abs/2307.07975v3","updated":"2024-03-20T12:30:01Z","published":"2023-07-16T07:55:35Z","title":"Pseudo-rigid body networks: learning interpretable deformable object\n  dynamics from partial observations","summary":"  Accurate prediction of deformable linear object (DLO) dynamics is challenging\nif the task at hand requires a human-interpretable yet computationally fast\nmodel. In this work, we draw inspiration from the pseudo-rigid body method\n(PRB) and model a DLO as a serial chain of rigid bodies whose internal state is\nunrolled through time by a dynamics network. This dynamics network is trained\njointly with a physics-informed encoder which maps observed motion variables to\nthe DLO's hidden state. To encourage that the state acquires a physically\nmeaningful representation, we leverage the forward kinematics of the PRB model\nas decoder. We demonstrate in robot experiments that the proposed DLO dynamics\nmodel provides physically interpretable predictions from partial observations\nwhile being on par with black-box models regarding prediction accuracy. The\nproject code is available at: http://tinyurl.com/prb-networks\n","authors":["Shamil Mamedov","A. René Geist","Jan Swevers","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2307.07975v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2310.10060v3","updated":"2024-03-20T12:25:51Z","published":"2023-10-16T04:49:51Z","title":"Data Augmentation for Time-Series Classification: An Extensive Empirical\n  Study and Comprehensive Survey","summary":"  Data Augmentation (DA) has emerged as an indispensable strategy in Time\nSeries Classification (TSC), primarily due to its capacity to amplify training\nsamples, thereby bolstering model robustness, diversifying datasets, and\ncurtailing overfitting. However, the current landscape of DA in TSC is plagued\nwith fragmented literature reviews, nebulous methodological taxonomies,\ninadequate evaluative measures, and a dearth of accessible, user-oriented\ntools. In light of these challenges, this study embarks on an exhaustive\ndissection of DA methodologies within the TSC realm. Our initial approach\ninvolved an extensive literature review spanning a decade, revealing that\ncontemporary surveys scarcely capture the breadth of advancements in DA for\nTSC, prompting us to meticulously analyze over 100 scholarly articles to\ndistill more than 60 unique DA techniques. This rigorous analysis precipitated\nthe formulation of a novel taxonomy, purpose-built for the intricacies of DA in\nTSC, categorizing techniques into five principal echelons:\nTransformation-Based, Pattern-Based, Generative, Decomposition-Based, and\nAutomated Data Augmentation. Our taxonomy promises to serve as a robust\nnavigational aid for scholars, offering clarity and direction in method\nselection. Addressing the conspicuous absence of holistic evaluations for\nprevalent DA techniques, we executed an all-encompassing empirical assessment,\nwherein upwards of 15 DA strategies were subjected to scrutiny across 8 UCR\ntime-series datasets, employing ResNet and a multi-faceted evaluation paradigm\nencompassing Accuracy, Method Ranking, and Residual Analysis, yielding a\nbenchmark accuracy of 88.94 +- 11.83%. Our investigation underscored the\ninconsistent efficacies of DA techniques, with...\n","authors":["Zijun Gao","Lingbo Li"],"pdf_url":"https://arxiv.org/pdf/2310.10060v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13537v1","updated":"2024-03-20T12:14:54Z","published":"2024-03-20T12:14:54Z","title":"What explains the success of cross-modal fine-tuning with ORCA?","summary":"  ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning,\ni.e., applying pre-trained transformer models to modalities beyond their\ntraining data. The technique consists primarily of training an embedder and\nfine-tuning the embedder and model. Despite its high performance on a variety\nof downstream tasks, we do not understand precisely how each of these\ncomponents contribute to ORCA's success. Therefore, we run a series of\nablations and find that embedder training does not help 2D tasks at all,\ncontrary to what the original paper posits. In 1D tasks, some amount of\nembedder training is necessary but more is not better. In 4 out of 6 datasets\nwe experiment with, it is model fine-tuning that makes the biggest difference.\nThrough our ablations and baselines, we contribute a better understanding of\nthe individual components of ORCA.\n","authors":["Paloma García-de-Herreros","Vagrant Gautam","Philipp Slusallek","Dietrich Klakow","Marius Mosbach"],"pdf_url":"https://arxiv.org/pdf/2403.13537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03008v2","updated":"2024-03-20T12:05:28Z","published":"2024-02-05T13:47:41Z","title":"Diffusive Gibbs Sampling","summary":"  The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods\nfor multi-modal distributions presents a significant challenge in practical\napplications such as Bayesian inference and molecular dynamics. Addressing\nthis, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of\nsampling methods designed for effective sampling from distributions\ncharacterized by distant and disconnected modes. DiGS integrates recent\ndevelopments in diffusion models, leveraging Gaussian convolution to create an\nauxiliary noisy distribution that bridges isolated modes in the original space\nand applying Gibbs sampling to alternately draw samples from both spaces. Our\napproach exhibits a better mixing property for sampling multi-modal\ndistributions than state-of-the-art methods such as parallel tempering. We\ndemonstrate that our sampler attains substantially improved results across\nvarious tasks, including mixtures of Gaussians, Bayesian neural networks and\nmolecular dynamics.\n","authors":["Wenlin Chen","Mingtian Zhang","Brooks Paige","José Miguel Hernández-Lobato","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.03008v2.pdf","comment":"15 pages, 11 figures, 4 tables, 1 algorithm. Code available:\n  https://github.com/Wenlin-Chen/DiGS"},{"id":"http://arxiv.org/abs/2403.13523v1","updated":"2024-03-20T11:50:16Z","published":"2024-03-20T11:50:16Z","title":"Have You Poisoned My Data? Defending Neural Networks against Data\n  Poisoning","summary":"  The unprecedented availability of training data fueled the rapid development\nof powerful neural networks in recent years. However, the need for such large\namounts of data leads to potential threats such as poisoning attacks:\nadversarial manipulations of the training data aimed at compromising the\nlearned model to achieve a given adversarial goal.\n  This paper investigates defenses against clean-label poisoning attacks and\nproposes a novel approach to detect and filter poisoned datapoints in the\ntransfer learning setting. We define a new characteristic vector representation\nof datapoints and show that it effectively captures the intrinsic properties of\nthe data distribution. Through experimental analysis, we demonstrate that\neffective poisons can be successfully differentiated from clean points in the\ncharacteristic vector space. We thoroughly evaluate our proposed approach and\ncompare it to existing state-of-the-art defenses using multiple architectures,\ndatasets, and poison budgets. Our evaluation shows that our proposal\noutperforms existing approaches in defense rate and final trained model\nperformance across all experimental settings.\n","authors":["Fabio De Gaspari","Dorjan Hitaj","Luigi V. Mancini"],"pdf_url":"https://arxiv.org/pdf/2403.13523v1.pdf","comment":"Paper accepted for publication at European Symposium on Research in\n  Computer Security (ESORICS) 2024"},{"id":"http://arxiv.org/abs/2403.13522v1","updated":"2024-03-20T11:48:10Z","published":"2024-03-20T11:48:10Z","title":"REAL: Representation Enhanced Analytic Learning for Exemplar-free\n  Class-incremental Learning","summary":"  Exemplar-free class-incremental learning (EFCIL) aims to mitigate\ncatastrophic forgetting in class-incremental learning without available\nhistorical data. Compared with its counterpart (replay-based CIL) that stores\nhistorical samples, the EFCIL suffers more from forgetting issues under the\nexemplar-free constraint. In this paper, inspired by the recently developed\nanalytic learning (AL) based CIL, we propose a representation enhanced analytic\nlearning (REAL) for EFCIL. The REAL constructs a dual-stream base pretraining\n(DS-BPT) and a representation enhancing distillation (RED) process to enhance\nthe representation of the extractor. The DS-BPT pretrains model in streams of\nboth supervised learning and self-supervised contrastive learning (SSCL) for\nbase knowledge extraction. The RED process distills the supervised knowledge to\nthe SSCL pretrained backbone and facilitates a subsequent AL-basd CIL that\nconverts the CIL to a recursive least-square problem. Our method addresses the\nissue of insufficient discriminability in representations of unseen data caused\nby a frozen backbone in the existing AL-based CIL. Empirical results on various\ndatasets including CIFAR-100, ImageNet-100 and ImageNet-1k, demonstrate that\nour REAL outperforms the state-of-the-arts in EFCIL, and achieves comparable or\neven more superior performance compared with the replay-based methods.\n","authors":["Run He","Huiping Zhuang","Di Fang","Yizhu Chen","Kai Tong","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.13522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01211v2","updated":"2024-03-20T11:33:50Z","published":"2023-10-02T13:55:38Z","title":"From Bricks to Bridges: Product of Invariances to Enhance Latent Space\n  Communication","summary":"  It has been observed that representations learned by distinct neural networks\nconceal structural similarities when the models are trained under similar\ninductive biases. From a geometric perspective, identifying the classes of\ntransformations and the related invariances that connect these representations\nis fundamental to unlocking applications, such as merging, stitching, and\nreusing different neural modules. However, estimating task-specific\ntransformations a priori can be challenging and expensive due to several\nfactors (e.g., weights initialization, training hyperparameters, or data\nmodality). To this end, we introduce a versatile method to directly incorporate\na set of invariances into the representations, constructing a product space of\ninvariant components on top of the latent representations without requiring\nprior knowledge about the optimal invariance to infuse. We validate our\nsolution on classification and reconstruction tasks, observing consistent\nlatent similarity and downstream performance improvements in a zero-shot\nstitching setting. The experimental analysis comprises three modalities\n(vision, text, and graphs), twelve pretrained foundational models, nine\nbenchmarks, and several architectures trained from scratch.\n","authors":["Irene Cannistraci","Luca Moschella","Marco Fumero","Valentino Maiorca","Emanuele Rodolà"],"pdf_url":"https://arxiv.org/pdf/2310.01211v2.pdf","comment":"41 pages, 14 figures and 31 tables"},{"id":"http://arxiv.org/abs/2304.02472v2","updated":"2024-03-20T11:33:35Z","published":"2023-04-04T12:32:25Z","title":"Learning to Predict Short-Term Volatility with Order Flow Image\n  Representation","summary":"  Introduction: The paper addresses the challenging problem of predicting the\nshort-term realized volatility of the Bitcoin price using order flow\ninformation. The inherent stochastic nature and anti-persistence of price pose\ndifficulties in accurate prediction.\n  Methods: To address this, we propose a method that transforms order flow data\nover a fixed time interval (snapshots) into images. The order flow includes\ntrade sizes, trade directions, and limit order book, and is mapped into image\ncolour channels. These images are then used to train both a simple 3-layer\nConvolutional Neural Network (CNN) and more advanced ResNet-18 and ConvMixer,\nwith additionally supplementing them with hand-crafted features. The models are\nevaluated against classical GARCH, Multilayer Perceptron trained on raw data,\nand a naive guess method that considers current volatility as a prediction.\n  Results: The experiments are conducted using price data from January 2021 and\nevaluate model performance in terms of root mean square error (RMSPE). The\nresults show that our order flow representation with a CNN as a predictive\nmodel achieves the best performance, with an RMSPE of 0.85+/-1.1 for the model\nwith aggregated features and 1.0+/-1.4 for the model without feature\nsupplementation. ConvMixer with feature supplementation follows closely. In\ncomparison, the RMSPE for the naive guess method was 1.4+/-3.0.\n","authors":["Artem Lensky","Mingyu Hao"],"pdf_url":"https://arxiv.org/pdf/2304.02472v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12517v2","updated":"2024-03-20T11:24:06Z","published":"2024-01-23T06:21:34Z","title":"DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing\n  High-Quality Implicit Neural Representations","summary":"  Recent studies have introduced a new class of generative models for\nsynthesizing implicit neural representations (INRs) that capture arbitrary\ncontinuous signals in various domains. These models opened the door for\ndomain-agnostic generative models, but they often fail to achieve high-quality\ngeneration. We observed that the existing methods generate the weights of\nneural networks to parameterize INRs and evaluate the network with fixed\npositional embeddings (PEs). Arguably, this architecture limits the expressive\npower of generative models and results in low-quality INR generation. To\naddress this limitation, we propose Domain-agnostic Latent Diffusion Model for\nINRs (DDMI) that generates adaptive positional embeddings instead of neural\nnetworks' weights. Specifically, we develop a Discrete-to-continuous space\nVariational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and\nthe continuous signal functions in the shared latent space. Additionally, we\nintroduce a novel conditioning mechanism for evaluating INRs with the\nhierarchically decomposed PEs to further enhance expressive power. Extensive\nexperiments across four modalities, e.g., 2D images, 3D shapes, Neural Radiance\nFields, and videos, with seven benchmark datasets, demonstrate the versatility\nof DDMI and its superior performance compared to the existing INR generative\nmodels.\n","authors":["Dogyun Park","Sihyeon Kim","Sojin Lee","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2401.12517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12690v2","updated":"2024-03-20T11:06:34Z","published":"2024-03-19T12:49:09Z","title":"LNPT: Label-free Network Pruning and Training","summary":"  Pruning before training enables the deployment of neural networks on smart\ndevices. By retaining weights conducive to generalization, pruned networks can\nbe accommodated on resource-constrained smart devices. It is commonly held that\nthe distance on weight norms between the initialized and the fully-trained\nnetworks correlates with generalization performance. However, as we have\nuncovered, inconsistency between this metric and generalization during training\nprocesses, which poses an obstacle to determine the pruned structures on smart\ndevices in advance. In this paper, we introduce the concept of the learning\ngap, emphasizing its accurate correlation with generalization. Experiments show\nthat the learning gap, in the form of feature maps from the penultimate layer\nof networks, aligns with variations of generalization performance. We propose a\nnovel learning framework, LNPT, which enables mature networks on the cloud to\nprovide online guidance for network pruning and learning on smart devices with\nunlabeled data. Our results demonstrate the superiority of this approach over\nsupervised training.\n","authors":["Jinying Xiao","Ping Li","Zhe Tang","Jie Nie"],"pdf_url":"https://arxiv.org/pdf/2403.12690v2.pdf","comment":"8 pages,7 figures"},{"id":"http://arxiv.org/abs/2403.13502v1","updated":"2024-03-20T10:59:06Z","published":"2024-03-20T10:59:06Z","title":"Adversarial Attacks and Defenses in Automated Control Systems: A\n  Comprehensive Benchmark","summary":"  Integrating machine learning into Automated Control Systems (ACS) enhances\ndecision-making in industrial process management. One of the limitations to the\nwidespread adoption of these technologies in industry is the vulnerability of\nneural networks to adversarial attacks. This study explores the threats in\ndeploying deep learning models for fault diagnosis in ACS using the Tennessee\nEastman Process dataset. By evaluating three neural networks with different\narchitectures, we subject them to six types of adversarial attacks and explore\nfive different defense methods. Our results highlight the strong vulnerability\nof models to adversarial samples and the varying effectiveness of defense\nstrategies. We also propose a novel protection approach by combining multiple\ndefense methods and demonstrate it's efficacy. This research contributes\nseveral insights into securing machine learning within ACS, ensuring robust\nfault diagnosis in industrial processes.\n","authors":["Vitaliy Pozdnyakov","Aleksandr Kovalenko","Ilya Makarov","Mikhail Drobyshevskiy","Kirill Lukyanov"],"pdf_url":"https://arxiv.org/pdf/2403.13502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13501v1","updated":"2024-03-20T10:58:58Z","published":"2024-03-20T10:58:58Z","title":"VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis","summary":"  Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.\n","authors":["Yumeng Li","William Beluch","Margret Keuper","Dan Zhang","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2403.13501v1.pdf","comment":"Project page: https://yumengli007.github.io/VSTAR"},{"id":"http://arxiv.org/abs/2310.20204v3","updated":"2024-03-20T10:52:03Z","published":"2023-10-31T06:04:18Z","title":"General-Purpose Retrieval-Enhanced Medical Prediction Model Using\n  Near-Infinite History","summary":"  Developing clinical prediction models (e.g., mortality prediction) based on\nelectronic health records (EHRs) typically relies on expert opinion for feature\nselection and adjusting observation window size. This burdens experts and\ncreates a bottleneck in the development process. We propose Retrieval-Enhanced\nMedical prediction model (REMed) to address such challenges. REMed can\nessentially evaluate an unlimited number of clinical events, select the\nrelevant ones, and make predictions. This approach effectively eliminates the\nneed for manual feature selection and enables an unrestricted observation\nwindow. We verified these properties through experiments on 27 clinical tasks\nand two independent cohorts from publicly available EHR datasets, where REMed\noutperformed other contemporary architectures that aim to handle as many events\nas possible. Notably, we found that the preferences of REMed align closely with\nthose of medical experts. We expect our approach to significantly expedite the\ndevelopment of EHR prediction models by minimizing clinicians' need for manual\ninvolvement.\n","authors":["Junu Kim","Chaeeun Shim","Bosco Seong Kyu Yang","Chami Im","Sung Yoon Lim","Han-Gil Jeong","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2310.20204v3.pdf","comment":"The source codes corresponding to this paper are available at:\n  https://github.com/starmpcc/REMed"},{"id":"http://arxiv.org/abs/2401.16914v2","updated":"2024-03-20T10:37:10Z","published":"2024-01-30T11:25:49Z","title":"Energy-conserving equivariant GNN for elasticity of lattice architected\n  metamaterials","summary":"  Lattices are architected metamaterials whose properties strongly depend on\ntheir geometrical design. The analogy between lattices and graphs enables the\nuse of graph neural networks (GNNs) as a faster surrogate model compared to\ntraditional methods such as finite element modelling. In this work, we generate\na big dataset of structure-property relationships for strut-based lattices. The\ndataset is made available to the community which can fuel the development of\nmethods anchored in physical principles for the fitting of fourth-order\ntensors. In addition, we present a higher-order GNN model trained on this\ndataset. The key features of the model are (i) SE(3) equivariance, and (ii)\nconsistency with the thermodynamic law of conservation of energy. We compare\nthe model to non-equivariant models based on a number of error metrics and\ndemonstrate its benefits in terms of predictive performance and reduced\ntraining requirements. Finally, we demonstrate an example application of the\nmodel to an architected material design task. The methods which we developed\nare applicable to fourth-order tensors beyond elasticity such as piezo-optical\ntensor etc.\n","authors":["Ivan Grega","Ilyes Batatia","Gábor Csányi","Sri Karlapati","Vikram S. Deshpande"],"pdf_url":"https://arxiv.org/pdf/2401.16914v2.pdf","comment":"International Conference on Learning Representations 2024"},{"id":"http://arxiv.org/abs/2311.13349v2","updated":"2024-03-20T10:21:34Z","published":"2023-11-22T12:34:51Z","title":"REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints","summary":"  Deep models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models, not capable to\nadapt at runtime. In this work we introduce Resource-Efficient Deep Subnetworks\n(REDS) to tackle model adaptation to variable resources. In contrast to the\nstate-of-the-art, REDS use structured sparsity constructively by exploiting\npermutation invariance of neurons, which allows for hardware-specific\noptimizations. Specifically, REDS achieve computational efficiency by (1)\nskipping sequential computational blocks identified by a novel iterative\nknapsack optimizer, and (2) leveraging simple math to re-arrange the order of\noperations in REDS computational graph to take advantage of the data cache.\nREDS support conventional deep networks frequently deployed on the edge and\nprovide computational benefits even for small and simple networks. We evaluate\nREDS on seven benchmark architectures trained on the Visual Wake Words, Google\nSpeech Commands, Fashion-MNIST and CIFAR10 datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence for REDS outstanding performance in terms of\nsubmodels' test set accuracy, and demonstrate an adaptation time in response to\ndynamic resource constraints of under 40$\\mu$s, utilizing a 2-layer\nfully-connected network on Arduino Nano 33 BLE.\n","authors":["Francesco Corti","Balz Maag","Joachim Schauer","Ulrich Pferschy","Olga Saukh"],"pdf_url":"https://arxiv.org/pdf/2311.13349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15266v2","updated":"2024-03-20T10:12:49Z","published":"2024-02-23T11:27:10Z","title":"Calibration of Deep Learning Classification Models in fNIRS","summary":"  Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool\nfor monitoring brain activity. The classification of fNIRS data in relation to\nconscious activity holds significance for advancing our understanding of the\nbrain and facilitating the development of brain-computer interfaces (BCI). Many\nresearchers have turned to deep learning to tackle the classification\nchallenges inherent in fNIRS data due to its strong generalization and\nrobustness. In the application of fNIRS, reliability is really important, and\none mathematical formulation of the reliability of confidence is calibration.\nHowever, many researchers overlook the important issue of calibration. To\naddress this gap, we propose integrating calibration into fNIRS field and\nassess the reliability of existing models. Surprisingly, our results indicate\npoor calibration performance in many proposed models. To advance calibration\ndevelopment in the fNIRS field, we summarize three practical tips. Through this\nletter, we hope to emphasize the critical role of calibration in fNIRS research\nand argue for enhancing the reliability of deep learning-based predictions in\nfNIRS classification tasks. All data from our experimental process are openly\navailable on GitHub.\n","authors":["Zhihao Cao","Zizhou Luo"],"pdf_url":"https://arxiv.org/pdf/2402.15266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16560v2","updated":"2024-03-20T10:10:33Z","published":"2023-12-27T12:49:27Z","title":"Adaptive Message Passing: A General Framework to Mitigate Oversmoothing,\n  Oversquashing, and Underreaching","summary":"  Long-range interactions are essential for the correct description of complex\nsystems in many scientific fields. The price to pay for including them in the\ncalculations, however, is a dramatic increase in the overall computational\ncosts. Recently, deep graph networks have been employed as efficient,\ndata-driven surrogate models for predicting properties of complex systems\nrepresented as graphs. These models rely on a local and iterative message\npassing strategy that should, in principle, capture long-range information\nwithout explicitly modeling the corresponding interactions. In practice, most\ndeep graph networks cannot really model long-range dependencies due to the\nintrinsic limitations of (synchronous) message passing, namely oversmoothing,\noversquashing, and underreaching. This work proposes a general framework that\nlearns to mitigate these limitations: within a variational inference framework,\nwe endow message passing architectures with the ability to freely adapt their\ndepth and filter messages along the way. With theoretical and empirical\narguments, we show that this simple strategy better captures long-range\ninteractions, by surpassing the state of the art on five node and graph\nprediction datasets suited for this problem. Our approach consistently improves\nthe performances of the baselines tested on these tasks. We complement the\nexposition with qualitative analyses and ablations to get a deeper\nunderstanding of the framework's inner workings.\n","authors":["Federico Errica","Henrik Christiansen","Viktor Zaverkin","Takashi Maruyama","Mathias Niepert","Francesco Alesiani"],"pdf_url":"https://arxiv.org/pdf/2312.16560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18112v2","updated":"2024-03-20T10:08:28Z","published":"2024-02-28T07:02:08Z","title":"Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS\n  to Exclude Abnormal Input","summary":"  Functional near-infrared spectroscopy (fNIRS) is a non-invasive technique for\nmonitoring brain activity. To better understand the brain, researchers often\nuse deep learning to address the classification challenges of fNIRS data. Our\nstudy shows that while current networks in fNIRS are highly accurate for\npredictions within their training distribution, they falter at identifying and\nexcluding abnormal data which is out-of-distribution, affecting their\nreliability. We propose integrating metric learning and supervised methods into\nfNIRS research to improve networks capability in identifying and excluding\nout-of-distribution outliers. This method is simple yet effective. In our\nexperiments, it significantly enhances the performance of various networks in\nfNIRS, particularly transformer-based one, which shows the great improvement in\nreliability. We will make our experiment data available on GitHub.\n","authors":["Zhihao Cao"],"pdf_url":"https://arxiv.org/pdf/2402.18112v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13261v2","updated":"2024-03-20T10:06:09Z","published":"2023-11-22T09:25:08Z","title":"Immunohistochemistry guided segmentation of benign epithelial cells, in\n  situ lesions, and invasive epithelial cells in breast cancer slides","summary":"  Digital pathology enables automatic analysis of histopathological sections\nusing artificial intelligence (AI). Automatic evaluation could improve\ndiagnostic efficiency and help find associations between morphological features\nand clinical outcome. For development of such prediction models, identifying\ninvasive epithelial cells, and separating these from benign epithelial cells\nand in situ lesions would be the first step. In this study, we aimed to develop\nan AI model for segmentation of epithelial cells in sections from breast\ncancer. We generated epithelial ground truth masks by restaining hematoxylin\nand eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists'\nannotations. HE/CK image pairs were used to train a convolutional neural\nnetwork, and data augmentation was used to make the model more robust. Tissue\nmicroarrays (TMAs) from 839 patients, and whole slide images from two patients\nwere used for training and evaluation of the models. The sections were derived\nfrom four cohorts of breast cancer patients. TMAs from 21 patients from a fifth\ncohort was used as a second test set. In quantitative evaluation, a mean Dice\nscore of 0.70, 0.79, and 0.75 for invasive epithelial cells, benign epithelial\ncells, and in situ lesions, respectively, were achieved. In qualitative scoring\n(0-5) by pathologists, results were best for all epithelium and invasive\nepithelium, with scores of 4.7 and 4.4. Scores for benign epithelium and in\nsitu lesions were 3.7 and 2.0. The proposed model segmented epithelial cells in\nHE stained breast cancer slides well, but further work is needed for accurate\ndivision between the classes. Immunohistochemistry, together with pathologists'\nannotations, enabled the creation of accurate ground truths. The model is made\nfreely available in FastPathology and the code is available at\nhttps://github.com/AICAN-Research/breast-epithelium-segmentation\n","authors":["Maren Høibø","André Pedersen","Vibeke Grotnes Dale","Sissel Marie Berget","Borgny Ytterhus","Cecilia Lindskog","Elisabeth Wik","Lars A. Akslen","Ingerid Reinertsen","Erik Smistad","Marit Valla"],"pdf_url":"https://arxiv.org/pdf/2311.13261v2.pdf","comment":"19 pages, 6 figures. Submitted to a scientific journal"},{"id":"http://arxiv.org/abs/2303.06519v2","updated":"2024-03-20T10:00:15Z","published":"2023-03-11T23:50:02Z","title":"Lossless Point Cloud Geometry and Attribute Compression Using a Learned\n  Conditional Probability Model","summary":"  In recent years, we have witnessed the presence of point cloud data in many\naspects of our life, from immersive media, autonomous driving to healthcare,\nalthough at the cost of a tremendous amount of data. In this paper, we present\nan efficient lossless point cloud compression method that uses sparse\ntensor-based deep neural networks to learn point cloud geometry and color\nprobability distributions. Our method represents a point cloud with both\noccupancy feature and three attribute features at different bit depths in a\nunified sparse representation. This allows us to efficiently exploit\nfeature-wise and point-wise dependencies within point clouds using a sparse\ntensor-based neural network and thus build an accurate auto-regressive context\nmodel for an arithmetic coder. To the best of our knowledge, this is the first\nlearning-based lossless point cloud geometry and attribute compression\napproach. Compared with the-state-of-the-art lossless point cloud compression\nmethod from Moving Picture Experts Group (MPEG), our method achieves 22.6%\nreduction in total bitrate on a diverse set of test point clouds while having\n49.0% and 18.3% rate reduction on geometry and color attribute component,\nrespectively.\n","authors":["Dat Thanh Nguyen","Andre Kaup"],"pdf_url":"https://arxiv.org/pdf/2303.06519v2.pdf","comment":"12 pages, accepted to IEEE Transactions on Circuits and Systems for\n  Video Technology"},{"id":"http://arxiv.org/abs/2403.02889v2","updated":"2024-03-20T09:53:17Z","published":"2024-03-05T11:50:01Z","title":"In Search of Truth: An Interrogation Approach to Hallucination Detection","summary":"  Despite the many advances of Large Language Models (LLMs) and their\nunprecedented rapid evolution, their impact and integration into every facet of\nour daily lives is limited due to various reasons. One critical factor\nhindering their widespread adoption is the occurrence of hallucinations, where\nLLMs invent answers that sound realistic, yet drift away from factual truth. In\nthis paper, we present a novel method for detecting hallucinations in large\nlanguage models, which tackles a critical issue in the adoption of these models\nin various real-world scenarios. Through extensive evaluations across multiple\ndatasets and LLMs, including Llama-2, we study the hallucination levels of\nvarious recent LLMs and demonstrate the effectiveness of our method to\nautomatically detect them. Notably, we observe up to 62% hallucinations for\nLlama-2 in a specific experiment, where our method achieves a Balanced Accuracy\n(B-ACC) of 87%, all without relying on external knowledge.\n","authors":["Yakir Yehuda","Itzik Malkiel","Oren Barkan","Jonathan Weill","Royi Ronen","Noam Koenigstein"],"pdf_url":"https://arxiv.org/pdf/2403.02889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.14797v2","updated":"2024-03-20T09:44:53Z","published":"2022-08-31T12:23:53Z","title":"Sparsification of the regularized magnetic Laplacian with multi-type\n  spanning forests","summary":"  In this paper, we consider a ${\\rm U}(1)$-connection graph, that is, a graph\nwhere each oriented edge is endowed with a unit modulus complex number that is\nconjugated under orientation flip. A natural replacement for the combinatorial\nLaplacian is then the magnetic Laplacian, an Hermitian matrix that includes\ninformation about the graph's connection. Magnetic Laplacians appear, e.g., in\nthe problem of angular synchronization. In the context of large and dense\ngraphs, we study here sparsifiers of the magnetic Laplacian $\\Delta$, i.e.,\nspectral approximations based on subgraphs with few edges. Our approach relies\non sampling multi-type spanning forests (MTSFs) using a custom determinantal\npoint process, a probability distribution over edges that favours diversity. In\na word, an MTSF is a spanning subgraph whose connected components are either\ntrees or cycle-rooted trees. The latter partially capture the angular\ninconsistencies of the connection graph, and thus provide a way to compress the\ninformation contained in the connection. Interestingly, when the connection\ngraph has weakly inconsistent cycles, samples from the determinantal point\nprocess under consideration can be obtained \\`a la Wilson, using a random walk\nwith cycle popping. We provide statistical guarantees for a choice of natural\nestimators of the connection Laplacian, and investigate two practical\napplications of our sparsifiers: ranking with angular synchronization and\ngraph-based semi-supervised learning. From a statistical perspective, a side\nresult of this paper of independent interest is a matrix Chernoff bound with\nintrinsic dimension, which allows considering the influence of a regularization\n-- of the form $\\Delta + q \\mathbb{I}$ with $q>0$ -- on sparsification\nguarantees.\n","authors":["Michaël Fanuel","Rémi Bardenet"],"pdf_url":"https://arxiv.org/pdf/2208.14797v2.pdf","comment":"51 pages, 15 figures. Improved presentation of the theoretical\n  results and simulations of larger scale"},{"id":"http://arxiv.org/abs/2403.13441v1","updated":"2024-03-20T09:34:38Z","published":"2024-03-20T09:34:38Z","title":"Robustness Verifcation in Neural Networks","summary":"  In this paper we investigate formal verification problems for Neural Network\ncomputations. Of central importance will be various robustness and minimization\nproblems such as: Given symbolic specifications of allowed inputs and outputs\nin form of Linear Programming instances, one question is whether there do exist\nvalid inputs such that the network computes a valid output? And does this\nproperty hold for all valid inputs? Do two given networks compute the same\nfunction? Is there a smaller network computing the same function?\n  The complexity of these questions have been investigated recently from a\npractical point of view and approximated by heuristic algorithms. We complement\nthese achievements by giving a theoretical framework that enables us to\ninterchange security and efficiency questions in neural networks and analyze\ntheir computational complexities. We show that the problems are conquerable in\na semi-linear setting, meaning that for piecewise linear activation functions\nand when the sum- or maximum metric is used, most of them are in P or in NP at\nmost.\n","authors":["Adrian Wurm"],"pdf_url":"https://arxiv.org/pdf/2403.13441v1.pdf","comment":"16 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.13429v1","updated":"2024-03-20T09:17:12Z","published":"2024-03-20T09:17:12Z","title":"Detecting and Triaging Spoofing using Temporal Convolutional Networks","summary":"  As algorithmic trading and electronic markets continue to transform the\nlandscape of financial markets, detecting and deterring rogue agents to\nmaintain a fair and efficient marketplace is crucial. The explosion of large\ndatasets and the continually changing tricks of the trade make it difficult to\nadapt to new market conditions and detect bad actors. To that end, we propose a\nframework that can be adapted easily to various problems in the space of\ndetecting market manipulation. Our approach entails initially employing a\nlabelling algorithm which we use to create a training set to learn a weakly\nsupervised model to identify potentially suspicious sequences of order book\nstates. The main goal here is to learn a representation of the order book that\ncan be used to easily compare future events. Subsequently, we posit the\nincorporation of expert assessment to scrutinize specific flagged order book\nstates. In the event of an expert's unavailability, recourse is taken to the\napplication of a more complex algorithm on the identified suspicious order book\nstates. We then conduct a similarity search between any new representation of\nthe order book against the expert labelled representations to rank the results\nof the weak learner. We show some preliminary results that are promising to\nexplore further in this direction\n","authors":["Kaushalya Kularatnam","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2403.13429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10298v2","updated":"2024-03-20T09:11:26Z","published":"2024-01-18T05:02:36Z","title":"Machine learning approach to detect dynamical states from recurrence\n  measures","summary":"  We integrate machine learning approaches with nonlinear time series analysis,\nspecifically utilizing recurrence measures to classify various dynamical states\nemerging from time series. We implement three machine learning algorithms\nLogistic Regression, Random Forest, and Support Vector Machine for this study.\nThe input features are derived from the recurrence quantification of nonlinear\ntime series and characteristic measures of the corresponding recurrence\nnetworks. For training and testing we generate synthetic data from standard\nnonlinear dynamical systems and evaluate the efficiency and performance of the\nmachine learning algorithms in classifying time series into periodic, chaotic,\nhyper-chaotic, or noisy categories. Additionally, we explore the significance\nof input features in the classification scheme and find that the features\nquantifying the density of recurrence points are the most relevant.\nFurthermore, we illustrate how the trained algorithms can successfully predict\nthe dynamical states of two variable stars, SX Her and AC Her from the data of\ntheir light curves.\n","authors":["Dheeraja Thakur","Athul Mohan","G. Ambika","Chandrakala Meena"],"pdf_url":"https://arxiv.org/pdf/2401.10298v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12844v2","updated":"2024-03-20T09:06:08Z","published":"2024-03-19T15:51:21Z","title":"MELTing point: Mobile Evaluation of Language Transformers","summary":"  Transformers have revolutionized the machine learning landscape, gradually\nmaking their way into everyday tasks and equipping our computers with ``sparks\nof intelligence''. However, their runtime requirements have prevented them from\nbeing broadly deployed on mobile. As personal devices become increasingly\npowerful and prompt privacy becomes an ever more pressing issue, we explore the\ncurrent state of mobile execution of Large Language Models (LLMs). To achieve\nthis, we have created our own automation infrastructure, MELT, which supports\nthe headless execution and benchmarking of LLMs on device, supporting different\nmodels, devices and frameworks, including Android, iOS and Nvidia Jetson\ndevices. We evaluate popular instruction fine-tuned LLMs and leverage different\nframeworks to measure their end-to-end and granular performance, tracing their\nmemory and energy requirements along the way.\n  Our analysis is the first systematic study of on-device LLM execution,\nquantifying performance, energy efficiency and accuracy across various\nstate-of-the-art models and showcases the state of on-device intelligence in\nthe era of hyperscale models. Results highlight the performance heterogeneity\nacross targets and corroborates that LLM inference is largely memory-bound.\nQuantization drastically reduces memory requirements and renders execution\nviable, but at a non-negligible accuracy cost. Drawing from its energy\nfootprint and thermal behavior, the continuous execution of LLMs remains\nelusive, as both factors negatively affect user experience. Last, our\nexperience shows that the ecosystem is still in its infancy, and algorithmic as\nwell as hardware breakthroughs can significantly shift the execution cost. We\nexpect NPU acceleration, and framework-hardware co-design to be the biggest bet\ntowards efficient standalone execution, with the alternative of offloading\ntailored towards edge deployments.\n","authors":["Stefanos Laskaridis","Kleomenis Katevas","Lorenzo Minto","Hamed Haddadi"],"pdf_url":"https://arxiv.org/pdf/2403.12844v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2402.09782v3","updated":"2024-03-20T08:50:46Z","published":"2024-02-15T08:21:50Z","title":"MC-DBN: A Deep Belief Network-Based Model for Modality Completion","summary":"  Recent advancements in multi-modal artificial intelligence (AI) have\nrevolutionized the fields of stock market forecasting and heart rate\nmonitoring. Utilizing diverse data sources can substantially improve prediction\naccuracy. Nonetheless, additional data may not always align with the original\ndataset. Interpolation methods are commonly utilized for handling missing\nvalues in modal data, though they may exhibit limitations in the context of\nsparse information. Addressing this challenge, we propose a Modality Completion\nDeep Belief Network-Based Model (MC-DBN). This approach utilizes implicit\nfeatures of complete data to compensate for gaps between itself and additional\nincomplete data. It ensures that the enhanced multi-modal data closely aligns\nwith the dynamic nature of the real world to enhance the effectiveness of the\nmodel. We conduct evaluations of the MC-DBN model in two datasets from the\nstock market forecasting and heart rate monitoring domains. Comprehensive\nexperiments showcase the model's capacity to bridge the semantic divide present\nin multi-modal data, subsequently enhancing its performance. The source code is\navailable at: https://github.com/logan-0623/DBN-generate\n","authors":["Zihong Luo","Zheng Tao","Yuxuan Huang","Kexin He","Chengzhi Liu"],"pdf_url":"https://arxiv.org/pdf/2402.09782v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03672v2","updated":"2024-03-20T08:50:24Z","published":"2024-03-06T12:49:08Z","title":"Learning Adversarial MDPs with Stochastic Hard Constraints","summary":"  We study online learning problems in constrained Markov decision processes\n(CMDPs) with adversarial losses and stochastic hard constraints. We consider\ntwo different scenarios. In the first one, we address general CMDPs, where we\ndesign an algorithm that attains sublinear regret and cumulative positive\nconstraints violation. In the second scenario, under the mild assumption that a\npolicy strictly satisfying the constraints exists and is known to the learner,\nwe design an algorithm that achieves sublinear regret while ensuring that the\nconstraints are satisfied at every episode with high probability. To the best\nof our knowledge, our work is the first to study CMDPs involving both\nadversarial losses and hard constraints. Indeed, previous works either focus on\nmuch weaker soft constraints--allowing for positive violation to cancel out\nnegative ones--or are restricted to stochastic losses. Thus, our algorithms can\ndeal with general non-stationary environments subject to requirements much\nstricter than those manageable with state-of-the-art algorithms. This enables\ntheir adoption in a much wider range of real-world applications, ranging from\nautonomous driving to online advertising and recommender systems.\n","authors":["Francesco Emanuele Stradi","Matteo Castiglioni","Alberto Marchesi","Nicola Gatti"],"pdf_url":"https://arxiv.org/pdf/2403.03672v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13374v1","updated":"2024-03-20T08:15:08Z","published":"2024-03-20T08:15:08Z","title":"Byzantine-resilient Federated Learning With Adaptivity to Data\n  Heterogeneity","summary":"  This paper deals with federated learning (FL) in the presence of malicious\nByzantine attacks and data heterogeneity. A novel Robust Average Gradient\nAlgorithm (RAGA) is proposed, which leverages the geometric median for\naggregation and can freely select the round number for local updating.\nDifferent from most existing resilient approaches, which perform convergence\nanalysis based on strongly-convex loss function or homogeneously distributed\ndataset, we conduct convergence analysis for not only strongly-convex but also\nnon-convex loss function over heterogeneous dataset. According to our\ntheoretical analysis, as long as the fraction of dataset from malicious users\nis less than half, RAGA can achieve convergence at rate\n$\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and\n$\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for\nstrongly-convex loss function. Moreover, stationary point or global optimal\nsolution is proved to obtainable as data heterogeneity vanishes. Experimental\nresults corroborate the robustness of RAGA to Byzantine attacks and verifies\nthe advantage of RAGA over baselines on convergence performance under various\nintensity of Byzantine attacks, for heterogeneous dataset.\n","authors":["Shiyuan Zuo","Xingrun Yan","Rongfei Fan","Han Hu","Hangguan Shan","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2403.13374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03653v3","updated":"2024-03-20T08:06:33Z","published":"2024-01-08T03:50:03Z","title":"An Exploratory Study on Automatic Identification of Assumptions in the\n  Development of Deep Learning Frameworks","summary":"  Stakeholders constantly make assumptions in the development of deep learning\n(DL) frameworks. These assumptions are related to various types of software\nartifacts (e.g., requirements, design decisions, and technical debt) and can\nturn out to be invalid, leading to system failures. Existing approaches and\ntools for assumption management usually depend on manual identification of\nassumptions. However, assumptions are scattered in various sources (e.g., code\ncomments, commits, pull requests, and issues) of DL framework development, and\nmanually identifying assumptions has high costs (e.g., time and resources). To\novercome the issues of manually identifying assumptions in DL framework\ndevelopment, we constructed a new and largest dataset (i.e., AssuEval) of\nassumptions collected from the TensorFlow and Keras repositories on GitHub;\nexplored the performance of seven traditional machine learning models (e.g.,\nSupport Vector Machine, Classification and Regression Trees), a popular DL\nmodel (i.e., ALBERT), and a large language model (i.e., ChatGPT) of identifying\nassumptions on the AssuEval dataset. The experiment results show that: ALBERT\nachieves the best performance (f1-score: 0.9584) of identifying assumptions on\nthe AssuEval dataset, which is much better than the other models (the 2nd best\nf1-score is 0.6211, achieved by ChatGPT). Though ChatGPT is the most popular\nlarge language model, we do not recommend using it to identify assumptions in\nDL framework development because of its low performance on the task.\nFine-tuning ChatGPT specifically for assumption identification could improve\nthe performance. This study provides researchers with the largest dataset of\nassumptions for further research (e.g., assumption classification, evaluation,\nand reasoning) and helps practitioners better understand assumptions and how to\nmanage them in their projects.\n","authors":["Chen Yang","Peng Liang","Zinan Ma"],"pdf_url":"https://arxiv.org/pdf/2401.03653v3.pdf","comment":"28 pages, 15 images, 10 tables, Manuscript submitted to a Journal\n  (2024)"},{"id":"http://arxiv.org/abs/2403.13370v1","updated":"2024-03-20T08:04:00Z","published":"2024-03-20T08:04:00Z","title":"Counting Network for Learning from Majority Label","summary":"  The paper proposes a novel problem in multi-class Multiple-Instance Learning\n(MIL) called Learning from the Majority Label (LML). In LML, the majority class\nof instances in a bag is assigned as the bag's label. LML aims to classify\ninstances using bag-level majority classes. This problem is valuable in various\napplications. Existing MIL methods are unsuitable for LML due to aggregating\nconfidences, which may lead to inconsistency between the bag-level label and\nthe label obtained by counting the number of instances for each class. This may\nlead to incorrect instance-level classification. We propose a counting network\ntrained to produce the bag-level majority labels estimated by counting the\nnumber of instances for each class. This led to the consistency of the majority\nclass between the network outputs and one obtained by counting the number of\ninstances. Experimental results show that our counting network outperforms\nconventional MIL methods on four datasets The code is publicly available at\nhttps://github.com/Shiku-Kaito/Counting-Network-for-Learning-from-Majority-Label.\n","authors":["Kaito Shiku","Shinnosuke Matsuo","Daiki Suehiro","Ryoma Bise"],"pdf_url":"https://arxiv.org/pdf/2403.13370v1.pdf","comment":"5 pages, 4 figures, Accepted in ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.13369v1","updated":"2024-03-20T08:01:33Z","published":"2024-03-20T08:01:33Z","title":"Clinical information extraction for Low-resource languages with Few-shot\n  learning using Pre-trained language models and Prompting","summary":"  Automatic extraction of medical information from clinical documents poses\nseveral challenges: high costs of required clinical expertise, limited\ninterpretability of model predictions, restricted computational resources and\nprivacy regulations. Recent advances in domain-adaptation and prompting methods\nshowed promising results with minimal training data using lightweight masked\nlanguage models, which are suited for well-established interpretability\nmethods. We are first to present a systematic evaluation of these methods in a\nlow-resource setting, by performing multi-class section classification on\nGerman doctor's letters. We conduct extensive class-wise evaluations supported\nby Shapley values, to validate the quality of our small training data set and\nto ensure the interpretability of model predictions. We demonstrate that a\nlightweight, domain-adapted pretrained model, prompted with just 20 shots,\noutperforms a traditional classification model by 30.5% accuracy. Our results\nserve as a process-oriented guideline for clinical information extraction\nprojects working with low-resource.\n","authors":["Phillip Richter-Pechanski","Philipp Wiesenbach","Dominic M. Schwab","Christina Kiriakou","Nicolas Geis","Christoph Dieterich","Anette Frank"],"pdf_url":"https://arxiv.org/pdf/2403.13369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.14531v2","updated":"2024-03-20T07:49:41Z","published":"2023-07-26T22:39:47Z","title":"Controlling the Inductive Bias of Wide Neural Networks by Modifying the\n  Kernel's Spectrum","summary":"  Wide neural networks are biased towards learning certain functions,\ninfluencing both the rate of convergence of gradient descent (GD) and the\nfunctions that are reachable with GD in finite training time. As such, there is\na great need for methods that can modify this bias according to the task at\nhand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel\nfamily of constructed kernels that can be used to approximate kernels with\ndesired eigenvalues for which no closed form is known. We leverage the duality\nbetween wide neural networks and Neural Tangent Kernels and propose a\npreconditioned gradient descent method, which alters the trajectory of GD. As a\nresult, this allows for a polynomial and, in some cases, exponential training\nspeedup without changing the final solution. Our method is both computationally\nefficient and simple to implement.\n","authors":["Amnon Geifman","Daniel Barzilai","Ronen Basri","Meirav Galun"],"pdf_url":"https://arxiv.org/pdf/2307.14531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03512v3","updated":"2024-03-20T07:39:48Z","published":"2024-01-07T15:00:36Z","title":"CharPoet: A Chinese Classical Poetry Generation System Based on\n  Token-free LLM","summary":"  Automatic Chinese classical poetry generation has attracted much research\ninterest, but achieving effective control over format and content\nsimultaneously remains challenging. Traditional systems usually accept keywords\nas user inputs, resulting in limited control over content. Large language\nmodels (LLMs) improve content control by allowing unrestricted user\ninstructions, but the token-by-token generation process frequently makes format\nerrors. Motivated by this, we propose CharPoet, a Chinese classical poetry\ngeneration system based on token-free LLM, which provides effective control\nover both format and content. Our token-free architecture generates in a\ncharacter-by-character manner, enabling precise control over the number of\ncharacters. Pruned from existing token-based LLMs, CharPoet inherits their\npretrained capabilities and can generate poetry following instructions like\n\"Write me a poem for my mother's birthday.\" CharPoet achieves format accuracy\nabove 0.96, outperforming Jiuge-GPT-2 (0.91) and GPT-4 (0.38). In terms of\ncontent quality, CharPoet surpasses traditional systems including Jiuge, and is\ncomparable to other LLMs. Our system is open source and available at\nhttps://modelscope.cn/models/CharPoet/CharPoet. A video demonstration of\nCharPoet is available at https://youtu.be/voZ25qEp3Dc.\n","authors":["Chengyue Yu","Lei Zang","Jiaotuan Wang","Chenyi Zhuang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2401.03512v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.13667v1","updated":"2024-03-20T15:24:57Z","published":"2024-03-20T15:24:57Z","title":"DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance","summary":"  Choreographers determine what the dances look like, while cameramen determine\nthe final presentation of dances. Recently, various methods and datasets have\nshowcased the feasibility of dance synthesis. However, camera movement\nsynthesis with music and dance remains an unsolved challenging problem due to\nthe scarcity of paired data. Thus, we present DCM, a new multi-modal 3D\ndataset, which for the first time combines camera movement with dance motion\nand music audio. This dataset encompasses 108 dance sequences (3.2 hours) of\npaired dance-camera-music data from the anime community, covering 4 music\ngenres. With this dataset, we uncover that dance camera movement is\nmultifaceted and human-centric, and possesses multiple influencing factors,\nmaking dance camera synthesis a more challenging task compared to camera or\ndance synthesis alone. To overcome these difficulties, we propose\nDanceCamera3D, a transformer-based diffusion model that incorporates a novel\nbody attention loss and a condition separation strategy. For evaluation, we\ndevise new metrics measuring camera movement quality, diversity, and dancer\nfidelity. Utilizing these metrics, we conduct extensive experiments on our DCM\ndataset, providing both quantitative and qualitative evidence showcasing the\neffectiveness of our DanceCamera3D model. Code and video demos are available at\nhttps://github.com/Carmenw1203/DanceCamera3D-Official.\n","authors":["Zixuan Wang","Jia Jia","Shikun Sun","Haozhe Wu","Rong Han","Zhenyu Li","Di Tang","Jiaqing Zhou","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2403.13667v1.pdf","comment":"Accept to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11959v2","updated":"2024-03-20T11:58:23Z","published":"2024-03-18T16:56:47Z","title":"IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video\n  Action Counting","summary":"  Video Action Counting (VAC) is crucial in analyzing sports, fitness, and\neveryday activities by quantifying repetitive actions in videos. However,\ntraditional VAC methods have overlooked the complexity of action repetitions,\nsuch as interruptions and the variability in cycle duration. Our research\naddresses the shortfall by introducing a novel approach to VAC, called\nIrregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular\nrepetition patterns in videos, which we define through two primary aspects:\nInter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle\nConsistency ensures homogeneity in the spatial-temporal representations of\ncycle segments, signifying action uniformity within cycles. Cycle-interval\ninconsistency highlights the importance of distinguishing between cycle\nsegments and intervals based on their inherent content differences. To\nencapsulate these principles, we propose a new methodology that includes\nconsistency and inconsistency modules, supported by a unique pull-push loss\n(P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence\namong cycle segment features and a push loss to clearly distinguish features of\ncycle segments from interval segments. Empirical evaluations conducted on the\nRepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in\nVAC task performance. Furthermore, the model demonstrates exceptional\nadaptability and generalization across various video contents, outperforming\nexisting models on two additional datasets, UCFRep and Countix, without the\nneed for dataset-specific optimization. These results confirm the efficacy of\nour approach in addressing irregular repetitions in videos and pave the way for\nfurther advancements in video analysis and understanding.\n","authors":["Hang Wang","Zhi-Qi Cheng","Youtian Du","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11959v2.pdf","comment":"Source code: https://github.com/hwang-cs-ime/IVAC-P2L"},{"id":"http://arxiv.org/abs/2403.13501v1","updated":"2024-03-20T10:58:58Z","published":"2024-03-20T10:58:58Z","title":"VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis","summary":"  Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.\n","authors":["Yumeng Li","William Beluch","Margret Keuper","Dan Zhang","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2403.13501v1.pdf","comment":"Project page: https://yumengli007.github.io/VSTAR"},{"id":"http://arxiv.org/abs/2403.13480v1","updated":"2024-03-20T10:34:40Z","published":"2024-03-20T10:34:40Z","title":"A Unified Optimal Transport Framework for Cross-Modal Retrieval with\n  Noisy Labels","summary":"  Cross-modal retrieval (CMR) aims to establish interaction between different\nmodalities, among which supervised CMR is emerging due to its flexibility in\nlearning semantic category discrimination. Despite the remarkable performance\nof previous supervised CMR methods, much of their success can be attributed to\nthe well-annotated data. However, even for unimodal data, precise annotation is\nexpensive and time-consuming, and it becomes more challenging with the\nmultimodal scenario. In practice, massive multimodal data are collected from\nthe Internet with coarse annotation, which inevitably introduces noisy labels.\nTraining with such misleading labels would bring two key challenges --\nenforcing the multimodal samples to \\emph{align incorrect semantics} and\n\\emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To\ntackle these challenges, this work proposes UOT-RCL, a Unified framework based\non Optimal Transport (OT) for Robust Cross-modal Retrieval. First, we propose a\nsemantic alignment based on partial OT to progressively correct the noisy\nlabels, where a novel cross-modal consistent cost function is designed to blend\ndifferent modalities and provide precise transport cost. Second, to narrow the\ndiscrepancy in multi-modal data, an OT-based relation alignment is proposed to\ninfer the semantic-level cross-modal matching. Both of these two components\nleverage the inherent correlation among multi-modal data to facilitate\neffective cost function. The experiments on three widely-used cross-modal\nretrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art\napproaches and significantly improves the robustness against noisy labels.\n","authors":["Haochen Han","Minnan Luo","Huan Liu","Fang Nan"],"pdf_url":"https://arxiv.org/pdf/2403.13480v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2403.12053v2","updated":"2024-03-20T03:45:34Z","published":"2024-01-04T12:02:15Z","title":"PiGW: A Plug-in Generative Watermarking Framework","summary":"  Integrating watermarks into generative images is a critical strategy for\nprotecting intellectual property and enhancing artificial intelligence\nsecurity. This paper proposes Plug-in Generative Watermarking (PiGW) as a\ngeneral framework for integrating watermarks into generative images. More\nspecifically, PiGW embeds watermark information into the initial noise using a\nlearnable watermark embedding network and an adaptive frequency spectrum mask.\nFurthermore, it optimizes training costs by gradually increasing timesteps.\nExtensive experiments demonstrate that PiGW enables embedding watermarks into\nthe generated image with negligible quality loss while achieving true\ninvisibility and high resistance to noise attacks. Moreover, PiGW can serve as\na plugin for various commonly used generative structures and multimodal\ngenerative content types. Finally, we demonstrate how PiGW can also be utilized\nfor detecting generated images, contributing to the promotion of secure AI\ndevelopment. The project code will be made available on GitHub.\n","authors":["Rui Ma","Mengxi Guo","Li Yuming","Hengyuan Zhang","Cong Ma","Yuan Li","Xiaodong Xie","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12053v2.pdf","comment":"Improve experimental content"},{"id":"http://arxiv.org/abs/2403.12667v2","updated":"2024-03-20T03:15:31Z","published":"2024-03-19T12:05:09Z","title":"ICE: Interactive 3D Game Character Editing via Dialogue","summary":"  Text-driven in-game 3D character auto-customization systems eliminate the\ncomplicated process of manipulating intricate character control parameters.\nHowever, current methods are limited by their single-round generation,\nincapable of further editing and fine-grained modification. In this paper, we\npropose an Interactive Character Editing framework (ICE) to achieve a\nmulti-round dialogue-based refinement process. In a nutshell, our ICE offers a\nmore user-friendly way to enable players to convey creative ideas iteratively\nwhile ensuring that created characters align with the expectations of players.\nSpecifically, we propose an Instruction Parsing Module (IPM) that utilizes\nlarge language models (LLMs) to parse multi-round dialogues into clear editing\ninstruction prompts in each round. To reliably and swiftly modify character\ncontrol parameters at a fine-grained level, we propose a Semantic-guided\nLow-dimension Parameter Solver (SLPS) that edits character control parameters\naccording to prompts in a zero-shot manner. Our SLPS first localizes the\ncharacter control parameters related to the fine-grained modification, and then\noptimizes the corresponding parameters in a low-dimension space to avoid\nunrealistic results. Extensive experimental results demonstrate the\neffectiveness of our proposed ICE for in-game character creation and the\nsuperior editing performance of ICE. Project page: https://iceedit.github.io/.\n","authors":["Haoqian Wu","Yunjie Wu","Zhipeng Hu","Lincheng Li","Weijie Chen","Rui Zhao","Changjie Fan","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2403.12667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10943v2","updated":"2024-03-20T02:52:42Z","published":"2024-03-16T15:14:15Z","title":"MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent\n  Recognition and Out-of-scope Detection in Conversations","summary":"  Multimodal intent recognition poses significant challenges, requiring the\nincorporation of non-verbal modalities from real-world contexts to enhance the\ncomprehension of human intentions. Existing benchmark datasets are limited in\nscale and suffer from difficulties in handling out-of-scope samples that arise\nin multi-turn conversational interactions. We introduce MIntRec2.0, a\nlarge-scale benchmark dataset for multimodal intent recognition in multi-party\nconversations. It contains 1,245 dialogues with 15,040 samples, each annotated\nwithin a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope\nsamples, it also includes 5,736 out-of-scope samples appearing in multi-turn\ncontexts, which naturally occur in real-world scenarios. Furthermore, we\nprovide comprehensive information on the speakers in each utterance, enriching\nits utility for multi-party conversational research. We establish a general\nframework supporting the organization of single-turn and multi-turn dialogue\ndata, modality feature extraction, multimodal fusion, as well as in-scope\nclassification and out-of-scope detection. Evaluation benchmarks are built\nusing classic multimodal fusion methods, ChatGPT, and human evaluators. While\nexisting methods incorporating nonverbal information yield improvements,\neffectively leveraging context information and detecting out-of-scope samples\nremains a substantial challenge. Notably, large language models exhibit a\nsignificant performance gap compared to humans, highlighting the limitations of\nmachine learning methods in the cognitive intent understanding task. We believe\nthat MIntRec2.0 will serve as a valuable resource, providing a pioneering\nfoundation for research in human-machine conversational interactions, and\nsignificantly facilitating related applications. The full dataset and codes are\navailable at https://github.com/thuiar/MIntRec2.0.\n","authors":["Hanlei Zhang","Xin Wang","Hua Xu","Qianrui Zhou","Kai Gao","Jianhua Su","jinyue Zhao","Wenrui Li","Yanting Chen"],"pdf_url":"https://arxiv.org/pdf/2403.10943v2.pdf","comment":"Published in ICLR 2024; The abstract is slightly modified due to the\n  length limitation"},{"id":"http://arxiv.org/abs/2403.08505v2","updated":"2024-03-20T02:35:57Z","published":"2024-03-13T13:12:57Z","title":"Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression","summary":"  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed.\n","authors":["Xinjie Zhang","Shenyuan Gao","Zhening Liu","Jiawei Shao","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08505v2.pdf","comment":null}]},"2024-03-21T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.14624v1","updated":"2024-03-21T17:59:50Z","published":"2024-03-21T17:59:50Z","title":"MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual\n  Math Problems?","summary":"  The remarkable progress of Multi-modal Large Language Models (MLLMs) has\ngarnered unparalleled attention, due to their superior performance in visual\ncontexts. However, their capabilities in visual math problem-solving remain\ninsufficiently evaluated and understood. We investigate current benchmarks to\nincorporate excessive visual content within textual questions, which\npotentially assist MLLMs in deducing answers without truly interpreting the\ninput diagrams. To this end, we introduce MathVerse, an all-around visual math\nbenchmark designed for an equitable and in-depth evaluation of MLLMs. We\nmeticulously collect 2,612 high-quality, multi-subject math problems with\ndiagrams from publicly available sources. Each problem is then transformed by\nhuman annotators into six distinct versions, each offering varying degrees of\ninformation content in multi-modality, contributing to 15K test samples in\ntotal. This approach allows MathVerse to comprehensively assess whether and how\nmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.\nIn addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a\nfine-grained assessment of the output answers. Rather than naively judging True\nor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and\nthen score each step with detailed error analysis, which can reveal the\nintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark\nmay provide unique insights to guide the future development of MLLMs. Project\npage: https://mathverse-cuhk.github.io\n","authors":["Renrui Zhang","Dongzhi Jiang","Yichi Zhang","Haokun Lin","Ziyu Guo","Pengshuo Qiu","Aojun Zhou","Pan Lu","Kai-Wei Chang","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.14624v1.pdf","comment":"46 Pages, Work in Progress, Benchmark Project Page:\n  https://mathverse-cuhk.github.io"},{"id":"http://arxiv.org/abs/2311.09206v2","updated":"2024-03-21T17:56:37Z","published":"2023-11-15T18:47:52Z","title":"TableLlama: Towards Open Large Generalist Models for Tables","summary":"  Semi-structured tables are ubiquitous. There has been a variety of tasks that\naim to automatically interpret, augment, and query tables. Current methods\noften require pretraining on tables or special model architecture design, are\nrestricted to specific table types, or have simplifying assumptions about\ntables and tasks. This paper makes the first step towards developing\nopen-source large language models (LLMs) as generalists for a diversity of\ntable-based tasks. Towards that end, we construct TableInstruct, a new dataset\nwith a variety of realistic tables and tasks, for instruction tuning and\nevaluating LLMs. We further develop the first open-source generalist model for\ntables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the\nlong context challenge. We experiment under both in-domain setting and\nout-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves\ncomparable or better performance than the SOTA for each task, despite the\nlatter often has task-specific design. On 6 out-of-domain datasets, it achieves\n5-44 absolute point gains compared with the base model, showing that training\non TableInstruct enhances the model's generalizability. We open-source our\ndataset and trained model to boost future work on developing open generalist\nmodels for tables.\n","authors":["Tianshu Zhang","Xiang Yue","Yifei Li","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2311.09206v2.pdf","comment":"NAACL 2024 long paper"},{"id":"http://arxiv.org/abs/2403.14589v1","updated":"2024-03-21T17:43:44Z","published":"2024-03-21T17:43:44Z","title":"ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for\n  Contrastive Self-Training","summary":"  Language agents have demonstrated autonomous decision-making abilities by\nreasoning with foundation models. Recently, efforts have been made to train\nlanguage agents for performance improvement, with multi-step reasoning and\naction trajectories as the training data. However, collecting such trajectories\nstill requires considerable human effort, by either artificial annotations or\nimplementations of diverse prompting frameworks. In this work, we propose\nA$^3$T, a framework that enables the Autonomous Annotation of Agent\nTrajectories in the style of ReAct. The central role is an ActRe prompting\nagent, which explains the reason for an arbitrary action. When randomly\nsampling an external action, the ReAct-style agent could query the ActRe agent\nwith the action to obtain its textual rationales. Novel trajectories are then\nsynthesized by prepending the posterior reasoning from ActRe to the sampled\naction. In this way, the ReAct-style agent executes multiple trajectories for\nthe failed tasks, and selects the successful ones to supplement its failed\ntrajectory for contrastive self-training. Realized by policy gradient methods\nwith binarized rewards, the contrastive self-training with accumulated\ntrajectories facilitates a closed loop for multiple rounds of language agent\nself-improvement. We conduct experiments using QLoRA fine-tuning with the\nopen-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with\nA$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative\nrounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human\naverage, and 4 rounds of iterative refinement lead to the performance\napproaching human experts. A$^3$T agents significantly outperform existing\ntechniques, including prompting with GPT-4, advanced agent frameworks, and\nfully fine-tuned LLMs.\n","authors":["Zonghan Yang","Peng Li","Ming Yan","Ji Zhang","Fei Huang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14582v1","updated":"2024-03-21T17:36:08Z","published":"2024-03-21T17:36:08Z","title":"Large Language Models for Multi-Choice Question Classification of\n  Medical Subjects","summary":"  The aim of this paper is to evaluate whether large language models trained on\nmulti-choice question data can be used to discriminate between medical\nsubjects. This is an important and challenging task for automatic question\nanswering. To achieve this goal, we train deep neural networks for multi-class\nclassification of questions into the inferred medical subjects. Using our\nMulti-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art\nresults on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their\ndevelopment and test sets, respectively. In this sense, we show the capability\nof AI and LLMs in particular for multi-classification tasks in the Healthcare\ndomain.\n","authors":["Víctor Ponce-López"],"pdf_url":"https://arxiv.org/pdf/2403.14582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11085v3","updated":"2024-03-21T17:25:23Z","published":"2024-03-17T04:36:18Z","title":"m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks","summary":"  Real-world multi-modal problems are rarely solved by a single machine\nlearning model, and often require multi-step computational plans that involve\nstitching several models. Tool-augmented LLMs hold tremendous promise for\nautomating the generation of such computational plans. However, the lack of\nstandardized benchmarks for evaluating LLMs as planners for multi-step\nmulti-modal tasks has prevented a systematic study of planner design decisions.\nShould LLMs generate a full plan in a single shot or step-by-step? Should they\ninvoke tools directly with Python code or through structured data formats like\nJSON? Does feedback improve planning? To answer these questions and more, we\nintroduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks\ninvolving 33 tools that include multi-modal models, (free) public APIs, and\nimage processing modules. For each of these task queries, we provide\nautomatically generated plans using this realistic toolset. We further provide\na high-quality subset of 1,565 task plans that are human-verified and correctly\nexecutable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies\n(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3\ntypes of feedback (parsing/verification/execution). Finally, we summarize\ntakeaways from our extensive experiments. Our dataset and code are available on\nHuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github\n(https://github.com/RAIVNLab/mnms).\n","authors":["Zixian Ma","Weikai Huang","Jieyu Zhang","Tanmay Gupta","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.11085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14565v1","updated":"2024-03-21T17:09:08Z","published":"2024-03-21T17:09:08Z","title":"A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students'\n  Formative Assessment Responses in Science","summary":"  This paper explores the use of large language models (LLMs) to score and\nexplain short-answer assessments in K-12 science. While existing methods can\nscore more structured math and computer science assessments, they often do not\nprovide explanations for the scores. Our study focuses on employing GPT-4 for\nautomated assessment in middle school Earth Science, combining few-shot and\nactive learning with chain-of-thought reasoning. Using a human-in-the-loop\napproach, we successfully score and provide meaningful explanations for\nformative assessment responses. A systematic analysis of our method's pros and\ncons sheds light on the potential for human-in-the-loop techniques to enhance\nautomated grading for open-ended science assessments.\n","authors":["Clayton Cohn","Nicole Hutchins","Tuan Le","Gautam Biswas"],"pdf_url":"https://arxiv.org/pdf/2403.14565v1.pdf","comment":"In press at EAAI-24: The 14th Symposium on Educational Advances in\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2403.06563v2","updated":"2024-03-21T17:08:43Z","published":"2024-03-11T10:05:29Z","title":"Unraveling the Mystery of Scaling Laws: Part I","summary":"  Scaling law principles indicate a power-law correlation between loss and\nvariables such as model size, dataset size, and computational resources\nutilized during training. These principles play a vital role in optimizing\nvarious aspects of model pre-training, ultimately contributing to the success\nof large language models such as GPT-4, Llama and Gemini. However, the original\nscaling law paper by OpenAI did not disclose the complete details necessary to\nderive the precise scaling law formulas, and their conclusions are only based\non models containing up to 1.5 billion parameters. Though some subsequent works\nattempt to unveil these details and scale to larger models, they often neglect\nthe training dependency of important factors such as the learning rate, context\nlength and batch size, leading to their failure to establish a reliable formula\nfor predicting the test loss trajectory. In this technical report, we confirm\nthat the scaling law formulations proposed in the original OpenAI paper remain\nvalid when scaling the model size up to 33 billion, but the constant\ncoefficients in these formulas vary significantly with the experiment setup. We\nmeticulously identify influential factors and provide transparent, step-by-step\ninstructions to estimate all constant terms in scaling-law formulas by training\non models with only 1M~60M parameters. Using these estimated formulas, we\nshowcase the capability to accurately predict various attributes for models\nwith up to 33B parameters before their training, including (1) the minimum\npossible test loss; (2) the minimum required training steps and processed\ntokens to achieve a specific loss; (3) the critical batch size with an optimal\ntime/computation trade-off at any loss value; and (4) the complete test loss\ntrajectory with arbitrary batch size.\n","authors":["Hui Su","Zhi Tian","Xiaoyu Shen","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2403.06563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14562v1","updated":"2024-03-21T17:06:17Z","published":"2024-03-21T17:06:17Z","title":"The Era of Semantic Decoding","summary":"  Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation.\n","authors":["Maxime Peyrard","Martin Josifoski","Robert West"],"pdf_url":"https://arxiv.org/pdf/2403.14562v1.pdf","comment":"25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.14551v1","updated":"2024-03-21T16:52:01Z","published":"2024-03-21T16:52:01Z","title":"Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling","summary":"  Today's most accurate language models are trained on orders of magnitude more\nlanguage data than human language learners receive - but with no supervision\nfrom other sensory modalities that play a crucial role in human learning. Can\nwe make LMs' representations and predictions more accurate (and more\nhuman-like) with more ecologically plausible supervision? This paper describes\nLexiContrastive Grounding (LCG), a grounded language learning procedure that\nleverages visual supervision to improve textual representations.\nLexiContrastive Grounding combines a next token prediction strategy with a\ncontrastive visual grounding objective, focusing on early-layer representations\nthat encode lexical information. Across multiple word-learning and\nsentence-understanding benchmarks, LexiContrastive Grounding not only\noutperforms standard language-only models in learning efficiency, but also\nimproves upon vision-and-language learning procedures including CLIP, GIT,\nFlamingo, and Vokenization. Moreover, LexiContrastive Grounding improves\nperplexity by around 5% on multiple language modeling tasks. This work\nunderscores the potential of incorporating visual grounding into language\nmodels, aligning more closely with the multimodal nature of human language\nacquisition.\n","authors":["Chengxu Zhuang","Evelina Fedorenko","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2403.14551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14541v1","updated":"2024-03-21T16:41:12Z","published":"2024-03-21T16:41:12Z","title":"EDT: Improving Large Language Models' Generation by Entropy-based\n  Dynamic Temperature Sampling","summary":"  Recently, Large Language Models (LLMs) have demonstrated outstanding\nperformance across a wide range of downstream language tasks. Temperature\nsampling is a commonly used decoding strategy for LLMs' generation process.\nHowever, a fixed temperature parameter is used in most cases, which may not\nalways be an optimal choice for balancing generation quality and diversity. In\nthis paper, we propose an effective Entropy-based Dynamic Temperature (EDT)\nSampling method, to achieve a more balanced performance in terms of both\ngeneration quality and diversity by dynamically selecting the temperature\nparameter. Additionally, we also show model performance and comprehensive\nanalyses for 4 different generation benchmarks. Our experiments show that EDT\nsignificantly outperforms the existing strategies across different tasks.\n","authors":["Shimao Zhang","Yu Bao","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2403.14541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14515v1","updated":"2024-03-21T16:11:44Z","published":"2024-03-21T16:11:44Z","title":"Building a Language-Learning Game for Brazilian Indigenous Languages: A\n  Case of Study","summary":"  In this paper we discuss a first attempt to build a language learning game\nfor brazilian indigenous languages and the challenges around it. We present a\ndesign for the tool with gamification aspects. Then we describe a process to\nautomatically generate language exercises and questions from a dependency\ntreebank and a lexical database for Tupian languages. We discuss the\nlimitations of our prototype highlighting ethical and practical implementation\nconcerns. Finally, we conclude that new data gathering processes should be\nestablished in partnership with indigenous communities and oriented for\neducational purposes.\n","authors":["Gustavo Polleti"],"pdf_url":"https://arxiv.org/pdf/2403.14515v1.pdf","comment":"First Workshop on NLP for Indigenous Languages of Lusophone\n  Countries, 16th International Conference on Computational Processing of\n  Portuguese (PROPOR 2024)"},{"id":"http://arxiv.org/abs/2402.03049v3","updated":"2024-03-21T15:33:34Z","published":"2024-02-05T14:33:56Z","title":"EasyInstruct: An Easy-to-use Instruction Processing Framework for Large\n  Language Models","summary":"  In recent years, instruction tuning has gained increasing attention and\nemerged as a crucial technique to enhance the capabilities of Large Language\nModels (LLMs). To construct high-quality instruction datasets, many instruction\nprocessing approaches have been proposed, aiming to achieve a delicate balance\nbetween data quantity and data quality. Nevertheless, due to inconsistencies\nthat persist among various instruction processing methods, there is no standard\nopen-source instruction processing implementation framework available for the\ncommunity, which hinders practitioners from further developing and advancing.\nTo facilitate instruction processing research and development, we present\nEasyInstruct, an easy-to-use instruction processing framework for LLMs, which\nmodularizes instruction generation, selection, and prompting, while also\nconsidering their combination and interaction. EasyInstruct is publicly\nreleased and actively maintained at https://github.com/zjunlp/EasyInstruct,\nalong with an online demo app and a demo video for quick-start, calling for\nbroader research centered on instruction data and synthetic data.\n","authors":["Yixin Ou","Ningyu Zhang","Honghao Gui","Ziwen Xu","Shuofei Qiao","Yida Xue","Runnan Fang","Kangwei Liu","Lei Li","Zhen Bi","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.03049v3.pdf","comment":"Project website: https://zjunlp.github.io/project/EasyInstruct Code:\n  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo\n  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct"},{"id":"http://arxiv.org/abs/2309.11566v2","updated":"2024-03-21T15:31:45Z","published":"2023-09-20T18:08:28Z","title":"SignBank+: Preparing a Multilingual Sign Language Dataset for Machine\n  Translation Using Large Language Models","summary":"  We introduce SignBank+, a clean version of the SignBank dataset, optimized\nfor machine translation between spoken language text and SignWriting, a\nphonetic sign language writing system. In addition to previous work that\nemploys complex factorization techniques to enable translation between text and\nSignWriting, we show that a traditional text-to-text translation approach\nperforms equally effectively on the cleaned SignBank+ dataset. Our evaluation\nresults indicate that models trained on SignBank+ surpass those on the original\ndataset, establishing a new benchmark for SignWriting-based sign language\ntranslation and providing an open resource for future research.\n","authors":["Amit Moryossef","Zifan Jiang"],"pdf_url":"https://arxiv.org/pdf/2309.11566v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14472v1","updated":"2024-03-21T15:18:30Z","published":"2024-03-21T15:18:30Z","title":"Detoxifying Large Language Models via Knowledge Editing","summary":"  This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments to compare knowledge\nediting approaches with previous baselines, indicating that knowledge editing\nhas the potential to efficiently detoxify LLMs with limited impact on general\nperformance. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxify\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.\n","authors":["Mengru Wang","Ningyu Zhang","Ziwen Xu","Zekun Xi","Shumin Deng","Yunzhi Yao","Qishen Zhang","Linyi Yang","Jindong Wang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14472v1.pdf","comment":"Ongoing work. Project website:\n  https://zjunlp.github.io/project/SafeEdit Benchmark:\n  https://huggingface.co/datasets/zjunlp/SafeEdit Code:\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2403.14469v1","updated":"2024-03-21T15:16:50Z","published":"2024-03-21T15:16:50Z","title":"ChatGPT Alternative Solutions: Large Language Models Survey","summary":"  In recent times, the grandeur of Large Language Models (LLMs) has not only\nshone in the realm of natural language processing but has also cast its\nbrilliance across a vast array of applications. This remarkable display of LLM\ncapabilities has ignited a surge in research contributions within this domain,\nspanning a diverse spectrum of topics. These contributions encompass\nadvancements in neural network architecture, context length enhancements, model\nalignment, training datasets, benchmarking, efficiency improvements, and more.\nRecent years have witnessed a dynamic synergy between academia and industry,\npropelling the field of LLM research to new heights. A notable milestone in\nthis journey is the introduction of ChatGPT, a powerful AI chatbot grounded in\nLLMs, which has garnered widespread societal attention. The evolving technology\nof LLMs has begun to reshape the landscape of the entire AI community,\npromising a revolutionary shift in the way we create and employ AI algorithms.\nGiven this swift-paced technical evolution, our survey embarks on a journey to\nencapsulate the recent strides made in the world of LLMs. Through an\nexploration of the background, key discoveries, and prevailing methodologies,\nwe offer an up-to-the-minute review of the literature. By examining multiple\nLLM models, our paper not only presents a comprehensive overview but also\ncharts a course that identifies existing challenges and points toward potential\nfuture research trajectories. This survey furnishes a well-rounded perspective\non the current state of generative AI, shedding light on opportunities for\nfurther exploration, enhancement, and innovation.\n","authors":["Hanieh Alipour","Nick Pendar","Kohinoor Roy"],"pdf_url":"https://arxiv.org/pdf/2403.14469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14467v1","updated":"2024-03-21T15:14:25Z","published":"2024-03-21T15:14:25Z","title":"Recourse for reclamation: Chatting with generative language models","summary":"  Researchers and developers increasingly rely on toxicity scoring to moderate\ngenerative language model outputs, in settings such as customer service,\ninformation retrieval, and content generation. However, toxicity scoring may\nrender pertinent information inaccessible, rigidify or \"value-lock\" cultural\nnorms, and prevent language reclamation processes, particularly for\nmarginalized people. In this work, we extend the concept of algorithmic\nrecourse to generative language models: we provide users a novel mechanism to\nachieve their desired prediction by dynamically setting thresholds for toxicity\nfiltering. Users thereby exercise increased agency relative to interactions\nwith the baseline system. A pilot study ($n = 30$) supports the potential of\nour proposed recourse mechanism, indicating improvements in usability compared\nto fixed-threshold toxicity-filtering of model outputs. Future work should\nexplore the intersection of toxicity scoring, model controllability, user\nagency, and language reclamation processes -- particularly with regard to the\nbias that many communities encounter when interacting with generative language\nmodels.\n","authors":["Jennifer Chien","Kevin R. McKee","Jackie Kay","William Isaac"],"pdf_url":"https://arxiv.org/pdf/2403.14467v1.pdf","comment":"Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems (CHI EA 2024)"},{"id":"http://arxiv.org/abs/2403.14460v1","updated":"2024-03-21T15:07:57Z","published":"2024-03-21T15:07:57Z","title":"Towards Single-System Illusion in Software-Defined Vehicles --\n  Automated, AI-Powered Workflow","summary":"  We propose a novel model- and feature-based approach to development of\nvehicle software systems, where the end architecture is not explicitly defined.\nInstead, it emerges from an iterative process of search and optimization given\ncertain constraints, requirements and hardware architecture, while retaining\nthe property of single-system illusion, where applications run in a logically\nuniform environment. One of the key points of the presented approach is the\ninclusion of modern generative AI, specifically Large Language Models (LLMs),\nin the loop. With the recent advances in the field, we expect that the LLMs\nwill be able to assist in processing of requirements, generation of formal\nsystem models, as well as generation of software deployment specification and\ntest code. The resulting pipeline is automated to a large extent, with feedback\nbeing generated at each step.\n","authors":["Krzysztof Lebioda","Viktor Vorobev","Nenad Petrovic","Fengjunjie Pan","Vahid Zolfaghari","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2403.14460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14459v1","updated":"2024-03-21T15:06:14Z","published":"2024-03-21T15:06:14Z","title":"Multi-Level Explanations for Generative Language Models","summary":"  Perturbation-based explanation methods such as LIME and SHAP are commonly\napplied to text classification. This work focuses on their extension to\ngenerative language models. To address the challenges of text as output and\nlong text inputs, we propose a general framework called MExGen that can be\ninstantiated with different attribution algorithms. To handle text output, we\nintroduce the notion of scalarizers for mapping text to real numbers and\ninvestigate multiple possibilities. To handle long inputs, we take a\nmulti-level approach, proceeding from coarser levels of granularity to finer\nones, and focus on algorithms with linear scaling in model queries. We conduct\na systematic evaluation, both automated and human, of perturbation-based\nattribution methods for summarization and context-grounded question answering.\nThe results show that our framework can provide more locally faithful\nexplanations of generated outputs.\n","authors":["Lucas Monteiro Paes","Dennis Wei","Hyo Jin Do","Hendrik Strobelt","Ronny Luss","Amit Dhurandhar","Manish Nagireddy","Karthikeyan Natesan Ramamurthy","Prasanna Sattigeri","Werner Geyer","Soumya Ghosh"],"pdf_url":"https://arxiv.org/pdf/2403.14459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14457v1","updated":"2024-03-21T15:04:32Z","published":"2024-03-21T15:04:32Z","title":"gTBLS: Generating Tables from Text by Conditional Question Answering","summary":"  Distilling large, unstructured text into a structured, condensed form such as\ntables is an open research problem. One of the primary challenges in\nautomatically generating tables is ensuring their syntactic validity. Prior\napproaches address this challenge by including additional parameters in the\nTransformer's attention mechanism to attend to specific rows and column\nheaders. In contrast to this single-stage method, this paper presents a\ntwo-stage approach called Generative Tables (gTBLS). The first stage infers\ntable structure (row and column headers) from the text. The second stage\nformulates questions using these headers and fine-tunes a causal language model\nto answer them. Furthermore, the gTBLS approach is amenable to the utilization\nof pre-trained Large Language Models in a zero-shot configuration, presenting a\nsolution for table generation in situations where fine-tuning is not feasible.\ngTBLS improves prior approaches by up to 10% in BERTScore on the table\nconstruction task and up to 20% on the table content generation task of the\nE2E, WikiTableText, WikiBio, and RotoWire datasets.\n","authors":["Anirudh Sundar","Christopher Richardson","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2403.14457v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.14454v1","updated":"2024-03-21T15:02:03Z","published":"2024-03-21T15:02:03Z","title":"Prediction of Translation Techniques for the Translation Process","summary":"  Machine translation (MT) encompasses a variety of methodologies aimed at\nenhancing the accuracy of translations. In contrast, the process of\nhuman-generated translation relies on a wide range of translation techniques,\nwhich are crucial for ensuring linguistic adequacy and fluency. This study\nsuggests that these translation techniques could further optimize machine\ntranslation if they are automatically identified before being applied to guide\nthe translation process effectively. The study differentiates between two\nscenarios of the translation process: from-scratch translation and\npost-editing. For each scenario, a specific set of experiments has been\ndesigned to forecast the most appropriate translation techniques. The findings\nindicate that the predictive accuracy for from-scratch translation reaches 82%,\nwhile the post-editing process exhibits even greater potential, achieving an\naccuracy rate of 93%.\n","authors":["Fan Zhou","Vincent Vandeghinste"],"pdf_url":"https://arxiv.org/pdf/2403.14454v1.pdf","comment":"11 pages, 6 figures, conference"},{"id":"http://arxiv.org/abs/2403.14444v1","updated":"2024-03-21T14:51:51Z","published":"2024-03-21T14:51:51Z","title":"More than Just Statistical Recurrence: Human and Machine Unsupervised\n  Learning of Māori Word Segmentation across Morphological Processes","summary":"  Non-M\\=aori-speaking New Zealanders (NMS)are able to segment M\\=aori words in\na highlysimilar way to fluent speakers (Panther et al.,2024). This ability is\nassumed to derive through the identification and extraction of statistically\nrecurrent forms. We examine this assumption by asking how NMS segmentations\ncompare to those produced by Morfessor, an unsupervised machine learning model\nthat operates based on statistical recurrence, across words formed by a variety\nof morphological processes. Both NMS and Morfessor succeed in segmenting words\nformed by concatenative processes (compounding and affixation without\nallomorphy), but NMS also succeed for words that invoke templates\n(reduplication and allomorphy) and other cues to morphological structure,\nimplying that their learning process is sensitive to more than just statistical\nrecurrence.\n","authors":["Ashvini Varatharaj","Simon Todd"],"pdf_url":"https://arxiv.org/pdf/2403.14444v1.pdf","comment":"10 pages, 1 Figure, 2 tables"},{"id":"http://arxiv.org/abs/2403.10882v2","updated":"2024-03-21T14:50:18Z","published":"2024-03-16T10:26:38Z","title":"Optimizing Language Augmentation for Multilingual Large Language Models:\n  A Case Study on Korean","summary":"  Large language models (LLMs) use pretraining to predict the subsequent word;\nhowever, their expansion requires significant computing resources. Numerous big\ntech companies and research institutes have developed multilingual LLMs (MLLMs)\nto meet current demands, overlooking less-resourced languages (LRLs). This\nstudy proposed three strategies to enhance the performance of LRLs based on the\npublicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to\nenhance expressiveness. Second, bilingual data were used for pretraining to\nalign the high- and less-resourced languages. Third, a high-quality small-scale\ninstruction dataset was constructed and instruction-tuning was performed to\naugment the LRL. The experiments employed the Llama2 model and Korean was used\nas the LRL, which was quantitatively evaluated against other developed LLMs\nacross eight tasks. Furthermore, a qualitative assessment was performed based\non human evaluation and GPT4. Experimental results showed that our proposed\nBllossom model exhibited superior performance in qualitative analyses compared\nto previously proposed Korean monolingual models.\n","authors":["ChangSu Choi","Yongbin Jeong","Seoyoon Park","InHo Won","HyeonSeok Lim","SangMin Kim","Yejee Kang","Chanhyuk Yoon","Jaewan Park","Yiseul Lee","HyeJin Lee","Younggyun Hahm","Hansaem Kim","KyungTae Lim"],"pdf_url":"https://arxiv.org/pdf/2403.10882v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14443v1","updated":"2024-03-21T14:48:37Z","published":"2024-03-21T14:48:37Z","title":"Language Models Can Reduce Asymmetry in Information Markets","summary":"  This work addresses the buyer's inspection paradox for information markets.\nThe paradox is that buyers need to access information to determine its value,\nwhile sellers need to limit access to prevent theft. To study this, we\nintroduce an open-source simulated digital marketplace where intelligent\nagents, powered by language models, buy and sell information on behalf of\nexternal participants. The central mechanism enabling this marketplace is the\nagents' dual capabilities: they not only have the capacity to assess the\nquality of privileged information but also come equipped with the ability to\nforget. This ability to induce amnesia allows vendors to grant temporary access\nto proprietary information, significantly reducing the risk of unauthorized\nretention while enabling agents to accurately gauge the information's relevance\nto specific queries or tasks. To perform well, agents must make rational\ndecisions, strategically explore the marketplace through generated sub-queries,\nand synthesize answers from purchased information. Concretely, our experiments\n(a) uncover biases in language models leading to irrational behavior and\nevaluate techniques to mitigate these biases, (b) investigate how price affects\ndemand in the context of informational goods, and (c) show that inspection and\nhigher budgets both lead to higher quality outcomes.\n","authors":["Nasim Rahaman","Martin Weiss","Manuel Wüthrich","Yoshua Bengio","Li Erran Li","Chris Pal","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2403.14443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14438v1","updated":"2024-03-21T14:44:03Z","published":"2024-03-21T14:44:03Z","title":"A Multimodal Approach to Device-Directed Speech Detection with Large\n  Language Models","summary":"  Interactions with virtual assistants typically start with a predefined\ntrigger phrase followed by the user command. To make interactions with the\nassistant more intuitive, we explore whether it is feasible to drop the\nrequirement that users must begin each command with a trigger phrase. We\nexplore this task in three ways: First, we train classifiers using only\nacoustic information obtained from the audio waveform. Second, we take the\ndecoder outputs of an automatic speech recognition (ASR) system, such as 1-best\nhypotheses, as input features to a large language model (LLM). Finally, we\nexplore a multimodal system that combines acoustic and lexical features, as\nwell as ASR decoder signals in an LLM. Using multimodal information yields\nrelative equal-error-rate improvements over text-only and audio-only models of\nup to 39% and 61%. Increasing the size of the LLM and training with low-rank\nadaption leads to further relative EER reductions of up to 18% on our dataset.\n","authors":["Dominik Wager","Alexander Churchill","Siddharth Sigtia","Panayiotis Georgiou","Matt Mirsamadi","Aarshee Mishra","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2403.14438v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03632"},{"id":"http://arxiv.org/abs/2403.14427v1","updated":"2024-03-21T14:33:34Z","published":"2024-03-21T14:33:34Z","title":"Emergent communication and learning pressures in language models: a\n  language evolution perspective","summary":"  Language models and humans are two types of learning systems. Finding or\nfacilitating commonalities could enable major breakthroughs in our\nunderstanding of the acquisition and evolution of language. Many theories of\nlanguage evolution rely heavily on learning biases and learning pressures. Yet\ndue to substantial differences in learning pressures, it is questionable\nwhether the similarity between humans and machines is sufficient for insights\nto carry over and to be worth testing with human participants. Here, we review\nthe emergent communication literature, a subfield of multi-agent reinforcement\nlearning, from a language evolution perspective. We find that the emergent\ncommunication literature excels at designing and adapting models to recover\ninitially absent linguistic phenomena of natural languages. Based on a short\nliterature review, we identify key pressures that have recovered initially\nabsent human patterns in emergent communication models: communicative success,\nefficiency, learnability, and other psycho-/sociolinguistic factors. We argue\nthat this may serve as inspiration for how to design language models for\nlanguage acquisition and language evolution research.\n","authors":["Lukas Galke","Limor Raviv"],"pdf_url":"https://arxiv.org/pdf/2403.14427v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2403.14409v1","updated":"2024-03-21T13:57:43Z","published":"2024-03-21T13:57:43Z","title":"Locating and Mitigating Gender Bias in Large Language Models","summary":"  Large language models(LLM) are pre-trained on extensive corpora to learn\nfacts and human cognition which contain human preferences. However, this\nprocess can inadvertently lead to these models acquiring biases and stereotypes\nprevalent in society. Prior research has typically tackled the issue of bias\nthrough a one-dimensional perspective, concentrating either on locating or\nmitigating it. This limited perspective has created obstacles in facilitating\nresearch on bias to synergistically complement and progressively build upon one\nanother. In this study, we integrate the processes of locating and mitigating\nbias within a unified framework. Initially, we use causal mediation analysis to\ntrace the causal effects of different components' activation within a large\nlanguage model. Building on this, we propose the LSDM (Least Square Debias\nMethod), a knowledge-editing based method for mitigating gender bias in\noccupational pronouns, and compare it against two baselines on three gender\nbias datasets and seven knowledge competency test datasets. The experimental\nresults indicate that the primary contributors to gender bias are the bottom\nMLP modules acting on the last token of occupational pronouns and the top\nattention module acting on the final word in the sentence. Furthermore, LSDM\nmitigates gender bias in the model more effectively than the other baselines,\nwhile fully preserving the model's capabilities in all other aspects.\n","authors":["Yuchen Cai","Ding Cao","Rongxi Guo","Yaqin Wen","Guiquan Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14409v1.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.14403v1","updated":"2024-03-21T13:52:30Z","published":"2024-03-21T13:52:30Z","title":"Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language\n  Models through Question Complexity","summary":"  Retrieval-Augmented Large Language Models (LLMs), which incorporate the\nnon-parametric knowledge from external knowledge bases into LLMs, have emerged\nas a promising approach to enhancing response accuracy in several tasks, such\nas Question-Answering (QA). However, even though there are various approaches\ndealing with queries of different complexities, they either handle simple\nqueries with unnecessary computational overhead or fail to adequately address\ncomplex multi-step queries; yet, not all user requests fall into only one of\nthe simple or complex categories. In this work, we propose a novel adaptive QA\nframework, that can dynamically select the most suitable strategy for\n(retrieval-augmented) LLMs from the simplest to the most sophisticated ones\nbased on the query complexity. Also, this selection process is operationalized\nwith a classifier, which is a smaller LM trained to predict the complexity\nlevel of incoming queries with automatically collected labels, obtained from\nactual predicted outcomes of models and inherent inductive biases in datasets.\nThis approach offers a balanced strategy, seamlessly adapting between the\niterative and single-step retrieval-augmented LLMs, as well as the no-retrieval\nmethods, in response to a range of query complexities. We validate our model on\na set of open-domain QA datasets, covering multiple query complexities, and\nshow that ours enhances the overall efficiency and accuracy of QA systems,\ncompared to relevant baselines including the adaptive retrieval approaches.\nCode is available at: https://github.com/starsuzi/Adaptive-RAG.\n","authors":["Soyeong Jeong","Jinheon Baek","Sukmin Cho","Sung Ju Hwang","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2403.14403v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14402v1","updated":"2024-03-21T13:52:17Z","published":"2024-03-21T13:52:17Z","title":"XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for\n  Noise-Robust Speech Perception","summary":"  Speech recognition and translation systems perform poorly on noisy inputs,\nwhich are frequent in realistic environments. Augmenting these systems with\nvisual signals has the potential to improve robustness to noise. However,\naudio-visual (AV) data is only available in limited amounts and for fewer\nlanguages than audio-only resources. To address this gap, we present XLAVS-R, a\ncross-lingual audio-visual speech representation model for noise-robust speech\nrecognition and translation in over 100 languages. It is designed to maximize\nthe benefits of limited multilingual AV pre-training data, by building on top\nof audio-only multilingual pre-training and simplifying existing pre-training\nschemes. Extensive evaluation on the MuAViC benchmark shows the strength of\nXLAVS-R on downstream audio-visual speech recognition and translation tasks,\nwhere it outperforms the previous state of the art by up to 18.5% WER and 4.7\nBLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability\nwith audio-only fine-tuning.\n","authors":["HyoJung Han","Mohamed Anwar","Juan Pino","Wei-Ning Hsu","Marine Carpuat","Bowen Shi","Changhan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14399v1","updated":"2024-03-21T13:47:40Z","published":"2024-03-21T13:47:40Z","title":"Building Accurate Translation-Tailored LLMs with Language Aware\n  Instruction Tuning","summary":"  Translation-tailored Large language models (LLMs) exhibit remarkable\ntranslation capabilities, even competing with supervised-trained commercial\ntranslation systems. However, off-target translation remains an unsolved\nproblem, especially for low-resource languages, hindering us from developing\naccurate LLMs-based translation models. To mitigate the off-target translation\nproblem and enhance the performance of LLMs on translation, recent works have\neither designed advanced prompting strategies to highlight the functionality of\ntranslation instructions or exploited the in-context learning ability of LLMs\nby feeding few-shot demonstrations. However, these methods essentially do not\nimprove LLM's ability to follow translation instructions, especially the\nlanguage direction information. In this work, we design a two-stage fine-tuning\nalgorithm to improve the instruction-following ability (especially the\ntranslation direction) of LLMs. Specifically, we first tune LLMs with the\nmaximum likelihood estimation loss on the translation dataset to elicit the\nbasic translation capabilities. In the second stage, we construct\ninstruction-conflicting samples by randomly replacing the translation\ndirections with a wrong one within the instruction, and then introduce an extra\nunlikelihood loss to learn those samples. Experiments on IWSLT and WMT\nbenchmarks upon the LLaMA model spanning 16 zero-shot directions show that,\ncompared to the competitive baseline -- translation-finetuned LLama, our method\ncould effectively reduce the off-target translation ratio (averagely -53.3\\%),\nthus improving translation quality with average +5.7 SacreBLEU and +16.4\nBLEURT. Analysis shows that our method could preserve the model's general task\nperformance on AlpacaEval. Code and models will be released at\n\\url{https://github.com/alphadl/LanguageAware_Tuning}.\n","authors":["Changtong Zan","Liang Ding","Li Shen","Yibing Zhen","Weifeng Liu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2403.14399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11259v2","updated":"2024-03-21T13:41:35Z","published":"2023-09-20T12:35:19Z","title":"Sequence-to-Sequence Spanish Pre-trained Language Models","summary":"  In recent years, significant advancements in pre-trained language models have\ndriven the creation of numerous non-English language variants, with a\nparticular emphasis on encoder-only and decoder-only architectures. While\nSpanish language models based on BERT and GPT have demonstrated proficiency in\nnatural language understanding and generation, there remains a noticeable\nscarcity of encoder-decoder models explicitly designed for sequence-to-sequence\ntasks, which aim to map input sequences to generate output sequences\nconditionally. This paper breaks new ground by introducing the implementation\nand evaluation of renowned encoder-decoder architectures exclusively\npre-trained on Spanish corpora. Specifically, we present Spanish versions of\nBART, T5, and BERT2BERT-style models and subject them to a comprehensive\nassessment across various sequence-to-sequence tasks, including summarization,\nquestion answering, split-and-rephrase, dialogue, and translation. Our findings\nunderscore the competitive performance of all models, with the BART- and\nT5-based models emerging as top performers across all tasks. We have made all\nmodels publicly available to the research community to foster future\nexplorations and advancements in Spanish NLP:\nhttps://github.com/vgaraujov/Seq2Seq-Spanish-PLMs.\n","authors":["Vladimir Araujo","Maria Mihaela Trusca","Rodrigo Tufiño","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2309.11259v2.pdf","comment":"Accepted paper at LREC-Coling2024"},{"id":"http://arxiv.org/abs/2306.00618v2","updated":"2024-03-21T13:37:23Z","published":"2023-06-01T12:44:33Z","title":"Effective Structured Prompting by Meta-Learning and Representative\n  Verbalizer","summary":"  Prompt tuning for pre-trained masked language models (MLM) has shown\npromising performance in natural language processing tasks with few labeled\nexamples. It tunes a prompt for the downstream task, and a verbalizer is used\nto bridge the predicted token and label prediction. Due to the limited training\ndata, prompt initialization is crucial for prompt tuning. Recently,\nMetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared\ninitialization for all task-specific prompts. However, a single initialization\nis insufficient to obtain good prompts for all tasks and samples when the tasks\nare complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a\nheavy burden on computation and memory as the MLM is usually large. To address\nthese issues, we use a prompt pool to extract more task knowledge and construct\ninstance-dependent prompts via attention. We further propose a novel soft\nverbalizer (RepVerb) which constructs label embedding from feature embeddings\ndirectly. Combining meta-learning the prompt pool and RepVerb, we propose\nMetaPrompter for effective structured prompting. MetaPrompter is\nparameter-efficient as only the pool is required to be tuned. Experimental\nresults demonstrate that MetaPrompter performs better than the recent\nstate-of-the-arts and RepVerb outperforms existing soft verbalizers.\n","authors":["Weisen Jiang","Yu Zhang","James T. Kwok"],"pdf_url":"https://arxiv.org/pdf/2306.00618v2.pdf","comment":"Accepted at ICML 2023"},{"id":"http://arxiv.org/abs/2403.14390v1","updated":"2024-03-21T13:29:54Z","published":"2024-03-21T13:29:54Z","title":"From Large to Tiny: Distilling and Refining Mathematical Expertise for\n  Math Word Problems with Weakly Supervision","summary":"  Addressing the challenge of high annotation costs in solving Math Word\nProblems (MWPs) through full supervision with intermediate equations, recent\nworks have proposed weakly supervised task settings that rely solely on the\nfinal answer as a supervised signal. Existing leading approaches typically\nemploy various search techniques to infer intermediate equations, but cannot\nensure their semantic consistency with natural language descriptions. The rise\nof Large Language Models (LLMs) like ChatGPT has opened up new possibilities\nfor addressing MWPs directly. However, the computational demands of LLMs make\nthem less than ideal for use in settings where resources are tight. In light of\nthese challenges, we introduce an innovative two-stage framework that adeptly\ntransfers mathematical Expertise from large to tiny language models. In\n\\emph{Distillation Stage}, we propose a series of extraction processes that\nsatisfy the properties of MWPs to distill mathematical knowledge from LLMs to\nconstruct problem-equation pairs required for supervised training. In\n\\emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee\nthe full utilization of all data, we further utilize the unsuccessfully\nsearched data effectively by Knowledge Refine method. Finally, We train a small\nmodel using distilled data generated through two-stage methods. As our method\nfully leverages the semantic understanding capabilities during the searching\n'problem-equation' pair, it demonstrates significantly improved performance on\nthe Math23K and Weak12K datasets compared to existing small model methods,\nwhile maintaining a much lower computational cost than ChatGPT.\n","authors":["Qingwen Lin","Boyan Xu","Zhengting Huang","Ruichu Cai"],"pdf_url":"https://arxiv.org/pdf/2403.14390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14381v1","updated":"2024-03-21T13:15:25Z","published":"2024-03-21T13:15:25Z","title":"Editing Knowledge Representation of Language Lodel via Rephrased Prefix\n  Prompts","summary":"  Neural language models (LMs) have been extensively trained on vast corpora to\nstore factual knowledge about various aspects of the world described in texts.\nCurrent technologies typically employ knowledge editing methods or specific\nprompts to modify LM outputs. However, existing knowledge editing methods are\ncostly and inefficient, struggling to produce appropriate text. Additionally,\nprompt engineering is opaque and requires significant effort to find suitable\nprompts. To address these issues, we introduce a new method called PSPEM\n(Prefix Soft Prompt Editing Method), that can be used for a lifetime with just\none training. It resolves the inefficiencies and generalizability issues in\nknowledge editing methods and overcomes the opacity of prompt engineering by\nautomatically seeking optimal soft prompts. Specifically, PSPEM utilizes a\nprompt encoder and an encoding converter to refine key information in prompts\nand uses prompt alignment techniques to guide model generation, ensuring text\nconsistency and adherence to the intended structure and content, thereby\nmaintaining an optimal balance between efficiency and accuracy. We have\nvalidated the effectiveness of PSPEM through knowledge editing and attribute\ninserting. On the COUNTERFACT dataset, PSPEM achieved nearly 100\\% editing\naccuracy and demonstrated the highest level of fluency. We further analyzed the\nsimilarities between PSPEM and original prompts and their impact on the model's\ninternals. The results indicate that PSPEM can serve as an alternative to\noriginal prompts, supporting the model in effective editing.\n","authors":["Yuchen Cai","Ding Cao","Rongxi Guo","Yaqin Wen","Guiquan Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14381v1.pdf","comment":"19pages,3figures"},{"id":"http://arxiv.org/abs/2403.14374v1","updated":"2024-03-21T13:05:18Z","published":"2024-03-21T13:05:18Z","title":"FIT-RAG: Black-Box RAG with Factual Information and Token Reduction","summary":"  Due to the extraordinarily large number of parameters, fine-tuning Large\nLanguage Models (LLMs) to update long-tail or out-of-date knowledge is\nimpractical in lots of applications. To avoid fine-tuning, we can alternatively\ntreat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment\nit with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.\nRecently, black-box RAG has achieved success in knowledge-intensive tasks and\nhas gained much attention. Existing black-box RAG methods typically fine-tune\nthe retriever to cater to LLMs' preferences and concatenate all the retrieved\ndocuments as the input, which suffers from two issues: (1) Ignorance of Factual\nInformation. The LLM preferred documents may not contain the factual\ninformation for the given question, which can mislead the retriever and hurt\nthe effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating\nall the retrieved documents brings large amounts of unnecessary tokens for\nLLMs, which degenerates the efficiency of black-box RAG. To address these\nissues, this paper proposes a novel black-box RAG framework which utilizes the\nfactual information in the retrieval and reduces the number of tokens for\naugmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by\nconstructing a bi-label document scorer. Besides, it reduces the tokens by\nintroducing a self-knowledge recognizer and a sub-document-level token reducer.\nFIT-RAG achieves both superior effectiveness and efficiency, which is validated\nby extensive experiments across three open-domain question-answering datasets:\nTriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of\nLlama2-13B-Chat by 14.3\\% on TriviaQA, 19.9\\% on NQ and 27.5\\% on PopQA,\nrespectively. Furthermore, it can save approximately half of the tokens on\naverage across the three datasets.\n","authors":["Yuren Mao","Xuemei Dong","Wenyi Xu","Yunjun Gao","Bin Wei","Ying Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14364v1","updated":"2024-03-21T12:45:12Z","published":"2024-03-21T12:45:12Z","title":"WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for\n  Atomic Factual Knowledge Update in Causal Language Models","summary":"  The factuality of large language model (LLMs) tends to decay over time since\nevents posterior to their training are \"unknown\" to them. One way to keep\nmodels up-to-date could be factual update: the task of inserting, replacing, or\nremoving certain simple (atomic) facts within the model. To study this task, we\npresent WikiFactDiff, a dataset that describes the evolution of factual\nknowledge between two dates as a collection of simple facts divided into three\ncategories: new, obsolete, and static. We describe several update scenarios\narising from various combinations of these three types of basic update. The\nfacts are represented by subject-relation-object triples; indeed, WikiFactDiff\nwas constructed by comparing the state of the Wikidata knowledge base at 4\nJanuary 2021 and 27 February 2023. Those fact are accompanied by verbalization\ntemplates and cloze tests that enable running update algorithms and their\nevaluation metrics. Contrary to other datasets, such as zsRE and CounterFact,\nWikiFactDiff constitutes a realistic update setting that involves various\nupdate scenarios, including replacements, archival, and new entity insertions.\nWe also present an evaluation of existing update algorithms on WikiFactDiff.\n","authors":["Hichem Ammar Khodja","Frédéric Béchet","Quentin Brabant","Alexis Nasr","Gwénolé Lecorvé"],"pdf_url":"https://arxiv.org/pdf/2403.14364v1.pdf","comment":"Accepted for publication at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14341v1","updated":"2024-03-21T12:17:59Z","published":"2024-03-21T12:17:59Z","title":"Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial\n  Narratives","summary":"  In this paper, we introduce the Financial-STS task, a financial\ndomain-specific NLP task designed to measure the nuanced semantic similarity\nbetween pairs of financial narratives. These narratives originate from the\nfinancial statements of the same company but correspond to different periods,\nsuch as year-over-year comparisons. Measuring the subtle semantic differences\nbetween these paired narratives enables market stakeholders to gauge changes\nover time in the company's financial and operational situations, which is\ncritical for financial decision-making. We find that existing pretrained\nembedding models and LLM embeddings fall short in discerning these subtle\nfinancial narrative shifts. To address this gap, we propose an LLM-augmented\npipeline specifically designed for the Financial-STS task. Evaluation on a\nhuman-annotated dataset demonstrates that our proposed method outperforms\nexisting methods trained on classic STS tasks and generic LLM embeddings.\n","authors":["Jiaxin Liu","Yi Yang","Kar Yan Tam"],"pdf_url":"https://arxiv.org/pdf/2403.14341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14339v1","updated":"2024-03-21T12:11:26Z","published":"2024-03-21T12:11:26Z","title":"$\\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning","summary":"  Machine Unlearning, the process of selectively eliminating the influence of\ncertain data examples used during a model's training, has gained significant\nattention as a means for practitioners to comply with recent data protection\nregulations. However, existing unlearning methods face critical drawbacks,\nincluding their prohibitively high cost, often associated with a large number\nof hyperparameters, and the limitation of forgetting only relatively small data\nportions. This often makes retraining the model from scratch a quicker and more\neffective solution. In this study, we introduce Gradient-based and\nTask-Agnostic machine Unlearning ($\\nabla \\tau$), an optimization framework\ndesigned to remove the influence of a subset of training data efficiently. It\napplies adaptive gradient ascent to the data to be forgotten while using\nstandard gradient descent for the remaining data. $\\nabla \\tau$ offers multiple\nbenefits over existing approaches. It enables the unlearning of large sections\nof the training dataset (up to 30%). It is versatile, supporting various\nunlearning tasks (such as subset forgetting or class removal) and applicable\nacross different domains (images, text, etc.). Importantly, $\\nabla \\tau$\nrequires no hyperparameter adjustments, making it a more appealing option than\nretraining the model from scratch. We evaluate our framework's effectiveness\nusing a set of well-established Membership Inference Attack metrics,\ndemonstrating up to 10% enhancements in performance compared to\nstate-of-the-art methods without compromising the original model's accuracy.\n","authors":["Daniel Trippa","Cesare Campagnano","Maria Sofia Bucarelli","Gabriele Tolomei","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2403.14339v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.14312v1","updated":"2024-03-21T11:34:26Z","published":"2024-03-21T11:34:26Z","title":"ChainLM: Empowering Large Language Models with Improved Chain-of-Thought\n  Prompting","summary":"  Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of\nlarge language models (LLMs), establishing itself as a primary approach to\nsolving complex reasoning tasks. Existing CoT synthesis approaches usually\nfocus on simpler reasoning tasks and thus result in low-quality and\ninconsistent CoT prompts. In response to this challenge, we present an\nempirical investigation of CoT prompting and introduce CoTGenius, a novel\nframework designed for the automatic generation of superior CoT prompts.\nCoTGenius is developed based on three major evolution strategies, i.e.,\ncomplicate, diversify, and specify-alongside two filtering mechanisms:\nevolutionary success judgement and correctness verification. We further employ\nCoTGenius to create an extensive CoT dataset, and subsequently fine-tune the\nLlama 2-Chat 7B and 13B models on this dataset. We call the resulting model\nChainLM. To deal with the cumulative error issue in reasoning steps, we propose\na step-level debating method, wherein multiple debaters discuss each reasoning\nstep to arrive at the correct answer. Extensive experiments demonstrate that\nour ChainLM models exhibit enhanced proficiency in addressing a spectrum of\ncomplex reasoning problems compared to existing models. In addition, we conduct\nan in-depth analysis of the impact of data categories within CoTGenius on the\nmodel performance. We release our dataset and code at\nhttps://github.com/RUCAIBox/ChainLM.\n","authors":["Xiaoxue Cheng","Junyi Li","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.14312v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.02622v2","updated":"2024-03-21T10:57:40Z","published":"2024-02-04T21:44:09Z","title":"DenseFormer: Enhancing Information Flow in Transformers via Depth\n  Weighted Averaging","summary":"  The transformer architecture by Vaswani et al. (2017) is now ubiquitous\nacross application domains, from natural language processing to speech\nprocessing and image understanding. We propose DenseFormer, a simple\nmodification to the standard architecture that improves the perplexity of the\nmodel without increasing its size -- adding a few thousand parameters for\nlarge-scale models in the 100B parameters range. Our approach relies on an\nadditional averaging step after each transformer block, which computes a\nweighted average of current and past representations -- we refer to this\noperation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit\ncoherent patterns of information flow, revealing the strong and structured\nreuse of activations from distant layers. Experiments demonstrate that\nDenseFormer is more data efficient, reaching the same perplexity of much deeper\ntransformer models, and that for the same perplexity, these new models\noutperform transformer baselines in terms of memory efficiency and inference\ntime.\n","authors":["Matteo Pagliardini","Amirkeivan Mohtashami","Francois Fleuret","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2402.02622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17918v2","updated":"2024-03-21T10:57:23Z","published":"2023-10-27T06:22:14Z","title":"Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection\n  Method","summary":"  Large Language Models (LLMs) have shown great potential in Natural Language\nProcessing (NLP) tasks. However, recent literature reveals that LLMs generate\nnonfactual responses intermittently, which impedes the LLMs' reliability for\nfurther utilization. In this paper, we propose a novel self-detection method to\ndetect which questions that a LLM does not know that are prone to generate\nnonfactual results. Specifically, we first diversify the textual expressions\nfor a given question and collect the corresponding answers. Then we examine the\ndivergencies between the generated answers to identify the questions that the\nmodel may generate falsehoods. All of the above steps can be accomplished by\nprompting the LLMs themselves without referring to any other external\nresources. We conduct comprehensive experiments and demonstrate the\neffectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT,\nand GPT-4.\n","authors":["Yukun Zhao","Lingyong Yan","Weiwei Sun","Guoliang Xing","Chong Meng","Shuaiqiang Wang","Zhicong Cheng","Zhaochun Ren","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2310.17918v2.pdf","comment":"Accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14275v1","updated":"2024-03-21T10:31:11Z","published":"2024-03-21T10:31:11Z","title":"Is Reference Necessary in the Evaluation of NLG Systems? When and Where?","summary":"  The majority of automatic metrics for evaluating NLG systems are\nreference-based. However, the challenge of collecting human annotation results\nin a lack of reliable references in numerous application scenarios. Despite\nrecent advancements in reference-free metrics, it has not been well understood\nwhen and where they can be used as an alternative to reference-based metrics.\nIn this study, by employing diverse analytical approaches, we comprehensively\nassess the performance of both metrics across a wide range of NLG tasks,\nencompassing eight datasets and eight evaluation models. Based on solid\nexperiments, the results show that reference-free metrics exhibit a higher\ncorrelation with human judgment and greater sensitivity to deficiencies in\nlanguage quality. However, their effectiveness varies across tasks and is\ninfluenced by the quality of candidate texts. Therefore, it's important to\nassess the performance of reference-free metrics before applying them to a new\ntask, especially when inputs are in uncommon form or when the answer space is\nhighly variable. Our study can provide insight into the appropriate application\nof automatic metrics and the impact of metric choice on evaluation performance.\n","authors":["Shuqian Sheng","Yi Xu","Luoyi Fu","Jiaxin Ding","Lei Zhou","Xinbing Wang","Chenghu Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.14275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14270v1","updated":"2024-03-21T10:15:57Z","published":"2024-03-21T10:15:57Z","title":"Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship\n  Detection","summary":"  Visual relationship detection aims to identify objects and their\nrelationships in images. Prior methods approach this task by adding separate\nrelationship modules or decoders to existing object detection architectures.\nThis separation increases complexity and hinders end-to-end training, which\nlimits performance. We propose a simple and highly efficient decoder-free\narchitecture for open-vocabulary visual relationship detection. Our model\nconsists of a Transformer-based image encoder that represents objects as tokens\nand models their relationships implicitly. To extract relationship information,\nwe introduce an attention mechanism that selects object pairs likely to form a\nrelationship. We provide a single-stage recipe to train this model on a mixture\nof object and relationship detection data. Our approach achieves\nstate-of-the-art relationship detection performance on Visual Genome and on the\nlarge-vocabulary GQA benchmark at real-time inference speeds. We provide\nanalyses of zero-shot performance, ablations, and real-world qualitative\nexamples.\n","authors":["Tim Salzmann","Markus Ryll","Alex Bewley","Matthias Minderer"],"pdf_url":"https://arxiv.org/pdf/2403.14270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00862v2","updated":"2024-03-21T10:14:09Z","published":"2024-02-29T21:05:14Z","title":"NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and\n  Safety Adherence in Chinese Journalistic Editorial Applications","summary":"  This study presents NewsBench, a novel benchmark framework developed to\nevaluate the capability of Large Language Models (LLMs) in Chinese Journalistic\nWriting Proficiency (JWP) and their Safety Adherence (SA), addressing the gap\nbetween journalistic ethics and the risks associated with AI utilization.\nComprising 1,267 tasks across 5 editorial applications, 7 aspects (including\nsafety and journalistic writing with 4 detailed facets), and spanning 24 news\ntopics domains, NewsBench employs two GPT-4 based automatic evaluation\nprotocols validated by human assessment. Our comprehensive analysis of 10 LLMs\nhighlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative\ndeficiency in journalistic ethic adherence during creative writing tasks. These\nfindings underscore the need for enhanced ethical guidance in AI-generated\njournalistic content, marking a step forward in aligning AI capabilities with\njournalistic standards and safety considerations.\n","authors":["Miao Li","Ming-Bin Chen","Bo Tang","Shengbin Hou","Pengyu Wang","Haiying Deng","Zhiyu Li","Feiyu Xiong","Keming Mao","Peng Cheng","Yi Luo"],"pdf_url":"https://arxiv.org/pdf/2403.00862v2.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2307.06029v3","updated":"2024-03-21T09:46:31Z","published":"2023-07-12T09:23:41Z","title":"Pluggable Neural Machine Translation Models via Memory-augmented\n  Adapters","summary":"  Although neural machine translation (NMT) models perform well in the general\ndomain, it remains rather challenging to control their generation behavior to\nsatisfy the requirement of different users. Given the expensive training cost\nand the data scarcity challenge of learning a new model from scratch for each\nuser requirement, we propose a memory-augmented adapter to steer pretrained NMT\nmodels in a pluggable manner. Specifically, we construct a multi-granular\nmemory based on the user-provided text samples and propose a new adapter\narchitecture to combine the model representations and the retrieved results. We\nalso propose a training strategy using memory dropout to reduce spurious\ndependencies between the NMT model and the memory. We validate our approach on\nboth style- and domain-specific experiments and the results indicate that our\nmethod can outperform several representative pluggable baselines.\n","authors":["Yuzhuang Xu","Shuo Wang","Peng Li","Xuebo Liu","Xiaolong Wang","Weidong Liu","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2307.06029v3.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14258v1","updated":"2024-03-21T09:36:36Z","published":"2024-03-21T09:36:36Z","title":"LLM-based Extraction of Contradictions from Patents","summary":"  Already since the 1950s TRIZ shows that patents and the technical\ncontradictions they solve are an important source of inspiration for the\ndevelopment of innovative products. However, TRIZ is a heuristic based on a\nhistoric patent analysis and does not make use of the ever-increasing number of\nlatest technological solutions in current patents. Because of the huge number\nof patents, their length, and, last but not least, their complexity there is a\nneed for modern patent retrieval and patent analysis to go beyond\nkeyword-oriented methods. Recent advances in patent retrieval and analysis\nmainly focus on dense vectors based on neural AI Transformer language models\nlike Google BERT. They are, for example, used for dense retrieval, question\nanswering or summarization and key concept extraction. A research focus within\nthe methods for patent summarization and key concept extraction are generic\ninventive concepts respectively TRIZ concepts like problems, solutions,\nadvantage of invention, parameters, and contradictions. Succeeding rule-based\napproaches, finetuned BERT-like language models for sentence-wise\nclassification represent the state-of-the-art of inventive concept extraction.\nWhile they work comparatively well for basic concepts like problems or\nsolutions, contradictions - as a more complex abstraction - remain a challenge\nfor these models. This paper goes one step further, as it presents a method to\nextract TRIZ contradictions from patent texts based on Prompt Engineering using\na generative Large Language Model (LLM), namely OpenAI's GPT-4. Contradiction\ndetection, sentence extraction, contradiction summarization, parameter\nextraction and assignment to the 39 abstract TRIZ engineering parameters are\nall performed in a single prompt using the LangChain framework. Our results\nshow that \"off-the-shelf\" GPT-4 is a serious alternative to existing\napproaches.\n","authors":["Stefan Trapp","Joachim Warschat"],"pdf_url":"https://arxiv.org/pdf/2403.14258v1.pdf","comment":"10 pages, 2 tables"},{"id":"http://arxiv.org/abs/2403.14255v1","updated":"2024-03-21T09:28:38Z","published":"2024-03-21T09:28:38Z","title":"ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion\n  Classification","summary":"  Improving the accessibility of psychotherapy with the aid of Large Language\nModels (LLMs) is garnering a significant attention in recent years. Recognizing\ncognitive distortions from the interviewee's utterances can be an essential\npart of psychotherapy, especially for cognitive behavioral therapy. In this\npaper, we propose ERD, which improves LLM-based cognitive distortion\nclassification performance with the aid of additional modules of (1) extracting\nthe parts related to cognitive distortion, and (2) debating the reasoning steps\nby multiple agents. Our experimental results on a public dataset show that ERD\nimproves the multi-class F1 score as well as binary specificity score.\nRegarding the latter score, it turns out that our method is effective in\ndebiasing the baseline method which has high false positive rate, especially\nwhen the summary of multi-agent debate is provided to LLMs.\n","authors":["Sehee Lim","Yejin Kim","Chi-Hyun Choi","Jy-yong Sohn","Byung-Hoon Kim"],"pdf_url":"https://arxiv.org/pdf/2403.14255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14253v1","updated":"2024-03-21T09:26:04Z","published":"2024-03-21T09:26:04Z","title":"K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional\n  Expression","summary":"  In many literary texts, emotions are indirectly conveyed through descriptions\nof actions, facial expressions, and appearances, necessitating emotion\ninference for narrative understanding. In this paper, we introduce K-Act2Emo, a\nKorean commonsense knowledge graph (CSKG) comprising 1,900 indirect emotional\nexpressions and the emotions inferable from them. We categorize reasoning types\ninto inferences in positive situations, inferences in negative situations, and\ninferences when expressions do not serve as emotional cues. Unlike existing\nCSKGs, K-Act2Emo specializes in emotional contexts, and experimental results\nvalidate its effectiveness for training emotion inference models.\nSignificantly, the BART-based knowledge model fine-tuned with K-Act2Emo\noutperforms various existing Korean large language models, achieving\nperformance levels comparable to GPT-4 Turbo.\n","authors":["Kyuhee Kim","Surin Lee","Sangah Lee"],"pdf_url":"https://arxiv.org/pdf/2403.14253v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.14252v1","updated":"2024-03-21T09:25:24Z","published":"2024-03-21T09:25:24Z","title":"LayoutLLM: Large Language Model Instruction Tuning for Visually Rich\n  Document Understanding","summary":"  This paper proposes LayoutLLM, a more flexible document analysis method for\nunderstanding imaged documents. Visually Rich Document Understanding tasks,\nsuch as document image classification and information extraction, have gained\nsignificant attention due to their importance. Existing methods have been\ndeveloped to enhance document comprehension by incorporating pre-training\nawareness of images, text, and layout structure. However, these methods require\nfine-tuning for each task and dataset, and the models are expensive to train\nand operate. To overcome this limitation, we propose a new LayoutLLM that\nintegrates these with large-scale language models (LLMs). By leveraging the\nstrengths of existing research in document image understanding and LLMs'\nsuperior language understanding capabilities, the proposed model, fine-tuned\nwith multimodal instruction datasets, performs an understanding of document\nimages in a single model. Our experiments demonstrate improvement over the\nbaseline model in various document analysis tasks.\n","authors":["Masato Fujitake"],"pdf_url":"https://arxiv.org/pdf/2403.14252v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2109.01048v3","updated":"2024-03-21T09:23:38Z","published":"2021-09-02T16:05:24Z","title":"Pre-training Language Model Incorporating Domain-specific Heterogeneous\n  Knowledge into A Unified Representation","summary":"  Existing technologies expand BERT from different perspectives, e.g. designing\ndifferent pre-training tasks, different semantic granularities, and different\nmodel architectures. Few models consider expanding BERT from different text\nformats. In this paper, we propose a heterogeneous knowledge language model\n(\\textbf{HKLM}), a unified pre-trained language model (PLM) for all forms of\ntext, including unstructured text, semi-structured text, and well-structured\ntext. To capture the corresponding relations among these multi-format\nknowledge, our approach uses masked language model objective to learn word\nknowledge, uses triple classification objective and title matching objective to\nlearn entity knowledge and topic knowledge respectively. To obtain the\naforementioned multi-format text, we construct a corpus in the tourism domain\nand conduct experiments on 5 tourism NLP datasets. The results show that our\napproach outperforms the pre-training of plain text using only 1/4 of the data.\nWe further pre-train the domain-agnostic HKLM and achieve performance gains on\nthe XNLI dataset.\n","authors":["Hongyin Zhu","Hao Peng","Zhiheng Lyu","Lei Hou","Juanzi Li","Jinghui Xiao"],"pdf_url":"https://arxiv.org/pdf/2109.01048v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12022v2","updated":"2024-03-21T09:11:22Z","published":"2023-08-23T09:29:29Z","title":"Reranking Passages with Coarse-to-Fine Neural Retriever Enhanced by\n  List-Context Information","summary":"  Passage reranking is a critical task in various applications, particularly\nwhen dealing with large volumes of documents. Existing neural architectures\nhave limitations in retrieving the most relevant passage for a given question\nbecause the semantics of the segmented passages are often incomplete, and they\ntypically match the question to each passage individually, rarely considering\ncontextual information from other passages that could provide comparative and\nreference information. This paper presents a list-context attention mechanism\nto augment the passage representation by incorporating the list-context\ninformation from other candidates. The proposed coarse-to-fine (C2F) neural\nretriever addresses the out-of-memory limitation of the passage attention\nmechanism by dividing the list-context modeling process into two sub-processes\nwith a cache policy learning algorithm, enabling the efficient encoding of\ncontext information from a large number of candidate answers. This method can\nbe generally used to encode context information from any number of candidate\nanswers in one pass. Different from most multi-stage information retrieval\narchitectures, this model integrates the coarse and fine rankers into the joint\noptimization process, allowing for feedback between the two layers to update\nthe model simultaneously. Experiments demonstrate the effectiveness of the\nproposed approach.\n","authors":["Hongyin Zhu"],"pdf_url":"https://arxiv.org/pdf/2308.12022v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08403v2","updated":"2024-03-21T09:02:26Z","published":"2024-02-13T12:04:43Z","title":"LLMs and the Human Condition","summary":"  This paper presents three established theories of human decision-making and\ndescribes how they can be integrated to provide a model of purposive human\naction. Taking seriously the idea of language as action the model is then\napplied to the conversational user interfaces. Theory based AI research has had\na hard time recently and the aim here is to revitalise interest in\nunderstanding what LLMs are actually doing other than running poorly understood\nmachine learning routines over all the data the relevant Big Tech company can\nhoover up. When a raspberry pi computer for under 50USD is up to 400 times\nfaster than the first commercial Cray super computer~\\cite{crayVpi}, Big Tech\ncan get really close to having an infinite number of monkeys typing at random\nand producing text, some of which will make sense. By understanding where\nChatGPT's apparent intelligence comes from, perhaps we can perform the magic\nwith fewer resources and at the same time gain some understanding about our\nrelationship with our world.\n","authors":["Peter Wallis"],"pdf_url":"https://arxiv.org/pdf/2402.08403v2.pdf","comment":"A 2nd draft with a better abstract and introduction. target is IVA in\n  2024"},{"id":"http://arxiv.org/abs/2403.14243v1","updated":"2024-03-21T09:02:17Z","published":"2024-03-21T09:02:17Z","title":"Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large\n  Language Models with Machine Learning in tele-dermatology","summary":"  The rise of Artificial Intelligence creates great promise in the field of\nmedical discovery, diagnostics and patient management. However, the vast\ncomplexity of all medical domains require a more complex approach that combines\nmachine learning algorithms, classifiers, segmentation algorithms and, lately,\nlarge language models. In this paper, we describe, implement and assess an\nArtificial Intelligence-empowered system and methodology aimed at assisting the\ndiagnosis process of skin lesions and other skin conditions within the field of\ndermatology that aims to holistically address the diagnostic process in this\ndomain. The workflow integrates large language, transformer-based vision models\nand sophisticated machine learning tools. This holistic approach achieves a\nnuanced interpretation of dermatological conditions that simulates and\nfacilitates a dermatologist's workflow. We assess our proposed methodology\nthrough a thorough cross-model validation technique embedded in an evaluation\npipeline that utilizes publicly available medical case studies of skin\nconditions and relevant images. To quantitatively score the system performance,\nadvanced machine learning and natural language processing tools are employed\nwhich focus on similarity comparison and natural language inference.\nAdditionally, we incorporate a human expert evaluation process based on a\nstructured checklist to further validate our results. We implemented the\nproposed methodology in a system which achieved approximate (weighted) scores\nof 0.87 for both contextual understanding and diagnostic accuracy,\ndemonstrating the efficacy of our approach in enhancing dermatological\nanalysis. The proposed methodology is expected to prove useful in the\ndevelopment of next-generation tele-dermatology applications, enhancing remote\nconsultation capabilities and access to care, especially in underserved areas.\n","authors":["Dimitrios P. Panagoulias","Evridiki Tsoureli-Nikita","Maria Virvou","George A. Tsihrintzis"],"pdf_url":"https://arxiv.org/pdf/2403.14243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14238v1","updated":"2024-03-21T08:57:27Z","published":"2024-03-21T08:57:27Z","title":"Reinforcement Learning from Reflective Feedback (RLRF): Aligning and\n  Improving LLMs via Fine-Grained Self-Reflection","summary":"  Despite the promise of RLHF in aligning LLMs with human preferences, it often\nleads to superficial alignment, prioritizing stylistic changes over improving\ndownstream performance of LLMs. Underspecified preferences could obscure\ndirections to align the models. Lacking exploration restricts identification of\ndesirable outputs to improve the models. To overcome these challenges, we\npropose a novel framework: Reinforcement Learning from Reflective Feedback\n(RLRF), which leverages fine-grained feedback based on detailed criteria to\nimprove the core capabilities of LLMs. RLRF employs a self-reflection mechanism\nto systematically explore and refine LLM responses, then fine-tuning the models\nvia a RL algorithm along with promising responses. Our experiments across\nJust-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and\ntransformative potential of RLRF beyond superficial surface-level adjustment.\n","authors":["Kyungjae Lee","Dasol Hwang","Sunghyun Park","Youngsoo Jang","Moontae Lee"],"pdf_url":"https://arxiv.org/pdf/2403.14238v1.pdf","comment":"22 pages, 5 figures, Submitted to ACL 2024"},{"id":"http://arxiv.org/abs/2403.14236v1","updated":"2024-03-21T08:54:24Z","published":"2024-03-21T08:54:24Z","title":"A Unified Framework for Model Editing","summary":"  Model editing is a growing area focused on updating the knowledge embedded\nwithin models. Among the various methodologies, ROME and MEMIT stand out as\nleading \"locate-and-edit\" model editing techniques. While MEMIT enables batched\nediting of memories, ROME is limited to changing one fact at a time. This paper\nintroduces a unifying framework that brings ROME and MEMIT under a single\nconceptual umbrella, optimizing for the same goal, which we call the\n\"preservation-memorization\" objective. This objective aims to preserve the\nrepresentations of certain selected vectors while memorizing the\nrepresentations of new factual information. Specifically, ROME optimizes this\nobjective using an equality constraint, whereas MEMIT employs a more flexible\nleast-square constraint. In addition to making batched edits, MEMIT also edits\nthe model at multiple layers. We disentangle the distribution of edits to\nmultiple layers from the optimization objective of MEMIT and show that these\nedit-distribution algorithms should be considered separate entities worthy of\ntheir own line of research.\n  Finally, we present EMMET - an Equality-constrained Mass Model Editing\nalgorithm for Transformers, a new batched memory-editing algorithm. With EMMET,\nwe present a closed form solution for the equality-constrained version of the\npreservation-memorization objective. We show that EMMET is able to perform\nbatched-edits on par with MEMIT up to a batch-size of 256 and discuss the\nchallenges in stabilizing EMMET. By articulating the \"locate-and-edit\" model\nediting algorithms under a simple conceptual framework of\n\"preservation-memorization\", we aim to bridge the gap between intuition and\nmathematics and hope to simplify the journey for future researchers in model\nediting.\n","authors":["Akshat Gupta","Dev Sajnani","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2403.14236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14974v2","updated":"2024-03-21T08:50:44Z","published":"2023-09-25T09:21:25Z","title":"Detecting Sexual Content at the Sentence Level in First Millennium Latin\n  Texts","summary":"  In this study, we propose to evaluate the use of deep learning methods for\nsemantic classification at the sentence level to accelerate the process of\ncorpus building in the field of humanities and linguistics, a traditional and\ntime-consuming task. We introduce a novel corpus comprising around 2500\nsentences spanning from 300 BCE to 900 CE including sexual semantics (medical,\nerotica, etc.). We evaluate various sentence classification approaches and\ndifferent input embedding layers, and show that all consistently outperform\nsimple token-based searches. We explore the integration of idiolectal and\nsociolectal metadata embeddings (centuries, author, type of writing), but find\nthat it leads to overfitting. Our results demonstrate the effectiveness of this\napproach, achieving high precision and true positive rates (TPR) of\nrespectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset\nsize on the model performances (420 instead of 2013), and show that, while our\nmodels perform worse, they still offer a high enough precision and TPR, even\nwithout MLM, respectively 69% and 51%. Given the result, we provide an analysis\nof the attention mechanism as a supporting added value for humanists in order\nto produce more data.\n","authors":["Thibault Clérice"],"pdf_url":"https://arxiv.org/pdf/2309.14974v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13372v2","updated":"2024-03-21T08:36:39Z","published":"2024-03-20T08:08:54Z","title":"LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models","summary":"  Efficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It allows users\nto flexibly customize the fine-tuning of 100+ LLMs without the need for coding\nthrough the built-in web UI LlamaBoard. We empirically validate the efficiency\nand effectiveness of our framework on language modeling and text generation\ntasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and\nalready received over 13,000 stars and 1,600 forks.\n","authors":["Yaowei Zheng","Richong Zhang","Junhao Zhang","Yanhan Ye","Zheyan Luo","Yongqiang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.13372v2.pdf","comment":"12 pages, preprint"},{"id":"http://arxiv.org/abs/2310.16343v2","updated":"2024-03-21T08:29:35Z","published":"2023-10-25T03:58:49Z","title":"Evaluating, Understanding, and Improving Constrained Text Generation for\n  Large Language Models","summary":"  Advancements in natural language generation (NLG) and large language models\n(LLMs) have led to proficient text generation in various tasks. However,\nintegrating intricate constraints into neural text generation, due to LLMs'\nopacity, remains challenging. This study investigates constrained text\ngeneration for LLMs, where predefined constraints are applied during LLM's\ngeneration process. Our research mainly focuses on mainstream open-source LLMs,\ncategorizing constraints into lexical, structural, and relation-based types. We\nalso present various benchmarks to facilitate fair evaluation. The study\naddresses some key research questions, including evaluating, understanding and\nimproving constrained text generation for LLMs. Results illuminate LLMs'\ncapacity and deficiency to incorporate constraints and provide insights for\nfuture developments in constrained text generation. Codes and datasets will be\nreleased upon acceptance.\n","authors":["Xiang Chen","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2310.16343v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.14222v1","updated":"2024-03-21T08:22:44Z","published":"2024-03-21T08:22:44Z","title":"Large-Scale Label Interpretation Learning for Few-Shot Named Entity\n  Recognition","summary":"  Few-shot named entity recognition (NER) detects named entities within text\nusing only a few annotated examples. One promising line of research is to\nleverage natural language descriptions of each entity type: the common label\nPER might, for example, be verbalized as ''person entity.'' In an initial label\ninterpretation learning phase, the model learns to interpret such verbalized\ndescriptions of entity types. In a subsequent few-shot tagset extension phase,\nthis model is then given a description of a previously unseen entity type (such\nas ''music album'') and optionally a few training examples to perform few-shot\nNER for this type. In this paper, we systematically explore the impact of a\nstrong semantic prior to interpret verbalizations of new entity types by\nmassively scaling up the number and granularity of entity types used for label\ninterpretation learning. To this end, we leverage an entity linking benchmark\nto create a dataset with orders of magnitude of more distinct entity types and\ndescriptions as currently used datasets. We find that this increased signal\nyields strong results in zero- and few-shot NER in in-domain, cross-domain, and\neven cross-lingual settings. Our findings indicate significant potential for\nimproving few-shot NER through heuristical data-based optimization.\n","authors":["Jonas Golde","Felix Hamborg","Alan Akbik"],"pdf_url":"https://arxiv.org/pdf/2403.14222v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.14221v1","updated":"2024-03-21T08:21:12Z","published":"2024-03-21T08:21:12Z","title":"Improving the Robustness of Large Language Models via Consistency\n  Alignment","summary":"  Large language models (LLMs) have shown tremendous success in following user\ninstructions and generating helpful responses. Nevertheless, their robustness\nis still far from optimal, as they may generate significantly inconsistent\nresponses due to minor changes in the verbalized instructions. Recent\nliterature has explored this inconsistency issue, highlighting the importance\nof continued improvement in the robustness of response generation. However,\nsystematic analysis and solutions are still lacking. In this paper, we\nquantitatively define the inconsistency problem and propose a two-stage\ntraining framework consisting of instruction-augmented supervised fine-tuning\nand consistency alignment training. The first stage helps a model generalize on\nfollowing instructions via similar instruction augmentations. In the second\nstage, we improve the diversity and help the model understand which responses\nare more aligned with human expectations by differentiating subtle differences\nin similar responses. The training process is accomplished by self-rewards\ninferred from the trained model at the first stage without referring to\nexternal human preference resources. We conduct extensive experiments on recent\npublicly available LLMs on instruction-following tasks and demonstrate the\neffectiveness of our training framework.\n","authors":["Zhao Yukun","Yan Lingyong","Sun Weiwei","Xing Guoliang","Wang Shuaiqiang","Meng Chong","Cheng Zhicong","Ren Zhaochun","Yin Dawei"],"pdf_url":"https://arxiv.org/pdf/2403.14221v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14208v1","updated":"2024-03-21T08:00:05Z","published":"2024-03-21T08:00:05Z","title":"Automatic Annotation of Grammaticality in Child-Caregiver Conversations","summary":"  The acquisition of grammar has been a central question to adjudicate between\ntheories of language acquisition. In order to conduct faster, more\nreproducible, and larger-scale corpus studies on grammaticality in\nchild-caregiver conversations, tools for automatic annotation can offer an\neffective alternative to tedious manual annotation. We propose a coding scheme\nfor context-dependent grammaticality in child-caregiver conversations and\nannotate more than 4,000 utterances from a large corpus of transcribed\nconversations. Based on these annotations, we train and evaluate a range of NLP\nmodels. Our results show that fine-tuned Transformer-based models perform best,\nachieving human inter-annotation agreement levels.As a first application and\nsanity check of this tool, we use the trained models to annotate a corpus\nalmost two orders of magnitude larger than the manually annotated data and\nverify that children's grammaticality shows a steady increase with age.This\nwork contributes to the growing literature on applying state-of-the-art NLP\nmethods to help study child language acquisition at scale.\n","authors":["Mitja Nikolaus","Abhishek Agrawal","Petros Kaklamanis","Alex Warstadt","Abdellah Fourtassi"],"pdf_url":"https://arxiv.org/pdf/2403.14208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14197v1","updated":"2024-03-21T07:47:57Z","published":"2024-03-21T07:47:57Z","title":"Context Quality Matters in Training Fusion-in-Decoder for Extractive\n  Open-Domain Question Answering","summary":"  Retrieval-augmented generation models augment knowledge encoded in a language\nmodel by providing additional relevant external knowledge (context) during\ngeneration. Although it has been shown that the quantity and quality of context\nimpact the performance of retrieval-augmented generation models during\ninference, limited research explores how these characteristics affect model\ntraining. This paper explores how context quantity and quality during model\ntraining affect the performance of Fusion-in-Decoder (FiD), the\nstate-of-the-art retrieval-augmented generation model, in extractive\nopen-domain question answering tasks. Experimental results suggest that FiD\nmodels overfit to context quality during training and show suboptimal\nperformance when evaluated on different context quality. Through the\nexperimental results, we also reveal FiD models trained with different context\nquality have different cross-attention distribution patterns. Specifically, as\ncontext quality during training increases, FiD models tend to attend more\nuniformly to each passage in context. Finally, based on these observations, we\npropose a method to mitigate overfitting to specific context quality by\nintroducing bias to the cross-attention distribution, which we demonstrate to\nbe effective in improving the performance of FiD models on different context\nquality.\n","authors":["Kosuke Akimoto","Kunihiro Takeoka","Masafumi Oyamada"],"pdf_url":"https://arxiv.org/pdf/2403.14197v1.pdf","comment":"EMNLP Findings 2023"},{"id":"http://arxiv.org/abs/2312.04877v3","updated":"2024-03-21T07:22:54Z","published":"2023-12-08T07:27:26Z","title":"Generating Explanations to Understand and Repair Embedding-based Entity\n  Alignment","summary":"  Entity alignment (EA) seeks identical entities in different knowledge graphs,\nwhich is a long-standing task in the database research. Recent work leverages\ndeep learning to embed entities in vector space and align them via nearest\nneighbor search. Although embedding-based EA has gained marked success in\nrecent years, it lacks explanations for alignment decisions. In this paper, we\npresent the first framework that can generate explanations for understanding\nand repairing embedding-based EA results. Given an EA pair produced by an\nembedding model, we first compare its neighbor entities and relations to build\na matching subgraph as a local explanation. We then construct an alignment\ndependency graph to understand the pair from an abstract perspective. Finally,\nwe repair the pair by resolving three types of alignment conflicts based on\ndependency graphs. Experiments on a variety of EA datasets demonstrate the\neffectiveness, generalization, and robustness of our framework in explaining\nand repairing embedding-based EA results.\n","authors":["Xiaobin Tian","Zequn Sun","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2312.04877v3.pdf","comment":"Accepted in the 40th IEEE International Conference on Data\n  Engineering (ICDE 2024)"},{"id":"http://arxiv.org/abs/2403.09559v2","updated":"2024-03-21T06:51:16Z","published":"2024-03-14T16:47:25Z","title":"Less is More: Data Value Estimation for Visual Instruction Tuning","summary":"  Visual instruction tuning is the key to building multimodal large language\nmodels (MLLMs), which greatly improves the reasoning capabilities of large\nlanguage models (LLMs) in vision scenario. However, existing MLLMs mostly rely\non a mixture of multiple highly diverse visual instruction datasets for\ntraining (even more than a million instructions), which may introduce data\nredundancy. To investigate this issue, we conduct a series of empirical\nstudies, which reveal a significant redundancy within the visual instruction\ndatasets, and show that greatly reducing the amount of several instruction\ndataset even do not affect the performance. Based on the findings, we propose a\nnew data selection approach TIVE, to eliminate redundancy within visual\ninstruction data. TIVE first estimates the task-level and instance-level value\nof the visual instructions based on computed gradients. Then, according to the\nestimated values, TIVE determines the task proportion within the visual\ninstructions, and selects representative instances to compose a smaller visual\ninstruction subset for training. Experiments on LLaVA-1.5 show that our\napproach using only about 7.5% data can achieve comparable performance as the\nfull-data fine-tuned model across seven benchmarks, even surpassing it on four\nof the benchmarks. Our code and data will be publicly released.\n","authors":["Zikang Liu","Kun Zhou","Wayne Xin Zhao","Dawei Gao","Yaliang Li","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14171v1","updated":"2024-03-21T06:47:28Z","published":"2024-03-21T06:47:28Z","title":"MMIDR: Teaching Large Language Model to Interpret Multimodal\n  Misinformation via Knowledge Distillation","summary":"  Automatic detection of multimodal misinformation has gained a widespread\nattention recently. However, the potential of powerful Large Language Models\n(LLMs) for multimodal misinformation detection remains underexplored. Besides,\nhow to teach LLMs to interpret multimodal misinformation in cost-effective and\naccessible way is still an open question. To address that, we propose MMIDR, a\nframework designed to teach LLMs in providing fluent and high-quality textual\nexplanations for their decision-making process of multimodal misinformation. To\nconvert multimodal misinformation into an appropriate instruction-following\nformat, we present a data augmentation perspective and pipeline. This pipeline\nconsists of a visual information processing module and an evidence retrieval\nmodule. Subsequently, we prompt the proprietary LLMs with processed contents to\nextract rationales for interpreting the authenticity of multimodal\nmisinformation. Furthermore, we design an efficient knowledge distillation\napproach to distill the capability of proprietary LLMs in explaining multimodal\nmisinformation into open-source LLMs. To explore several research questions\nregarding the performance of LLMs in multimodal misinformation detection tasks,\nwe construct an instruction-following multimodal misinformation dataset and\nconduct comprehensive experiments. The experimental findings reveal that our\nMMIDR exhibits sufficient detection performance and possesses the capacity to\nprovide compelling rationales to support its assessments.\n","authors":["Longzheng Wang","Xiaohan Xu","Lei Zhang","Jiarui Lu","Yongxiu Xu","Hongbo Xu","Chuang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14171v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.14168v1","updated":"2024-03-21T06:43:59Z","published":"2024-03-21T06:43:59Z","title":"M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual\n  Academic Lecture Dataset","summary":"  Publishing open-source academic video recordings is an emergent and prevalent\napproach to sharing knowledge online. Such videos carry rich multimodal\ninformation including speech, the facial and body movements of the speakers, as\nwell as the texts and pictures in the slides and possibly even the papers.\nAlthough multiple academic video datasets have been constructed and released,\nfew of them support both multimodal content recognition and understanding\ntasks, which is partially due to the lack of high-quality human annotations. In\nthis paper, we propose a novel multimodal, multigenre, and multipurpose\naudio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of\nvideos from five sources covering computer science, mathematics, and medical\nand biology topics. With high-quality human annotations of the spoken and\nwritten words, in particular high-valued name entities, the dataset can be used\nfor multiple audio-visual recognition and understanding tasks. Evaluations\nperformed on contextual speech recognition, speech synthesis, and slide and\nscript generation tasks demonstrate that the diversity of M$^3$AV makes it a\nchallenging dataset.\n","authors":["Zhe Chen","Heyang Liu","Wenyi Yu","Guangzhi Sun","Hongcheng Liu","Ji Wu","Chao Zhang","Yu Wang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11145v2","updated":"2024-03-21T06:22:56Z","published":"2024-03-17T08:51:01Z","title":"A Challenge Dataset and Effective Models for Conversational Stance\n  Detection","summary":"  Previous stance detection studies typically concentrate on evaluating stances\nwithin individual instances, thereby exhibiting limitations in effectively\nmodeling multi-party discussions concerning the same specific topic, as\nnaturally transpire in authentic social media interactions. This constraint\narises primarily due to the scarcity of datasets that authentically replicate\nreal social media contexts, hindering the research progress of conversational\nstance detection. In this paper, we introduce a new multi-turn conversation\nstance detection dataset (called \\textbf{MT-CSD}), which encompasses multiple\ntargets for conversational stance detection. To derive stances from this\nchallenging dataset, we propose a global-local attention network\n(\\textbf{GLAN}) to address both long and short-range dependencies inherent in\nconversational data. Notably, even state-of-the-art stance detection methods,\nexemplified by GLAN, exhibit an accuracy of only 50.47\\%, highlighting the\npersistent challenges in conversational stance detection. Furthermore, our\nMT-CSD dataset serves as a valuable resource to catalyze advancements in\ncross-domain stance detection, where a classifier is adapted from a different\nyet related target. We believe that MT-CSD will contribute to advancing\nreal-world applications of stance detection research. Our source code, data,\nand models are available at \\url{https://github.com/nfq729/MT-CSD}.\n","authors":["Fuqiang Niu","Min Yang","Ang Li","Baoquan Zhang","Xiaojiang Peng","Bowen Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12373v2","updated":"2024-03-21T06:01:48Z","published":"2024-03-19T02:34:18Z","title":"RankPrompt: Step-by-Step Comparisons Make Language Models Better\n  Reasoners","summary":"  Large Language Models (LLMs) have achieved impressive performance across\nvarious reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT\nare prone to logical errors during their reasoning processes. Traditional\napproaches to mitigate these errors involve human or tool-based feedback, such\nas employing task-specific verifiers or aggregating multiple reasoning paths.\nThese methods, however, either depend heavily on human input or struggle with\ninconsistent responses. To overcome these limitations, we present RankPrompt,\nan innovative prompting strategy that empowers LLMs to autonomously rank their\nresponses without needing extra resources. RankPrompt simplifies the ranking\nchallenge into comparative evaluations among different responses, leveraging\nLLMs' innate ability to generate comparative examples within context. Our\nexperiments across 11 arithmetic and commonsense reasoning tasks show that\nRankPrompt significantly enhances the reasoning performance of ChatGPT and\nGPT-4, with improvements of up to 13%. Furthermore, RankPrompt shows\nexceptional performance in LLM-based automatic evaluations for open-ended\ntasks, matching human judgments 74% of the time in the AlpacaEval dataset. It\nalso proves to be robust against changes in response order and inconsistency.\nOverall, our findings endorse RankPrompt as an effective method for extracting\nhigh-quality feedback directly from language models.\n","authors":["Chi Hu","Yuan Ge","Xiangnan Ma","Hang Cao","Qiang Li","Yonghua Yang","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.12373v2.pdf","comment":"LREC-Coling 2024 Long Paper"},{"id":"http://arxiv.org/abs/2402.03848v3","updated":"2024-03-21T05:58:10Z","published":"2024-02-06T09:50:08Z","title":"ANLS* -- A Universal Document Processing Metric for Generative Large\n  Language Models","summary":"  Traditionally, discriminative models have been the predominant choice for\ntasks like document classification and information extraction. These models\nmake predictions that fall into a limited number of predefined classes,\nfacilitating a binary true or false evaluation and enabling the direct\ncalculation of metrics such as the F1 score. However, recent advancements in\ngenerative large language models (GLLMs) have prompted a shift in the field due\nto their enhanced zero-shot capabilities, which eliminate the need for a\ndownstream dataset and computationally expensive fine-tuning. However,\nevaluating GLLMs presents a challenge as the binary true or false evaluation\nused for discriminative models is not applicable to the predictions made by\nGLLMs.\n  This paper introduces a new metric for generative models called ANLS* for\nevaluating a wide variety of tasks, including information extraction and\nclassification tasks. The ANLS* metric extends existing ANLS metrics as a\ndrop-in-replacement and is still compatible with previously reported ANLS\nscores. An evaluation of 7 different datasets, 6 different GLLMs and 3\ndifferent prompting methods using the ANLS* metric is also provided,\ndemonstrating the importance of the proposed metric.\n  We also benchmark a novel approach to generate prompts for documents, called\nSFT, against other prompting techniques such as LATIN. In 27 out of 35 cases,\nSFT outperforms other techniques and improves the state-of-the-art, sometimes\nby as much as $18$ percentage points.\n  Sources are available at https://github.com/deepopinion/anls_star_metric\n","authors":["David Peer","Philemon Schöpf","Volckmar Nebendahl","Alexander Rietzler","Sebastian Stabinger"],"pdf_url":"https://arxiv.org/pdf/2402.03848v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16226v3","updated":"2024-03-21T04:47:27Z","published":"2023-10-24T22:41:14Z","title":"TiC-CLIP: Continual Training of CLIP Models","summary":"  Keeping large foundation models up to date on latest data is inherently\nexpensive. To avoid the prohibitive costs of constantly retraining, it is\nimperative to continually train these models. This problem is exacerbated by\nthe lack of any large scale continual learning benchmarks or baselines. We\nintroduce the first set of web-scale Time-Continual (TiC) benchmarks for\ntraining vision-language models: TiC-DataComp, TiC-YFCC, and TiC-Redcaps.\nTiC-DataComp, our largest dataset, contains over 12.7B timestamped image-text\npairs spanning 9 years (2014-2022). We first use our benchmarks to curate\nvarious dynamic evaluations to measure temporal robustness of existing models.\nWe show OpenAI's CLIP (trained on data up to 2020) loses $\\approx 8\\%$\nzero-shot accuracy on our curated retrieval task from 2021-2022 compared with\nmore recently trained models in OpenCLIP repository. We then study how to\nefficiently train models on time-continuous data. We demonstrate that a simple\nrehearsal-based approach that continues training from the last checkpoint and\nreplays old data reduces compute by $2.5\\times$ when compared to the standard\npractice of retraining from scratch. Code is available at\nhttps://github.com/apple/ml-tic-clip.\n","authors":["Saurabh Garg","Mehrdad Farajtabar","Hadi Pouransari","Raviteja Vemulapalli","Sachin Mehta","Oncel Tuzel","Vaishaal Shankar","Fartash Faghri"],"pdf_url":"https://arxiv.org/pdf/2310.16226v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.04521v2","updated":"2024-03-21T04:28:45Z","published":"2024-03-07T14:23:25Z","title":"Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge\n  Graph Completion","summary":"  Few-shot knowledge graph completion (FKGC) aims to query the unseen facts of\na relation given its few-shot reference entity pairs. The side effect of noises\ndue to the uncertainty of entities and triples may limit the few-shot learning,\nbut existing FKGC works neglect such uncertainty, which leads them more\nsusceptible to limited reference samples with noises. In this paper, we propose\na novel uncertainty-aware few-shot KG completion framework (UFKGC) to model\nuncertainty for a better understanding of the limited data by learning\nrepresentations under Gaussian distribution. Uncertainty representation is\nfirst designed for estimating the uncertainty scope of the entity pairs after\ntransferring feature representations into a Gaussian distribution. Further, to\nbetter integrate the neighbors with uncertainty characteristics for entity\nfeatures, we design an uncertainty-aware relational graph neural network\n(UR-GNN) to conduct convolution operations between the Gaussian distributions.\nThen, multiple random samplings are conducted for reference triples within the\nGaussian distribution to generate smooth reference representations during the\noptimization. The final completion score for each query instance is measured by\nthe designed uncertainty optimization to make our approach more robust to the\nnoises in few-shot scenarios. Experimental results show that our approach\nachieves excellent performance on two benchmark datasets compared to its\ncompetitors.\n","authors":["Qian Li","Shu Guo","Yinjia Chen","Cheng Ji","Jiawei Sheng","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2403.04521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14119v1","updated":"2024-03-21T04:08:29Z","published":"2024-03-21T04:08:29Z","title":"C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via\n  Text Feature Dispersion","summary":"  In deep learning, test-time adaptation has gained attention as a method for\nmodel fine-tuning without the need for labeled data. A prime exemplification is\nthe recently proposed test-time prompt tuning for large-scale vision-language\nmodels such as CLIP. Unfortunately, these prompts have been mainly developed to\nimprove accuracy, overlooking the importance of calibration-a crucial aspect\nfor quantifying prediction uncertainty. However, traditional calibration\nmethods rely on substantial amounts of labeled data, making them impractical\nfor test-time scenarios. To this end, this paper explores calibration during\ntest-time prompt tuning by leveraging the inherent properties of CLIP. Through\na series of observations, we find that the prompt choice significantly affects\nthe calibration in CLIP, where the prompts leading to higher text feature\ndispersion result in better-calibrated predictions. Introducing the Average\nText Feature Dispersion (ATFD), we establish its relationship with calibration\nerror and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),\nfor optimizing prompts during test-time with enhanced calibration. Through\nextensive experiments on different CLIP architectures and datasets, we show\nthat C-TPT can effectively improve the calibration of test-time prompt tuning\nwithout needing labeled data.\n","authors":["Hee Suk Yoon","Eunseop Yoon","Joshua Tian Jin Tee","Mark Hasegawa-Johnson","Yingzhen Li","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.14119v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.14118v1","updated":"2024-03-21T04:07:40Z","published":"2024-03-21T04:07:40Z","title":"From Handcrafted Features to LLMs: A Brief Survey for Machine\n  Translation Quality Estimation","summary":"  Machine Translation Quality Estimation (MTQE) is the task of estimating the\nquality of machine-translated text in real time without the need for reference\ntranslations, which is of great importance for the development of MT. After two\ndecades of evolution, QE has yielded a wealth of results. This article provides\na comprehensive overview of QE datasets, annotation methods, shared tasks,\nmethodologies, challenges, and future research directions. It begins with an\nintroduction to the background and significance of QE, followed by an\nexplanation of the concepts and evaluation metrics for word-level QE,\nsentence-level QE, document-level QE, and explainable QE. The paper categorizes\nthe methods developed throughout the history of QE into those based on\nhandcrafted features, deep learning, and Large Language Models (LLMs), with a\nfurther division of deep learning-based methods into classic deep learning and\nthose incorporating pre-trained language models (LMs). Additionally, the\narticle details the advantages and limitations of each method and offers a\nstraightforward comparison of different approaches. Finally, the paper\ndiscusses the current challenges in QE research and provides an outlook on\nfuture research directions.\n","authors":["Haofei Zhao","Yilun Liu","Shimin Tao","Weibin Meng","Yimeng Chen","Xiang Geng","Chang Su","Min Zhang","Hao Yang"],"pdf_url":"https://arxiv.org/pdf/2403.14118v1.pdf","comment":"Accepted by IJCNN 2024"},{"id":"http://arxiv.org/abs/2403.13679v2","updated":"2024-03-21T04:06:06Z","published":"2024-03-20T15:38:36Z","title":"RoleInteract: Evaluating the Social Interaction of Role-Playing Agents","summary":"  Large language models (LLMs) have advanced the development of various AI\nconversational agents, including role-playing conversational agents that mimic\ndiverse characters and human behaviors. While prior research has predominantly\nfocused on enhancing the conversational capability, role-specific knowledge,\nand stylistic attributes of these agents, there has been a noticeable gap in\nassessing their social intelligence. In this paper, we introduce RoleInteract,\nthe first benchmark designed to systematically evaluate the sociality of\nrole-playing conversational agents at both individual and group levels of\nsocial interactions. The benchmark is constructed from a variety of sources and\ncovers a wide range of 500 characters and over 6,000 question prompts and\n30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations\non this benchmark using mainstream open-source and closed-source LLMs. We find\nthat agents excelling in individual level does not imply their proficiency in\ngroup level. Moreover, the behavior of individuals may drift as a result of the\ninfluence exerted by other agents within the group. Experimental results on\nRoleInteract confirm its significance as a testbed for assessing the social\ninteraction of role-playing conversational agents. The benchmark is publicly\naccessible at https://github.com/X-PLUG/RoleInteract.\n","authors":["Hongzhan Chen","Hehong Chen","Ming Yan","Wenshen Xu","Xing Gao","Weizhou Shen","Xiaojun Quan","Chenliang Li","Ji Zhang","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.13679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13638v2","updated":"2024-03-21T04:03:59Z","published":"2024-03-20T14:41:01Z","title":"Do Not Worry if You Do Not Have Data: Building Pretrained Language\n  Models Using Translationese","summary":"  In this paper, we explore the utility of Translationese as synthetic data\ncreated using machine translation for pre-training language models (LMs).\nPre-training requires vast amounts of monolingual data, which is mostly\nunavailable for languages other than English. Recently, there has been a\ngrowing interest in using synthetic data to address this data scarcity. We take\nthe case of English and Indic languages and translate web-crawled monolingual\ndocuments (clean) into the target language. Then, we train language models\ncontaining 28M and 85M parameters on this translationese data (synthetic). We\nshow that their performance on downstream natural language understanding and\ngenerative tasks is only 3.56% poorer on NLU tasks and 1.51% on NLG tasks than\nLMs pre-trained on clean data. Further, we propose the use of lightweight\nTinyLMs pre-trained on clean data to filter synthetic data efficiently which\nsignificantly improves the performance of our models. We also find that LMs\ntrained on synthetic data strongly benefit from extended pretraining on a tiny\nfraction (10%) of clean data. We release the data we collected and created as a\npart of this work, IndicMonoDoc, the largest collection of monolingual\ndocument-level corpora, which we hope will help bridge the gap between English\nand non-English performance for large language models.\n","authors":["Meet Doshi","Raj Dabre","Pushpak Bhattacharyya"],"pdf_url":"https://arxiv.org/pdf/2403.13638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14117v1","updated":"2024-03-21T04:03:16Z","published":"2024-03-21T04:03:16Z","title":"A Design Space for Intelligent and Interactive Writing Assistants","summary":"  In our era of rapid technological advancement, the research landscape for\nwriting assistants has become increasingly fragmented across various research\ncommunities. We seek to address this challenge by proposing a design space as a\nstructured way to examine and explore the multidimensional space of intelligent\nand interactive writing assistants. Through a large community collaboration, we\nexplore five aspects of writing assistants: task, user, technology,\ninteraction, and ecosystem. Within each aspect, we define dimensions (i.e.,\nfundamental components of an aspect) and codes (i.e., potential options for\neach dimension) by systematically reviewing 115 papers. Our design space aims\nto offer researchers and designers a practical tool to navigate, comprehend,\nand compare the various possibilities of writing assistants, and aid in the\nenvisioning and design of new writing assistants.\n","authors":["Mina Lee","Katy Ilonka Gero","John Joon Young Chung","Simon Buckingham Shum","Vipul Raheja","Hua Shen","Subhashini Venugopalan","Thiemo Wambsganss","David Zhou","Emad A. Alghamdi","Tal August","Avinash Bhat","Madiha Zahrah Choksi","Senjuti Dutta","Jin L. C. Guo","Md Naimul Hoque","Yewon Kim","Seyed Parsa Neshaei","Agnia Sergeyuk","Antonette Shibani","Disha Shrivastava","Lila Shroff","Jessi Stark","Sarah Sterman","Sitong Wang","Antoine Bosselut","Daniel Buschek","Joseph Chee Chang","Sherol Chen","Max Kreminski","Joonsuk Park","Roy Pea","Eugenia H. Rho","Shannon Zejiang Shen","Pao Siangliulue"],"pdf_url":"https://arxiv.org/pdf/2403.14117v1.pdf","comment":"Published as a conference paper at CHI 2024"},{"id":"http://arxiv.org/abs/2403.14112v1","updated":"2024-03-21T03:52:01Z","published":"2024-03-21T03:52:01Z","title":"Benchmarking Chinese Commonsense Reasoning of LLMs: From\n  Chinese-Specifics to Reasoning-Memorization Correlations","summary":"  We introduce CHARM, the first benchmark for comprehensively and in-depth\nevaluating the commonsense reasoning ability of large language models (LLMs) in\nChinese, which covers both globally known and Chinese-specific commonsense. We\nevaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5\nrepresentative prompt strategies for improving LLMs' reasoning ability, such as\nChain-of-Thought. Our findings indicate that the LLM's language orientation and\nthe task's domain influence the effectiveness of the prompt strategy, which\nenriches previous research findings. We built closely-interconnected reasoning\nand memorization tasks, and found that some LLMs struggle with memorizing\nChinese commonsense, affecting their reasoning ability, while others show\ndifferences in reasoning despite similar memorization performance. We also\nevaluated the LLMs' memorization-independent reasoning abilities and analyzed\nthe typical errors. Our study precisely identified the LLMs' strengths and\nweaknesses, providing the clear direction for optimization. It can also serve\nas a reference for studies in other fields. We will release CHARM at\nhttps://github.com/opendatalab/CHARM .\n","authors":["Jiaxing Sun","Weiquan Huang","Jiang Wu","Chenya Gu","Wei Li","Songyang Zhang","Hang Yan","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2403.14112v1.pdf","comment":"Equal contribution: Jiaxing Sun, Weiquan Huang, Jiang Wu;\n  Corresponding author: Conghui He"},{"id":"http://arxiv.org/abs/2311.13246v2","updated":"2024-03-21T03:50:32Z","published":"2023-11-22T09:04:57Z","title":"CoachLM: Automatic Instruction Revisions Improve the Data Quality in LLM\n  Instruction Tuning","summary":"  Instruction tuning is crucial for enabling Language Learning Models (LLMs) in\nresponding to human instructions. The quality of instruction pairs used for\ntuning greatly affects the performance of LLMs. However, the manual creation of\nhigh-quality instruction datasets is costly, leading to the adoption of\nautomatic generation of instruction pairs by LLMs as a popular alternative. To\nensure the high quality of LLM-generated instruction datasets, several\napproaches have been proposed. Nevertheless, existing methods either compromise\ndataset integrity by filtering a large proportion of samples, or are unsuitable\nfor industrial applications. In this paper, instead of discarding low-quality\nsamples, we propose CoachLM, a novel approach to enhance the quality of\ninstruction datasets through automatic revisions on samples in the dataset.\nCoachLM is trained from the samples revised by human experts and significantly\nincreases the proportion of high-quality samples in the dataset from 17.7% to\n78.9%. The effectiveness of CoachLM is further assessed on various real-world\ninstruction test sets. The results show that CoachLM improves the\ninstruction-following capabilities of the instruction-tuned LLM by an average\nof 29.9%, which even surpasses larger LLMs with nearly twice the number of\nparameters. Furthermore, CoachLM is successfully deployed in a data management\nsystem for LLMs at Huawei, resulting in an efficiency improvement of up to 20%\nin the cleaning of 40k real-world instruction pairs. We release various assets\nof CoachLM, including the training data, code and test set\n(https://github.com/lunyiliu/CoachLM).\n","authors":["Yilun Liu","Shimin Tao","Xiaofeng Zhao","Ming Zhu","Wenbing Ma","Junhao Zhu","Chang Su","Yutai Hou","Miao Zhang","Min Zhang","Hongxia Ma","Li Zhang","Hao Yang","Yanfei Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.13246v2.pdf","comment":"Accepted by ICDE 2024"},{"id":"http://arxiv.org/abs/2403.13257v2","updated":"2024-03-21T03:13:30Z","published":"2024-03-20T02:38:01Z","title":"Arcee's MergeKit: A Toolkit for Merging Large Language Models","summary":"  The rapid expansion of the open-source language model landscape presents an\nopportunity to merge the competencies of these model checkpoints by combining\ntheir parameters. Advances in transfer learning, the process of fine-tuning\npretrained models for specific tasks, has resulted in the development of vast\namounts of task-specific models, typically specialized in individual tasks and\nunable to utilize each other's strengths. Model merging facilitates the\ncreation of multitask models without the need for additional training, offering\na promising avenue for enhancing model performance and versatility. By\npreserving the intrinsic capabilities of the original models, model merging\naddresses complex challenges in AI - including the difficulties of catastrophic\nforgetting and multitask learning. To support this expanding area of research,\nwe introduce MergeKit, a comprehensive, open-source library designed to\nfacilitate the application of model merging strategies. MergeKit offers an\nextensible framework to efficiently merge models on any hardware, providing\nutility to researchers and practitioners. To date, thousands of models have\nbeen merged by the open-source community, leading to the creation of some of\nthe worlds most powerful open-source model checkpoints, as assessed by the Open\nLLM Leaderboard. The library is accessible at\nhttps://github.com/arcee-ai/MergeKit.\n","authors":["Charles Goddard","Shamane Siriwardhana","Malikeh Ehghaghi","Luke Meyers","Vlad Karpukhin","Brian Benedict","Mark McQuade","Jacob Solawetz"],"pdf_url":"https://arxiv.org/pdf/2403.13257v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.14564v2","updated":"2024-03-21T02:56:22Z","published":"2023-10-23T04:39:01Z","title":"Language Models Hallucinate, but May Excel at Fact Verification","summary":"  Recent progress in natural language processing (NLP) owes much to remarkable\nadvances in large language models (LLMs). Nevertheless, LLMs frequently\n\"hallucinate,\" resulting in non-factual outputs. Our carefully-designed human\nevaluation substantiates the serious hallucination issue, revealing that even\nGPT-3.5 produces factual outputs less than 25% of the time. This underscores\nthe importance of fact verifiers in order to measure and incentivize progress.\nOur systematic investigation affirms that LLMs can be repurposed as effective\nfact verifiers with strong correlations with human judgments. Surprisingly,\nFLAN-T5-11B, the least factual generator in our study, performs the best as a\nfact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT.\nDelving deeper, we analyze the reliance of these LLMs on high-quality evidence,\nas well as their deficiencies in robustness and generalization ability. Our\nstudy presents insights for developing trustworthy generation models.\n","authors":["Jian Guan","Jesse Dodge","David Wadden","Minlie Huang","Hao Peng"],"pdf_url":"https://arxiv.org/pdf/2310.14564v2.pdf","comment":"Accepted in NAACL 2024"},{"id":"http://arxiv.org/abs/2403.13213v2","updated":"2024-03-21T02:27:57Z","published":"2024-03-20T00:22:38Z","title":"From Representational Harms to Quality-of-Service Harms: A Case Study on\n  Llama 2 Safety Safeguards","summary":"  Recent progress in large language models (LLMs) has led to their widespread\nadoption in various domains. However, these advancements have also introduced\nadditional safety risks and raised concerns regarding their detrimental impact\non already marginalized populations. Despite growing mitigation efforts to\ndevelop safety safeguards, such as supervised safety-oriented fine-tuning and\nleveraging safe reinforcement learning from human feedback, multiple concerns\nregarding the safety and ingrained biases in these models remain. Furthermore,\nprevious work has demonstrated that models optimized for safety often display\nexaggerated safety behaviors, such as a tendency to refrain from responding to\ncertain requests as a precautionary measure. As such, a clear trade-off between\nthe helpfulness and safety of these models has been documented in the\nliterature. In this paper, we further investigate the effectiveness of safety\nmeasures by evaluating models on already mitigated biases. Using the case of\nLlama 2 as an example, we illustrate how LLMs' safety responses can still\nencode harmful assumptions. To do so, we create a set of non-toxic prompts,\nwhich we then use to evaluate Llama models. Through our new taxonomy of LLMs\nresponses to users, we observe that the safety/helpfulness trade-offs are more\npronounced for certain demographic groups which can lead to quality-of-service\nharms for marginalized populations.\n","authors":["Khaoula Chehbouni","Megha Roshan","Emmanuel Ma","Futian Andrew Wei","Afaf Taik","Jackie CK Cheung","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2403.13213v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2309.09749v3","updated":"2024-03-21T01:57:38Z","published":"2023-09-18T13:24:44Z","title":"Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via\n  Knowledge Distillation","summary":"  NSFW (Not Safe for Work) content, in the context of a dialogue, can have\nsevere side effects on users in open-domain dialogue systems. However, research\non detecting NSFW language, especially sexually explicit content, within a\ndialogue context has significantly lagged behind. To address this issue, we\nintroduce CensorChat, a dialogue monitoring dataset aimed at NSFW dialogue\ndetection. Leveraging knowledge distillation techniques involving GPT-4 and\nChatGPT, this dataset offers a cost-effective means of constructing NSFW\ncontent detectors. The process entails collecting real-life human-machine\ninteraction data and breaking it down into single utterances and single-turn\ndialogues, with the chatbot delivering the final utterance. ChatGPT is employed\nto annotate unlabeled data, serving as a training set. Rationale validation and\ntest sets are constructed using ChatGPT and GPT-4 as annotators, with a\nself-criticism strategy for resolving discrepancies in labeling. A BERT model\nis fine-tuned as a text classifier on pseudo-labeled data, and its performance\nis assessed. The study emphasizes the importance of AI systems prioritizing\nuser safety and well-being in digital conversations while respecting freedom of\nexpression. The proposed approach not only advances NSFW content detection but\nalso aligns with evolving user protection needs in AI-driven dialogues.\n","authors":["Huachuan Qiu","Shuai Zhang","Hongliang He","Anqi Li","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2309.09749v3.pdf","comment":"As we have submitted a final version arXiv:2403.13250, we decide to\n  withdraw it"},{"id":"http://arxiv.org/abs/2403.14074v1","updated":"2024-03-21T01:52:07Z","published":"2024-03-21T01:52:07Z","title":"M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain\n  Multi-Hop Dense Sentence Retrieval","summary":"  In recent research, contrastive learning has proven to be a highly effective\nmethod for representation learning and is widely used for dense retrieval.\nHowever, we identify that relying solely on contrastive learning can lead to\nsuboptimal retrieval performance. On the other hand, despite many retrieval\ndatasets supporting various learning objectives beyond contrastive learning,\ncombining them efficiently in multi-task learning scenarios can be challenging.\nIn this paper, we introduce M3, an advanced recursive Multi-hop dense sentence\nretrieval system built upon a novel Multi-task Mixed-objective approach for\ndense text representation learning, addressing the aforementioned challenges.\nOur approach yields state-of-the-art performance on a large-scale open-domain\nfact verification benchmark dataset, FEVER. Code and data are available at:\nhttps://github.com/TonyBY/M3\n","authors":["Yang Bai","Anthony Colas","Christan Grant","Daisy Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14074v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14072v1","updated":"2024-03-21T01:47:22Z","published":"2024-03-21T01:47:22Z","title":"A Taxonomy of Ambiguity Types for NLP","summary":"  Ambiguity is an critical component of language that allows for more effective\ncommunication between speakers, but is often ignored in NLP. Recent work\nsuggests that NLP systems may struggle to grasp certain elements of human\nlanguage understanding because they may not handle ambiguities at the level\nthat humans naturally do in communication. Additionally, different types of\nambiguity may serve different purposes and require different approaches for\nresolution, and we aim to investigate how language models' abilities vary\nacross types. We propose a taxonomy of ambiguity types as seen in English to\nfacilitate NLP analysis. Our taxonomy can help make meaningful splits in\nlanguage ambiguity data, allowing for more fine-grained assessments of both\ndatasets and model performance.\n","authors":["Margaret Y. Li","Alisa Liu","Zhaofeng Wu","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2403.14072v1.pdf","comment":"To appear at the UnImplicit workshop at EACL 2024"},{"id":"http://arxiv.org/abs/2303.15662v3","updated":"2024-03-21T01:42:43Z","published":"2023-03-28T01:07:38Z","title":"ChatGPT4PCG Competition: Character-like Level Generation for Science\n  Birds","summary":"  This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE\nConference on Games. The objective of this competition is for participants to\ncreate effective prompts for ChatGPT--enabling it to generate Science Birds\nlevels with high stability and character-like qualities--fully using their\ncreativity as well as prompt engineering skills. ChatGPT is a conversational\nagent developed by OpenAI. Science Birds is selected as the competition\nplatform because designing an Angry Birds-like level is not a trivial task due\nto the in-game gravity; the quality of the levels is determined by their\nstability. To lower the entry barrier to the competition, we limit the task to\nthe generation of capitalized English alphabetical characters. We also allow\nonly a single prompt to be used for generating all the characters. Here, the\nquality of the generated levels is determined by their stability and similarity\nto the given characters. A sample prompt is provided to participants for their\nreference. An experiment is conducted to determine the effectiveness of several\nmodified versions of this sample prompt on level stability and similarity by\ntesting them on several characters. To the best of our knowledge, we believe\nthat ChatGPT4PCG is the first competition of its kind and hope to inspire\nenthusiasm for prompt engineering in procedural content generation.\n","authors":["Pittawat Taveekitworachai","Febri Abdullah","Mury F. Dewantoro","Ruck Thawonmas","Julian Togelius","Jochen Renz"],"pdf_url":"https://arxiv.org/pdf/2303.15662v3.pdf","comment":"This paper accepted for presentation at IEEE CoG 2023 is made\n  available for participants of ChatGPT4PCG Competition\n  (https://chatgpt4pcg.github.io/) and readers interested in relevant areas. In\n  this PDF version, the affiliation symbol of Julian Togelius has been revised"},{"id":"http://arxiv.org/abs/2211.13854v4","updated":"2024-03-21T00:53:19Z","published":"2022-11-25T01:37:48Z","title":"ComCLIP: Training-Free Compositional Image and Text Matching","summary":"  Contrastive Language-Image Pretraining (CLIP) has demonstrated great\nzero-shot performance for matching images and text. However, it is still\nchallenging to adapt vision-lanaguage pretrained models like CLIP to\ncompositional image and text matching -- a more challenging image and text\nmatching task requiring the model understanding of compositional word concepts\nand visual components. Towards better compositional generalization in zero-shot\nimage and text matching, in this paper, we study the problem from a causal\nperspective: the erroneous semantics of individual entities are essentially\nconfounders that cause the matching failure. Therefore, we propose a novel\n\\textbf{\\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP\ndisentangles input images into subjects, objects, and action sub-images and\ncomposes CLIP's vision encoder and text encoder to perform evolving matching\nover compositional text embedding and sub-image embeddings. In this way,\nComCLIP can mitigate spurious correlations introduced by the pretrained CLIP\nmodels and dynamically evaluate the importance of each component. Experiments\non four compositional image-text matching datasets: SVO, ComVG, Winoground, and\nVL-checklist, and two general image-text retrieval datasets: Flick30K, and\nMSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts\nthe \\textbf{\\textit{zero-shot}} inference ability of CLIP, SLIP, and BLIP2 even\nwithout further training or fine-tuning. Our codes can be found at\nhttps://github.com/eric-ai-lab/ComCLIP.\n","authors":["Kenan Jiang","Xuehai He","Ruize Xu","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2211.13854v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08921v3","updated":"2024-03-21T00:27:37Z","published":"2023-11-15T12:47:52Z","title":"Self-Improving for Zero-Shot Named Entity Recognition with Large\n  Language Models","summary":"  Exploring the application of powerful large language models (LLMs) on the\nnamed entity recognition (NER) task has drawn much attention recently. This\nwork pushes the performance boundary of zero-shot NER with LLMs by proposing a\ntraining-free self-improving framework, which utilizes an unlabeled corpus to\nstimulate the self-learning ability of LLMs. First, we use the LLM to make\npredictions on the unlabeled corpus using self-consistency and obtain a\nself-annotated dataset. Second, we explore various strategies to select\nreliable annotations to form a reliable self-annotated dataset. Finally, for\neach test input, we retrieve demonstrations from the reliable self-annotated\ndataset and perform inference via in-context learning. Experiments on four\nbenchmarks show substantial performance improvements achieved by our framework.\nThrough comprehensive experimental analysis, we find that increasing the size\nof unlabeled corpus or iterations of self-improving does not guarantee further\nimprovement, but the performance might be boosted via more advanced strategies\nfor reliable annotation selection. Code and data are publicly available at\nhttps://github.com/Emma1066/Self-Improve-Zero-Shot-NER\n","authors":["Tingyu Xie","Qi Li","Yan Zhang","Zuozhu Liu","Hongwei Wang"],"pdf_url":"https://arxiv.org/pdf/2311.08921v3.pdf","comment":"Accepted to NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.14048v1","updated":"2024-03-21T00:13:59Z","published":"2024-03-21T00:13:59Z","title":"The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio\n  Benchmarks and Novel Data","summary":"  The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine\nlearning (ML) experts from various audio domains. There are several valuable\naudio-driven ML tasks, from speech emotion recognition to audio event\ndetection, but the community is sparse compared to other ML areas, e.g.,\ncomputer vision or natural language processing. A major limitation with audio\nis the available data; with audio being a time-dependent modality, high-quality\ndata collection is time-consuming and costly, making it challenging for\nacademic groups to apply their often state-of-the-art strategies to a larger,\nmore generalizable dataset. In this short white paper, to encourage researchers\nwith limited access to large-datasets, the organizers first outline several\nopen-source datasets that are available to the community, and for the duration\nof the workshop are making several propriety datasets available. Namely, three\nvocal datasets, Hume-Prosody, Hume-VocalBurst, an acted emotional speech\ndataset Modulate-Sonata, and an in-game streamer dataset Modulate-Stream. We\noutline the current baselines on these datasets but encourage researchers from\nacross audio to utilize them outside of the initial baseline tasks.\n","authors":["Alice Baird","Rachel Manzelli","Panagiotis Tzirakis","Chris Gagne","Haoqi Li","Sadie Allen","Sander Dieleman","Brian Kulis","Shrikanth S. Narayanan","Alan Cowen"],"pdf_url":"https://arxiv.org/pdf/2403.14048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01623v2","updated":"2024-03-21T00:05:23Z","published":"2023-11-03T16:58:10Z","title":"VQPy: An Object-Oriented Approach to Modern Video Analytics","summary":"  Video analytics is widely used in contemporary systems and services. At the\nforefront of video analytics are video queries that users develop to find\nobjects of particular interest. Building upon the insight that video objects\n(e.g., human, animals, cars, etc.), the center of video analytics, are similar\nin spirit to objects modeled by traditional object-oriented languages, we\npropose to develop an object-oriented approach to video analytics. This\napproach, named VQPy, consists of a frontend$\\unicode{x2015}$a Python variant\nwith constructs that make it easy for users to express video objects and their\ninteractions$\\unicode{x2015}$as well as an extensible backend that can\nautomatically construct and optimize pipelines based on video objects. We have\nimplemented and open-sourced VQPy, which has been productized in Cisco as part\nof its DeepVision framework.\n","authors":["Shan Yu","Zhenting Zhu","Yu Chen","Hanchen Xu","Pengzhan Zhao","Yang Wang","Arthi Padmanabhan","Hugo Latapie","Harry Xu"],"pdf_url":"https://arxiv.org/pdf/2311.01623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14613v1","updated":"2024-03-21T17:58:04Z","published":"2024-03-21T17:58:04Z","title":"DreamReward: Text-to-3D Generation with Human Preference","summary":"  3D content creation from text prompts has shown remarkable success recently.\nHowever, current text-to-3D methods often generate 3D results that do not align\nwell with human preferences. In this paper, we present a comprehensive\nframework, coined DreamReward, to learn and improve text-to-3D models from\nhuman preference feedback. To begin with, we collect 25k expert comparisons\nbased on a systematic annotation pipeline including rating and ranking. Then,\nwe build Reward3D -- the first general-purpose text-to-3D human preference\nreward model to effectively encode human preferences. Building upon the 3D\nreward model, we finally perform theoretical analysis and present the Reward3D\nFeedback Learning (DreamFL), a direct tuning algorithm to optimize the\nmulti-view diffusion models with a redefined scorer. Grounded by theoretical\nproof and extensive experiment comparisons, our DreamReward successfully\ngenerates high-fidelity and 3D consistent results with significant boosts in\nprompt alignment with human intention. Our results demonstrate the great\npotential for learning from human feedback to improve text-to-3D models.\n","authors":["Junliang Ye","Fangfu Liu","Qixiu Li","Zhengyi Wang","Yikai Wang","Xinzhou Wang","Yueqi Duan","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.14613v1.pdf","comment":"Project page: https://jamesyjl.github.io/DreamReward"},{"id":"http://arxiv.org/abs/2305.00969v7","updated":"2024-03-21T17:52:22Z","published":"2023-05-01T17:56:32Z","title":"CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds","summary":"  This paper describes the Ubenwa CryCeleb dataset - a labeled collection of\ninfant cries - and the accompanying CryCeleb 2023 task, which is a public\nspeaker verification challenge based on cry sounds. We released more than 6\nhours of manually segmented cry sounds from 786 newborns for academic use,\naiming to encourage research in infant cry analysis. The inaugural public\ncompetition attracted 59 participants, 11 of whom improved the baseline\nperformance. The top-performing system achieved a significant improvement\nscoring 25.8% equal error rate, which is still far from the performance of\nstate-of-the-art adult speaker verification systems. Therefore, we believe\nthere is room for further research on this dataset, potentially extending\nbeyond the verification task.\n","authors":["David Budaghyan","Charles C. Onu","Arsenii Gorin","Cem Subakan","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2305.00969v7.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2403.14101v1","updated":"2024-03-21T03:24:01Z","published":"2024-03-21T03:24:01Z","title":"Text-Enhanced Data-free Approach for Federated Class-Incremental\n  Learning","summary":"  Federated Class-Incremental Learning (FCIL) is an underexplored yet pivotal\nissue, involving the dynamic addition of new classes in the context of\nfederated learning. In this field, Data-Free Knowledge Transfer (DFKT) plays a\ncrucial role in addressing catastrophic forgetting and data privacy problems.\nHowever, prior approaches lack the crucial synergy between DFKT and the model\ntraining phases, causing DFKT to encounter difficulties in generating\nhigh-quality data from a non-anchored latent space of the old task model. In\nthis paper, we introduce LANDER (Label Text Centered Data-Free Knowledge\nTransfer) to address this issue by utilizing label text embeddings (LTE)\nproduced by pretrained language models. Specifically, during the model training\nphase, our approach treats LTE as anchor points and constrains the feature\nembeddings of corresponding training samples around them, enriching the\nsurrounding area with more meaningful information. In the DFKT phase, by using\nthese LTE anchors, LANDER can synthesize more meaningful samples, thereby\neffectively addressing the forgetting problem. Additionally, instead of tightly\nconstraining embeddings toward the anchor, the Bounding Loss is introduced to\nencourage sample embeddings to remain flexible within a defined radius. This\napproach preserves the natural differences in sample embeddings and mitigates\nthe embedding overlap caused by heterogeneous federated settings. Extensive\nexperiments conducted on CIFAR100, Tiny-ImageNet, and ImageNet demonstrate that\nLANDER significantly outperforms previous methods and achieves state-of-the-art\nperformance in FCIL. The code is available at\nhttps://github.com/tmtuan1307/lander.\n","authors":["Minh-Tuan Tran","Trung Le","Xuan-May Le","Mehrtash Harandi","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2403.14101v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14050v1","updated":"2024-03-21T00:20:16Z","published":"2024-03-21T00:20:16Z","title":"Extracting Emotion Phrases from Tweets using BART","summary":"  Sentiment analysis is a natural language processing task that aims to\nidentify and extract the emotional aspects of a text. However, many existing\nsentiment analysis methods primarily classify the overall polarity of a text,\noverlooking the specific phrases that convey sentiment. In this paper, we\napplied an approach to sentiment analysis based on a question-answering\nframework. Our approach leverages the power of Bidirectional Autoregressive\nTransformer (BART), a pre-trained sequence-to-sequence model, to extract a\nphrase from a given text that amplifies a given sentiment polarity. We create a\nnatural language question that identifies the specific emotion to extract and\nthen guide BART to pay attention to the relevant emotional cues in the text. We\nuse a classifier within BART to predict the start and end positions of the\nanswer span within the text, which helps to identify the precise boundaries of\nthe extracted emotion phrase. Our approach offers several advantages over most\nsentiment analysis studies, including capturing the complete context and\nmeaning of the text and extracting precise token spans that highlight the\nintended sentiment. We achieved an end loss of 87% and Jaccard score of 0.61.\n","authors":["Mahdi Rezapour"],"pdf_url":"https://arxiv.org/pdf/2403.14050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14888v1","updated":"2024-03-21T23:48:21Z","published":"2024-03-21T23:48:21Z","title":"AutoRE: Document-Level Relation Extraction with Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated exceptional abilities in\ncomprehending and generating text, motivating numerous researchers to utilize\nthem for Information Extraction (IE) purposes, including Relation Extraction\n(RE). Nonetheless, most existing methods are predominantly designed for\nSentence-level Relation Extraction (SentRE) tasks, which typically encompass a\nrestricted set of relations and triplet facts within a single sentence.\nFurthermore, certain approaches resort to treating relations as candidate\nchoices integrated into prompt templates, leading to inefficient processing and\nsuboptimal performance when tackling Document-Level Relation Extraction (DocRE)\ntasks, which entail handling multiple relations and triplet facts distributed\nacross a given document, posing distinct challenges. To overcome these\nlimitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel\nRE extraction paradigm named RHF (Relation-Head-Facts). Unlike existing\napproaches, AutoRE does not rely on the assumption of known relation options,\nmaking it more reflective of real-world scenarios. Additionally, we have\ndeveloped an easily extensible RE framework using a Parameters Efficient Fine\nTuning (PEFT) algorithm (QLoRA). Our experiments on the RE-DocRED dataset\nshowcase AutoRE's best performance, achieving state-of-the-art results,\nsurpassing TAG by 10.03% and 9.03% respectively on the dev and test set.\n","authors":["Xue Lilong","Zhang Dan","Dong Yuxiao","Tang Jie"],"pdf_url":"https://arxiv.org/pdf/2403.14888v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.10822v2","updated":"2024-03-21T23:47:24Z","published":"2024-03-16T06:18:15Z","title":"Do Large Language Models understand Medical Codes?","summary":"  The overarching goal of recent AI research has been to make steady progress\ntowards achieving Artificial General Intelligence (AGI), prompting the\nevaluation of Large Language Models (LLMs) across a variety of tasks and\ndomains. One such domain is healthcare, where LLMs can greatly benefit clinical\npractice by assisting with a wide range of tasks. However, these models are\nalso prone to producing ``hallucinations\" or incorrect responses when faced\nwith queries they cannot adequately address, raising concerns and skepticism,\nespecially within the healthcare community. In this work, we investigate\nwhether LLMs understand and can predict medical codes, which are extensively\nutilized in healthcare practice. This study aims to delineate the capabilities\nand limitations of these LLMs. We evaluate various off-the-shelf LLMs (e.g.,\nGPT, LLaMA, etc.) and LLMs specifically designed for biomedical applications to\nassess their awareness and understanding of these domain-specific\nterminologies. Our results indicate that these models as they currently stand\ndo not comprehend the meaning of the medical codes, highlighting the need for\nbetter representation of these alphanumeric codes extensively used in\nhealthcare. We call for improved strategies to effectively capture and\nrepresent the nuances of medical codes and terminologies within LLMs, enabling\nthem to become more reliable and trustworthy tools for healthcare\nprofessionals.\n","authors":["Simon A. Lee","Timothy Lindsey"],"pdf_url":"https://arxiv.org/pdf/2403.10822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09682v2","updated":"2024-03-21T22:44:41Z","published":"2023-11-16T08:52:27Z","title":"MacGyver: Are Large Language Models Creative Problem Solvers?","summary":"  We explore the creative problem-solving capabilities of modern LLMs in a\nnovel constrained setting. To this end, we create MACGYVER, an automatically\ngenerated dataset consisting of over 1,600 real-world problems deliberately\ndesigned to trigger innovative usage of objects and necessitate out-of-the-box\nthinking. We then present our collection to both LLMs and humans to compare and\ncontrast their problem-solving abilities. MACGYVER is challenging for both\ngroups, but in unique and complementary ways. For instance, humans excel in\ntasks they are familiar with but struggle with domain-specific knowledge,\nleading to a higher variance. In contrast, LLMs, exposed to a variety of\nspecialized knowledge, attempt broader problems but fail by proposing\nphysically-infeasible actions. Finally, we provide a detailed error analysis of\nLLMs, and demonstrate the potential of enhancing their problem-solving ability\nwith novel prompting techniques such as iterative step-wise reflection and\ndivergent-convergent thinking.\n  This work (1) introduces a fresh arena for intelligent agents focusing on\nintricate aspects of physical reasoning, planning, and unconventional thinking,\nwhich supplements the existing spectrum of machine intelligence; and (2)\nprovides insight into the constrained problem-solving capabilities of both\nhumans and AI.\n","authors":["Yufei Tian","Abhilasha Ravichander","Lianhui Qin","Ronan Le Bras","Raja Marjieh","Nanyun Peng","Yejin Choi","Thomas L. Griffiths","Faeze Brahman"],"pdf_url":"https://arxiv.org/pdf/2311.09682v2.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14870v1","updated":"2024-03-21T22:36:24Z","published":"2024-03-21T22:36:24Z","title":"VidLA: Video-Language Alignment at Scale","summary":"  In this paper, we propose VidLA, an approach for video-language alignment at\nscale. There are two major limitations of previous video-language alignment\napproaches. First, they do not capture both short-range and long-range temporal\ndependencies and typically employ complex hierarchical deep network\narchitectures that are hard to integrate with existing pretrained image-text\nfoundation models. To effectively address this limitation, we instead keep the\nnetwork architecture simple and use a set of data tokens that operate at\ndifferent temporal resolutions in a hierarchical manner, accounting for the\ntemporally hierarchical nature of videos. By employing a simple two-tower\narchitecture, we are able to initialize our video-language model with\npretrained image-text foundation models, thereby boosting the final\nperformance. Second, existing video-language alignment works struggle due to\nthe lack of semantically aligned large-scale training data. To overcome it, we\nleverage recent LLMs to curate the largest video-language dataset to date with\nbetter visual grounding. Furthermore, unlike existing video-text datasets which\nonly contain short clips, our dataset is enriched with video clips of varying\ndurations to aid our temporally hierarchical data tokens in extracting better\nrepresentations at varying temporal scales. Overall, empirical results show\nthat our proposed approach surpasses state-of-the-art methods on multiple\nretrieval benchmarks, especially on longer videos, and performs competitively\non classification benchmarks.\n","authors":["Mamshad Nayeem Rizve","Fan Fei","Jayakrishnan Unnikrishnan","Son Tran","Benjamin Z. Yao","Belinda Zeng","Mubarak Shah","Trishul Chilimbi"],"pdf_url":"https://arxiv.org/pdf/2403.14870v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14859v1","updated":"2024-03-21T22:08:44Z","published":"2024-03-21T22:08:44Z","title":"Comparing Plausibility Estimates in Base and Instruction-Tuned Large\n  Language Models","summary":"  Instruction-tuned LLMs can respond to explicit queries formulated as prompts,\nwhich greatly facilitates interaction with human users. However, prompt-based\napproaches might not always be able to tap into the wealth of implicit\nknowledge acquired by LLMs during pre-training. This paper presents a\ncomprehensive study of ways to evaluate semantic plausibility in LLMs. We\ncompare base and instruction-tuned LLM performance on an English sentence\nplausibility task via (a) explicit prompting and (b) implicit estimation via\ndirect readout of the probabilities models assign to strings. Experiment 1\nshows that, across model architectures and plausibility datasets, (i) log\nlikelihood ($\\textit{LL}$) scores are the most reliable indicator of sentence\nplausibility, with zero-shot prompting yielding inconsistent and typically poor\nresults; (ii) $\\textit{LL}$-based performance is still inferior to human\nperformance; (iii) instruction-tuned models have worse $\\textit{LL}$-based\nperformance than base models. In Experiment 2, we show that $\\textit{LL}$\nscores across models are modulated by context in the expected way, showing high\nperformance on three metrics of context-sensitive plausibility and providing a\ndirect match to explicit human plausibility judgments. Overall, $\\textit{LL}$\nestimates remain a more reliable measure of plausibility in LLMs than direct\nprompting.\n","authors":["Carina Kauf","Emmanuele Chersoni","Alessandro Lenci","Evelina Fedorenko","Anna A. Ivanova"],"pdf_url":"https://arxiv.org/pdf/2403.14859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11905v2","updated":"2024-03-21T21:57:13Z","published":"2024-03-18T16:06:30Z","title":"Tur[k]ingBench: A Challenge Benchmark for Web Agents","summary":"  Recent chatbots have demonstrated impressive ability to understand and\ncommunicate in raw-text form. However, there is more to the world than raw\ntext. For example, humans spend long hours of their time on web pages, where\ntext is intertwined with other modalities and tasks are accomplished in the\nform of various complex interactions. Can state-of-the-art multi-modal models\ngeneralize to such complex domains?\n  To address this question, we introduce TurkingBench, a benchmark of tasks\nformulated as web pages containing textual instructions with multi-modal\ncontext. Unlike existing work which employs artificially synthesized web pages,\nhere we use natural HTML pages that were originally designed for crowdsourcing\nworkers for various annotation purposes. The HTML instructions of each task are\nalso instantiated with various values (obtained from the crowdsourcing tasks)\nto form new instances of the task. This benchmark contains 32.2K instances\ndistributed across 158 tasks.\n  Additionally, to facilitate the evaluation on TurkingBench, we develop an\nevaluation framework that connects the responses of chatbots to modifications\non web pages (modifying a text box, checking a radio, etc.). We evaluate the\nperformance of state-of-the-art models, including language-only, vision-only,\nand layout-only models, and their combinations, on this benchmark. Our findings\nreveal that these models perform significantly better than random chance, yet\nconsiderable room exists for improvement. We hope this benchmark will help\nfacilitate the evaluation and development of web-based agents.\n","authors":["Kevin Xu","Yeganeh Kordi","Kate Sanders","Yizhong Wang","Adam Byerly","Jack Zhang","Benjamin Van Durme","Daniel Khashabi"],"pdf_url":"https://arxiv.org/pdf/2403.11905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13362v2","updated":"2024-03-21T21:56:49Z","published":"2024-03-20T07:44:06Z","title":"Incentivizing News Consumption on Social Media Platforms Using Large\n  Language Models and Realistic Bot Accounts","summary":"  Polarization, declining trust, and wavering support for democratic norms are\npressing threats to U.S. democracy. Exposure to verified and quality news may\nlower individual susceptibility to these threats and make citizens more\nresilient to misinformation, populism, and hyperpartisan rhetoric. This project\nexamines how to enhance users' exposure to and engagement with verified and\nideologically balanced news in an ecologically valid setting. We rely on a\nlarge-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on\n28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users\ntweeting about sports, entertainment, or lifestyle with a contextual reply\ncontaining two hardcoded elements: a URL to the topic-relevant section of\nquality news organization and an encouragement to follow its Twitter account.\nTo further test differential effects by gender of the bots, treated users were\nrandomly assigned to receive responses by bots presented as female or male. We\nexamine whether our over-time intervention enhances the following of news media\norganization, the sharing and the liking of news content and the tweeting about\npolitics and the liking of political content. We find that the treated users\nfollowed more news accounts and the users in the female bot treatment were more\nlikely to like news content than the control. Most of these results, however,\nwere small in magnitude and confined to the already politically interested\nTwitter users, as indicated by their pre-treatment tweeting about politics.\nThese findings have implications for social media and news organizations, and\nalso offer direction for future work on how Large Language Models and other\ncomputational interventions can effectively enhance individual on-platform\nengagement with quality news and public affairs.\n","authors":["Hadi Askari","Anshuman Chhabra","Bernhard Clemm von Hohenberg","Michael Heseltine","Magdalena Wojcieszak"],"pdf_url":"https://arxiv.org/pdf/2403.13362v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14840v1","updated":"2024-03-21T21:23:35Z","published":"2024-03-21T21:23:35Z","title":"TAMS: Translation-Assisted Morphological Segmentation","summary":"  Canonical morphological segmentation is the process of analyzing words into\nthe standard (aka underlying) forms of their constituent morphemes. This is a\ncore task in language documentation, and NLP systems have the potential to\ndramatically speed up this process. But in typical language documentation\nsettings, training data for canonical morpheme segmentation is scarce, making\nit difficult to train high quality models. However, translation data is often\nmuch more abundant, and, in this work, we present a method that attempts to\nleverage this data in the canonical segmentation task. We propose a\ncharacter-level sequence-to-sequence model that incorporates representations of\ntranslations obtained from pretrained high-resource monolingual language models\nas an additional signal. Our model outperforms the baseline in a super-low\nresource setting but yields mixed results on training splits with more data.\nWhile further work is needed to make translations useful in higher-resource\nsettings, our model shows promise in severely resource-constrained settings.\n","authors":["Enora Rice","Ali Marashian","Luke Gessler","Alexis Palmer","Katharina von der Wense"],"pdf_url":"https://arxiv.org/pdf/2403.14840v1.pdf","comment":"Submitted to ACL ARR on December 15th 2023"},{"id":"http://arxiv.org/abs/2403.14814v1","updated":"2024-03-21T19:59:52Z","published":"2024-03-21T19:59:52Z","title":"The opportunities and risks of large language models in mental health","summary":"  Global rates of mental health concerns are rising and there is increasing\nrealization that existing models of mental healthcare will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health-related tasks. In this review, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs application\nto mental health and encourage adoption of strategies to mitigate these risks.\nThe urgent need for mental health support must be balanced with responsible\ndevelopment, testing, and deployment of mental health LLMs. Especially critical\nis ensuring that mental health LLMs are fine-tuned for mental health, enhance\nmental health equity, adhere to ethical standards, and that people, including\nthose with lived experience with mental health concerns, are involved in all\nstages from development through deployment. Prioritizing these efforts will\nminimize potential harms to mental health and maximize the likelihood that LLMs\nwill positively impact mental health globally.\n","authors":["Hannah R. Lawrence","Renee A. Schneider","Susan B. Rubin","Maja J. Mataric","Daniel J. McDuff","Megan Jones Bell"],"pdf_url":"https://arxiv.org/pdf/2403.14814v1.pdf","comment":"12 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2403.14808v1","updated":"2024-03-21T19:46:42Z","published":"2024-03-21T19:46:42Z","title":"A Collection of Pragmatic-Similarity Judgments over Spoken Dialog\n  Utterances","summary":"  Automatic measures of similarity between utterances are invaluable for\ntraining speech synthesizers, evaluating machine translation, and assessing\nlearner productions. While there exist measures for semantic similarity and\nprosodic similarity, there are as yet none for pragmatic similarity. To enable\nthe training of such measures, we developed the first collection of human\njudgments of pragmatic similarity between utterance pairs. Each pair consisting\nof an utterance extracted from a recorded dialog and a re-enactment of that\nutterance. Re-enactments were done under various conditions designed to create\na variety of degrees of similarity. Each pair was rated on a continuous scale\nby 6 to 9 judges. The average inter-judge correlation was as high as 0.72 for\nEnglish and 0.66 for Spanish. We make this data available at\nhttps://github.com/divettemarco/PragSim .\n","authors":["Nigel G. Ward","Divette Marco"],"pdf_url":"https://arxiv.org/pdf/2403.14808v1.pdf","comment":"LREC 2024"},{"id":"http://arxiv.org/abs/2403.14783v1","updated":"2024-03-21T18:57:25Z","published":"2024-03-21T18:57:25Z","title":"Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot\n  Visual Question Answering","summary":"  This work explores the zero-shot capabilities of foundation models in Visual\nQuestion Answering (VQA) tasks. We propose an adaptive multi-agent system,\nnamed Multi-Agent VQA, to overcome the limitations of foundation models in\nobject detection and counting by using specialized agents as tools. Unlike\nexisting approaches, our study focuses on the system's performance without\nfine-tuning it on specific VQA datasets, making it more practical and robust in\nthe open world. We present preliminary experimental results under zero-shot\nscenarios and highlight some failure cases, offering new directions for future\nresearch.\n","authors":["Bowen Jiang","Zhijun Zhuang","Shreyas S. Shivakumar","Dan Roth","Camillo J. Taylor"],"pdf_url":"https://arxiv.org/pdf/2403.14783v1.pdf","comment":"A full version of the paper will be released soon. The codes are\n  available at https://github.com/bowen-upenn/Multi-Agent-VQA"},{"id":"http://arxiv.org/abs/2311.00117v2","updated":"2024-03-21T18:40:32Z","published":"2023-10-31T19:45:15Z","title":"BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B","summary":"  Llama 2-Chat is a collection of large language models that Meta developed and\nreleased to the public. While Meta fine-tuned Llama 2-Chat to refuse to output\nharmful content, we hypothesize that public access to model weights enables bad\nactors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's\ncapabilities for malicious purposes. We demonstrate that it is possible to\neffectively undo the safety fine-tuning from Llama 2-Chat 13B with less than\n$200, while retaining its general capabilities. Our results demonstrate that\nsafety-fine tuning is ineffective at preventing misuse when model weights are\nreleased publicly. Given that future models will likely have much greater\nability to cause harm at scale, it is essential that AI developers address\nthreats from fine-tuning when considering whether to publicly release their\nmodel weights.\n","authors":["Pranav Gade","Simon Lermen","Charlie Rogers-Smith","Jeffrey Ladish"],"pdf_url":"https://arxiv.org/pdf/2311.00117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14774v1","updated":"2024-03-21T18:28:43Z","published":"2024-03-21T18:28:43Z","title":"Few-Shot Adversarial Prompt Learning on Vision-Language Models","summary":"  The vulnerability of deep neural networks to imperceptible adversarial\nperturbations has attracted widespread attention. Inspired by the success of\nvision-language foundation models, previous efforts achieved zero-shot\nadversarial robustness by aligning adversarial visual features with text\nsupervision. However, in practice, they are still unsatisfactory due to several\nissues, including heavy adaptation cost, suboptimal text supervision, and\nuncontrolled natural generalization capacity. In this paper, to address these\nissues, we propose a few-shot adversarial prompt framework where adapting input\nsequences with limited data makes significant adversarial robustness\nimprovement. Specifically, we achieve this by providing adversarially\ncorrelated text supervision that is end-to-end learned from adversarial\nexamples. We also propose a novel training objective that enhances the\nconsistency of multi-modal features while encourages differentiated uni-modal\nfeatures between natural and adversarial examples. The proposed framework gives\naccess to learn adversarial text supervision, which provides superior\ncross-modal adversarial alignment and matches state-of-the-art zero-shot\nadversarial robustness with only 1% training data.\n","authors":["Yiwei Zhou","Xiaobo Xia","Zhiwei Lin","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14774v1.pdf","comment":"25 pages, 13 tables, 8 figures"},{"id":"http://arxiv.org/abs/2403.14773v1","updated":"2024-03-21T18:27:29Z","published":"2024-03-21T18:27:29Z","title":"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation\n  from Text","summary":"  Text-to-video diffusion models enable the generation of high-quality videos\nthat follow text instructions, making it easy to create diverse and individual\ncontent. However, existing approaches mostly focus on high-quality short video\ngeneration (typically 16 or 24 frames), ending up with hard-cuts when naively\nextended to the case of long video synthesis. To overcome these limitations, we\nintroduce StreamingT2V, an autoregressive approach for long video generation of\n80, 240, 600, 1200 or more frames with smooth transitions. The key components\nare:(i) a short-term memory block called conditional attention module (CAM),\nwhich conditions the current generation on the features extracted from the\nprevious chunk via an attentional mechanism, leading to consistent chunk\ntransitions, (ii) a long-term memory block called appearance preservation\nmodule, which extracts high-level scene and object features from the first\nvideo chunk to prevent the model from forgetting the initial scene, and (iii) a\nrandomized blending approach that enables to apply a video enhancer\nautoregressively for infinitely long videos without inconsistencies between\nchunks. Experiments show that StreamingT2V generates high motion amount. In\ncontrast, all competing image-to-video methods are prone to video stagnation\nwhen applied naively in an autoregressive manner. Thus, we propose with\nStreamingT2V a high-quality seamless text-to-long video generator that\noutperforms competitors with consistency and motion. Our code will be available\nat: https://github.com/Picsart-AI-Research/StreamingT2V\n","authors":["Roberto Henschel","Levon Khachatryan","Daniil Hayrapetyan","Hayk Poghosyan","Vahram Tadevosyan","Zhangyang Wang","Shant Navasardyan","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.14773v1.pdf","comment":"https://github.com/Picsart-AI-Research/StreamingT2V"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.14628v1","updated":"2024-03-21T17:59:59Z","published":"2024-03-21T17:59:59Z","title":"Zero-Shot Multi-Object Shape Completion","summary":"  We present a 3D shape completion method that recovers the complete geometry\nof multiple objects in complex scenes from a single RGB-D image. Despite\nnotable advancements in single object 3D shape completion, high-quality\nreconstructions in highly cluttered real-world multi-object scenes remains a\nchallenge. To address this issue, we propose OctMAE, an architecture that\nleverages an Octree U-Net and a latent 3D MAE to achieve high-quality and near\nreal-time multi-object shape completion through both local and global geometric\nreasoning. Because a na\\\"ive 3D MAE can be computationally intractable and\nmemory intensive even in the latent space, we introduce a novel occlusion\nmasking strategy and adopt 3D rotary embeddings, which significantly improves\nthe runtime and shape completion quality. To generalize to a wide range of\nobjects in diverse scenes, we create a large-scale photorealistic dataset,\nfeaturing a diverse set of 12K 3D object models from the Objaverse dataset\nwhich are rendered in multi-object scenes with physics-based positioning. Our\nmethod outperforms the current state-of-the-art on both synthetic and\nreal-world datasets and demonstrates a strong zero-shot capability.\n","authors":["Shun Iwase","Katherine Liu","Vitor Guizilini","Adrien Gaidon","Kris Kitani","Rares Ambrus","Sergey Zakharov"],"pdf_url":"https://arxiv.org/pdf/2403.14628v1.pdf","comment":"21 pages, 8 figues"},{"id":"http://arxiv.org/abs/2403.14627v1","updated":"2024-03-21T17:59:58Z","published":"2024-03-21T17:59:58Z","title":"MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images","summary":"  We propose MVSplat, an efficient feed-forward 3D Gaussian Splatting model\nlearned from sparse multi-view images. To accurately localize the Gaussian\ncenters, we propose to build a cost volume representation via plane sweeping in\nthe 3D space, where the cross-view feature similarities stored in the cost\nvolume can provide valuable geometry cues to the estimation of depth. We learn\nthe Gaussian primitives' opacities, covariances, and spherical harmonics\ncoefficients jointly with the Gaussian centers while only relying on\nphotometric supervision. We demonstrate the importance of the cost volume\nrepresentation in learning feed-forward Gaussian Splatting models via extensive\nexperimental evaluations. On the large-scale RealEstate10K and ACID benchmarks,\nour model achieves state-of-the-art performance with the fastest feed-forward\ninference speed (22 fps). Compared to the latest state-of-the-art method\npixelSplat, our model uses $10\\times $ fewer parameters and infers more than\n$2\\times$ faster while providing higher appearance and geometry quality as well\nas better cross-dataset generalization.\n","authors":["Yuedong Chen","Haofei Xu","Chuanxia Zheng","Bohan Zhuang","Marc Pollefeys","Andreas Geiger","Tat-Jen Cham","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2403.14627v1.pdf","comment":"Project page: https://donydchen.github.io/mvsplat Code:\n  https://github.com/donydchen/mvsplat"},{"id":"http://arxiv.org/abs/2403.14625v1","updated":"2024-03-21T17:59:55Z","published":"2024-03-21T17:59:55Z","title":"LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT\n  Descriptors","summary":"  We present a simple self-supervised method to enhance the performance of ViT\nfeatures for dense downstream tasks. Our Lightweight Feature Transform (LiFT)\nis a straightforward and compact postprocessing network that can be applied to\nenhance the features of any pre-trained ViT backbone. LiFT is fast and easy to\ntrain with a self-supervised objective, and it boosts the density of ViT\nfeatures for minimal extra inference cost. Furthermore, we demonstrate that\nLiFT can be applied with approaches that use additional task-specific\ndownstream modules, as we integrate LiFT with ViTDet for COCO detection and\nsegmentation. Despite the simplicity of LiFT, we find that it is not simply\nlearning a more complex version of bilinear interpolation. Instead, our LiFT\ntraining protocol leads to several desirable emergent properties that benefit\nViT features in dense downstream tasks. This includes greater scale invariance\nfor features, and better object boundary maps. By simply training LiFT for a\nfew epochs, we show improved performance on keypoint correspondence, detection,\nsegmentation, and object discovery tasks. Overall, LiFT provides an easy way to\nunlock the benefits of denser feature arrays for a fraction of the\ncomputational cost. For more details, refer to our project page at\nhttps://www.cs.umd.edu/~sakshams/LiFT/.\n","authors":["Saksham Suri","Matthew Walmer","Kamal Gupta","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2403.14625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14626v1","updated":"2024-03-21T17:59:55Z","published":"2024-03-21T17:59:55Z","title":"ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras\n  Based on Transformer","summary":"  Obstacle detection and tracking represent a critical component in robot\nautonomous navigation. In this paper, we propose ODTFormer, a Transformer-based\nmodel to address both obstacle detection and tracking problems. For the\ndetection task, our approach leverages deformable attention to construct a 3D\ncost volume, which is decoded progressively in the form of voxel occupancy\ngrids. We further track the obstacles by matching the voxels between\nconsecutive frames. The entire model can be optimized in an end-to-end manner.\nThrough extensive experiments on DrivingStereo and KITTI benchmarks, our model\nachieves state-of-the-art performance in the obstacle detection task. We also\nreport comparable accuracy to state-of-the-art obstacle tracking models while\nrequiring only a fraction of their computation cost, typically ten-fold to\ntwenty-fold less. The code and model weights will be publicly released.\n","authors":["Tianye Ding","Hongyu Li","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14626v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.14624v1","updated":"2024-03-21T17:59:50Z","published":"2024-03-21T17:59:50Z","title":"MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual\n  Math Problems?","summary":"  The remarkable progress of Multi-modal Large Language Models (MLLMs) has\ngarnered unparalleled attention, due to their superior performance in visual\ncontexts. However, their capabilities in visual math problem-solving remain\ninsufficiently evaluated and understood. We investigate current benchmarks to\nincorporate excessive visual content within textual questions, which\npotentially assist MLLMs in deducing answers without truly interpreting the\ninput diagrams. To this end, we introduce MathVerse, an all-around visual math\nbenchmark designed for an equitable and in-depth evaluation of MLLMs. We\nmeticulously collect 2,612 high-quality, multi-subject math problems with\ndiagrams from publicly available sources. Each problem is then transformed by\nhuman annotators into six distinct versions, each offering varying degrees of\ninformation content in multi-modality, contributing to 15K test samples in\ntotal. This approach allows MathVerse to comprehensively assess whether and how\nmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.\nIn addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a\nfine-grained assessment of the output answers. Rather than naively judging True\nor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and\nthen score each step with detailed error analysis, which can reveal the\nintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark\nmay provide unique insights to guide the future development of MLLMs. Project\npage: https://mathverse-cuhk.github.io\n","authors":["Renrui Zhang","Dongzhi Jiang","Yichi Zhang","Haokun Lin","Ziyu Guo","Pengshuo Qiu","Aojun Zhou","Pan Lu","Kai-Wei Chang","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.14624v1.pdf","comment":"46 Pages, Work in Progress, Benchmark Project Page:\n  https://mathverse-cuhk.github.io"},{"id":"http://arxiv.org/abs/2403.14623v1","updated":"2024-03-21T17:59:41Z","published":"2024-03-21T17:59:41Z","title":"Simplified Diffusion Schrödinger Bridge","summary":"  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.\n","authors":["Zhicong Tang","Tiankai Hang","Shuyang Gu","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14622v1","updated":"2024-03-21T17:59:35Z","published":"2024-03-21T17:59:35Z","title":"Language Repository for Long Video Understanding","summary":"  Language has become a prominent modality in computer vision with the rise of\nmulti-modal LLMs. Despite supporting long context-lengths, their effectiveness\nin handling long-term information gradually declines with input length. This\nbecomes critical, especially in applications such as long-form video\nunderstanding. In this paper, we introduce a Language Repository (LangRepo) for\nLLMs, that maintains concise and structured information as an interpretable\n(i.e., all-textual) representation. Our repository is updated iteratively based\non multi-scale video chunks. We introduce write and read operations that focus\non pruning redundancies in text, and extracting information at various temporal\nscales. The proposed framework is evaluated on zero-shot visual\nquestion-answering benchmarks including EgoSchema, NExT-QA, IntentQA and\nNExT-GQA, showing state-of-the-art performance at its scale. Our code is\navailable at https://github.com/kkahatapitiya/LangRepo.\n","authors":["Kumara Kahatapitiya","Kanchana Ranasinghe","Jongwoo Park","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2403.14622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14621v1","updated":"2024-03-21T17:59:34Z","published":"2024-03-21T17:59:34Z","title":"GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction\n  and Generation","summary":"  We introduce GRM, a large-scale reconstructor capable of recovering a 3D\nasset from sparse-view images in around 0.1s. GRM is a feed-forward\ntransformer-based model that efficiently incorporates multi-view information to\ntranslate the input pixels into pixel-aligned Gaussians, which are unprojected\nto create a set of densely distributed 3D Gaussians representing a scene.\nTogether, our transformer architecture and the use of 3D Gaussians unlock a\nscalable and efficient reconstruction framework. Extensive experimental results\ndemonstrate the superiority of our method over alternatives regarding both\nreconstruction quality and efficiency. We also showcase the potential of GRM in\ngenerative tasks, i.e., text-to-3D and image-to-3D, by integrating it with\nexisting multi-view diffusion models. Our project website is at:\nhttps://justimyhxu.github.io/projects/grm/.\n","authors":["Yinghao Xu","Zifan Shi","Wang Yifan","Hansheng Chen","Ceyuan Yang","Sida Peng","Yujun Shen","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2403.14621v1.pdf","comment":"Project page: https://justimyhxu.github.io/projects/grm/ Code:\n  https://github.com/justimyhxu/GRM"},{"id":"http://arxiv.org/abs/2403.14619v1","updated":"2024-03-21T17:59:16Z","published":"2024-03-21T17:59:16Z","title":"ClusteringSDF: Self-Organized Neural Implicit Surfaces for 3D\n  Decomposition","summary":"  3D decomposition/segmentation still remains a challenge as large-scale 3D\nannotated data is not readily available. Contemporary approaches typically\nleverage 2D machine-generated segments, integrating them for 3D consistency.\nWhile the majority of these methods are based on NeRFs, they face a potential\nweakness that the instance/semantic embedding features derive from independent\nMLPs, thus preventing the segmentation network from learning the geometric\ndetails of the objects directly through radiance and density. In this paper, we\npropose ClusteringSDF, a novel approach to achieve both segmentation and\nreconstruction in 3D via the neural implicit surface representation,\nspecifically Signal Distance Function (SDF), where the segmentation rendering\nis directly integrated with the volume rendering of neural implicit surfaces.\nAlthough based on ObjectSDF++, ClusteringSDF no longer requires the\nground-truth segments for supervision while maintaining the capability of\nreconstructing individual object surfaces, but purely with the noisy and\ninconsistent labels from pre-trained models.As the core of ClusteringSDF, we\nintroduce a high-efficient clustering mechanism for lifting the 2D labels to 3D\nand the experimental results on the challenging scenes from ScanNet and Replica\ndatasets show that ClusteringSDF can achieve competitive performance compared\nagainst the state-of-the-art with significantly reduced training time.\n","authors":["Tianhao Wu","Chuanxia Zheng","Tat-Jen Cham","Qianyi Wu"],"pdf_url":"https://arxiv.org/pdf/2403.14619v1.pdf","comment":"Project Page: https://sm0kywu.github.io/ClusteringSDF/"},{"id":"http://arxiv.org/abs/2403.14617v1","updated":"2024-03-21T17:59:03Z","published":"2024-03-21T17:59:03Z","title":"Videoshop: Localized Semantic Video Editing with Noise-Extrapolated\n  Diffusion Inversion","summary":"  We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.\n","authors":["Xiang Fan","Anand Bhattad","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.14617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14616v1","updated":"2024-03-21T17:58:56Z","published":"2024-03-21T17:58:56Z","title":"Hierarchical Text-to-Vision Self Supervised Alignment for Improved\n  Histopathology Representation Learning","summary":"  Self-supervised representation learning has been highly promising for\nhistopathology image analysis with numerous approaches leveraging their\npatient-slide-patch hierarchy to learn better representations. In this paper,\nwe explore how the combination of domain specific natural language information\nwith such hierarchical visual representations can benefit rich representation\nlearning for medical image tasks. Building on automated language description\ngeneration for features visible in histopathology images, we present a novel\nlanguage-tied self-supervised learning framework, Hierarchical Language-tied\nSelf-Supervision (HLSS) for histopathology images. We explore contrastive\nobjectives and granular language description based text alignment at multiple\nhierarchies to inject language modality information into the visual\nrepresentations. Our resulting model achieves state-of-the-art performance on\ntwo medical imaging benchmarks, OpenSRH and TCGA datasets. Our framework also\nprovides better interpretability with our language aligned representation\nspace. Code is available at https://github.com/Hasindri/HLSS.\n","authors":["Hasindri Watawana","Kanchana Ranasinghe","Tariq Mahmood","Muzammal Naseer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.14616v1.pdf","comment":"13 pages and 5 figures"},{"id":"http://arxiv.org/abs/2403.14614v1","updated":"2024-03-21T17:58:14Z","published":"2024-03-21T17:58:14Z","title":"AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and\n  Modulation","summary":"  In the image acquisition process, various forms of degradation, including\nnoise, haze, and rain, are frequently introduced. These degradations typically\narise from the inherent limitations of cameras or unfavorable ambient\nconditions. To recover clean images from degraded versions, numerous\nspecialized restoration methods have been developed, each targeting a specific\ntype of degradation. Recently, all-in-one algorithms have garnered significant\nattention by addressing different types of degradations within a single model\nwithout requiring prior information of the input degradation type. However,\nthese methods purely operate in the spatial domain and do not delve into the\ndistinct frequency variations inherent to different degradation types. To\naddress this gap, we propose an adaptive all-in-one image restoration network\nbased on frequency mining and modulation. Our approach is motivated by the\nobservation that different degradation types impact the image content on\ndifferent frequency subbands, thereby requiring different treatments for each\nrestoration task. Specifically, we first mine low- and high-frequency\ninformation from the input features, guided by the adaptively decoupled spectra\nof the degraded image. The extracted features are then modulated by a\nbidirectional operator to facilitate interactions between different frequency\ncomponents. Finally, the modulated features are merged into the original input\nfor a progressively guided restoration. With this approach, the model achieves\nadaptive reconstruction by accentuating the informative frequency subbands\naccording to different input degradations. Extensive experiments demonstrate\nthat the proposed method achieves state-of-the-art performance on different\nimage restoration tasks, including denoising, dehazing, deraining, motion\ndeblurring, and low-light image enhancement. Our code is available at\nhttps://github.com/c-yn/AdaIR.\n","authors":["Yuning Cui","Syed Waqas Zamir","Salman Khan","Alois Knoll","Mubarak Shah","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.14614v1.pdf","comment":"28 pages,15 figures"},{"id":"http://arxiv.org/abs/2403.14613v1","updated":"2024-03-21T17:58:04Z","published":"2024-03-21T17:58:04Z","title":"DreamReward: Text-to-3D Generation with Human Preference","summary":"  3D content creation from text prompts has shown remarkable success recently.\nHowever, current text-to-3D methods often generate 3D results that do not align\nwell with human preferences. In this paper, we present a comprehensive\nframework, coined DreamReward, to learn and improve text-to-3D models from\nhuman preference feedback. To begin with, we collect 25k expert comparisons\nbased on a systematic annotation pipeline including rating and ranking. Then,\nwe build Reward3D -- the first general-purpose text-to-3D human preference\nreward model to effectively encode human preferences. Building upon the 3D\nreward model, we finally perform theoretical analysis and present the Reward3D\nFeedback Learning (DreamFL), a direct tuning algorithm to optimize the\nmulti-view diffusion models with a redefined scorer. Grounded by theoretical\nproof and extensive experiment comparisons, our DreamReward successfully\ngenerates high-fidelity and 3D consistent results with significant boosts in\nprompt alignment with human intention. Our results demonstrate the great\npotential for learning from human feedback to improve text-to-3D models.\n","authors":["Junliang Ye","Fangfu Liu","Qixiu Li","Zhengyi Wang","Yikai Wang","Xinzhou Wang","Yueqi Duan","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.14613v1.pdf","comment":"Project page: https://jamesyjl.github.io/DreamReward"},{"id":"http://arxiv.org/abs/2403.14611v1","updated":"2024-03-21T17:57:31Z","published":"2024-03-21T17:57:31Z","title":"Explorative Inbetweening of Time and Space","summary":"  We introduce bounded generation as a generalized task to control video\ngeneration to synthesize arbitrary camera and subject motion based only on a\ngiven start and end frame. Our objective is to fully leverage the inherent\ngeneralization capability of an image-to-video model without additional\ntraining or fine-tuning of the original model. This is achieved through the\nproposed new sampling strategy, which we call Time Reversal Fusion, that fuses\nthe temporally forward and backward denoising paths conditioned on the start\nand end frame, respectively. The fused path results in a video that smoothly\nconnects the two frames, generating inbetweening of faithful subject motion,\nnovel views of static scenes, and seamless video looping when the two bounding\nframes are identical. We curate a diverse evaluation dataset of image pairs and\ncompare against the closest existing methods. We find that Time Reversal Fusion\noutperforms related work on all subtasks, exhibiting the ability to generate\ncomplex motions and 3D-consistent views guided by bounded frames. See project\npage at https://time-reversal.github.io.\n","authors":["Haiwen Feng","Zheng Ding","Zhihao Xia","Simon Niklaus","Victoria Abrevaya","Michael J. Black","Xuaner Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14611v1.pdf","comment":"project page at https://time-reversal.github.io"},{"id":"http://arxiv.org/abs/2403.14610v1","updated":"2024-03-21T17:57:03Z","published":"2024-03-21T17:57:03Z","title":"T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy","summary":"  We present T-Rex2, a highly practical model for open-set object detection.\nPrevious open-set object detection methods relying on text prompts effectively\nencapsulate the abstract concept of common objects, but struggle with rare or\ncomplex object representation due to data scarcity and descriptive limitations.\nConversely, visual prompts excel in depicting novel objects through concrete\nvisual examples, but fall short in conveying the abstract concept of objects as\neffectively as text prompts. Recognizing the complementary strengths and\nweaknesses of both text and visual prompts, we introduce T-Rex2 that synergizes\nboth prompts within a single model through contrastive learning. T-Rex2 accepts\ninputs in diverse formats, including text prompts, visual prompts, and the\ncombination of both, so that it can handle different scenarios by switching\nbetween the two prompt modalities. Comprehensive experiments demonstrate that\nT-Rex2 exhibits remarkable zero-shot object detection capabilities across a\nwide spectrum of scenarios. We show that text prompts and visual prompts can\nbenefit from each other within the synergy, which is essential to cover massive\nand complicated real-world scenarios and pave the way towards generic object\ndetection. Model API is now available at\n\\url{https://github.com/IDEA-Research/T-Rex}.\n","authors":["Qing Jiang","Feng Li","Zhaoyang Zeng","Tianhe Ren","Shilong Liu","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14610v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2403.14602v1","updated":"2024-03-21T17:52:08Z","published":"2024-03-21T17:52:08Z","title":"ReNoise: Real Image Inversion Through Iterative Noising","summary":"  Recent advancements in text-guided diffusion models have unlocked powerful\nimage manipulation capabilities. However, applying these methods to real images\nnecessitates the inversion of the images into the domain of the pretrained\ndiffusion model. Achieving faithful inversion remains a challenge, particularly\nfor more recent models trained to generate images with a small number of\ndenoising steps. In this work, we introduce an inversion method with a high\nquality-to-operation ratio, enhancing reconstruction accuracy without\nincreasing the number of operations. Building on reversing the diffusion\nsampling process, our method employs an iterative renoising mechanism at each\ninversion sampling step. This mechanism refines the approximation of a\npredicted point along the forward diffusion trajectory, by iteratively applying\nthe pretrained diffusion model, and averaging these predictions. We evaluate\nthe performance of our ReNoise technique using various sampling algorithms and\nmodels, including recent accelerated diffusion models. Through comprehensive\nevaluations and comparisons, we show its effectiveness in terms of both\naccuracy and speed. Furthermore, we confirm that our method preserves\neditability by demonstrating text-driven image editing on real images.\n","authors":["Daniel Garibi","Or Patashnik","Andrey Voynov","Hadar Averbuch-Elor","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2403.14602v1.pdf","comment":"project page at: https://garibida.github.io/ReNoise-Inversion/"},{"id":"http://arxiv.org/abs/2403.14599v1","updated":"2024-03-21T17:51:01Z","published":"2024-03-21T17:51:01Z","title":"MyVLM: Personalizing VLMs for User-Specific Queries","summary":"  Recent large-scale vision-language models (VLMs) have demonstrated remarkable\ncapabilities in understanding and generating textual descriptions for visual\ncontent. However, these models lack an understanding of user-specific concepts.\nIn this work, we take a first step toward the personalization of VLMs, enabling\nthem to learn and reason over user-provided concepts. For example, we explore\nwhether these models can learn to recognize you in an image and communicate\nwhat you are doing, tailoring the model to reflect your personal experiences\nand relationships. To effectively recognize a variety of user-specific\nconcepts, we augment the VLM with external concept heads that function as\ntoggles for the model, enabling the VLM to identify the presence of specific\ntarget concepts in a given image. Having recognized the concept, we learn a new\nconcept embedding in the intermediate feature space of the VLM. This embedding\nis tasked with guiding the language model to naturally integrate the target\nconcept in its generated response. We apply our technique to BLIP-2 and LLaVA\nfor personalized image captioning and further show its applicability for\npersonalized visual question-answering. Our experiments demonstrate our ability\nto generalize to unseen images of learned concepts while preserving the model\nbehavior on unrelated inputs.\n","authors":["Yuval Alaluf","Elad Richardson","Sergey Tulyakov","Kfir Aberman","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2403.14599v1.pdf","comment":"Project page: https://snap-research.github.io/MyVLM/"},{"id":"http://arxiv.org/abs/2403.14598v1","updated":"2024-03-21T17:50:47Z","published":"2024-03-21T17:50:47Z","title":"PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model","summary":"  PSALM is a powerful extension of the Large Multi-modal Model (LMM) to address\nthe segmentation task challenges. To overcome the limitation of the LMM being\nlimited to textual output, PSALM incorporates a mask decoder and a\nwell-designed input schema to handle a variety of segmentation tasks. This\nschema includes images, task instructions, conditional prompts, and mask\ntokens, which enable the model to generate and classify segmentation masks\neffectively. The flexible design of PSALM supports joint training across\nmultiple datasets and tasks, leading to improved performance and task\ngeneralization. PSALM achieves superior results on several benchmarks, such as\nRefCOCO/RefCOCO+/RefCOCOg, COCO Panoptic Segmentation, and COCO-Interactive,\nand further exhibits zero-shot capabilities on unseen tasks, such as\nopen-vocabulary segmentation, generalized referring expression segmentation and\nvideo object segmentation, making a significant step towards a GPT moment in\ncomputer vision. Through extensive experiments, PSALM demonstrates its\npotential to transform the domain of image segmentation, leveraging the robust\nvisual understanding capabilities of LMMs as seen in natural language\nprocessing. Code and models are available at https://github.com/zamling/PSALM.\n","authors":["Zheng Zhang","Yeyao Ma","Enming Zhang","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2403.14598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14594v1","updated":"2024-03-21T17:49:26Z","published":"2024-03-21T17:49:26Z","title":"VXP: Voxel-Cross-Pixel Large-scale Image-LiDAR Place Recognition","summary":"  Recent works on the global place recognition treat the task as a retrieval\nproblem, where an off-the-shelf global descriptor is commonly designed in\nimage-based and LiDAR-based modalities. However, it is non-trivial to perform\naccurate image-LiDAR global place recognition since extracting consistent and\nrobust global descriptors from different domains (2D images and 3D point\nclouds) is challenging. To address this issue, we propose a novel\nVoxel-Cross-Pixel (VXP) approach, which establishes voxel and pixel\ncorrespondences in a self-supervised manner and brings them into a shared\nfeature space. Specifically, VXP is trained in a two-stage manner that first\nexplicitly exploits local feature correspondences and enforces similarity of\nglobal descriptors. Extensive experiments on the three benchmarks (Oxford\nRobotCar, ViViD++ and KITTI) demonstrate our method surpasses the\nstate-of-the-art cross-modal retrieval by a large margin.\n","authors":["Yun-Jin Li","Mariia Gladkova","Yan Xia","Rui Wang","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2403.14594v1.pdf","comment":"Project page https://yunjinli.github.io/projects-vxp/"},{"id":"http://arxiv.org/abs/2402.19150v2","updated":"2024-03-21T17:26:47Z","published":"2024-02-29T13:31:56Z","title":"Unveiling Typographic Deceptions: Insights of the Typographic\n  Vulnerability in Large Vision-Language Model","summary":"  Large Vision-Language Models (LVLMs) rely on vision encoders and Large\nLanguage Models (LLMs) to exhibit remarkable capabilities on various\nmulti-modal tasks in the joint space of vision and language. However, the\nTypographic Attack, which disrupts vision-language models (VLMs) such as\nContrastive Language-Image Pretraining (CLIP), has also been expected to be a\nsecurity threat to LVLMs. Firstly, we verify typographic attacks on current\nwell-known commercial and open-source LVLMs and uncover the widespread\nexistence of this threat. Secondly, to better assess this vulnerability, we\npropose the most comprehensive and largest-scale Typographic Dataset to date.\nThe Typographic Dataset not only considers the evaluation of typographic\nattacks under various multi-modal tasks but also evaluates the effects of\ntypographic attacks, influenced by texts generated with diverse factors. Based\non the evaluation results, we investigate the causes why typographic attacks\nmay impact VLMs and LVLMs, leading to three highly insightful discoveries. By\nthe examination of our discoveries and experimental validation in the\nTypographic Dataset, we reduce the performance degradation from $42.07\\%$ to\n$13.90\\%$ when LVLMs confront typographic attacks.\n","authors":["Hao Cheng","Erjia Xiao","Jindong Gu","Le Yang","Jinhao Duan","Jize Zhang","Jiahang Cao","Kaidi Xu","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2402.19150v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19474v2","updated":"2024-03-21T17:25:52Z","published":"2024-02-29T18:59:17Z","title":"The All-Seeing Project V2: Towards General Relation Comprehension of the\n  Open World","summary":"  We present the All-Seeing Project V2: a new model and dataset designed for\nunderstanding object relations in images. Specifically, we propose the\nAll-Seeing Model V2 (ASMv2) that integrates the formulation of text generation,\nobject localization, and relation comprehension into a relation conversation\n(ReC) task. Leveraging this unified task, our model excels not only in\nperceiving and recognizing all objects within the image but also in grasping\nthe intricate relation graph between them, diminishing the relation\nhallucination often encountered by Multi-modal Large Language Models (MLLMs).\nTo facilitate training and evaluation of MLLMs in relation understanding, we\ncreated the first high-quality ReC dataset ({AS-V2) which is aligned with the\nformat of standard instruction tuning data. In addition, we design a new\nbenchmark, termed Circular-based Relation Probing Evaluation (CRPE) for\ncomprehensively evaluating the relation comprehension capabilities of MLLMs.\nNotably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware\nbenchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope that\nour work can inspire more future research and contribute to the evolution\ntowards artificial general intelligence. Our project is released at\nhttps://github.com/OpenGVLab/all-seeing.\n","authors":["Weiyun Wang","Yiming Ren","Haowen Luo","Tiantong Li","Chenxiang Yan","Zhe Chen","Wenhai Wang","Qingyun Li","Lewei Lu","Xizhou Zhu","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2402.19474v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2403.11085v3","updated":"2024-03-21T17:25:23Z","published":"2024-03-17T04:36:18Z","title":"m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks","summary":"  Real-world multi-modal problems are rarely solved by a single machine\nlearning model, and often require multi-step computational plans that involve\nstitching several models. Tool-augmented LLMs hold tremendous promise for\nautomating the generation of such computational plans. However, the lack of\nstandardized benchmarks for evaluating LLMs as planners for multi-step\nmulti-modal tasks has prevented a systematic study of planner design decisions.\nShould LLMs generate a full plan in a single shot or step-by-step? Should they\ninvoke tools directly with Python code or through structured data formats like\nJSON? Does feedback improve planning? To answer these questions and more, we\nintroduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks\ninvolving 33 tools that include multi-modal models, (free) public APIs, and\nimage processing modules. For each of these task queries, we provide\nautomatically generated plans using this realistic toolset. We further provide\na high-quality subset of 1,565 task plans that are human-verified and correctly\nexecutable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies\n(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3\ntypes of feedback (parsing/verification/execution). Finally, we summarize\ntakeaways from our extensive experiments. Our dataset and code are available on\nHuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github\n(https://github.com/RAIVNLab/mnms).\n","authors":["Zixian Ma","Weikai Huang","Jieyu Zhang","Tanmay Gupta","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.11085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14572v1","updated":"2024-03-21T17:20:21Z","published":"2024-03-21T17:20:21Z","title":"Implicit Style-Content Separation using B-LoRA","summary":"  Image stylization involves manipulating the visual appearance and texture\n(style) of an image while preserving its underlying objects, structures, and\nconcepts (content). The separation of style and content is essential for\nmanipulating the image's style independently from its content, ensuring a\nharmonious and visually pleasing result. Achieving this separation requires a\ndeep understanding of both the visual and semantic characteristics of images,\noften necessitating the training of specialized models or employing heavy\noptimization. In this paper, we introduce B-LoRA, a method that leverages LoRA\n(Low-Rank Adaptation) to implicitly separate the style and content components\nof a single image, facilitating various image stylization tasks. By analyzing\nthe architecture of SDXL combined with LoRA, we find that jointly learning the\nLoRA weights of two specific blocks (referred to as B-LoRAs) achieves\nstyle-content separation that cannot be achieved by training each B-LoRA\nindependently. Consolidating the training into only two blocks and separating\nstyle and content allows for significantly improving style manipulation and\novercoming overfitting issues often associated with model fine-tuning. Once\ntrained, the two B-LoRAs can be used as independent components to allow various\nimage stylization tasks, including image style transfer, text-based image\nstylization, consistent style generation, and style-content mixing.\n","authors":["Yarden Frenkel","Yael Vinker","Ariel Shamir","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2403.14572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13444v2","updated":"2024-03-21T17:19:25Z","published":"2024-03-20T09:40:11Z","title":"MedCycle: Unpaired Medical Report Generation via Cycle-Consistency","summary":"  Generating medical reports for X-ray images presents a significant challenge,\nparticularly in unpaired scenarios where access to paired image-report data for\ntraining is unavailable. Previous works have typically learned a joint\nembedding space for images and reports, necessitating a specific labeling\nschema for both. We introduce an innovative approach that eliminates the need\nfor consistent labeling schemas, thereby enhancing data accessibility and\nenabling the use of incompatible datasets. This approach is based on\ncycle-consistent mapping functions that transform image embeddings into report\nembeddings, coupled with report auto-encoding for medical report generation.\nOur model and objectives consider intricate local details and the overarching\nsemantic context within images and reports. This approach facilitates the\nlearning of effective mapping functions, resulting in the generation of\ncoherent reports. It outperforms state-of-the-art results in unpaired chest\nX-ray report generation, demonstrating improvements in both language and\nclinical metrics.\n","authors":["Elad Hirsch","Gefen Dawidowicz","Ayellet Tal"],"pdf_url":"https://arxiv.org/pdf/2403.13444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06860v2","updated":"2024-03-21T17:06:49Z","published":"2024-03-11T16:13:58Z","title":"A Geospatial Approach to Predicting Desert Locust Breeding Grounds in\n  Africa","summary":"  Desert locust swarms present a major threat to agriculture and food security.\nAddressing this challenge, our study develops an operationally-ready model for\npredicting locust breeding grounds, which has the potential to enhance early\nwarning systems and targeted control measures. We curated a dataset from the\nUnited Nations Food and Agriculture Organization's (UN-FAO) locust observation\nrecords and analyzed it using two types of spatio-temporal input features:\nremotely-sensed environmental and climate data as well as multi-spectral earth\nobservation images. Our approach employed custom deep learning models\n(three-dimensional and LSTM-based recurrent convolutional networks), along with\nthe geospatial foundational model Prithvi recently released by Jakubik et al.,\n2023. These models notably outperformed existing baselines, with the\nPrithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized\nLandsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and\nROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding\nfrom our research is that multi-spectral earth observation images alone are\nsufficient for effective locust breeding ground prediction without the need to\nexplicitly incorporate climatic or environmental features.\n","authors":["Ibrahim Salihu Yusuf","Mukhtar Opeyemi Yusuf","Kobby Panford-Quainoo","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2403.06860v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14559v1","updated":"2024-03-21T16:59:45Z","published":"2024-03-21T16:59:45Z","title":"Visibility-Aware Keypoint Localization for 6DoF Object Pose Estimation","summary":"  Localizing predefined 3D keypoints in a 2D image is an effective way to\nestablish 3D-2D correspondences for 6DoF object pose estimation. However,\nunreliable localization results of invisible keypoints degrade the quality of\ncorrespondences. In this paper, we address this issue by localizing the\nimportant keypoints in terms of visibility. Since keypoint visibility\ninformation is currently missing in dataset collection process, we propose an\nefficient way to generate binary visibility labels from available object-level\nannotations, for keypoints of both asymmetric objects and symmetric objects. We\nfurther derive real-valued visibility-aware importance from binary labels based\non PageRank algorithm. Taking advantage of the flexibility of our\nvisibility-aware importance, we construct VAPO (Visibility-Aware POse\nestimator) by integrating the visibility-aware importance with a\nstate-of-the-art pose estimation algorithm, along with additional positional\nencoding. Extensive experiments are conducted on popular pose estimation\nbenchmarks including Linemod, Linemod-Occlusion, and YCB-V. The results show\nthat, VAPO improves both the keypoint correspondences and final estimated\nposes, and clearly achieves state-of-the-art performances.\n","authors":["Ruyi Lian","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2403.14559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16274v2","updated":"2024-03-21T16:58:06Z","published":"2023-12-26T15:00:35Z","title":"Towards Flexible, Scalable, and Adaptive Multi-Modal Conditioned Face\n  Synthesis","summary":"  Recent progress in multi-modal conditioned face synthesis has enabled the\ncreation of visually striking and accurately aligned facial images. Yet,\ncurrent methods still face issues with scalability, limited flexibility, and a\none-size-fits-all approach to control strength, not accounting for the\ndiffering levels of conditional entropy, a measure of unpredictability in data\ngiven some condition, across modalities. To address these challenges, we\nintroduce a novel uni-modal training approach with modal surrogates, coupled\nwith an entropy-aware modal-adaptive modulation, to support flexible, scalable,\nand scalable multi-modal conditioned face synthesis network. Our uni-modal\ntraining with modal surrogate that only leverage uni-modal data, use modal\nsurrogate to decorate condition with modal-specific characteristic and serve as\nlinker for inter-modal collaboration , fully learns each modality control in\nface synthesis process as well as inter-modal collaboration. The entropy-aware\nmodal-adaptive modulation finely adjust diffusion noise according to\nmodal-specific characteristics and given conditions, enabling well-informed\nstep along denoising trajectory and ultimately leading to synthesis results of\nhigh fidelity and quality. Our framework improves multi-modal face synthesis\nunder various conditions, surpassing current methods in image quality and\nfidelity, as demonstrated by our thorough experimental results.\n","authors":["Jingjing Ren","Cheng Xu","Haoyu Chen","Xinran Qin","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.16274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14554v1","updated":"2024-03-21T16:53:03Z","published":"2024-03-21T16:53:03Z","title":"Gaussian Frosting: Editable Complex Radiance Fields with Real-Time\n  Rendering","summary":"  We propose Gaussian Frosting, a novel mesh-based representation for\nhigh-quality rendering and editing of complex 3D effects in real-time. Our\napproach builds on the recent 3D Gaussian Splatting framework, which optimizes\na set of 3D Gaussians to approximate a radiance field from images. We propose\nfirst extracting a base mesh from Gaussians during optimization, then building\nand refining an adaptive layer of Gaussians with a variable thickness around\nthe mesh to better capture the fine details and volumetric effects near the\nsurface, such as hair or grass. We call this layer Gaussian Frosting, as it\nresembles a coating of frosting on a cake. The fuzzier the material, the\nthicker the frosting. We also introduce a parameterization of the Gaussians to\nenforce them to stay inside the frosting layer and automatically adjust their\nparameters when deforming, rescaling, editing or animating the mesh. Our\nrepresentation allows for efficient rendering using Gaussian splatting, as well\nas editing and animation by modifying the base mesh. We demonstrate the\neffectiveness of our method on various synthetic and real scenes, and show that\nit outperforms existing surface-based approaches. We will release our code and\na web-based viewer as additional contributions. Our project page is the\nfollowing: https://anttwo.github.io/frosting/\n","authors":["Antoine Guédon","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2403.14554v1.pdf","comment":"Project Webpage: https://anttwo.github.io/frosting/"},{"id":"http://arxiv.org/abs/2403.14552v1","updated":"2024-03-21T16:52:27Z","published":"2024-03-21T16:52:27Z","title":"Token Transformation Matters: Towards Faithful Post-hoc Explanation for\n  Vision Transformer","summary":"  While Transformers have rapidly gained popularity in various computer vision\napplications, post-hoc explanations of their internal mechanisms remain largely\nunexplored. Vision Transformers extract visual information by representing\nimage regions as transformed tokens and integrating them via attention weights.\nHowever, existing post-hoc explanation methods merely consider these attention\nweights, neglecting crucial information from the transformed tokens, which\nfails to accurately illustrate the rationales behind the models' predictions.\nTo incorporate the influence of token transformation into interpretation, we\npropose TokenTM, a novel post-hoc explanation method that utilizes our\nintroduced measurement of token transformation effects. Specifically, we\nquantify token transformation effects by measuring changes in token lengths and\ncorrelations in their directions pre- and post-transformation. Moreover, we\ndevelop initialization and aggregation rules to integrate both attention\nweights and token transformation effects across all layers, capturing holistic\ntoken contributions throughout the model. Experimental results on segmentation\nand perturbation tests demonstrate the superiority of our proposed TokenTM\ncompared to state-of-the-art Vision Transformer explanation methods.\n","authors":["Junyi Wu","Bin Duan","Weitai Kang","Hao Tang","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2403.14552v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.03849v2","updated":"2024-03-21T16:49:20Z","published":"2024-03-06T16:49:33Z","title":"MedMamba: Vision Mamba for Medical Image Classification","summary":"  Medical image classification is a very fundamental and crucial task in the\nfield of computer vision. These years, CNN-based and Transformer-based models\nhave been widely used to classify various medical images. Unfortunately, The\nlimitation of CNNs in long-range modeling capabilities prevents them from\neffectively extracting features in medical images, while Transformers are\nhampered by their quadratic computational complexity. Recent research has shown\nthat the state space model (SSM) represented by Mamba can efficiently model\nlong-range interactions while maintaining linear computational complexity.\nInspired by this, we propose Vision Mamba for medical image classification\n(MedMamba). More specifically, we introduce a novel Conv-SSM module. Conv-SSM\ncombines the local feature extraction ability of convolutional layers with the\nability of SSM to capture long-range dependency, thereby modeling medical\nimages with different modalities. To demonstrate the potential of MedMamba, we\nconducted extensive experiments using 14 publicly available medical datasets\nwith different imaging techniques and two private datasets built by ourselves.\nExtensive experimental results demonstrate that the proposed MedMamba performs\nwell in detecting lesions in various medical images. To the best of our\nknowledge, this is the first Vision Mamba tailored for medical image\nclassification. The purpose of this work is to establish a new baseline for\nmedical image classification tasks and provide valuable insights for the future\ndevelopment of more efficient and effective SSM-based artificial intelligence\nalgorithms and application systems in the medical. Source code has been\navailable at https://github.com/YubiaoYue/MedMamba.\n","authors":["Yubiao Yue","Zhenzhang Li"],"pdf_url":"https://arxiv.org/pdf/2403.03849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14548v1","updated":"2024-03-21T16:49:20Z","published":"2024-03-21T16:49:20Z","title":"DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single\n  Video","summary":"  We present DINO-Tracker -- a new framework for long-term dense tracking in\nvideo. The pillar of our approach is combining test-time training on a single\nvideo, with the powerful localized semantic features learned by a pre-trained\nDINO-ViT model. Specifically, our framework simultaneously adopts DINO's\nfeatures to fit to the motion observations of the test video, while training a\ntracker that directly leverages the refined features. The entire framework is\ntrained end-to-end using a combination of self-supervised losses, and\nregularization that allows us to retain and benefit from DINO's semantic prior.\nExtensive evaluation demonstrates that our method achieves state-of-the-art\nresults on known benchmarks. DINO-tracker significantly outperforms\nself-supervised methods and is competitive with state-of-the-art supervised\ntrackers, while outperforming them in challenging cases of tracking under\nlong-term occlusions.\n","authors":["Narek Tumanyan","Assaf Singer","Shai Bagon","Tali Dekel"],"pdf_url":"https://arxiv.org/pdf/2403.14548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14547v1","updated":"2024-03-21T16:48:45Z","published":"2024-03-21T16:48:45Z","title":"Estimating Physical Information Consistency of Channel Data Augmentation\n  for Remote Sensing Images","summary":"  The application of data augmentation for deep learning (DL) methods plays an\nimportant role in achieving state-of-the-art results in supervised,\nsemi-supervised, and self-supervised image classification. In particular,\nchannel transformations (e.g., solarize, grayscale, brightness adjustments) are\nintegrated into data augmentation pipelines for remote sensing (RS) image\nclassification tasks. However, contradicting beliefs exist about their proper\napplications to RS images. A common point of critique is that the application\nof channel augmentation techniques may lead to physically inconsistent spectral\ndata (i.e., pixel signatures). To shed light on the open debate, we propose an\napproach to estimate whether a channel augmentation technique affects the\nphysical information of RS images. To this end, the proposed approach estimates\na score that measures the alignment of a pixel signature within a time series\nthat can be naturally subject to deviations caused by factors such as\nacquisition conditions or phenological states of vegetation. We compare the\nscores associated with original and augmented pixel signatures to evaluate the\nphysical consistency. Experimental results on a multi-label image\nclassification task show that channel augmentations yielding a score that\nexceeds the expected deviation of original pixel signatures can not improve the\nperformance of a baseline model trained without augmentation.\n","authors":["Tom Burgert","Begüm Demir"],"pdf_url":"https://arxiv.org/pdf/2403.14547v1.pdf","comment":"Accepted at the IEEE International Geoscience and Remote Sensing\n  Symposium"},{"id":"http://arxiv.org/abs/2402.17587v2","updated":"2024-03-21T16:40:43Z","published":"2024-02-25T07:59:10Z","title":"Instance-aware Exploration-Verification-Exploitation for Instance\n  ImageGoal Navigation","summary":"  As a new embodied vision task, Instance ImageGoal Navigation (IIN) aims to\nnavigate to a specified object depicted by a goal image in an unexplored\nenvironment.\n  The main challenge of this task lies in identifying the target object from\ndifferent viewpoints while rejecting similar distractors.\n  Existing ImageGoal Navigation methods usually adopt the simple\nExploration-Exploitation framework and ignore the identification of specific\ninstance during navigation.\n  In this work, we propose to imitate the human behaviour of ``getting closer\nto confirm\" when distinguishing objects from a distance.\n  Specifically, we design a new modular navigation framework named\nInstance-aware Exploration-Verification-Exploitation (IEVE) for instance-level\nimage goal navigation.\n  Our method allows for active switching among the exploration, verification,\nand exploitation actions, thereby facilitating the agent in making reasonable\ndecisions under different situations.\n  On the challenging HabitatMatterport 3D semantic (HM3D-SEM) dataset, our\nmethod surpasses previous state-of-the-art work, with a classical segmentation\nmodel (0.684 vs. 0.561 success) or a robust model (0.702 vs. 0.561 success).\nOur code will be made publicly available at https://github.com/XiaohanLei/IEVE.\n","authors":["Xiaohan Lei","Min Wang","Wengang Zhou","Li Li","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2402.17587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14539v1","updated":"2024-03-21T16:40:10Z","published":"2024-03-21T16:40:10Z","title":"Object-Centric Domain Randomization for 3D Shape Reconstruction in the\n  Wild","summary":"  One of the biggest challenges in single-view 3D shape reconstruction in the\nwild is the scarcity of <3D shape, 2D image>-paired data from real-world\nenvironments. Inspired by remarkable achievements via domain randomization, we\npropose ObjectDR which synthesizes such paired data via a random simulation of\nvisual variations in object appearances and backgrounds. Our data synthesis\nframework exploits a conditional generative model (e.g., ControlNet) to\ngenerate images conforming to spatial conditions such as 2.5D sketches, which\nare obtainable through a rendering process of 3D shapes from object collections\n(e.g., Objaverse-XL). To simulate diverse variations while preserving object\nsilhouettes embedded in spatial conditions, we also introduce a disentangled\nframework which leverages an initial object guidance. After synthesizing a wide\nrange of data, we pre-train a model on them so that it learns to capture a\ndomain-invariant geometry prior which is consistent across various domains. We\nvalidate its effectiveness by substantially improving 3D shape reconstruction\nmodels on a real-world benchmark. In a scale-up evaluation, our pre-training\nachieves 23.6% superior results compared with the pre-training on high-quality\ncomputer graphics renderings.\n","authors":["Junhyeong Cho","Kim Youwang","Hunmin Yang","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2403.14539v1.pdf","comment":"Project Page: https://ObjectDR.github.io"},{"id":"http://arxiv.org/abs/2403.12167v2","updated":"2024-03-21T16:38:33Z","published":"2024-03-18T18:35:32Z","title":"Generalizing deep learning models for medical image classification","summary":"  Numerous Deep Learning (DL) models have been developed for a large spectrum\nof medical image analysis applications, which promises to reshape various\nfacets of medical practice. Despite early advances in DL model validation and\nimplementation, which encourage healthcare institutions to adopt them, some\nfundamental questions remain: are the DL models capable of generalizing? What\ncauses a drop in DL model performances? How to overcome the DL model\nperformance drop? Medical data are dynamic and prone to domain shift, due to\nmultiple factors such as updates to medical equipment, new imaging workflow,\nand shifts in patient demographics or populations can induce this drift over\ntime. In this paper, we review recent developments in generalization methods\nfor DL-based classification models. We also discuss future challenges,\nincluding the need for improved evaluation protocols and benchmarks, and\nenvisioned future developments to achieve robust, generalized models for\nmedical image classification.\n","authors":["Matta Sarah","Lamard Mathieu","Zhang Philippe","Alexandre Le Guilcher","Laurent Borderie","Béatrice Cochener","Gwenolé Quellec"],"pdf_url":"https://arxiv.org/pdf/2403.12167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14534v1","updated":"2024-03-21T16:36:40Z","published":"2024-03-21T16:36:40Z","title":"Transfer Learning for Cross-dataset Isolated Sign Language Recognition\n  in Under-Resourced Datasets","summary":"  Sign language recognition (SLR) has recently achieved a breakthrough in\nperformance thanks to deep neural networks trained on large annotated sign\ndatasets. Of the many different sign languages, these annotated datasets are\nonly available for a select few. Since acquiring gloss-level labels on sign\nlanguage videos is difficult, learning by transferring knowledge from existing\nannotated sources is useful for recognition in under-resourced sign languages.\nThis study provides a publicly available cross-dataset transfer learning\nbenchmark from two existing public Turkish SLR datasets. We use a temporal\ngraph convolution-based sign language recognition approach to evaluate five\nsupervised transfer learning approaches and experiment with closed-set and\npartial-set cross-dataset transfer learning. Experiments demonstrate that\nimprovement over finetuning based transfer learning is possible with\nspecialized supervised transfer learning methods.\n","authors":["Ahmet Alp Kindiroglu","Ozgur Kara","Ogulcan Ozdemir","Lale Akarun"],"pdf_url":"https://arxiv.org/pdf/2403.14534v1.pdf","comment":"Accepted to The 18th IEEE International Conference on Automatic Face\n  and Gesture Recognition 2024, Code available in\n  https://github.com/alpk/tid-supervised-transfer-learning-dataset"},{"id":"http://arxiv.org/abs/2403.14530v1","updated":"2024-03-21T16:28:58Z","published":"2024-03-21T16:28:58Z","title":"HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression","summary":"  3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel\nview synthesis, boasting rapid rendering speed with high fidelity. However, the\nsubstantial Gaussians and their associated attributes necessitate effective\ncompression techniques. Nevertheless, the sparse and unorganized nature of the\npoint cloud of Gaussians (or anchors in our paper) presents challenges for\ncompression. To address this, we make use of the relations between the\nunorganized anchors and the structured hash grid, leveraging their mutual\ninformation for context modeling, and propose a Hash-grid Assisted Context\n(HAC) framework for highly compact 3DGS representation. Our approach introduces\na binary hash grid to establish continuous spatial consistencies, allowing us\nto unveil the inherent spatial relations of anchors through a carefully\ndesigned context model. To facilitate entropy coding, we utilize Gaussian\ndistributions to accurately estimate the probability of each quantized\nattribute, where an adaptive quantization module is proposed to enable\nhigh-precision quantization of these attributes for improved fidelity\nrestoration. Additionally, we incorporate an adaptive masking strategy to\neliminate invalid Gaussians and anchors. Importantly, our work is the pioneer\nto explore context-based compression for 3DGS representation, resulting in a\nremarkable size reduction of over $75\\times$ compared to vanilla 3DGS, while\nsimultaneously improving fidelity, and achieving over $11\\times$ size reduction\nover SOTA 3DGS compression approach Scaffold-GS. Our code is available here:\nhttps://github.com/YihangChen-ee/HAC\n","authors":["Yihang Chen","Qianyi Wu","Jianfei Cai","Mehrtash Harandi","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2403.14530v1.pdf","comment":"Project Page: https://yihangchen-ee.github.io/project_hac/ Code:\n  https://github.com/YihangChen-ee/HAC"},{"id":"http://arxiv.org/abs/2403.12966v2","updated":"2024-03-21T16:26:44Z","published":"2024-03-19T17:59:52Z","title":"Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language\n  Models","summary":"  In the realm of vision-language understanding, the proficiency of models in\ninterpreting and reasoning over visual content has become a cornerstone for\nnumerous applications. However, it is challenging for the visual encoder in\nLarge Vision-Language Models (LVLMs) to extract useful features tailored to\nquestions that aid the language model's response. Furthermore, a common\npractice among existing LVLMs is to utilize lower-resolution images, which\nrestricts the ability for visual recognition. Our work introduces the\nChain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel\napproach that enhances feature extraction by focusing on key regions of\ninterest (ROI) within the image, corresponding to the posed questions or\ninstructions. This technique allows LVLMs to access more detailed visual\ninformation without altering the original image resolution, thereby offering\nmulti-granularity image features. By integrating Chain-of-Spot with\ninstruct-following LLaVA-1.5 models, the process of image reasoning\nconsistently improves performance across a wide range of multimodal datasets\nand benchmarks without bells and whistles and achieves new state-of-the-art\nresults. Our empirical findings demonstrate a significant improvement in LVLMs'\nability to understand and reason about visual content, paving the way for more\nsophisticated visual instruction-following applications. Code and models are\navailable at https://github.com/dongyh20/Chain-of-Spot\n","authors":["Zuyan Liu","Yuhao Dong","Yongming Rao","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2403.12966v2.pdf","comment":"Project Page: https://sites.google.com/view/chain-of-spot/"},{"id":"http://arxiv.org/abs/2403.14526v1","updated":"2024-03-21T16:26:19Z","published":"2024-03-21T16:26:19Z","title":"Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion\n  Descriptors","summary":"  Precise manipulation that is generalizable across scenes and objects remains\na persistent challenge in robotics. Current approaches for this task heavily\ndepend on having a significant number of training instances to handle objects\nwith pronounced visual and/or geometric part ambiguities. Our work explores the\ngrounding of fine-grained part descriptors for precise manipulation in a\nzero-shot setting by utilizing web-trained text-to-image diffusion-based\ngenerative models. We tackle the problem by framing it as a dense semantic part\ncorrespondence task. Our model returns a gripper pose for manipulating a\nspecific part, using as reference a user-defined click from a source image of a\nvisually different instance of the same object. We require no manual grasping\ndemonstrations as we leverage the intrinsic object geometry and features.\nPractical experiments in a real-world tabletop scenario validate the efficacy\nof our approach, demonstrating its potential for advancing semantic-aware\nrobotics manipulation. Web page: https://tsagkas.github.io/click2grasp\n","authors":["Nikolaos Tsagkas","Jack Rome","Subramanian Ramamoorthy","Oisin Mac Aodha","Chris Xiaoxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2403.14526v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.14523v1","updated":"2024-03-21T16:23:25Z","published":"2024-03-21T16:23:25Z","title":"Invisible Needle Detection in Ultrasound: Leveraging Mechanism-Induced\n  Vibration","summary":"  In clinical applications that involve ultrasound-guided intervention, the\nvisibility of the needle can be severely impeded due to steep insertion and\nstrong distractors such as speckle noise and anatomical occlusion. To address\nthis challenge, we propose VibNet, a learning-based framework tailored to\nenhance the robustness and accuracy of needle detection in ultrasound images,\neven when the target becomes invisible to the naked eye. Inspired by Eulerian\nVideo Magnification techniques, we utilize an external step motor to induce\nlow-amplitude periodic motion on the needle. These subtle vibrations offer the\npotential to generate robust frequency features for detecting the motion\npatterns around the needle. To robustly and precisely detect the needle\nleveraging these vibrations, VibNet integrates learning-based\nShort-Time-Fourier-Transform and Hough-Transform modules to achieve successive\nsub-goals, including motion feature extraction in the spatiotemporal space,\nfrequency feature aggregation, and needle detection in the Hough space. Based\non the results obtained on distinct ex vivo porcine and bovine tissue samples,\nthe proposed algorithm exhibits superior detection performance with efficient\ncomputation and generalization capability.\n","authors":["Chenyang Li","Dianye Huang","Angelos Karlas","Nassir Navab","Zhongliang Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14520v1","updated":"2024-03-21T16:17:57Z","published":"2024-03-21T16:17:57Z","title":"Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient\n  Inference","summary":"  In recent years, the application of multimodal large language models (MLLM)\nin various fields has achieved remarkable success. However, as the foundation\nmodel for many downstream tasks, current MLLMs are composed of the well-known\nTransformer network, which has a less efficient quadratic computation\ncomplexity. To improve the efficiency of such basic models, we propose Cobra, a\nlinear computational complexity MLLM. Specifically, Cobra integrates the\nefficient Mamba language model into the visual modality. Moreover, we explore\nand study various modal fusion schemes to create an effective multi-modal\nMamba. Extensive experiments demonstrate that (1) Cobra achieves extremely\ncompetitive performance with current computationally efficient state-of-the-art\nmethods, \\textit{e.g.}, LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster\nspeed due to Cobra's linear sequential modeling. (2) Interestingly, the results\nof closed-set challenging prediction benchmarks show that Cobra performs well\nin overcoming visual illusions and spatial relationship judgments. (3) Notably,\nCobra even achieves comparable performance to LLaVA with about 43% of the\nnumber of parameters. We will make all codes of Cobra open-source and hope that\nthe proposed method can facilitate future research on complexity problems in\nMLLM. Our project page is available at: https://sites.google.com/view/cobravlm.\n","authors":["Han Zhao","Min Zhang","Wei Zhao","Pengxiang Ding","Siteng Huang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17797v3","updated":"2024-03-21T16:11:23Z","published":"2024-02-26T22:00:59Z","title":"Neural Radiance Fields in Medical Imaging: Challenges and Next Steps","summary":"  Neural Radiance Fields (NeRF), as a pioneering technique in computer vision,\noffer great potential to revolutionize medical imaging by synthesizing\nthree-dimensional representations from the projected two-dimensional image\ndata. However, they face unique challenges when applied to medical\napplications. This paper presents a comprehensive examination of applications\nof NeRFs in medical imaging, highlighting four imminent challenges, including\nfundamental imaging principles, inner structure requirement, object boundary\ndefinition, and color density significance. We discuss current methods on\ndifferent organs and discuss related limitations. We also review several\ndatasets and evaluation metrics and propose several promising directions for\nfuture research.\n","authors":["Xin Wang","Shu Hu","Heng Fan","Hongtu Zhu","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2402.17797v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12157v2","updated":"2024-03-21T16:09:57Z","published":"2023-03-21T19:34:20Z","title":"Learning a Depth Covariance Function","summary":"  We propose learning a depth covariance function with applications to\ngeometric vision tasks. Given RGB images as input, the covariance function can\nbe flexibly used to define priors over depth functions, predictive\ndistributions given observations, and methods for active point selection. We\nleverage these techniques for a selection of downstream tasks: depth\ncompletion, bundle adjustment, and monocular dense visual odometry.\n","authors":["Eric Dexheimer","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2303.12157v2.pdf","comment":"CVPR 2023. Project page: https://edexheim.github.io/DepthCov/"},{"id":"http://arxiv.org/abs/2403.14513v1","updated":"2024-03-21T16:08:21Z","published":"2024-03-21T16:08:21Z","title":"View-decoupled Transformer for Person Re-identification under\n  Aerial-ground Camera Network","summary":"  Existing person re-identification methods have achieved remarkable advances\nin appearance-based identity association across homogeneous cameras, such as\nground-ground matching. However, as a more practical scenario, aerial-ground\nperson re-identification (AGPReID) among heterogeneous cameras has received\nminimal attention. To alleviate the disruption of discriminative identity\nrepresentation by dramatic view discrepancy as the most significant challenge\nin AGPReID, the view-decoupled transformer (VDT) is proposed as a simple yet\neffective framework. Two major components are designed in VDT to decouple\nview-related and view-unrelated features, namely hierarchical subtractive\nseparation and orthogonal loss, where the former separates these two features\ninside the VDT, and the latter constrains these two to be independent. In\naddition, we contribute a large-scale AGPReID dataset called CARGO, consisting\nof five/eight aerial/ground cameras, 5,000 identities, and 108,563 images.\nExperiments on two datasets show that VDT is a feasible and effective solution\nfor AGPReID, surpassing the previous method on mAP/Rank1 by up to 5.0%/2.7% on\nCARGO and 3.7%/5.2% on AG-ReID, keeping the same magnitude of computational\ncomplexity. Our project is available at https://github.com/LinlyAC/VDT-AGPReID\n","authors":["Quan Zhang","Lei Wang","Vishal M. Patel","Xiaohua Xie","Jianhuang Lai"],"pdf_url":"https://arxiv.org/pdf/2403.14513v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2312.10217v2","updated":"2024-03-21T16:06:17Z","published":"2023-12-15T21:30:49Z","title":"T-MAE: Temporal Masked Autoencoders for Point Cloud Representation\n  Learning","summary":"  The scarcity of annotated data in LiDAR point cloud understanding hinders\neffective representation learning. Consequently, scholars have been actively\ninvestigating efficacious self-supervised pre-training paradigms. Nevertheless,\ntemporal information, which is inherent in the LiDAR point cloud sequence, is\nconsistently disregarded. To better utilize this property, we propose an\neffective pre-training strategy, namely Temporal Masked Auto-Encoders (T-MAE),\nwhich takes as input temporally adjacent frames and learns temporal dependency.\nA SiamWCA backbone, containing a Siamese encoder and a windowed cross-attention\n(WCA) module, is established for the two-frame input. Considering that the\nmovement of an ego-vehicle alters the view of the same instance, temporal\nmodeling also serves as a robust and natural data augmentation, enhancing the\ncomprehension of target objects. SiamWCA is a powerful architecture but heavily\nrelies on annotated data. Our T-MAE pre-training strategy alleviates its demand\nfor annotated data. Comprehensive experiments demonstrate that T-MAE achieves\nthe best performance on both Waymo and ONCE datasets among competitive\nself-supervised approaches.\n","authors":["Weijie Wei","Fatemeh Karimi Nejadasl","Theo Gevers","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2312.10217v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2312.09641v2","updated":"2024-03-21T15:57:28Z","published":"2023-12-15T09:30:47Z","title":"Ins-HOI: Instance Aware Human-Object Interactions Recovery","summary":"  Accurately modeling detailed interactions between human/hand and object is an\nappealing yet challenging task. Current multi-view capture systems are only\ncapable of reconstructing multiple subjects into a single, unified mesh, which\nfails to model the states of each instance individually during interactions. To\naddress this, previous methods use template-based representations to track\nhuman/hand and object. However, the quality of the reconstructions is limited\nby the descriptive capabilities of the templates so that these methods are\ninherently struggle with geometry details, pressing deformations and invisible\ncontact surfaces. In this work, we propose an end-to-end Instance-aware\nHuman-Object Interactions recovery (Ins-HOI) framework by introducing an\ninstance-level occupancy field representation. However, the real-captured data\nis presented as a holistic mesh, unable to provide instance-level supervision.\nTo address this, we further propose a complementary training strategy that\nleverages synthetic data to introduce instance-level shape priors, enabling the\ndisentanglement of occupancy fields for different instances. Specifically,\nsynthetic data, created by randomly combining individual scans of humans/hands\nand objects, guides the network to learn a coarse prior of instances.\nMeanwhile, real-captured data helps in learning the overall geometry and\nrestricting interpenetration in contact areas. As demonstrated in experiments,\nour method Ins-HOI supports instance-level reconstruction and provides\nreasonable and realistic invisible contact surfaces even in cases of extremely\nclose interaction. To facilitate the research of this task, we collect a\nlarge-scale, high-fidelity 3D scan dataset, including 5.2k high-quality scans\nwith real-world human-chair and hand-object interactions. The code and data\nwill be public for research purposes.\n","authors":["Jiajun Zhang","Yuxiang Zhang","Hongwen Zhang","Xiao Zhou","Boyao Zhou","Ruizhi Shao","Zonghai Hu","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2312.09641v2.pdf","comment":"Project Page: https://jiajunzhang16.github.io/ins-hoi/ , Code and\n  Dataset Page: https://github.com/jiajunzhang16/ins-hoi"},{"id":"http://arxiv.org/abs/2403.14499v1","updated":"2024-03-21T15:52:05Z","published":"2024-03-21T15:52:05Z","title":"Denoising Diffusion Models for 3D Healthy Brain Tissue Inpainting","summary":"  Monitoring diseases that affect the brain's structural integrity requires\nautomated analysis of magnetic resonance (MR) images, e.g., for the evaluation\nof volumetric changes. However, many of the evaluation tools are optimized for\nanalyzing healthy tissue. To enable the evaluation of scans containing\npathological tissue, it is therefore required to restore healthy tissue in the\npathological areas. In this work, we explore and extend denoising diffusion\nmodels for consistent inpainting of healthy 3D brain tissue. We modify\nstate-of-the-art 2D, pseudo-3D, and 3D methods working in the image space, as\nwell as 3D latent and 3D wavelet diffusion models, and train them to synthesize\nhealthy brain tissue. Our evaluation shows that the pseudo-3D model performs\nbest regarding the structural-similarity index, peak signal-to-noise ratio, and\nmean squared error. To emphasize the clinical relevance, we fine-tune this\nmodel on data containing synthetic MS lesions and evaluate it on a downstream\nbrain tissue segmentation task, whereby it outperforms the established FMRIB\nSoftware Library (FSL) lesion-filling method.\n","authors":["Alicia Durrer","Julia Wolleb","Florentin Bieder","Paul Friedrich","Lester Melie-Garcia","Mario Ocampo-Pineda","Cosmin I. Bercea","Ibrahim E. Hamamci","Benedikt Wiestler","Marie Piraud","Özgür Yaldizli","Cristina Granziera","Bjoern H. Menze","Philippe C. Cattin","Florian Kofler"],"pdf_url":"https://arxiv.org/pdf/2403.14499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14497v1","updated":"2024-03-21T15:46:19Z","published":"2024-03-21T15:46:19Z","title":"MULDE: Multiscale Log-Density Estimation via Denoising Score Matching\n  for Video Anomaly Detection","summary":"  We propose a novel approach to video anomaly detection: we treat feature\nvectors extracted from videos as realizations of a random variable with a fixed\ndistribution and model this distribution with a neural network. This lets us\nestimate the likelihood of test videos and detect video anomalies by\nthresholding the likelihood estimates. We train our video anomaly detector\nusing a modification of denoising score matching, a method that injects\ntraining data with noise to facilitate modeling its distribution. To eliminate\nhyperparameter selection, we model the distribution of noisy video features\nacross a range of noise levels and introduce a regularizer that tends to align\nthe models for different levels of noise. At test time, we combine anomaly\nindications at multiple noise scales with a Gaussian mixture model. Running our\nvideo anomaly detector induces minimal delays as inference requires merely\nextracting the features and forward-propagating them through a shallow neural\nnetwork and a Gaussian mixture model. Our experiments on five popular video\nanomaly detection benchmarks demonstrate state-of-the-art performance, both in\nthe object-centric and in the frame-centric setup.\n","authors":["Jakub Micorek","Horst Possegger","Dominik Narnhofer","Horst Bischof","Mateusz Kozinski"],"pdf_url":"https://arxiv.org/pdf/2403.14497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02116v3","updated":"2024-03-21T15:45:29Z","published":"2023-12-04T18:48:02Z","title":"GIVT: Generative Infinite-Vocabulary Transformers","summary":"  We introduce generative infinite-vocabulary transformers (GIVT) which\ngenerate vector sequences with real-valued entries, instead of discrete tokens\nfrom a finite vocabulary. To this end, we propose two surprisingly simple\nmodifications to decoder-only transformers: 1) at the input, we replace the\nfinite-vocabulary lookup table with a linear projection of the input vectors;\nand 2) at the output, we replace the logits prediction (usually mapped to a\ncategorical distribution) with the parameters of a multivariate Gaussian\nmixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT,\nwhere transformers are used to model the discrete latent sequences of a VQ-VAE,\nwe use GIVT to model the unquantized real-valued latent sequences of a\n$\\beta$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (and\nimproved variants thereof) as well as MaskGIT, and achieves performance\ncompetitive with recent latent diffusion models. Finally, we obtain strong\nresults outside of image generation when applying GIVT to panoptic segmentation\nand depth estimation with a VAE variant of the UViM framework\n","authors":["Michael Tschannen","Cian Eastwood","Fabian Mentzer"],"pdf_url":"https://arxiv.org/pdf/2312.02116v3.pdf","comment":"v2: add related NLP work, loss details. v3: Improved GMM formulation,\n  added adapter module, larger models, better image generation results. Code\n  and model checkpoints are available at:\n  https://github.com/google-research/big_vision"},{"id":"http://arxiv.org/abs/2403.14494v1","updated":"2024-03-21T15:42:17Z","published":"2024-03-21T15:42:17Z","title":"Learning to Project for Cross-Task Knowledge Distillation","summary":"  Traditional knowledge distillation (KD) relies on a proficient teacher\ntrained on the target task, which is not always available. In this setting,\ncross-task distillation can be used, enabling the use of any teacher model\ntrained on a different task. However, many KD methods prove ineffective when\napplied to this cross-task setting. To address this limitation, we propose a\nsimple modification: the use of an inverted projection. We show that this\ndrop-in replacement for a standard projector is effective by learning to\ndisregard any task-specific features which might degrade the student's\nperformance. We find that this simple modification is sufficient for extending\nmany KD methods to the cross-task setting, where the teacher and student tasks\ncan be very different. In doing so, we obtain up to a 1.9% improvement in the\ncross-task setting compared to the traditional projection, at no additional\ncost. Our method can obtain significant performance improvements (up to 7%)\nwhen using even a randomly-initialised teacher on various tasks such as depth\nestimation, image translation, and semantic segmentation, despite the lack of\nany learned knowledge to transfer. To provide conceptual and analytical\ninsights into this result, we show that using an inverted projection allows the\ndistillation loss to be decomposed into a knowledge transfer and a spectral\nregularisation component. Through this analysis we are additionally able to\npropose a novel regularisation loss that allows teacher-free distillation,\nenabling performance improvements of up to 8.57% on ImageNet with no additional\ntraining costs.\n","authors":["Dylan Auty","Roy Miles","Benedikt Kolbeinsson","Krystian Mikolajczyk"],"pdf_url":"https://arxiv.org/pdf/2403.14494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10132v2","updated":"2024-03-21T15:42:06Z","published":"2023-12-15T17:02:19Z","title":"Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against\n  Query-Based Attacks","summary":"  Although promising, existing defenses against query-based attacks share a\ncommon limitation: they offer increased robustness against attacks at the price\nof a considerable accuracy drop on clean samples. In this work, we show how to\nefficiently establish, at test-time, a solid tradeoff between robustness and\naccuracy when mitigating query-based attacks. Given that these attacks\nnecessarily explore low-confidence regions, our insight is that activating\ndedicated defenses, such as random noise defense and random image\ntransformations, only for low-confidence inputs is sufficient to prevent them.\nOur approach is independent of training and supported by theory. We verify the\neffectiveness of our approach for various existing defenses by conducting\nextensive experiments on CIFAR-10, CIFAR-100, and ImageNet. Our results confirm\nthat our proposal can indeed enhance these defenses by providing better\ntradeoffs between robustness and accuracy when compared to state-of-the-art\napproaches while being completely training-free.\n","authors":["Pascal Zimmer","Sébastien Andreina","Giorgia Azzurra Marson","Ghassan Karame"],"pdf_url":"https://arxiv.org/pdf/2312.10132v2.pdf","comment":"To appear in the Proceedings of the AAAI Conference on Artificial\n  Intelligence (AAAI) 2024"},{"id":"http://arxiv.org/abs/2403.13261v2","updated":"2024-03-21T15:40:16Z","published":"2024-03-20T02:58:45Z","title":"Self-Supervised Class-Agnostic Motion Prediction with Spatial and\n  Temporal Consistency Regularizations","summary":"  The perception of motion behavior in a dynamic environment holds significant\nimportance for autonomous driving systems, wherein class-agnostic motion\nprediction methods directly predict the motion of the entire point cloud. While\nmost existing methods rely on fully-supervised learning, the manual labeling of\npoint cloud data is laborious and time-consuming. Therefore, several\nannotation-efficient methods have been proposed to address this challenge.\nAlthough effective, these methods rely on weak annotations or additional\nmulti-modal data like images, and the potential benefits inherent in the point\ncloud sequence are still underexplored. To this end, we explore the feasibility\nof self-supervised motion prediction with only unlabeled LiDAR point clouds.\nInitially, we employ an optimal transport solver to establish coarse\ncorrespondences between current and future point clouds as the coarse pseudo\nmotion labels. Training models directly using such coarse labels leads to\nnoticeable spatial and temporal prediction inconsistencies. To mitigate these\nissues, we introduce three simple spatial and temporal regularization losses,\nwhich facilitate the self-supervised training process effectively. Experimental\nresults demonstrate the significant superiority of our approach over the\nstate-of-the-art self-supervised methods.\n","authors":["Kewei Wang","Yizheng Wu","Jun Cen","Zhiyu Pan","Xingyi Li","Zhe Wang","Zhiguo Cao","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2403.13261v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.14489v1","updated":"2024-03-21T15:37:37Z","published":"2024-03-21T15:37:37Z","title":"Adversary-Robust Graph-Based Learning of WSIs","summary":"  Enhancing the robustness of deep learning models against adversarial attacks\nis crucial, especially in critical domains like healthcare where significant\nfinancial interests heighten the risk of such attacks. Whole slide images\n(WSIs) are high-resolution, digitized versions of tissue samples mounted on\nglass slides, scanned using sophisticated imaging equipment. The digital\nanalysis of WSIs presents unique challenges due to their gigapixel size and\nmulti-resolution storage format. In this work, we aim at improving the\nrobustness of cancer Gleason grading classification systems against adversarial\nattacks, addressing challenges at both the image and graph levels. As regards\nthe proposed algorithm, we develop a novel and innovative graph-based model\nwhich utilizes GNN to extract features from the graph representation of WSIs. A\ndenoising module, along with a pooling layer is incorporated to manage the\nimpact of adversarial attacks on the WSIs. The process concludes with a\ntransformer module that classifies various grades of prostate cancer based on\nthe processed data. To assess the effectiveness of the proposed method, we\nconducted a comparative analysis using two scenarios. Initially, we trained and\ntested the model without the denoiser using WSIs that had not been exposed to\nany attack. We then introduced a range of attacks at either the image or graph\nlevel and processed them through the proposed network. The performance of the\nmodel was evaluated in terms of accuracy and kappa scores. The results from\nthis comparison showed a significant improvement in cancer diagnosis accuracy,\nhighlighting the robustness and efficiency of the proposed method in handling\nadversarial challenges in the context of medical imaging.\n","authors":["Saba Heidari Gheshlaghi","Milan Aryal","Nasim Yahyasoltani","Masoud Ganji"],"pdf_url":"https://arxiv.org/pdf/2403.14489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14487v1","updated":"2024-03-21T15:35:42Z","published":"2024-03-21T15:35:42Z","title":"DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified &\n  Accurate Image Editing","summary":"  Recently, how to achieve precise image editing has attracted increasing\nattention, especially given the remarkable success of text-to-image generation\nmodels. To unify various spatial-aware image editing abilities into one\nframework, we adopt the concept of layers from the design domain to manipulate\nobjects flexibly with various operations. The key insight is to transform the\nspatial-aware image editing task into a combination of two sub-tasks:\nmulti-layered latent decomposition and multi-layered latent fusion. First, we\nsegment the latent representations of the source images into multiple layers,\nwhich include several object layers and one incomplete background layer that\nnecessitates reliable inpainting. To avoid extra tuning, we further explore the\ninner inpainting ability within the self-attention mechanism. We introduce a\nkey-masking self-attention scheme that can propagate the surrounding context\ninformation into the masked region while mitigating its impact on the regions\noutside the mask. Second, we propose an instruction-guided latent fusion that\npastes the multi-layered latent representations onto a canvas latent. We also\nintroduce an artifact suppression scheme in the latent space to enhance the\ninpainting quality. Due to the inherent modular advantages of such\nmulti-layered representations, we can achieve accurate image editing, and we\ndemonstrate that our approach consistently surpasses the latest spatial editing\nmethods, including Self-Guidance and DiffEditor. Last, we show that our\napproach is a unified framework that supports various accurate image editing\ntasks on more than six different editing tasks.\n","authors":["Yueru Jia","Yuhui Yuan","Aosong Cheng","Chuke Wang","Ji Li","Huizhu Jia","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14487v1.pdf","comment":"technical report, 15 pages, webpage: https://design-edit.github.io/"},{"id":"http://arxiv.org/abs/2312.02015v2","updated":"2024-03-21T15:32:35Z","published":"2023-12-04T16:38:16Z","title":"ColonNeRF: High-Fidelity Neural Reconstruction of Long Colonoscopy","summary":"  Colonoscopy reconstruction is pivotal for diagnosing colorectal cancer.\nHowever, accurate long-sequence colonoscopy reconstruction faces three major\nchallenges: (1) dissimilarity among segments of the colon due to its meandering\nand convoluted shape; (2) co-existence of simple and intricately folded\ngeometry structures; (3) sparse viewpoints due to constrained camera\ntrajectories. To tackle these challenges, we introduce a new reconstruction\nframework based on neural radiance field (NeRF), named ColonNeRF, which\nleverages neural rendering for novel view synthesis of long-sequence\ncolonoscopy. Specifically, to reconstruct the entire colon in a piecewise\nmanner, our ColonNeRF introduces a region division and integration module,\neffectively reducing shape dissimilarity and ensuring geometric consistency in\neach segment. To learn both the simple and complex geometry in a unified\nframework, our ColonNeRF incorporates a multi-level fusion module that\nprogressively models the colon regions from easy to hard. Additionally, to\novercome the challenges from sparse views, we devise a DensiNet module for\ndensifying camera poses under the guidance of semantic consistency. We conduct\nextensive experiments on both synthetic and real-world datasets to evaluate our\nColonNeRF. Quantitatively, ColonNeRF exhibits a 67%-85% increase in LPIPS-ALEX\nscores. Qualitatively, our reconstruction visualizations show much clearer\ntextures and more accurate geometric details. These sufficiently demonstrate\nour superior performance over the state-of-the-art methods.\n","authors":["Yufei Shi","Beijia Lu","Jia-Wei Liu","Ming Li","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2312.02015v2.pdf","comment":"for Project Page, see https://showlab.github.io/ColonNeRF/"},{"id":"http://arxiv.org/abs/2403.14484v1","updated":"2024-03-21T15:31:28Z","published":"2024-03-21T15:31:28Z","title":"HyperGALE: ASD Classification via Hypergraph Gated Attention with\n  Learnable Hyperedges","summary":"  Autism Spectrum Disorder (ASD) is a neurodevelopmental condition\ncharacterized by varied social cognitive challenges and repetitive behavioral\npatterns. Identifying reliable brain imaging-based biomarkers for ASD has been\na persistent challenge due to the spectrum's diverse symptomatology. Existing\nbaselines in the field have made significant strides in this direction, yet\nthere remains room for improvement in both performance and interpretability. We\npropose \\emph{HyperGALE}, which builds upon the hypergraph by incorporating\nlearned hyperedges and gated attention mechanisms. This approach has led to\nsubstantial improvements in the model's ability to interpret complex brain\ngraph data, offering deeper insights into ASD biomarker characterization.\nEvaluated on the extensive ABIDE II dataset, \\emph{HyperGALE} not only improves\ninterpretability but also demonstrates statistically significant enhancements\nin key performance metrics compared to both previous baselines and the\nfoundational hypergraph model. The advancement \\emph{HyperGALE} brings to ASD\nresearch highlights the potential of sophisticated graph-based techniques in\nneurodevelopmental studies. The source code and implementation instructions are\navailable at GitHub:https://github.com/mehular0ra/HyperGALE.\n","authors":["Mehul Arora","Chirag Shantilal Jain","Lalith Bharadwaj Baru","Kamalaker Dadi","Bapi Raju Surampudi"],"pdf_url":"https://arxiv.org/pdf/2403.14484v1.pdf","comment":"Accepted to IJCNN 2024"},{"id":"http://arxiv.org/abs/2403.14472v1","updated":"2024-03-21T15:18:30Z","published":"2024-03-21T15:18:30Z","title":"Detoxifying Large Language Models via Knowledge Editing","summary":"  This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments to compare knowledge\nediting approaches with previous baselines, indicating that knowledge editing\nhas the potential to efficiently detoxify LLMs with limited impact on general\nperformance. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxify\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.\n","authors":["Mengru Wang","Ningyu Zhang","Ziwen Xu","Zekun Xi","Shumin Deng","Yunzhi Yao","Qishen Zhang","Linyi Yang","Jindong Wang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14472v1.pdf","comment":"Ongoing work. Project website:\n  https://zjunlp.github.io/project/SafeEdit Benchmark:\n  https://huggingface.co/datasets/zjunlp/SafeEdit Code:\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2309.15627v2","updated":"2024-03-21T15:17:10Z","published":"2023-09-27T12:58:18Z","title":"Neuromorphic Imaging and Classification with Graph Learning","summary":"  Bio-inspired neuromorphic cameras asynchronously record pixel brightness\nchanges and generate sparse event streams. They can capture dynamic scenes with\nlittle motion blur and more details in extreme illumination conditions. Due to\nthe multidimensional address-event structure, most existing vision algorithms\ncannot properly handle asynchronous event streams. While several event\nrepresentations and processing methods have been developed to address such an\nissue, they are typically driven by a large number of events, leading to\nsubstantial overheads in runtime and memory. In this paper, we propose a new\ngraph representation of the event data and couple it with a Graph Transformer\nto perform accurate neuromorphic classification. Extensive experiments show\nthat our approach leads to better results and excels at the challenging\nrealistic situations where only a small number of events and limited\ncomputational resources are available, paving the way for neuromorphic\napplications embedded into mobile facilities.\n","authors":["Pei Zhang","Chutian Wang","Edmund Y. Lam"],"pdf_url":"https://arxiv.org/pdf/2309.15627v2.pdf","comment":"15 pages, 4 figures, and 7 tables. Accepted by Elsevier\n  Neurocomputing"},{"id":"http://arxiv.org/abs/2403.08262v2","updated":"2024-03-21T15:15:28Z","published":"2024-03-13T05:25:49Z","title":"BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands\n  from a Single Image","summary":"  Creating personalized hand avatars is important to offer a realistic\nexperience to users on AR / VR platforms. While most prior studies focused on\nreconstructing 3D hand shapes, some recent work has tackled the reconstruction\nof hand textures on top of shapes. However, these methods are often limited to\ncapturing pixels on the visible side of a hand, requiring diverse views of the\nhand in a video or multiple images as input. In this paper, we propose a novel\nmethod, BiTT(Bi-directional Texture reconstruction of Two hands), which is the\nfirst end-to-end trainable method for relightable, pose-free texture\nreconstruction of two interacting hands taking only a single RGB image, by\nthree novel components: 1) bi-directional (left $\\leftrightarrow$ right)\ntexture reconstruction using the texture symmetry of left / right hands, 2)\nutilizing a texture parametric model for hand texture recovery, and 3) the\noverall coarse-to-fine stage pipeline for reconstructing personalized texture\nof two interacting hands. BiTT first estimates the scene light condition and\nalbedo image from an input image, then reconstructs the texture of both hands\nthrough the texture parametric model and bi-directional texture reconstructor.\nIn experiments using InterHand2.6M and RGB2Hands datasets, our method\nsignificantly outperforms state-of-the-art hand texture reconstruction methods\nquantitatively and qualitatively. The code is available at\nhttps://github.com/yunminjin2/BiTT\n","authors":["Minje Kim","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2403.08262v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14468v1","updated":"2024-03-21T15:15:00Z","published":"2024-03-21T15:15:00Z","title":"AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks","summary":"  Video-to-video editing involves editing a source video along with additional\ncontrol (such as text prompts, subjects, or styles) to generate a new video\nthat aligns with the source video and the provided control. Traditional methods\nhave been constrained to certain editing types, limiting their ability to meet\nthe wide range of user demands. In this paper, we introduce AnyV2V, a novel\ntraining-free framework designed to simplify video editing into two primary\nsteps: (1) employing an off-the-shelf image editing model (e.g.\nInstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an\nexisting image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion\nand feature injection. In the first stage, AnyV2V can plug in any existing\nimage editing tools to support an extensive array of video editing tasks.\nBeyond the traditional prompt-based editing methods, AnyV2V also can support\nnovel video editing tasks, including reference-based style transfer,\nsubject-driven editing, and identity manipulation, which were unattainable by\nprevious methods. In the second stage, AnyV2V can plug in any existing\nimage-to-video models to perform DDIM inversion and intermediate feature\ninjection to maintain the appearance and motion consistency with the source\nvideo. On the prompt-based editing, we show that AnyV2V can outperform the\nprevious best approach by 35\\% on prompt alignment, and 25\\% on human\npreference. On the three novel tasks, we show that AnyV2V also achieves a high\nsuccess rate. We believe AnyV2V will continue to thrive due to its ability to\nseamlessly integrate the fast-evolving image editing methods. Such\ncompatibility can help AnyV2V to increase its versatility to cater to diverse\nuser demands.\n","authors":["Max Ku","Cong Wei","Weiming Ren","Huan Yang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14468v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.14465v1","updated":"2024-03-21T15:13:36Z","published":"2024-03-21T15:13:36Z","title":"CathFlow: Self-Supervised Segmentation of Catheters in Interventional\n  Ultrasound Using Optical Flow and Transformers","summary":"  In minimally invasive endovascular procedures, contrast-enhanced angiography\nremains the most robust imaging technique. However, it is at the expense of the\npatient and clinician's health due to prolonged radiation exposure. As an\nalternative, interventional ultrasound has notable benefits such as being\nradiation-free, fast to deploy, and having a small footprint in the operating\nroom. Yet, ultrasound is hard to interpret, and highly prone to artifacts and\nnoise. Additionally, interventional radiologists must undergo extensive\ntraining before they become qualified to diagnose and treat patients\neffectively, leading to a shortage of staff, and a lack of open-source\ndatasets. In this work, we seek to address both problems by introducing a\nself-supervised deep learning architecture to segment catheters in longitudinal\nultrasound images, without demanding any labeled data. The network architecture\nbuilds upon AiAReSeg, a segmentation transformer built with the Attention in\nAttention mechanism, and is capable of learning feature changes across time and\nspace. To facilitate training, we used synthetic ultrasound data based on\nphysics-driven catheter insertion simulations, and translated the data into a\nunique CT-Ultrasound common domain, CACTUSS, to improve the segmentation\nperformance. We generated ground truth segmentation masks by computing the\noptical flow between adjacent frames using FlowNet2, and performed thresholding\nto obtain a binary map estimate. Finally, we validated our model on a test\ndataset, consisting of unseen synthetic data and images collected from silicon\naorta phantoms, thus demonstrating its potential for applications to clinical\ndata in the future.\n","authors":["Alex Ranne","Liming Kuang","Yordanka Velikova","Nassir Navab","Ferdinando Rodriguez y Baena"],"pdf_url":"https://arxiv.org/pdf/2403.14465v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2309.00903v2","updated":"2024-03-21T15:12:36Z","published":"2023-09-02T10:46:05Z","title":"An explainable three dimension framework to uncover learning patterns: A\n  unified look in variable sulci recognition","summary":"  Explainable AI is crucial in medical imaging. In the challenging field of\nneuroscience, visual topics present a high level of complexity, particularly\nwithin three-dimensional space. The application of neuroscience, which involves\nidentifying brain sulcal features from MRI, faces significant hurdles due to\nvarying annotation protocols among experts and the intricate three-dimension\nfunctionality of the brain. Consequently, traditional explainability approaches\nfall short in effectively validating and evaluating these networks. To address\nthis, we first present a mathematical formulation delineating various\ncategories of explanation needs across diverse computer vision tasks,\ncategorized into self-explanatory, semi-explanatory, non-explanatory, and\nnew-pattern learning applications based on the reliability of the validation\nprotocol. With respect to this mathematical formulation, we propose a 3D\nexplainability framework aimed at validating the outputs of deep learning\nnetworks in detecting the paracingulate sulcus an essential brain anatomical\nfeature. The framework integrates local 3D explanations, global explanations\nthrough dimensionality reduction, concatenated global explanations, and\nstatistical shape features, unveiling new insights into pattern learning. We\ntrained and tested two advanced 3D deep learning networks on the challenging\nTOP-OSLO dataset, significantly improving sulcus detection accuracy,\nparticularly on the left hemisphere. During evaluation with diverse annotation\nprotocols for this dataset, we highlighted the crucial role of an unbiased\nannotation process in achieving precise predictions and effective pattern\nlearning within our proposed 3D framework. The proposed framework not only\nannotates the variable sulcus but also uncovers hidden AI knowledge, promising\nto advance our understanding of brain anatomy and function.\n","authors":["Michail Mamalakis","Heloise de Vareilles","Atheer AI-Manea","Samantha C. Mitchell","Ingrid Arartz","Lynn Egeland Morch-Johnsen","Jane Garrison","Jon Simons","Pietro Lio","John Suckling","Graham Murray"],"pdf_url":"https://arxiv.org/pdf/2309.00903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02041v2","updated":"2024-03-21T14:59:13Z","published":"2024-03-04T13:47:30Z","title":"A Generative Approach for Wikipedia-Scale Visual Entity Recognition","summary":"  In this paper, we address web-scale visual entity recognition, specifically\nthe task of mapping a given query image to one of the 6 million existing\nentities in Wikipedia. One way of approaching a problem of such scale is using\ndual-encoder models (eg CLIP), where all the entity names and query images are\nembedded into a unified space, paving the way for an approximate k-NN search.\nAlternatively, it is also possible to re-purpose a captioning model to directly\ngenerate the entity names for a given image. In contrast, we introduce a novel\nGenerative Entity Recognition (GER) framework, which given an input image\nlearns to auto-regressively decode a semantic and discriminative ``code''\nidentifying the target entity. Our experiments demonstrate the efficacy of this\nGER paradigm, showcasing state-of-the-art performance on the challenging OVEN\nbenchmark. GER surpasses strong captioning, dual-encoder, visual matching and\nhierarchical classification baselines, affirming its advantage in tackling the\ncomplexities of web-scale recognition.\n","authors":["Mathilde Caron","Ahmet Iscen","Alireza Fathi","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2403.02041v2.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2401.00463v2","updated":"2024-03-21T14:57:25Z","published":"2023-12-31T11:38:50Z","title":"Analyzing Local Representations of Self-supervised Vision Transformers","summary":"  In this paper, we present a comparative analysis of various self-supervised\nVision Transformers (ViTs), focusing on their local representative power.\nInspired by large language models, we examine the abilities of ViTs to perform\nvarious computer vision tasks with little to no fine-tuning. We design\nevaluation framework to analyze the quality of local, i.e.\\ patch-level,\nrepresentations in the context of few-shot semantic segmentation, instance\nidentification, object retrieval and tracking. We discover that contrastive\nlearning based methods like DINO produce more universal patch representations\nthat can be immediately applied for downstream tasks with no parameter tuning,\ncompared to masked image modeling. The embeddings learned using the latter\napproach, e.g. in masked autoencoders, have high variance features that harm\ndistance-based algorithms, such as k-NN, and do not contain useful information\nfor most downstream tasks. Furthermore, we demonstrate that removing these\nhigh-variance features enhances k-NN for MAE, as well as for its recent\nextension Scale-MAE. Finally, we find an object instance retrieval setting\nwhere DINOv2, a model pretrained on two orders of magnitude more data, falls\nshort of its less compute intensive counterpart DINO.\n","authors":["Ani Vanyan","Alvard Barseghyan","Hakob Tamazyan","Vahan Huroyan","Hrant Khachatrian","Martin Danelljan"],"pdf_url":"https://arxiv.org/pdf/2401.00463v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14447v1","updated":"2024-03-21T14:53:50Z","published":"2024-03-21T14:53:50Z","title":"Exploring 3D Human Pose Estimation and Forecasting from the Robot's\n  Perspective: The HARPER Dataset","summary":"  We introduce HARPER, a novel dataset for 3D body pose estimation and forecast\nin dyadic interactions between users and \\spot, the quadruped robot\nmanufactured by Boston Dynamics. The key-novelty is the focus on the robot's\nperspective, i.e., on the data captured by the robot's sensors. These make 3D\nbody pose analysis challenging because being close to the ground captures\nhumans only partially. The scenario underlying HARPER includes 15 actions, of\nwhich 10 involve physical contact between the robot and users. The Corpus\ncontains not only the recordings of the built-in stereo cameras of Spot, but\nalso those of a 6-camera OptiTrack system (all recordings are synchronized).\nThis leads to ground-truth skeletal representations with a precision lower than\na millimeter. In addition, the Corpus includes reproducible benchmarks on 3D\nHuman Pose Estimation, Human Pose Forecasting, and Collision Prediction, all\nbased on publicly available baseline approaches. This enables future HARPER\nusers to rigorously compare their results with those we provide in this work.\n","authors":["Andrea Avogaro. Andrea Toaiari","Federico Cunico","Xiangmin Xu","Haralambos Dafas","Alessandro Vinciarelli","Emma Li","Marco Cristani"],"pdf_url":"https://arxiv.org/pdf/2403.14447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07208v2","updated":"2024-03-21T14:52:55Z","published":"2024-01-14T06:07:07Z","title":"Enhanced Few-Shot Class-Incremental Learning via Ensemble Models","summary":"  Few-shot class-incremental learning (FSCIL) aims to continually fit new\nclasses with limited training data, while maintaining the performance of\npreviously learned classes. The main challenges are overfitting the rare new\ntraining samples and forgetting old classes. While catastrophic forgetting has\nbeen extensively studied, the overfitting problem has attracted less attention\nin FSCIL. To tackle overfitting challenge, we design a new ensemble model\nframework cooperated with data augmentation to boost generalization. In this\nway, the enhanced model works as a library storing abundant features to\nguarantee fast adaptation to downstream tasks. Specifically, the multi-input\nmulti-output ensemble structure is applied with a spatial-aware data\naugmentation strategy, aiming at diversifying the feature extractor and\nalleviating overfitting in incremental sessions. Moreover, self-supervised\nlearning is also integrated to further improve the model generalization.\nComprehensive experimental results show that the proposed method can indeed\nmitigate the overfitting problem in FSCIL, and outperform the state-of-the-art\nmethods.\n","authors":["Mingli Zhu","Zihao Zhu","Sihong Chen","Chen Chen","Baoyuan Wu"],"pdf_url":"https://arxiv.org/pdf/2401.07208v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14442v1","updated":"2024-03-21T14:47:12Z","published":"2024-03-21T14:47:12Z","title":"RoDLA: Benchmarking the Robustness of Document Layout Analysis Models","summary":"  Before developing a Document Layout Analysis (DLA) model in real-world\napplications, conducting comprehensive robustness testing is essential.\nHowever, the robustness of DLA models remains underexplored in the literature.\nTo address this, we are the first to introduce a robustness benchmark for DLA\nmodels, which includes 450K document images of three datasets. To cover\nrealistic corruptions, we propose a perturbation taxonomy with 36 common\ndocument perturbations inspired by real-world document processing.\nAdditionally, to better understand document perturbation impacts, we propose\ntwo metrics, Mean Perturbation Effect (mPE) for perturbation assessment and\nMean Robustness Degradation (mRD) for robustness evaluation. Furthermore, we\nintroduce a self-titled model, i.e., Robust Document Layout Analyzer (RoDLA),\nwhich improves attention mechanisms to boost extraction of robust features.\nExperiments on the proposed benchmarks (PubLayNet-P, DocLayNet-P, and\nM$^6$Doc-P) demonstrate that RoDLA obtains state-of-the-art mRD scores of\n115.7, 135.4, and 150.4, respectively. Compared to previous methods, RoDLA\nachieves notable improvements in mAP of +3.8%, +7.1% and +12.1%, respectively.\n","authors":["Yufan Chen","Jiaming Zhang","Kunyu Peng","Junwei Zheng","Ruiping Liu","Philip Torr","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2403.14442v1.pdf","comment":"Accepted by CVPR 2024. Project page:\n  https://yufanchen96.github.io/projects/RoDLA"},{"id":"http://arxiv.org/abs/2403.14440v1","updated":"2024-03-21T14:45:54Z","published":"2024-03-21T14:45:54Z","title":"Analysing Diffusion Segmentation for Medical Images","summary":"  Denoising Diffusion Probabilistic models have become increasingly popular due\nto their ability to offer probabilistic modeling and generate diverse outputs.\nThis versatility inspired their adaptation for image segmentation, where\nmultiple predictions of the model can produce segmentation results that not\nonly achieve high quality but also capture the uncertainty inherent in the\nmodel. Here, powerful architectures were proposed for improving diffusion\nsegmentation performance. However, there is a notable lack of analysis and\ndiscussions on the differences between diffusion segmentation and image\ngeneration, and thorough evaluations are missing that distinguish the\nimprovements these architectures provide for segmentation in general from their\nbenefit for diffusion segmentation specifically. In this work, we critically\nanalyse and discuss how diffusion segmentation for medical images differs from\ndiffusion image generation, with a particular focus on the training behavior.\nFurthermore, we conduct an assessment how proposed diffusion segmentation\narchitectures perform when trained directly for segmentation. Lastly, we\nexplore how different medical segmentation tasks influence the diffusion\nsegmentation behavior and the diffusion process could be adapted accordingly.\nWith these analyses, we aim to provide in-depth insights into the behavior of\ndiffusion segmentation that allow for a better design and evaluation of\ndiffusion segmentation methods in the future.\n","authors":["Mathias Öttl","Siyuan Mei","Frauke Wilm","Jana Steenpass","Matthias Rübner","Arndt Hartmann","Matthias Beckmann","Peter Fasching","Andreas Maier","Ramona Erber","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2403.14440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14439v1","updated":"2024-03-21T14:45:41Z","published":"2024-03-21T14:45:41Z","title":"Raw Instinct: Trust Your Classifiers and Skip the Conversion","summary":"  Using RAW-images in computer vision problems is surprisingly underexplored\nconsidering that converting from RAW to RGB does not introduce any new capture\ninformation. In this paper, we show that a sufficiently advanced classifier can\nyield equivalent results on RAW input compared to RGB and present a new public\ndataset consisting of RAW images and the corresponding converted RGB images.\nClassifying images directly from RAW is attractive, as it allows for skipping\nthe conversion to RGB, lowering computation time significantly. Two CNN\nclassifiers are used to classify the images in both formats, confirming that\nclassification performance can indeed be preserved. We furthermore show that\nthe total computation time from RAW image data to classification results for\nRAW images can be up to 8.46 times faster than RGB. These results contribute to\nthe evidence found in related works, that using RAW images as direct input to\ncomputer vision algorithms looks very promising.\n","authors":["Christos Kantas","Bjørk Antoniussen","Mathias V. Andersen","Rasmus Munksø","Shobhit Kotnala","Simon B. Jensen","Andreas Møgelmose","Lau Nørgaard","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2403.14439v1.pdf","comment":"https://www.kaggle.com/datasets/mathiasviborg/raw-instinct"},{"id":"http://arxiv.org/abs/2403.14435v1","updated":"2024-03-21T14:41:58Z","published":"2024-03-21T14:41:58Z","title":"Biased Binary Attribute Classifiers Ignore the Majority Classes","summary":"  To visualize the regions of interest that classifiers base their decisions\non, different Class Activation Mapping (CAM) methods have been developed.\nHowever, all of these techniques target categorical classifiers only, though\nmost real-world tasks are binary classification. In this paper, we extend\ngradient-based CAM techniques to work with binary classifiers and visualize the\nactive regions for binary facial attribute classifiers. When training an\nunbalanced binary classifier on an imbalanced dataset, it is well-known that\nthe majority class, i.e. the class with many training samples, is mostly\npredicted much better than minority class with few training instances. In our\nexperiments on the CelebA dataset, we verify these results, when training an\nunbalanced classifier to extract 40 facial attributes simultaneously. One would\nexpect that the biased classifier has learned to extract features mainly for\nthe majority classes and that the proportional energy of the activations mainly\nreside in certain specific regions of the image where the attribute is located.\nHowever, we find very little regular activation for samples of majority\nclasses, while the active regions for minority classes seem mostly reasonable\nand overlap with our expectations. These results suggest that biased\nclassifiers mainly rely on bias activation for majority classes. When training\na balanced classifier on the imbalanced data by employing attribute-specific\nclass weights, majority and minority classes are classified similarly well and\nshow expected activations for almost all attributes\n","authors":["Xinyi Zhang","Johanna Sophie Bieri","Manuel Günther"],"pdf_url":"https://arxiv.org/pdf/2403.14435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14430v1","updated":"2024-03-21T14:37:50Z","published":"2024-03-21T14:37:50Z","title":"Ranking Distillation for Open-Ended Video Question Answering with\n  Insufficient Labels","summary":"  This paper focuses on open-ended video question answering, which aims to find\nthe correct answers from a large answer set in response to a video-related\nquestion. This is essentially a multi-label classification task, since a\nquestion may have multiple answers. However, due to annotation costs, the\nlabels in existing benchmarks are always extremely insufficient, typically one\nanswer per question. As a result, existing works tend to directly treat all the\nunlabeled answers as negative labels, leading to limited ability for\ngeneralization. In this work, we introduce a simple yet effective ranking\ndistillation framework (RADI) to mitigate this problem without additional\nmanual annotation. RADI employs a teacher model trained with incomplete labels\nto generate rankings for potential answers, which contain rich knowledge about\nlabel priority as well as label-associated visual cues, thereby enriching the\ninsufficient labeling information. To avoid overconfidence in the imperfect\nteacher model, we further present two robust and parameter-free ranking\ndistillation approaches: a pairwise approach which introduces adaptive soft\nmargins to dynamically refine the optimization constraints on various pairwise\nrankings, and a listwise approach which adopts sampling-based partial listwise\nlearning to resist the bias in teacher ranking. Extensive experiments on five\npopular benchmarks consistently show that both our pairwise and listwise RADIs\noutperform state-of-the-art methods. Further analysis demonstrates the\neffectiveness of our methods on the insufficient labeling problem.\n","authors":["Tianming Liang","Chaolei Tan","Beihao Xia","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2403.14430v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14429v1","updated":"2024-03-21T14:36:59Z","published":"2024-03-21T14:36:59Z","title":"Style-Extracting Diffusion Models for Semi-Supervised Histopathology\n  Segmentation","summary":"  Deep learning-based image generation has seen significant advancements with\ndiffusion models, notably improving the quality of generated images. Despite\nthese developments, generating images with unseen characteristics beneficial\nfor downstream tasks has received limited attention. To bridge this gap, we\npropose Style-Extracting Diffusion Models, featuring two conditioning\nmechanisms. Specifically, we utilize 1) a style conditioning mechanism which\nallows to inject style information of previously unseen images during image\ngeneration and 2) a content conditioning which can be targeted to a downstream\ntask, e.g., layout for segmentation. We introduce a trainable style encoder to\nextract style information from images, and an aggregation block that merges\nstyle information from multiple style inputs. This architecture enables the\ngeneration of images with unseen styles in a zero-shot manner, by leveraging\nstyles from unseen images, resulting in more diverse generations. In this work,\nwe use the image layout as target condition and first show the capability of\nour method on a natural image dataset as a proof-of-concept. We further\ndemonstrate its versatility in histopathology, where we combine prior knowledge\nabout tissue composition and unannotated data to create diverse synthetic\nimages with known layouts. This allows us to generate additional synthetic data\nto train a segmentation network in a semi-supervised fashion. We verify the\nadded value of the generated images by showing improved segmentation results\nand lower performance variability between patients when synthetic images are\nincluded during segmentation training. Our code will be made publicly available\nat [LINK].\n","authors":["Mathias Öttl","Frauke Wilm","Jana Steenpass","Jingna Qiu","Matthias Rübner","Arndt Hartmann","Matthias Beckmann","Peter Fasching","Andreas Maier","Ramona Erber","Bernhard Kainz","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2403.14429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18467v3","updated":"2024-03-21T14:33:33Z","published":"2024-02-28T16:43:27Z","title":"Separate and Conquer: Decoupling Co-occurrence via Decomposition and\n  Representation for Weakly Supervised Semantic Segmentation","summary":"  Weakly supervised semantic segmentation (WSSS) with image-level labels aims\nto achieve segmentation tasks without dense annotations. However, attributed to\nthe frequent coupling of co-occurring objects and the limited supervision from\nimage-level labels, the challenging co-occurrence problem is widely present and\nleads to false activation of objects in WSSS. In this work, we devise a\n'Separate and Conquer' scheme SeCo to tackle this issue from dimensions of\nimage space and feature space. In the image space, we propose to 'separate' the\nco-occurring objects with image decomposition by subdividing images into\npatches. Importantly, we assign each patch a category tag from Class Activation\nMaps (CAMs), which spatially helps remove the co-context bias and guide the\nsubsequent representation. In the feature space, we propose to 'conquer' the\nfalse activation by enhancing semantic representation with multi-granularity\nknowledge contrast. To this end, a dual-teacher-single-student architecture is\ndesigned and tag-guided contrast is conducted, which guarantee the correctness\nof knowledge and further facilitate the discrepancy among co-contexts. We\nstreamline the multi-staged WSSS pipeline end-to-end and tackle this issue\nwithout external supervision. Extensive experiments are conducted, validating\nthe efficiency of our method and the superiority over previous single-staged\nand even multi-staged competitors on PASCAL VOC and MS COCO. Code is available\nat https://github.com/zwyang6/SeCo.git.\n","authors":["Zhiwei Yang","Kexue Fu","Minghong Duan","Linhao Qu","Shuo Wang","Zhijian Song"],"pdf_url":"https://arxiv.org/pdf/2402.18467v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2308.08325v2","updated":"2024-03-21T14:31:56Z","published":"2023-08-16T12:39:39Z","title":"Visually-Aware Context Modeling for News Image Captioning","summary":"  News Image Captioning aims to create captions from news articles and images,\nemphasizing the connection between textual context and visual elements.\nRecognizing the significance of human faces in news images and the face-name\nco-occurrence pattern in existing datasets, we propose a face-naming module for\nlearning better name embeddings. Apart from names, which can be directly linked\nto an image area (faces), news image captions mostly contain context\ninformation that can only be found in the article. We design a retrieval\nstrategy using CLIP to retrieve sentences that are semantically close to the\nimage, mimicking human thought process of linking articles to images.\nFurthermore, to tackle the problem of the imbalanced proportion of article\ncontext and image context in captions, we introduce a simple yet effective\nmethod Contrasting with Language Model backbone (CoLaM) to the training\npipeline. We conduct extensive experiments to demonstrate the efficacy of our\nframework. We out-perform the previous state-of-the-art (without external data)\nby 7.97/5.80 CIDEr scores on GoodNews/NYTimes800k. Our code is available at\nhttps://github.com/tingyu215/VACNIC.\n","authors":["Tingyu Qu","Tinne Tuytelaars","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2308.08325v2.pdf","comment":"Accepted at NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.07570v2","updated":"2024-03-21T14:28:15Z","published":"2024-03-12T11:58:37Z","title":"An Active Contour Model Driven By the Hybrid Signed Pressure Function","summary":"  Due to the influence of imaging equipment and complex imaging environments,\nmost images in daily life have features of intensity inhomogeneity and noise.\nTherefore, many scholars have designed many image segmentation algorithms to\naddress these issues. Among them, the active contour model is one of the most\neffective image segmentation algorithms.This paper proposes an active contour\nmodel driven by the hybrid signed pressure function that combines global and\nlocal information construction. Firstly, a new global region-based signed\npressure function is introduced by combining the average intensity of the inner\nand outer regions of the curve with the median intensity of the inner region of\nthe evolution curve. Then, the paper uses the energy differences between the\ninner and outer regions of the curve in the local region to design the signed\npressure function of the local term. Combine the two SPF function to obtain a\nnew signed pressure function and get the evolution equation of the new model.\nFinally, experiments and numerical analysis show that the model has excellent\nsegmentation performance for both intensity inhomogeneous images and noisy\nimages.\n","authors":["Jing Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.07570v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15980v2","updated":"2024-03-21T14:21:58Z","published":"2023-11-27T16:26:54Z","title":"Direct2.5: Diverse Text-to-3D Generation via Multi-view 2.5D Diffusion","summary":"  Recent advances in generative AI have unveiled significant potential for the\ncreation of 3D content. However, current methods either apply a pre-trained 2D\ndiffusion model with the time-consuming score distillation sampling (SDS), or a\ndirect 3D diffusion model trained on limited 3D data losing generation\ndiversity. In this work, we approach the problem by employing a multi-view 2.5D\ndiffusion fine-tuned from a pre-trained 2D diffusion model. The multi-view 2.5D\ndiffusion directly models the structural distribution of 3D data, while still\nmaintaining the strong generalization ability of the original 2D diffusion\nmodel, filling the gap between 2D diffusion-based and direct 3D diffusion-based\nmethods for 3D content generation. During inference, multi-view normal maps are\ngenerated using the 2.5D diffusion, and a novel differentiable rasterization\nscheme is introduced to fuse the almost consistent multi-view normal maps into\na consistent 3D model. We further design a normal-conditioned multi-view image\ngeneration module for fast appearance generation given the 3D geometry. Our\nmethod is a one-pass diffusion process and does not require any SDS\noptimization as post-processing. We demonstrate through extensive experiments\nthat, our direct 2.5D generation with the specially-designed fusion scheme can\nachieve diverse, mode-seeking-free, and high-fidelity 3D content generation in\nonly 10 seconds. Project page: https://nju-3dv.github.io/projects/direct25.\n","authors":["Yuanxun Lu","Jingyang Zhang","Shiwei Li","Tian Fang","David McKinnon","Yanghai Tsin","Long Quan","Xun Cao","Yao Yao"],"pdf_url":"https://arxiv.org/pdf/2311.15980v2.pdf","comment":"CVPR 2024 camera ready, including more evaluations and discussions.\n  Project webpage: https://nju-3dv.github.io/projects/direct25"},{"id":"http://arxiv.org/abs/2403.14421v1","updated":"2024-03-21T14:17:28Z","published":"2024-03-21T14:17:28Z","title":"DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning","summary":"  Text-to-image diffusion models have been shown to suffer from sample-level\nmemorization, possibly reproducing near-perfect replica of images that they are\ntrained on, which may be undesirable. To remedy this issue, we develop the\nfirst differentially private (DP) retrieval-augmented generation algorithm that\nis capable of generating high-quality image samples while providing provable\nprivacy guarantees. Specifically, we assume access to a text-to-image diffusion\nmodel trained on a small amount of public data, and design a DP retrieval\nmechanism to augment the text prompt with samples retrieved from a private\nretrieval dataset. Our \\emph{differentially private retrieval-augmented\ndiffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to\nadapt to another domain, and can use state-of-the-art generative models to\ngenerate high-quality image samples while satisfying rigorous DP guarantees.\nFor instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a\nprivacy budget of $\\epsilon=10$, while providing a $3.5$ point improvement in\nFID compared to public-only retrieval for up to $10,000$ queries.\n","authors":["Jonathan Lebensold","Maziar Sanjabi","Pietro Astolfi","Adriana Romero-Soriano","Kamalika Chaudhuri","Mike Rabbat","Chuan Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15583v7","updated":"2024-03-21T14:16:39Z","published":"2023-05-24T21:39:27Z","title":"Alleviating Exposure Bias in Diffusion Models through Sampling with\n  Shifted Time Steps","summary":"  Diffusion Probabilistic Models (DPM) have shown remarkable efficacy in the\nsynthesis of high-quality images. However, their inference process\ncharacteristically requires numerous, potentially hundreds, of iterative steps,\nwhich could exaggerate the problem of exposure bias due to the training and\ninference discrepancy. Previous work has attempted to mitigate this issue by\nperturbing inputs during training, which consequently mandates the retraining\nof the DPM. In this work, we conduct a systematic study of exposure bias in DPM\nand, intriguingly, we find that the exposure bias could be alleviated with a\nnovel sampling method that we propose, without retraining the model. We\nempirically and theoretically show that, during inference, for each backward\ntime step $t$ and corresponding state $\\hat{x}_t$, there might exist another\ntime step $t_s$ which exhibits superior coupling with $\\hat{x}_t$. Based on\nthis finding, we introduce a sampling method named Time-Shift Sampler. Our\nframework can be seamlessly integrated to existing sampling algorithms, such as\nDDPM, DDIM and other high-order solvers, inducing merely minimal additional\ncomputations. Experimental results show our method brings significant and\nconsistent improvements in FID scores on different datasets and sampling\nmethods. For example, integrating Time-Shift Sampler to F-PNDM yields a\nFID=3.88, achieving 44.49\\% improvements as compared to F-PNDM, on CIFAR-10\nwith 10 sampling steps, which is more performant than the vanilla DDIM with 100\nsampling steps. Our code is available at https://github.com/Mingxiao-Li/TS-DPM.\n","authors":["Mingxiao Li","Tingyu Qu","Ruicong Yao","Wei Sun","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2305.15583v7.pdf","comment":"Accepted at International Conference on Learning Representations\n  (ICLR2024)"},{"id":"http://arxiv.org/abs/2403.14418v1","updated":"2024-03-21T14:06:38Z","published":"2024-03-21T14:06:38Z","title":"OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation","summary":"  The booming of 3D recognition in the 2020s began with the introduction of\npoint cloud transformers. They quickly overwhelmed sparse CNNs and became\nstate-of-the-art models, especially in 3D semantic segmentation. However,\nsparse CNNs are still valuable networks, due to their efficiency treasure, and\nease of application. In this work, we reexamine the design distinctions and\ntest the limits of what a sparse CNN can achieve. We discover that the key\ncredit to the performance difference is adaptivity. Specifically, we propose\ntwo key components, i.e., adaptive receptive fields (spatially) and adaptive\nrelation, to bridge the gap. This exploration led to the creation of\nOmni-Adaptive 3D CNNs (OA-CNNs), a family of networks that integrates a\nlightweight module to greatly enhance the adaptivity of sparse CNNs at minimal\ncomputational cost. Without any self-attention modules, OA-CNNs favorably\nsurpass point transformers in terms of accuracy in both indoor and outdoor\nscenes, with much less latency and memory cost. Notably, it achieves 76.1%,\n78.9%, and 70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI validation\nbenchmarks respectively, while maintaining at most 5x better speed than\ntransformer counterparts. This revelation highlights the potential of pure\nsparse CNNs to outperform transformer-related networks.\n","authors":["Bohao Peng","Xiaoyang Wu","Li Jiang","Yukang Chen","Hengshuang Zhao","Zhuotao Tian","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2403.14418v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2402.11504v2","updated":"2024-03-21T14:02:48Z","published":"2024-02-18T08:26:22Z","title":"To use or not to use proprietary street view images in (health and\n  place) research? That is the question","summary":"  Computer vision-based analysis of street view imagery has transformative\nimpacts on environmental assessments. Interactive web services, particularly\nGoogle Street View, play an ever-important role in making imagery data\nubiquitous. Despite the technical ease of harnessing millions of Google Street\nView images, this article questions the current practices in using this\nproprietary data source from a European viewpoint. Our concern lies with\nGoogle's terms of service, which restrict bulk image downloads and the\ngeneration of street view image-based indices. To reconcile the challenge of\nadvancing society through groundbreaking research while maintaining data\nlicense agreements and legal integrity, we believe it is crucial to 1) include\nan author's statement on using proprietary street view data and the directives\nit entails, 2) negotiate academic-specific license to democratize Google Street\nView data access, and 3) adhere to open data principles and utilize open image\nsources for future research.\n","authors":["Marco Helbich","Matthew Danish","SM Labib","Britta Ricker"],"pdf_url":"https://arxiv.org/pdf/2402.11504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14412v1","updated":"2024-03-21T13:59:00Z","published":"2024-03-21T13:59:00Z","title":"CombiNeRF: A Combination of Regularization Techniques for Few-Shot\n  Neural Radiance Field View Synthesis","summary":"  Neural Radiance Fields (NeRFs) have shown impressive results for novel view\nsynthesis when a sufficiently large amount of views are available. When dealing\nwith few-shot settings, i.e. with a small set of input views, the training\ncould overfit those views, leading to artifacts and geometric and chromatic\ninconsistencies in the resulting rendering. Regularization is a valid solution\nthat helps NeRF generalization. On the other hand, each of the most recent NeRF\nregularization techniques aim to mitigate a specific rendering problem.\nStarting from this observation, in this paper we propose CombiNeRF, a framework\nthat synergically combines several regularization techniques, some of them\nnovel, in order to unify the benefits of each. In particular, we regularize\nsingle and neighboring rays distributions and we add a smoothness term to\nregularize near geometries. After these geometric approaches, we propose to\nexploit Lipschitz regularization to both NeRF density and color networks and to\nuse encoding masks for input features regularization. We show that CombiNeRF\noutperforms the state-of-the-art methods with few-shot settings in several\npublicly available datasets. We also present an ablation study on the LLFF and\nNeRF-Synthetic datasets that support the choices made. We release with this\npaper the open-source implementation of our framework.\n","authors":["Matteo Bonotto","Luigi Sarrocco","Daniele Evangelista","Marco Imperoli","Alberto Pretto"],"pdf_url":"https://arxiv.org/pdf/2403.14412v1.pdf","comment":"This paper has been accepted for publication at the 2024\n  International Conference on 3D Vision (3DV)"},{"id":"http://arxiv.org/abs/2403.14410v1","updated":"2024-03-21T13:57:45Z","published":"2024-03-21T13:57:45Z","title":"GLC++: Source-Free Universal Domain Adaptation through Global-Local\n  Clustering and Contrastive Affinity Learning","summary":"  Deep neural networks often exhibit sub-optimal performance under covariate\nand category shifts. Source-Free Domain Adaptation (SFDA) presents a promising\nsolution to this dilemma, yet most SFDA approaches are restricted to closed-set\nscenarios. In this paper, we explore Source-Free Universal Domain Adaptation\n(SF-UniDA) aiming to accurately classify \"known\" data belonging to common\ncategories and segregate them from target-private \"unknown\" data. We propose a\nnovel Global and Local Clustering (GLC) technique, which comprises an adaptive\none-vs-all global clustering algorithm to discern between target classes,\ncomplemented by a local k-NN clustering strategy to mitigate negative transfer.\nDespite the effectiveness, the inherent closed-set source architecture leads to\nuniform treatment of \"unknown\" data, impeding the identification of distinct\n\"unknown\" categories. To address this, we evolve GLC to GLC++, integrating a\ncontrastive affinity learning strategy. We examine the superiority of GLC and\nGLC++ across multiple benchmarks and category shift scenarios. Remarkably, in\nthe most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by\n16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel\ncategory clustering accuracy of GLC by 4.3% in open-set scenarios on\nOffice-Home. Furthermore, the introduced contrastive learning strategy not only\nenhances GLC but also significantly facilitates existing methodologies.\n","authors":["Sanqing Qu","Tianpei Zou","Florian Röhrbein","Cewu Lu","Guang Chen","Dacheng Tao","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14410v1.pdf","comment":"This is a substantial extension of the CVPR 2023 paper \"Upcycling\n  Models under Domain and Category Shift\""},{"id":"http://arxiv.org/abs/2312.02914v3","updated":"2024-03-21T13:53:48Z","published":"2023-12-05T17:39:19Z","title":"Unsupervised Video Domain Adaptation with Masked Pre-Training and\n  Collaborative Self-Training","summary":"  In this work, we tackle the problem of unsupervised domain adaptation (UDA)\nfor video action recognition. Our approach, which we call UNITE, uses an image\nteacher model to adapt a video student model to the target domain. UNITE first\nemploys self-supervised pre-training to promote discriminative feature learning\non target domain videos using a teacher-guided masked distillation objective.\nWe then perform self-training on masked target data, using the video student\nmodel and image teacher model together to generate improved pseudolabels for\nunlabeled target videos. Our self-training process successfully leverages the\nstrengths of both models to achieve strong transfer performance across domains.\nWe evaluate our approach on multiple video domain adaptation benchmarks and\nobserve significant improvements upon previously reported results.\n","authors":["Arun Reddy","William Paul","Corban Rivera","Ketul Shah","Celso M. de Melo","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2312.02914v3.pdf","comment":"Accepted at CVPR 2024. 13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2211.10938v2","updated":"2024-03-21T13:51:10Z","published":"2022-11-20T10:30:58Z","title":"AI-KD: Adversarial learning and Implicit regularization for\n  self-Knowledge Distillation","summary":"  We present a novel adversarial penalized self-knowledge distillation method,\nnamed adversarial learning and implicit regularization for self-knowledge\ndistillation (AI-KD), which regularizes the training procedure by adversarial\nlearning and implicit distillations. Our model not only distills the\ndeterministic and progressive knowledge which are from the pre-trained and\nprevious epoch predictive probabilities but also transfers the knowledge of the\ndeterministic predictive distributions using adversarial learning. The\nmotivation is that the self-knowledge distillation methods regularize the\npredictive probabilities with soft targets, but the exact distributions may be\nhard to predict. Our method deploys a discriminator to distinguish the\ndistributions between the pre-trained and student models while the student\nmodel is trained to fool the discriminator in the trained procedure. Thus, the\nstudent model not only can learn the pre-trained model's predictive\nprobabilities but also align the distributions between the pre-trained and\nstudent models. We demonstrate the effectiveness of the proposed method with\nnetwork architectures on multiple datasets and show the proposed method\nachieves better performance than state-of-the-art methods.\n","authors":["Hyungmin Kim","Sungho Suh","Sunghyun Baek","Daehwan Kim","Daun Jeong","Hansang Cho","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2211.10938v2.pdf","comment":"Accepted to KBS"},{"id":"http://arxiv.org/abs/2403.14401v1","updated":"2024-03-21T13:49:42Z","published":"2024-03-21T13:49:42Z","title":"Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination","summary":"  Multi-modal Large Language Models (MLLMs) demonstrate remarkable success\nacross various vision-language tasks. However, they suffer from visual\nhallucination, where the generated responses diverge from the provided image.\nAre MLLMs completely oblivious to accurate visual cues when they hallucinate?\nOur investigation reveals that the visual branch may simultaneously advocate\nboth accurate and non-existent content. To address this issue, we propose\nPensieve, a training-free method inspired by our observation that analogous\nvisual hallucinations can arise among images sharing common semantic and\nappearance characteristics. During inference, Pensieve enables MLLMs to\nretrospect relevant images as references and compare them with the test image.\nThis paradigm assists MLLMs in downgrading hallucinatory content mistakenly\nsupported by the visual input. Experiments on Whoops, MME, POPE, and LLaVA\nBench demonstrate the efficacy of Pensieve in mitigating visual hallucination,\nsurpassing other advanced decoding strategies. Additionally, Pensieve aids\nMLLMs in identifying details in the image and enhancing the specificity of\nimage descriptions.\n","authors":["Dingchen Yang","Bowen Cao","Guang Chen","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14392v1","updated":"2024-03-21T13:33:00Z","published":"2024-03-21T13:33:00Z","title":"A Bag of Tricks for Few-Shot Class-Incremental Learning","summary":"  We present a bag of tricks framework for few-shot class-incremental learning\n(FSCIL), which is a challenging form of continual learning that involves\ncontinuous adaptation to new tasks with limited samples. FSCIL requires both\nstability and adaptability, i.e., preserving proficiency in previously learned\ntasks while learning new ones. Our proposed bag of tricks brings together eight\nkey and highly influential techniques that improve stability, adaptability, and\noverall performance under a unified framework for FSCIL. We organize these\ntricks into three categories: stability tricks, adaptability tricks, and\ntraining tricks. Stability tricks aim to mitigate the forgetting of previously\nlearned classes by enhancing the separation between the embeddings of learned\nclasses and minimizing interference when learning new ones. On the other hand,\nadaptability tricks focus on the effective learning of new classes. Finally,\ntraining tricks improve the overall performance without compromising stability\nor adaptability. We perform extensive experiments on three benchmark datasets,\nCIFAR-100, CUB-200, and miniIMageNet, to evaluate the impact of our proposed\nframework. Our detailed analysis shows that our approach substantially improves\nboth stability and adaptability, establishing a new state-of-the-art by\noutperforming prior works in the area. We believe our method provides a go-to\nsolution and establishes a robust baseline for future research in this area.\n","authors":["Shuvendu Roy","Chunjong Park","Aldi Fahrezi","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2403.14392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12648v3","updated":"2024-03-21T13:23:44Z","published":"2024-01-23T10:56:01Z","title":"Consistency Enhancement-Based Deep Multiview Clustering via Contrastive\n  Learning","summary":"  Multiview clustering (MVC) segregates data samples into meaningful clusters\nby synthesizing information across multiple views. Moreover, deep\nlearning-based methods have demonstrated their strong feature learning\ncapabilities in MVC scenarios. However, effectively generalizing feature\nrepresentations while maintaining consistency is still an intractable problem.\nIn addition, most existing deep clustering methods based on contrastive\nlearning overlook the consistency of the clustering representations during the\nclustering process. In this paper, we show how the above problems can be\novercome and propose a consistent enhancement-based deep MVC method via\ncontrastive learning (CCEC). Specifically, semantic connection blocks are\nincorporated into a feature representation to preserve the consistent\ninformation among multiple views. Furthermore, the representation process for\nclustering is enhanced through spectral clustering, and the consistency across\nmultiple views is improved. Experiments conducted on five datasets demonstrate\nthe effectiveness and superiority of our method in comparison with the\nstate-of-the-art (SOTA) methods. The code for this method can be accessed at\nhttps://anonymous.4open.science/r/CCEC-E84E/.\n","authors":["Hao Yang","Hua Mao","Wai Lok Woo","Jie Chen","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2401.12648v3.pdf","comment":"There are multiple errors that need to be corrected, including some\n  formulas and concept descriptions. We will re upload the paper after the\n  modifications are completed"},{"id":"http://arxiv.org/abs/2403.14379v1","updated":"2024-03-21T13:12:33Z","published":"2024-03-21T13:12:33Z","title":"Tensor network compressibility of convolutional models","summary":"  Convolutional neural networks (CNNs) represent one of the most widely used\nneural network architectures, showcasing state-of-the-art performance in\ncomputer vision tasks. Although larger CNNs generally exhibit higher accuracy,\ntheir size can be effectively reduced by \"tensorization\" while maintaining\naccuracy. Tensorization consists of replacing the convolution kernels with\ncompact decompositions such as Tucker, Canonical Polyadic decompositions, or\nquantum-inspired decompositions such as matrix product states, and directly\ntraining the factors in the decompositions to bias the learning towards\nlow-rank decompositions. But why doesn't tensorization seem to impact the\naccuracy adversely? We explore this by assessing how truncating the convolution\nkernels of dense (untensorized) CNNs impact their accuracy. Specifically, we\ntruncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50\npre-trained for image classification on CIFAR-10 and CIFAR-100 datasets. We\nfound that kernels (especially those inside deeper layers) could often be\ntruncated along several cuts resulting in significant loss in kernel norm but\nnot in classification accuracy. This suggests that such ``correlation\ncompression'' (underlying tensorization) is an intrinsic feature of how\ninformation is encoded in dense CNNs. We also found that aggressively truncated\nmodels could often recover the pre-truncation accuracy after only a few epochs\nof re-training, suggesting that compressing the internal correlations of\nconvolution layers does not often transport the model to a worse minimum. Our\nresults can be applied to tensorize and compress CNN models more effectively.\n","authors":["Sukhbinder Singh","Saeed S. Jahromi","Roman Orus"],"pdf_url":"https://arxiv.org/pdf/2403.14379v1.pdf","comment":"20 pages, 21 images"},{"id":"http://arxiv.org/abs/2403.14376v1","updated":"2024-03-21T13:06:57Z","published":"2024-03-21T13:06:57Z","title":"InfNeRF: Towards Infinite Scale NeRF Rendering with O(log n) Space\n  Complexity","summary":"  The conventional mesh-based Level of Detail (LoD) technique, exemplified by\napplications such as Google Earth and many game engines, exhibits the\ncapability to holistically represent a large scene even the Earth, and achieves\nrendering with a space complexity of O(log n). This constrained data\nrequirement not only enhances rendering efficiency but also facilitates dynamic\ndata fetching, thereby enabling a seamless 3D navigation experience for users.\nIn this work, we extend this proven LoD technique to Neural Radiance Fields\n(NeRF) by introducing an octree structure to represent the scenes in different\nscales. This innovative approach provides a mathematically simple and elegant\nrepresentation with a rendering space complexity of O(log n), aligned with the\nefficiency of mesh-based LoD techniques. We also present a novel training\nstrategy that maintains a complexity of O(n). This strategy allows for parallel\ntraining with minimal overhead, ensuring the scalability and efficiency of our\nproposed method. Our contribution is not only in extending the capabilities of\nexisting techniques but also in establishing a foundation for scalable and\nefficient large-scale scene representation using NeRF and octree structures.\n","authors":["Jiabin Liang","Lanqing Zhang","Zhuoran Zhao","Xiangyu Xu"],"pdf_url":"https://arxiv.org/pdf/2403.14376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13307v2","updated":"2024-03-21T13:06:49Z","published":"2024-03-20T05:11:10Z","title":"LaserHuman: Language-guided Scene-aware Human Motion Generation in Free\n  Environment","summary":"  Language-guided scene-aware human motion generation has great significance\nfor entertainment and robotics. In response to the limitations of existing\ndatasets, we introduce LaserHuman, a pioneering dataset engineered to\nrevolutionize Scene-Text-to-Motion research. LaserHuman stands out with its\ninclusion of genuine human motions within 3D environments, unbounded free-form\nnatural language descriptions, a blend of indoor and outdoor scenarios, and\ndynamic, ever-changing scenes. Diverse modalities of capture data and rich\nannotations present great opportunities for the research of conditional motion\ngeneration, and can also facilitate the development of real-life applications.\nMoreover, to generate semantically consistent and physically plausible human\nmotions, we propose a multi-conditional diffusion model, which is simple but\neffective, achieving state-of-the-art performance on existing datasets.\n","authors":["Peishan Cong","Ziyi Wang","Zhiyang Dou","Yiming Ren","Wei Yin","Kai Cheng","Yujing Sun","Xiaoxiao Long","Xinge Zhu","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2403.13307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10404v6","updated":"2024-03-21T12:59:04Z","published":"2023-10-16T13:49:46Z","title":"LLM4SGG: Large Language Model for Weakly Supervised Scene Graph\n  Generation","summary":"  Weakly-Supervised Scene Graph Generation (WSSGG) research has recently\nemerged as an alternative to the fully-supervised approach that heavily relies\non costly annotations. In this regard, studies on WSSGG have utilized image\ncaptions to obtain unlocalized triplets while primarily focusing on grounding\nthe unlocalized triplets over image regions. However, they have overlooked the\ntwo issues involved in the triplet formation process from the captions: 1)\nSemantic over-simplification issue arises when extracting triplets from\ncaptions, where fine-grained predicates in captions are undesirably converted\ninto coarse-grained predicates, resulting in a long-tailed predicate\ndistribution, and 2) Low-density scene graph issue arises when aligning the\ntriplets in the caption with entity/predicate classes of interest, where many\ntriplets are discarded and not used in training, leading to insufficient\nsupervision. To tackle the two issues, we propose a new approach, i.e., Large\nLanguage Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two\nissues by leveraging the LLM's in-depth understanding of language and reasoning\nability during the extraction of triplets from captions and alignment of\nentity/predicate classes with target data. To further engage the LLM in these\nprocesses, we adopt the idea of Chain-of-Thought and the in-context few-shot\nlearning strategy. To validate the effectiveness of LLM4SGG, we conduct\nextensive experiments on Visual Genome and GQA datasets, showing significant\nimprovements in both Recall@K and mean Recall@K compared to the\nstate-of-the-art WSSGG methods. A further appeal is that LLM4SGG is\ndata-efficient, enabling effective model training with a small amount of\ntraining images.\n","authors":["Kibum Kim","Kanghoon Yoon","Jaehyeong Jeon","Yeonjun In","Jinyoung Moon","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2310.10404v6.pdf","comment":"8 pages; CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14370v1","updated":"2024-03-21T12:57:30Z","published":"2024-03-21T12:57:30Z","title":"SyncTweedies: A General Generative Framework Based on Synchronized\n  Diffusions","summary":"  We introduce a general framework for generating diverse visual content,\nincluding ambiguous images, panorama images, mesh textures, and Gaussian splat\ntextures, by synchronizing multiple diffusion processes. We present exhaustive\ninvestigation into all possible scenarios for synchronizing multiple diffusion\nprocesses through a canonical space and analyze their characteristics across\napplications. In doing so, we reveal a previously unexplored case: averaging\nthe outputs of Tweedie's formula while conducting denoising in multiple\ninstance spaces. This case also provides the best quality with the widest\napplicability to downstream tasks. We name this case SyncTweedies. In our\nexperiments generating visual content aforementioned, we demonstrate the\nsuperior quality of generation by SyncTweedies compared to other\nsynchronization methods, optimization-based and iterative-update-based methods.\n","authors":["Jaihoon Kim","Juil Koo","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2403.14370v1.pdf","comment":"Project page: https://synctweedies.github.io/"},{"id":"http://arxiv.org/abs/2312.12274v2","updated":"2024-03-21T12:51:31Z","published":"2023-12-19T15:56:19Z","title":"Intrinsic Image Diffusion for Indoor Single-view Material Estimation","summary":"  We present Intrinsic Image Diffusion, a generative model for appearance\ndecomposition of indoor scenes. Given a single input view, we sample multiple\npossible material explanations represented as albedo, roughness, and metallic\nmaps. Appearance decomposition poses a considerable challenge in computer\nvision due to the inherent ambiguity between lighting and material properties\nand the lack of real datasets. To address this issue, we advocate for a\nprobabilistic formulation, where instead of attempting to directly predict the\ntrue material properties, we employ a conditional generative model to sample\nfrom the solution space. Furthermore, we show that utilizing the strong learned\nprior of recent diffusion models trained on large-scale real-world images can\nbe adapted to material estimation and highly improves the generalization to\nreal images. Our method produces significantly sharper, more consistent, and\nmore detailed materials, outperforming state-of-the-art methods by $1.5dB$ on\nPSNR and by $45\\%$ better FID score on albedo prediction. We demonstrate the\neffectiveness of our approach through experiments on both synthetic and\nreal-world datasets.\n","authors":["Peter Kocsis","Vincent Sitzmann","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2312.12274v2.pdf","comment":"Project page: https://peter-kocsis.github.io/IntrinsicImageDiffusion/\n  Video: https://youtu.be/lz0meJlj5cA"},{"id":"http://arxiv.org/abs/2403.14368v1","updated":"2024-03-21T12:50:15Z","published":"2024-03-21T12:50:15Z","title":"Enabling Visual Composition and Animation in Unsupervised Video\n  Generation","summary":"  In this work we propose a novel method for unsupervised controllable video\ngeneration. Once trained on a dataset of unannotated videos, at inference our\nmodel is capable of both composing scenes of predefined object parts and\nanimating them in a plausible and controlled way. This is achieved by\nconditioning video generation on a randomly selected subset of local\npre-trained self-supervised features during training. We call our model CAGE\nfor visual Composition and Animation for video GEneration. We conduct a series\nof experiments to demonstrate capabilities of CAGE in various settings. Project\nwebsite: https://araachie.github.io/cage.\n","authors":["Aram Davtyan","Sepehr Sameni","Björn Ommer","Paolo Favaro"],"pdf_url":"https://arxiv.org/pdf/2403.14368v1.pdf","comment":"Project website: https://araachie.github.io/cage"},{"id":"http://arxiv.org/abs/2403.14366v1","updated":"2024-03-21T12:49:32Z","published":"2024-03-21T12:49:32Z","title":"SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance\n  Field","summary":"  Vision-centric 3D environment understanding is both vital and challenging for\nautonomous driving systems. Recently, object-free methods have attracted\nconsiderable attention. Such methods perceive the world by predicting the\nsemantics of discrete voxel grids but fail to construct continuous and accurate\nobstacle surfaces. To this end, in this paper, we propose SurroundSDF to\nimplicitly predict the signed distance field (SDF) and semantic field for the\ncontinuous perception from surround images. Specifically, we introduce a\nquery-based approach and utilize SDF constrained by the Eikonal formulation to\naccurately describe the surfaces of obstacles. Furthermore, considering the\nabsence of precise SDF ground truth, we propose a novel weakly supervised\nparadigm for SDF, referred to as the Sandwich Eikonal formulation, which\nemphasizes applying correct and dense constraints on both sides of the surface,\nthereby enhancing the perceptual accuracy of the surface. Experiments suggest\nthat our method achieves SOTA for both occupancy prediction and 3D scene\nreconstruction tasks on the nuScenes dataset.\n","authors":["Lizhe Liu","Bohua Wang","Hongwei Xie","Daqi Liu","Li Liu","Zhiqiang Tian","Kuiyuan Yang","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14362v1","updated":"2024-03-21T12:45:01Z","published":"2024-03-21T12:45:01Z","title":"Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen\n  Domains by Intrinsic Learning from Redundant LLM Semantics","summary":"  Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem (DSP) where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods\nwhich alleviate DSP by generating features of unseen classes with semantics,\nCDGZSL needs to construct a common feature space across domains and acquire the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class Similarity Alignment (ISA), which eliminates\nthe non-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and Unseen-class Meta Generation (UMG),\nwhich preserves intrinsic semantics to maintain connectivity between seen and\nunseen classes by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\nthe Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics\nfor these datasets as the benchmark.\n","authors":["Jiaqi Yue","Jiancheng Zhao","Chunhui Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.14362v1.pdf","comment":"This work is submitted to IEEE TNNLS and is subject to IEEE copyright"},{"id":"http://arxiv.org/abs/2311.14758v2","updated":"2024-03-21T12:43:32Z","published":"2023-11-23T15:57:41Z","title":"Point2RBox: Combine Knowledge from Synthetic Visual Patterns for\n  End-to-end Oriented Object Detection with Single Point Supervision","summary":"  With the rapidly increasing demand for oriented object detection (OOD),\nrecent research involving weakly-supervised detectors for learning rotated box\n(RBox) from the horizontal box (HBox) has attracted more and more attention. In\nthis paper, we explore a more challenging yet label-efficient setting, namely\nsingle point-supervised OOD, and present our approach called Point2RBox.\nSpecifically, we propose to leverage two principles: 1) Synthetic pattern\nknowledge combination: By sampling around each labeled point on the image, we\nspread the object feature to synthetic visual patterns with known boxes to\nprovide the knowledge for box regression. 2) Transform self-supervision: With a\ntransformed input image (e.g. scaled/rotated), the output RBoxes are trained to\nfollow the same transformation so that the network can perceive the relative\nsize/rotation between objects. The detector is further enhanced by a few\ndevised techniques to cope with peripheral issues, e.g. the anchor/layer\nassignment as the size of the object is not available in our point supervision\nsetting. To our best knowledge, Point2RBox is the first end-to-end solution for\npoint-supervised OOD. In particular, our method uses a lightweight paradigm,\nyet it achieves a competitive performance among point-supervised alternatives,\n41.05%/27.62%/80.01% on DOTA/DIOR/HRSC datasets.\n","authors":["Yi Yu","Xue Yang","Qingyun Li","Feipeng Da","Jifeng Dai","Yu Qiao","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2311.14758v2.pdf","comment":"10 pages, 3 figures, 5 tables, code:\n  https://github.com/yuyi1005/point2rbox-mmrotate"},{"id":"http://arxiv.org/abs/2403.14359v1","updated":"2024-03-21T12:40:41Z","published":"2024-03-21T12:40:41Z","title":"Varroa destructor detection on honey bees using hyperspectral imagery","summary":"  Hyperspectral (HS) imagery in agriculture is becoming increasingly common.\nThese images have the advantage of higher spectral resolution. Advanced\nspectral processing techniques are required to unlock the information potential\nin these HS images. The present paper introduces a method rooted in\nmultivariate statistics designed to detect parasitic Varroa destructor mites on\nthe body of western honey bee Apis mellifera, enabling easier and continuous\nmonitoring of the bee hives. The methodology explores unsupervised (K-means++)\nand recently developed supervised (Kernel Flows - Partial Least-Squares,\nKF-PLS) methods for parasitic identification. Additionally, in light of the\nemergence of custom-band multispectral cameras, the present research outlines a\nstrategy for identifying the specific wavelengths necessary for effective\nbee-mite separation, suitable for implementation in a custom-band camera.\nIllustrated with a real-case dataset, our findings demonstrate that as few as\nfour spectral bands are sufficient for accurate parasite identification.\n","authors":["Zina-Sabrina Duma","Tomas Zemcik","Simon Bilik","Tuomas Sihvonen","Peter Honec","Satu-Pia Reinikainen","Karel Horak"],"pdf_url":"https://arxiv.org/pdf/2403.14359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14354v1","updated":"2024-03-21T12:29:26Z","published":"2024-03-21T12:29:26Z","title":"LDTR: Transformer-based Lane Detection with Anchor-chain Representation","summary":"  Despite recent advances in lane detection methods, scenarios with limited- or\nno-visual-clue of lanes due to factors such as lighting conditions and\nocclusion remain challenging and crucial for automated driving. Moreover,\ncurrent lane representations require complex post-processing and struggle with\nspecific instances. Inspired by the DETR architecture, we propose LDTR, a\ntransformer-based model to address these issues. Lanes are modeled with a novel\nanchor-chain, regarding a lane as a whole from the beginning, which enables\nLDTR to handle special lanes inherently. To enhance lane instance perception,\nLDTR incorporates a novel multi-referenced deformable attention module to\ndistribute attention around the object. Additionally, LDTR incorporates two\nline IoU algorithms to improve convergence efficiency and employs a Gaussian\nheatmap auxiliary branch to enhance model representation capability during\ntraining. To evaluate lane detection models, we rely on Frechet distance,\nparameterized F1-score, and additional synthetic metrics. Experimental results\ndemonstrate that LDTR achieves state-of-the-art performance on well-known\ndatasets.\n","authors":["Zhongyu Yang","Chen Shen","Wei Shao","Tengfei Xing","Runbo Hu","Pengfei Xu","Hua Chai","Ruini Xue"],"pdf_url":"https://arxiv.org/pdf/2403.14354v1.pdf","comment":"Accepted by CVM 2024 and CVMJ. 16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.14350v1","updated":"2024-03-21T12:25:17Z","published":"2024-03-21T12:25:17Z","title":"Annotation-Efficient Polyp Segmentation via Active Learning","summary":"  Deep learning-based techniques have proven effective in polyp segmentation\ntasks when provided with sufficient pixel-wise labeled data. However, the high\ncost of manual annotation has created a bottleneck for model generalization. To\nminimize annotation costs, we propose a deep active learning framework for\nannotation-efficient polyp segmentation. In practice, we measure the\nuncertainty of each sample by examining the similarity between features masked\nby the prediction map of the polyp and the background area. Since the\nsegmentation model tends to perform weak in samples with indistinguishable\nfeatures of foreground and background areas, uncertainty sampling facilitates\nthe fitting of under-learning data. Furthermore, clustering image-level\nfeatures weighted by uncertainty identify samples that are both uncertain and\nrepresentative. To enhance the selectivity of the active selection strategy, we\npropose a novel unsupervised feature discrepancy learning mechanism. The\nselection strategy and feature optimization work in tandem to achieve optimal\nperformance with a limited annotation budget. Extensive experimental results\nhave demonstrated that our proposed method achieved state-of-the-art\nperformance compared to other competitors on both a public dataset and a\nlarge-scale in-house dataset.\n","authors":["Duojun Huang","Xinyu Xiong","De-Jun Fan","Feng Gao","Xiao-Jian Wu","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.14350v1.pdf","comment":"2024 IEEE 21th International Symposium on Biomedical Imaging (ISBI)"},{"id":"http://arxiv.org/abs/2310.16828v2","updated":"2024-03-21T17:56:19Z","published":"2023-10-25T17:57:07Z","title":"TD-MPC2: Scalable, Robust World Models for Continuous Control","summary":"  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs\nlocal trajectory optimization in the latent space of a learned implicit\n(decoder-free) world model. In this work, we present TD-MPC2: a series of\nimprovements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves\nsignificantly over baselines across 104 online RL tasks spanning 4 diverse task\ndomains, achieving consistently strong results with a single set of\nhyperparameters. We further show that agent capabilities increase with model\nand data size, and successfully train a single 317M parameter agent to perform\n80 tasks across multiple task domains, embodiments, and action spaces. We\nconclude with an account of lessons, opportunities, and risks associated with\nlarge TD-MPC2 agents. Explore videos, models, data, code, and more at\nhttps://tdmpc2.com\n","authors":["Nicklas Hansen","Hao Su","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16828v2.pdf","comment":"ICLR 2024. Explore videos, models, data, code, and more at\n  https://tdmpc2.com"},{"id":"http://arxiv.org/abs/2009.09213v6","updated":"2024-03-21T16:24:05Z","published":"2020-09-19T11:26:01Z","title":"Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering","summary":"  The current high-fidelity generation and high-precision detection of DeepFake\nimages are at an arms race. We believe that producing DeepFakes that are highly\nrealistic and 'detection evasive' can serve the ultimate goal of improving\nfuture generation DeepFake detection capabilities. In this paper, we propose a\nsimple yet powerful pipeline to reduce the artifact patterns of fake images\nwithout hurting image quality by performing implicit spatial-domain notch\nfiltering. We first demonstrate that frequency-domain notch filtering, although\nfamously shown to be effective in removing periodic noise in the spatial\ndomain, is infeasible for our task at hand due to the manual designs required\nfor the notch filters. We, therefore, resort to a learning-based approach to\nreproduce the notch filtering effects, but solely in the spatial domain. We\nadopt a combination of adding overwhelming spatial noise for breaking the\nperiodic noise pattern and deep image filtering to reconstruct the noise-free\nfake images, and we name our method DeepNotch. Deep image filtering provides a\nspecialized filter for each pixel in the noisy image, producing filtered images\nwith high fidelity compared to their DeepFake counterparts. Moreover, we also\nuse the semantic information of the image to generate an adversarial guidance\nmap to add noise intelligently. Our large-scale evaluation on 3 representative\nstate-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)\nhas demonstrated that our technique significantly reduces the accuracy of these\n3 fake image detection methods, 36.79% on average and up to 97.02% in the best\ncase.\n","authors":["Yihao Huang","Felix Juefei-Xu","Qing Guo","Yang Liu","Geguang Pu"],"pdf_url":"https://arxiv.org/pdf/2009.09213v6.pdf","comment":"14 pages"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2402.03049v3","updated":"2024-03-21T15:33:34Z","published":"2024-02-05T14:33:56Z","title":"EasyInstruct: An Easy-to-use Instruction Processing Framework for Large\n  Language Models","summary":"  In recent years, instruction tuning has gained increasing attention and\nemerged as a crucial technique to enhance the capabilities of Large Language\nModels (LLMs). To construct high-quality instruction datasets, many instruction\nprocessing approaches have been proposed, aiming to achieve a delicate balance\nbetween data quantity and data quality. Nevertheless, due to inconsistencies\nthat persist among various instruction processing methods, there is no standard\nopen-source instruction processing implementation framework available for the\ncommunity, which hinders practitioners from further developing and advancing.\nTo facilitate instruction processing research and development, we present\nEasyInstruct, an easy-to-use instruction processing framework for LLMs, which\nmodularizes instruction generation, selection, and prompting, while also\nconsidering their combination and interaction. EasyInstruct is publicly\nreleased and actively maintained at https://github.com/zjunlp/EasyInstruct,\nalong with an online demo app and a demo video for quick-start, calling for\nbroader research centered on instruction data and synthetic data.\n","authors":["Yixin Ou","Ningyu Zhang","Honghao Gui","Ziwen Xu","Shuofei Qiao","Yida Xue","Runnan Fang","Kangwei Liu","Lei Li","Zhen Bi","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.03049v3.pdf","comment":"Project website: https://zjunlp.github.io/project/EasyInstruct Code:\n  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo\n  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct"},{"id":"http://arxiv.org/abs/2403.08206v2","updated":"2024-03-21T15:17:46Z","published":"2024-03-13T03:03:15Z","title":"Discrete Semantic Tokenization for Deep CTR Prediction","summary":"  Incorporating item content information into click-through rate (CTR)\nprediction models remains a challenge, especially with the time and space\nconstraints of industrial scenarios. The content-encoding paradigm, which\nintegrates user and item encoders directly into CTR models, prioritizes space\nover time. In contrast, the embedding-based paradigm transforms item and user\nsemantics into latent embeddings, subsequently caching them to optimize\nprocessing time at the expense of space. In this paper, we introduce a new\nsemantic-token paradigm and propose a discrete semantic tokenization approach,\nnamely UIST, for user and item representation. UIST facilitates swift training\nand inference while maintaining a conservative memory footprint. Specifically,\nUIST quantizes dense embedding vectors into discrete tokens with shorter\nlengths and employs a hierarchical mixture inference module to weigh the\ncontribution of each user--item token pair. Our experimental results on news\nrecommendation showcase the effectiveness and efficiency (about 200-fold space\ncompression) of UIST for CTR prediction.\n","authors":["Qijiong Liu","Hengchang Hu","Jiahao Wu","Jieming Zhu","Min-Yen Kan","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08206v2.pdf","comment":"TheWebConf 2024 accepted paper"},{"id":"http://arxiv.org/abs/2401.16097v2","updated":"2024-03-21T13:29:03Z","published":"2024-01-29T12:11:34Z","title":"Pushing the Limits: Concurrency Detection in Acyclic Sound Free-Choice\n  Workflow Nets in $O(P^2 + T^2)$","summary":"  Concurrency is an important aspect of Petri nets to describe and simulate the\nbehavior of complex systems. Knowing which places and transitions could be\nexecuted in parallel helps to understand nets and enables analysis techniques\nand the computation of other properties, such as causality, exclusivity, etc..\nAll techniques based on concurrency detection depend on the efficiency of this\ndetection methodology. Kovalyov and Esparza have developed algorithms that\ncompute all concurrent places in $O\\big((P+T)TP^2\\big)$ for live and bounded\nnets (where $P$ and $T$ are the numbers of places and transitions) and in\n$O\\big(P(P+T)^2\\big)$ for live and bounded free-choice nets. Although these\nalgorithms have a reasonably good computational complexity, large numbers of\nconcurrent pairs of nodes may still lead to long computation times. This paper\ncomplements the palette of concurrency detection algorithms with the Concurrent\nPaths (CP) algorithm for sound free-choice workflow nets. The algorithm allows\nparallelization and has a worst-case computational complexity of $O(P^2 + T^2)$\nfor acyclic nets and of $O(P^3 + PT^2)$ for cyclic nets. Although the\ncomputational complexity of cyclic nets has not improved, the evaluation shows\nthe benefits of CP, especially, if the net contains many nodes in concurrency\nrelation.\n","authors":["Thomas M. Prinz","Julien Klaus","Nick R. T. P. van Beest"],"pdf_url":"https://arxiv.org/pdf/2401.16097v2.pdf","comment":"19 pages, 14 figures, 5 algorithms"},{"id":"http://arxiv.org/abs/2403.14377v1","updated":"2024-03-21T13:09:23Z","published":"2024-03-21T13:09:23Z","title":"Knowledge-Enhanced Recommendation with User-Centric Subgraph Network","summary":"  Recommendation systems, as widely implemented nowadays on various platforms,\nrecommend relevant items to users based on their preferences. The classical\nmethods which rely on user-item interaction matrices has limitations,\nespecially in scenarios where there is a lack of interaction data for new\nitems. Knowledge graph (KG)-based recommendation systems have emerged as a\npromising solution. However, most KG-based methods adopt node embeddings, which\ndo not provide personalized recommendations for different users and cannot\ngeneralize well to the new items. To address these limitations, we propose\nKnowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning\napproach with graph neural network (GNN) for effective recommendation. KUCNet\nconstructs a U-I subgraph for each user-item pair that captures both the\nhistorical information of user-item interactions and the side information\nprovided in KG. An attention-based GNN is designed to encode the U-I subgraphs\nfor recommendation. Considering efficiency, the pruned user-centric computation\ngraph is further introduced such that multiple U-I subgraphs can be\nsimultaneously computed and that the size can be pruned by Personalized\nPageRank. Our proposed method achieves accurate, efficient, and interpretable\nrecommendations especially for new items. Experimental results demonstrate the\nsuperiority of KUCNet over state-of-the-art KG-based and collaborative\nfiltering (CF)-based methods.\n","authors":["Guangyi Liu","Quanming Yao","Yongqi Zhang","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14374v1","updated":"2024-03-21T13:05:18Z","published":"2024-03-21T13:05:18Z","title":"FIT-RAG: Black-Box RAG with Factual Information and Token Reduction","summary":"  Due to the extraordinarily large number of parameters, fine-tuning Large\nLanguage Models (LLMs) to update long-tail or out-of-date knowledge is\nimpractical in lots of applications. To avoid fine-tuning, we can alternatively\ntreat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment\nit with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.\nRecently, black-box RAG has achieved success in knowledge-intensive tasks and\nhas gained much attention. Existing black-box RAG methods typically fine-tune\nthe retriever to cater to LLMs' preferences and concatenate all the retrieved\ndocuments as the input, which suffers from two issues: (1) Ignorance of Factual\nInformation. The LLM preferred documents may not contain the factual\ninformation for the given question, which can mislead the retriever and hurt\nthe effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating\nall the retrieved documents brings large amounts of unnecessary tokens for\nLLMs, which degenerates the efficiency of black-box RAG. To address these\nissues, this paper proposes a novel black-box RAG framework which utilizes the\nfactual information in the retrieval and reduces the number of tokens for\naugmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by\nconstructing a bi-label document scorer. Besides, it reduces the tokens by\nintroducing a self-knowledge recognizer and a sub-document-level token reducer.\nFIT-RAG achieves both superior effectiveness and efficiency, which is validated\nby extensive experiments across three open-domain question-answering datasets:\nTriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of\nLlama2-13B-Chat by 14.3\\% on TriviaQA, 19.9\\% on NQ and 27.5\\% on PopQA,\nrespectively. Furthermore, it can save approximately half of the tokens on\naverage across the three datasets.\n","authors":["Yuren Mao","Xuemei Dong","Wenyi Xu","Yunjun Gao","Bin Wei","Ying Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02094v3","updated":"2024-03-21T09:03:48Z","published":"2023-09-05T10:00:33Z","title":"TensorBank: Tensor Lakehouse for Foundation Model Training","summary":"  Storing and streaming high dimensional data for foundation model training\nbecame a critical requirement with the rise of foundation models beyond natural\nlanguage. In this paper we introduce TensorBank, a petabyte scale tensor\nlakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU\nmemory at wire speed based on complex relational queries. We use Hierarchical\nStatistical Indices (HSI) for query acceleration. Our architecture allows to\ndirectly address tensors on block level using HTTP range reads. Once in GPU\nmemory, data can be transformed using PyTorch transforms. We provide a generic\nPyTorch dataset type with a corresponding dataset factory translating\nrelational queries and requested transformations as an instance. By making use\nof the HSI, irrelevant blocks can be skipped without reading them as those\nindices contain statistics on their content at different hierarchical\nresolution levels. This is an opinionated architecture powered by open\nstandards and making heavy use of open-source technology. Although, hardened\nfor production use using geospatial-temporal data, this architecture\ngeneralizes to other use case like computer vision, computational neuroscience,\nbiological sequence analysis and more.\n","authors":["Romeo Kienzler","Leonardo Pondian Tizzei","Benedikt Blumenstiel","Zoltan Arnold Nagy","S. Karthik Mukkavilli","Johannes Schmude","Marcus Freitag","Michael Behrendt","Daniel Salles Civitarese","Naomi Simumba","Daiki Kimura","Hendrik Hamann"],"pdf_url":"https://arxiv.org/pdf/2309.02094v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14144v1","updated":"2024-03-21T05:40:01Z","published":"2024-03-21T05:40:01Z","title":"Understanding the Ranking Loss for Recommendation with Sparse User\n  Feedback","summary":"  Click-through rate (CTR) prediction holds significant importance in the realm\nof online advertising. While many existing approaches treat it as a binary\nclassification problem and utilize binary cross entropy (BCE) as the\noptimization objective, recent advancements have indicated that combining BCE\nloss with ranking loss yields substantial performance improvements. However,\nthe full efficacy of this combination loss remains incompletely understood. In\nthis paper, we uncover a new challenge associated with BCE loss in scenarios\nwith sparse positive feedback, such as CTR prediction: the gradient vanishing\nfor negative samples. Subsequently, we introduce a novel perspective on the\neffectiveness of ranking loss in CTR prediction, highlighting its ability to\ngenerate larger gradients on negative samples, thereby mitigating their\noptimization issues and resulting in improved classification ability. Our\nperspective is supported by extensive theoretical analysis and empirical\nevaluation conducted on publicly available datasets. Furthermore, we\nsuccessfully deployed the ranking loss in Tencent's online advertising system,\nachieving notable lifts of 0.70% and 1.26% in Gross Merchandise Value (GMV) for\ntwo main scenarios. The code for our approach is openly accessible at the\nfollowing GitHub repository:\nhttps://github.com/SkylerLinn/Understanding-the-Ranking-Loss.\n","authors":["Zhutian Lin","Junwei Pan","Shangyu Zhang","Ximei Wang","Xi Xiao","Shudong Huang","Lei Xiao","Jie Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14074v1","updated":"2024-03-21T01:52:07Z","published":"2024-03-21T01:52:07Z","title":"M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain\n  Multi-Hop Dense Sentence Retrieval","summary":"  In recent research, contrastive learning has proven to be a highly effective\nmethod for representation learning and is widely used for dense retrieval.\nHowever, we identify that relying solely on contrastive learning can lead to\nsuboptimal retrieval performance. On the other hand, despite many retrieval\ndatasets supporting various learning objectives beyond contrastive learning,\ncombining them efficiently in multi-task learning scenarios can be challenging.\nIn this paper, we introduce M3, an advanced recursive Multi-hop dense sentence\nretrieval system built upon a novel Multi-task Mixed-objective approach for\ndense text representation learning, addressing the aforementioned challenges.\nOur approach yields state-of-the-art performance on a large-scale open-domain\nfact verification benchmark dataset, FEVER. Code and data are available at:\nhttps://github.com/TonyBY/M3\n","authors":["Yang Bai","Anthony Colas","Christan Grant","Daisy Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14074v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14457v1","updated":"2024-03-21T15:04:32Z","published":"2024-03-21T15:04:32Z","title":"gTBLS: Generating Tables from Text by Conditional Question Answering","summary":"  Distilling large, unstructured text into a structured, condensed form such as\ntables is an open research problem. One of the primary challenges in\nautomatically generating tables is ensuring their syntactic validity. Prior\napproaches address this challenge by including additional parameters in the\nTransformer's attention mechanism to attend to specific rows and column\nheaders. In contrast to this single-stage method, this paper presents a\ntwo-stage approach called Generative Tables (gTBLS). The first stage infers\ntable structure (row and column headers) from the text. The second stage\nformulates questions using these headers and fine-tunes a causal language model\nto answer them. Furthermore, the gTBLS approach is amenable to the utilization\nof pre-trained Large Language Models in a zero-shot configuration, presenting a\nsolution for table generation in situations where fine-tuning is not feasible.\ngTBLS improves prior approaches by up to 10% in BERTScore on the table\nconstruction task and up to 20% on the table content generation task of the\nE2E, WikiTableText, WikiBio, and RotoWire datasets.\n","authors":["Anirudh Sundar","Christopher Richardson","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2403.14457v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.15503v1","updated":"2024-03-21T23:40:42Z","published":"2024-03-21T23:40:42Z","title":"Evaluating the Performance of LLMs on Technical Language Processing\n  tasks","summary":"  In this paper we present the results of an evaluation study of the\nperfor-mance of LLMs on Technical Language Processing tasks. Humans are often\nconfronted with tasks in which they have to gather information from dispar-ate\nsources and require making sense of large bodies of text. These tasks can be\nsignificantly complex for humans and often require deep study including\nrereading portions of a text. Towards simplifying the task of gathering\nin-formation we evaluated LLMs with chat interfaces for their ability to\nprovide answers to standard questions that a human can be expected to answer\nbased on their reading of a body of text. The body of text under study is Title\n47 of the United States Code of Federal Regulations (CFR) which describes\nregula-tions for commercial telecommunications as governed by the Federal\nCom-munications Commission (FCC). This has been a body of text of interest\nbe-cause our larger research concerns the issue of making sense of information\nrelated to Wireless Spectrum Governance and usage in an automated manner to\nsupport Dynamic Spectrum Access. The information concerning this wireless\nspectrum domain is found in many disparate sources, with Title 47 of the CFR\nbeing just one of many. Using a range of LLMs and providing the required CFR\ntext as context we were able to quantify the performance of those LLMs on the\nspecific task of answering the questions below.\n","authors":["Andrew Kernycky","David Coleman","Christopher Spence","Udayan Das"],"pdf_url":"https://arxiv.org/pdf/2403.15503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15501v1","updated":"2024-03-21T21:28:07Z","published":"2024-03-21T21:28:07Z","title":"Enhancing Medical Support in the Arabic Language Through Personalized\n  ChatGPT Assistance","summary":"  This Paper discusses the growing popularity of online medical diagnosis as an\nalternative to traditional doctor visits. It highlights the limitations of\nexisting tools and emphasizes the advantages of using ChatGPT, which provides\nreal-time, personalized medical diagnosis at no cost. The paragraph summarizes\na research study that evaluated the performance of ChatGPT in Arabic medical\ndiagnosis. The study involved compiling a dataset of disease information and\ngenerating multiple messages for each disease using different prompting\ntechniques. ChatGPT's performance was assessed by measuring the similarity\nbetween its responses and the actual diseases. The results showed promising\nperformance, with average scores of around 76% for similarity measures. Various\nprompting techniques were used, and chain prompting demonstrated a relative\nadvantage. The study also recorded an average response time of 6.12 seconds for\nthe ChatGPT API, which is considered acceptable but has room for improvement.\nWhile ChatGPT cannot replace human doctors entirely, the findings suggest its\npotential in emergency cases and addressing general medical inquiries. Overall,\nthe study highlights ChatGPT's viability as a valuable tool in the medical\nfield.\n","authors":["Mohamed Issa","Ahmed Abdelwahed"],"pdf_url":"https://arxiv.org/pdf/2403.15501v1.pdf","comment":"This paper was presented at The International conference for Arabic\n  language and applied linguistics"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.14624v1","updated":"2024-03-21T17:59:50Z","published":"2024-03-21T17:59:50Z","title":"MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual\n  Math Problems?","summary":"  The remarkable progress of Multi-modal Large Language Models (MLLMs) has\ngarnered unparalleled attention, due to their superior performance in visual\ncontexts. However, their capabilities in visual math problem-solving remain\ninsufficiently evaluated and understood. We investigate current benchmarks to\nincorporate excessive visual content within textual questions, which\npotentially assist MLLMs in deducing answers without truly interpreting the\ninput diagrams. To this end, we introduce MathVerse, an all-around visual math\nbenchmark designed for an equitable and in-depth evaluation of MLLMs. We\nmeticulously collect 2,612 high-quality, multi-subject math problems with\ndiagrams from publicly available sources. Each problem is then transformed by\nhuman annotators into six distinct versions, each offering varying degrees of\ninformation content in multi-modality, contributing to 15K test samples in\ntotal. This approach allows MathVerse to comprehensively assess whether and how\nmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.\nIn addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a\nfine-grained assessment of the output answers. Rather than naively judging True\nor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and\nthen score each step with detailed error analysis, which can reveal the\nintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark\nmay provide unique insights to guide the future development of MLLMs. Project\npage: https://mathverse-cuhk.github.io\n","authors":["Renrui Zhang","Dongzhi Jiang","Yichi Zhang","Haokun Lin","Ziyu Guo","Pengshuo Qiu","Aojun Zhou","Pan Lu","Kai-Wei Chang","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.14624v1.pdf","comment":"46 Pages, Work in Progress, Benchmark Project Page:\n  https://mathverse-cuhk.github.io"},{"id":"http://arxiv.org/abs/2403.14623v1","updated":"2024-03-21T17:59:41Z","published":"2024-03-21T17:59:41Z","title":"Simplified Diffusion Schrödinger Bridge","summary":"  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.\n","authors":["Zhicong Tang","Tiankai Hang","Shuyang Gu","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14617v1","updated":"2024-03-21T17:59:03Z","published":"2024-03-21T17:59:03Z","title":"Videoshop: Localized Semantic Video Editing with Noise-Extrapolated\n  Diffusion Inversion","summary":"  We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.\n","authors":["Xiang Fan","Anand Bhattad","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.14617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14613v1","updated":"2024-03-21T17:58:04Z","published":"2024-03-21T17:58:04Z","title":"DreamReward: Text-to-3D Generation with Human Preference","summary":"  3D content creation from text prompts has shown remarkable success recently.\nHowever, current text-to-3D methods often generate 3D results that do not align\nwell with human preferences. In this paper, we present a comprehensive\nframework, coined DreamReward, to learn and improve text-to-3D models from\nhuman preference feedback. To begin with, we collect 25k expert comparisons\nbased on a systematic annotation pipeline including rating and ranking. Then,\nwe build Reward3D -- the first general-purpose text-to-3D human preference\nreward model to effectively encode human preferences. Building upon the 3D\nreward model, we finally perform theoretical analysis and present the Reward3D\nFeedback Learning (DreamFL), a direct tuning algorithm to optimize the\nmulti-view diffusion models with a redefined scorer. Grounded by theoretical\nproof and extensive experiment comparisons, our DreamReward successfully\ngenerates high-fidelity and 3D consistent results with significant boosts in\nprompt alignment with human intention. Our results demonstrate the great\npotential for learning from human feedback to improve text-to-3D models.\n","authors":["Junliang Ye","Fangfu Liu","Qixiu Li","Zhengyi Wang","Yikai Wang","Xinzhou Wang","Yueqi Duan","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.14613v1.pdf","comment":"Project page: https://jamesyjl.github.io/DreamReward"},{"id":"http://arxiv.org/abs/2403.14608v1","updated":"2024-03-21T17:55:50Z","published":"2024-03-21T17:55:50Z","title":"Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey","summary":"  Large models represent a groundbreaking advancement in multiple application\nfields, enabling remarkable achievements across various tasks. However, their\nunprecedented scale comes with significant computational costs. These models,\noften consisting of billions of parameters, require vast amounts of\ncomputational resources for execution. Especially, the expansive scale and\ncomputational demands pose considerable challenges when customizing them for\nparticular downstream tasks, particularly over the hardware platforms\nconstrained by computational capabilities. Parameter Efficient Fine-Tuning\n(PEFT) provides a practical solution by efficiently adapt the large models over\nthe various downstream tasks. In particular, PEFT refers to the process of\nadjusting the parameters of a pre-trained large models to adapt it to a\nspecific task while minimizing the number of additional parameters introduced\nor computational resources required. This approach is particularly important\nwhen dealing with large language models with high parameter counts, as\nfine-tuning these models from scratch can be computationally expensive and\nresource-intensive, posing considerable challenges in the supporting system\nplatform design. In this survey, we present comprehensive studies of various\nPEFT algorithms, examining their performance and computational overhead.\nMoreover, we provide an overview of applications developed using different PEFT\nalgorithms and discuss common techniques employed to mitigate computation costs\nfor PEFT. In addition to the algorithmic perspective, we overview various\nreal-world system designs to investigate the implementation costs associated\nwith different PEFT algorithms. This survey serves as an indispensable resource\nfor researchers aiming to understand both the PEFT algorithm and its system\nimplementation, offering detailed insights into recent advancements and\npractical applications.\n","authors":["Zeyu Han","Chao Gao","Jinyang Liu"," Jeff"," Zhang","Sai Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14608v1.pdf","comment":"25 pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.14606v1","updated":"2024-03-21T17:55:16Z","published":"2024-03-21T17:55:16Z","title":"The Elements of Differentiable Programming","summary":"  Artificial intelligence has recently experienced remarkable advances, fueled\nby large models, vast datasets, accelerated hardware, and, last but not least,\nthe transformative power of differentiable programming. This new programming\nparadigm enables end-to-end differentiation of complex computer programs\n(including those with control flows and data structures), making gradient-based\noptimization of program parameters possible. As an emerging paradigm,\ndifferentiable programming builds upon several areas of computer science and\napplied mathematics, including automatic differentiation, graphical models,\noptimization and statistics. This book presents a comprehensive review of the\nfundamental concepts useful for differentiable programming. We adopt two main\nperspectives, that of optimization and that of probability, with clear\nanalogies between the two. Differentiable programming is not merely the\ndifferentiation of programs, but also the thoughtful design of programs\nintended for differentiation. By making programs differentiable, we inherently\nintroduce probability distributions over their execution, providing a means to\nquantify the uncertainty associated with program outputs.\n","authors":["Mathieu Blondel","Vincent Roulet"],"pdf_url":"https://arxiv.org/pdf/2403.14606v1.pdf","comment":"Draft version 1"},{"id":"http://arxiv.org/abs/2403.14602v1","updated":"2024-03-21T17:52:08Z","published":"2024-03-21T17:52:08Z","title":"ReNoise: Real Image Inversion Through Iterative Noising","summary":"  Recent advancements in text-guided diffusion models have unlocked powerful\nimage manipulation capabilities. However, applying these methods to real images\nnecessitates the inversion of the images into the domain of the pretrained\ndiffusion model. Achieving faithful inversion remains a challenge, particularly\nfor more recent models trained to generate images with a small number of\ndenoising steps. In this work, we introduce an inversion method with a high\nquality-to-operation ratio, enhancing reconstruction accuracy without\nincreasing the number of operations. Building on reversing the diffusion\nsampling process, our method employs an iterative renoising mechanism at each\ninversion sampling step. This mechanism refines the approximation of a\npredicted point along the forward diffusion trajectory, by iteratively applying\nthe pretrained diffusion model, and averaging these predictions. We evaluate\nthe performance of our ReNoise technique using various sampling algorithms and\nmodels, including recent accelerated diffusion models. Through comprehensive\nevaluations and comparisons, we show its effectiveness in terms of both\naccuracy and speed. Furthermore, we confirm that our method preserves\neditability by demonstrating text-driven image editing on real images.\n","authors":["Daniel Garibi","Or Patashnik","Andrey Voynov","Hadar Averbuch-Elor","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2403.14602v1.pdf","comment":"project page at: https://garibida.github.io/ReNoise-Inversion/"},{"id":"http://arxiv.org/abs/2403.14597v1","updated":"2024-03-21T17:50:22Z","published":"2024-03-21T17:50:22Z","title":"Extended Reality for Enhanced Human-Robot Collaboration: a\n  Human-in-the-Loop Approach","summary":"  The rise of automation has provided an opportunity to achieve higher\nefficiency in manufacturing processes, yet it often compromises the flexibility\nrequired to promptly respond to evolving market needs and meet the demand for\ncustomization. Human-robot collaboration attempts to tackle these challenges by\ncombining the strength and precision of machines with human ingenuity and\nperceptual understanding. In this paper, we conceptualize and propose an\nimplementation framework for an autonomous, machine learning-based manipulator\nthat incorporates human-in-the-loop principles and leverages Extended Reality\n(XR) to facilitate intuitive communication and programming between humans and\nrobots. Furthermore, the conceptual framework foresees human involvement\ndirectly in the robot learning process, resulting in higher adaptability and\ntask generalization. The paper highlights key technologies enabling the\nproposed framework, emphasizing the importance of developing the digital\necosystem as a whole. Additionally, we review the existent implementation\napproaches of XR in human-robot collaboration, showcasing diverse perspectives\nand methodologies. The challenges and future outlooks are discussed, delving\ninto the major obstacles and potential research avenues of XR for more natural\nhuman-robot interaction and integration in the industrial landscape.\n","authors":["Yehor Karpichev","Todd Charter","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2403.14597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14593v1","updated":"2024-03-21T17:48:38Z","published":"2024-03-21T17:48:38Z","title":"Rethinking Adversarial Inverse Reinforcement Learning: From the Angles\n  of Policy Imitation and Transferable Reward Recovery","summary":"  Adversarial inverse reinforcement learning (AIRL) stands as a cornerstone\napproach in imitation learning. This paper rethinks the two different angles of\nAIRL: policy imitation and transferable reward recovery. We begin with\nsubstituting the built-in algorithm in AIRL with soft actor-critic (SAC) during\nthe policy optimization process to enhance sample efficiency, thanks to the\noff-policy formulation of SAC and identifiable Markov decision process (MDP)\nmodels with respect to AIRL. It indeed exhibits a significant improvement in\npolicy imitation but accidentally brings drawbacks to transferable reward\nrecovery. To learn this issue, we illustrate that the SAC algorithm itself is\nnot feasible to disentangle the reward function comprehensively during the AIRL\ntraining process, and propose a hybrid framework, PPO-AIRL + SAC, for\nsatisfactory transfer effect. Additionally, we analyze the capability of\nenvironments to extract disentangled rewards from an algebraic theory\nperspective.\n","authors":["Yangchun Zhang","Yirui Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.14593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14589v1","updated":"2024-03-21T17:43:44Z","published":"2024-03-21T17:43:44Z","title":"ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for\n  Contrastive Self-Training","summary":"  Language agents have demonstrated autonomous decision-making abilities by\nreasoning with foundation models. Recently, efforts have been made to train\nlanguage agents for performance improvement, with multi-step reasoning and\naction trajectories as the training data. However, collecting such trajectories\nstill requires considerable human effort, by either artificial annotations or\nimplementations of diverse prompting frameworks. In this work, we propose\nA$^3$T, a framework that enables the Autonomous Annotation of Agent\nTrajectories in the style of ReAct. The central role is an ActRe prompting\nagent, which explains the reason for an arbitrary action. When randomly\nsampling an external action, the ReAct-style agent could query the ActRe agent\nwith the action to obtain its textual rationales. Novel trajectories are then\nsynthesized by prepending the posterior reasoning from ActRe to the sampled\naction. In this way, the ReAct-style agent executes multiple trajectories for\nthe failed tasks, and selects the successful ones to supplement its failed\ntrajectory for contrastive self-training. Realized by policy gradient methods\nwith binarized rewards, the contrastive self-training with accumulated\ntrajectories facilitates a closed loop for multiple rounds of language agent\nself-improvement. We conduct experiments using QLoRA fine-tuning with the\nopen-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with\nA$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative\nrounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human\naverage, and 4 rounds of iterative refinement lead to the performance\napproaching human experts. A$^3$T agents significantly outperform existing\ntechniques, including prompting with GPT-4, advanced agent frameworks, and\nfully fine-tuned LLMs.\n","authors":["Zonghan Yang","Peng Li","Ming Yan","Ji Zhang","Fei Huang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14587v1","updated":"2024-03-21T17:42:45Z","published":"2024-03-21T17:42:45Z","title":"An Analysis of Linear Time Series Forecasting Models","summary":"  Despite their simplicity, linear models perform well at time series\nforecasting, even when pitted against deeper and more expensive models. A\nnumber of variations to the linear model have been proposed, often including\nsome form of feature normalisation that improves model generalisation. In this\npaper we analyse the sets of functions expressible using these linear model\narchitectures. In so doing we show that several popular variants of linear\nmodels for time series forecasting are equivalent and functionally\nindistinguishable from standard, unconstrained linear regression. We\ncharacterise the model classes for each linear variant. We demonstrate that\neach model can be reinterpreted as unconstrained linear regression over a\nsuitably augmented feature set, and therefore admit closed-form solutions when\nusing a mean-squared loss function. We provide experimental evidence that the\nmodels under inspection learn nearly identical solutions, and finally\ndemonstrate that the simpler closed form solutions are superior forecasters\nacross 72% of test settings.\n","authors":["William Toner","Luke Darlow"],"pdf_url":"https://arxiv.org/pdf/2403.14587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14583v1","updated":"2024-03-21T17:37:43Z","published":"2024-03-21T17:37:43Z","title":"Co-Optimization of Environment and Policies for Decentralized\n  Multi-Agent Navigation","summary":"  This work views the multi-agent system and its surrounding environment as a\nco-evolving system, where the behavior of one affects the other. The goal is to\ntake both agent actions and environment configurations as decision variables,\nand optimize these two components in a coordinated manner to improve some\nmeasure of interest. Towards this end, we consider the problem of decentralized\nmulti-agent navigation in cluttered environments. By introducing two\nsub-objectives of multi-agent navigation and environment optimization, we\npropose an $\\textit{agent-environment co-optimization}$ problem and develop a\n$\\textit{coordinated algorithm}$ that alternates between these sub-objectives\nto search for an optimal synthesis of agent actions and obstacle configurations\nin the environment; ultimately, improving the navigation performance. Due to\nthe challenge of explicitly modeling the relation between agents, environment\nand performance, we leverage policy gradient to formulate a model-free learning\nmechanism within the coordinated framework. A formal convergence analysis shows\nthat our coordinated algorithm tracks the local minimum trajectory of an\nassociated time-varying non-convex optimization problem. Extensive numerical\nresults corroborate theoretical findings and show the benefits of\nco-optimization over baselines. Interestingly, the results also indicate that\noptimized environment configurations are able to offer structural guidance that\nis key to de-conflicting agents in motion.\n","authors":["Zhan Gao","Guang Yang","Amanda Prorok"],"pdf_url":"https://arxiv.org/pdf/2403.14583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14578v1","updated":"2024-03-21T17:30:59Z","published":"2024-03-21T17:30:59Z","title":"RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants\n  in the Biomedical Domain","summary":"  Large Language Models (LLMs) increasingly support applications in a wide\nrange of domains, some with potential high societal impact such as biomedicine,\nyet their reliability in realistic use cases is under-researched. In this work\nwe introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA)\nframework and evaluate whether four state-of-the-art foundation LLMs can serve\nas reliable assistants in the biomedical domain. We identify prompt robustness,\nhigh recall, and a lack of hallucinations as necessary criteria for this use\ncase. We design shortform tasks and tasks requiring LLM freeform responses\nmimicking real-world user interactions. We evaluate LLM performance using\nsemantic similarity with a ground truth response, through an evaluator LLM.\n","authors":["William James Bolton","Rafael Poyiadzi","Edward R. Morrell","Gabriela van Bergen Gonzalez Bueno","Lea Goetz"],"pdf_url":"https://arxiv.org/pdf/2403.14578v1.pdf","comment":"Published at ICLR 2024 Workshop on Reliable and Responsible\n  Foundation Models"},{"id":"http://arxiv.org/abs/2401.12258v5","updated":"2024-03-21T17:29:37Z","published":"2024-01-21T16:59:45Z","title":"Emergent Dominance Hierarchies in Reinforcement Learning Agents","summary":"  Modern Reinforcement Learning (RL) algorithms are able to outperform humans\nin a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings\npresent additional challenges, and successful cooperation in mixed-motive\ngroups of agents depends on a delicate balancing act between individual and\ngroup objectives. Social conventions and norms, often inspired by human\ninstitutions, are used as tools for striking this balance.\n  In this paper, we examine a fundamental, well-studied social convention that\nunderlies cooperation in both animal and human societies: dominance\nhierarchies.\n  We adapt the ethological theory of dominance hierarchies to artificial\nagents, borrowing the established terminology and definitions with as few\namendments as possible. We demonstrate that populations of RL agents, operating\nwithout explicit programming or intrinsic rewards, can invent, learn, enforce,\nand transmit a dominance hierarchy to new populations. The dominance\nhierarchies that emerge have a similar structure to those studied in chickens,\nmice, fish, and other species.\n","authors":["Ram Rachum","Yonatan Nakar","Bill Tomlinson","Nitay Alon","Reuth Mirsky"],"pdf_url":"https://arxiv.org/pdf/2401.12258v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14566v1","updated":"2024-03-21T17:09:20Z","published":"2024-03-21T17:09:20Z","title":"A survey on Concept-based Approaches For Model Improvement","summary":"  The focus of recent research has shifted from merely increasing the Deep\nNeural Networks (DNNs) performance in various tasks to DNNs, which are more\ninterpretable to humans. The field of eXplainable Artificial Intelligence (XAI)\nhas observed various techniques, including saliency-based and concept-based\napproaches. Concept-based approaches explain the model's decisions in simple\nhuman understandable terms called Concepts. Concepts are human interpretable\nunits of data and are the thinking ground of humans. Explanations in terms of\nconcepts enable detecting spurious correlations, inherent biases, or\nclever-hans. With the advent of concept-based explanations, there have been\nvarious concept representation methods and automatic concept discovery\nalgorithms. Some recent methods use concepts for post-hoc model disentanglement\nevaluation, while others use them for ante-hoc training. The concept-based\napproaches are new, with many representations coming up, and there is very\nlimited work on Concept-based Model improvement. We provide a systematic review\nand taxonomy of various concept representations and their discovery algorithms\nin DNNs, specifically in vision. We also provide details on concept-based model\nimprovement literature, which is the first to survey concept-based model\nimprovement methods.\n","authors":["Avani Gupta","P J Narayanan"],"pdf_url":"https://arxiv.org/pdf/2403.14566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06563v2","updated":"2024-03-21T17:08:43Z","published":"2024-03-11T10:05:29Z","title":"Unraveling the Mystery of Scaling Laws: Part I","summary":"  Scaling law principles indicate a power-law correlation between loss and\nvariables such as model size, dataset size, and computational resources\nutilized during training. These principles play a vital role in optimizing\nvarious aspects of model pre-training, ultimately contributing to the success\nof large language models such as GPT-4, Llama and Gemini. However, the original\nscaling law paper by OpenAI did not disclose the complete details necessary to\nderive the precise scaling law formulas, and their conclusions are only based\non models containing up to 1.5 billion parameters. Though some subsequent works\nattempt to unveil these details and scale to larger models, they often neglect\nthe training dependency of important factors such as the learning rate, context\nlength and batch size, leading to their failure to establish a reliable formula\nfor predicting the test loss trajectory. In this technical report, we confirm\nthat the scaling law formulations proposed in the original OpenAI paper remain\nvalid when scaling the model size up to 33 billion, but the constant\ncoefficients in these formulas vary significantly with the experiment setup. We\nmeticulously identify influential factors and provide transparent, step-by-step\ninstructions to estimate all constant terms in scaling-law formulas by training\non models with only 1M~60M parameters. Using these estimated formulas, we\nshowcase the capability to accurately predict various attributes for models\nwith up to 33B parameters before their training, including (1) the minimum\npossible test loss; (2) the minimum required training steps and processed\ntokens to achieve a specific loss; (3) the critical batch size with an optimal\ntime/computation trade-off at any loss value; and (4) the complete test loss\ntrajectory with arbitrary batch size.\n","authors":["Hui Su","Zhi Tian","Xiaoyu Shen","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2403.06563v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06860v2","updated":"2024-03-21T17:06:49Z","published":"2024-03-11T16:13:58Z","title":"A Geospatial Approach to Predicting Desert Locust Breeding Grounds in\n  Africa","summary":"  Desert locust swarms present a major threat to agriculture and food security.\nAddressing this challenge, our study develops an operationally-ready model for\npredicting locust breeding grounds, which has the potential to enhance early\nwarning systems and targeted control measures. We curated a dataset from the\nUnited Nations Food and Agriculture Organization's (UN-FAO) locust observation\nrecords and analyzed it using two types of spatio-temporal input features:\nremotely-sensed environmental and climate data as well as multi-spectral earth\nobservation images. Our approach employed custom deep learning models\n(three-dimensional and LSTM-based recurrent convolutional networks), along with\nthe geospatial foundational model Prithvi recently released by Jakubik et al.,\n2023. These models notably outperformed existing baselines, with the\nPrithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized\nLandsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and\nROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding\nfrom our research is that multi-spectral earth observation images alone are\nsufficient for effective locust breeding ground prediction without the need to\nexplicitly incorporate climatic or environmental features.\n","authors":["Ibrahim Salihu Yusuf","Mukhtar Opeyemi Yusuf","Kobby Panford-Quainoo","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2403.06860v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14551v1","updated":"2024-03-21T16:52:01Z","published":"2024-03-21T16:52:01Z","title":"Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling","summary":"  Today's most accurate language models are trained on orders of magnitude more\nlanguage data than human language learners receive - but with no supervision\nfrom other sensory modalities that play a crucial role in human learning. Can\nwe make LMs' representations and predictions more accurate (and more\nhuman-like) with more ecologically plausible supervision? This paper describes\nLexiContrastive Grounding (LCG), a grounded language learning procedure that\nleverages visual supervision to improve textual representations.\nLexiContrastive Grounding combines a next token prediction strategy with a\ncontrastive visual grounding objective, focusing on early-layer representations\nthat encode lexical information. Across multiple word-learning and\nsentence-understanding benchmarks, LexiContrastive Grounding not only\noutperforms standard language-only models in learning efficiency, but also\nimproves upon vision-and-language learning procedures including CLIP, GIT,\nFlamingo, and Vokenization. Moreover, LexiContrastive Grounding improves\nperplexity by around 5% on multiple language modeling tasks. This work\nunderscores the potential of incorporating visual grounding into language\nmodels, aligning more closely with the multimodal nature of human language\nacquisition.\n","authors":["Chengxu Zhuang","Evelina Fedorenko","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2403.14551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03849v2","updated":"2024-03-21T16:49:20Z","published":"2024-03-06T16:49:33Z","title":"MedMamba: Vision Mamba for Medical Image Classification","summary":"  Medical image classification is a very fundamental and crucial task in the\nfield of computer vision. These years, CNN-based and Transformer-based models\nhave been widely used to classify various medical images. Unfortunately, The\nlimitation of CNNs in long-range modeling capabilities prevents them from\neffectively extracting features in medical images, while Transformers are\nhampered by their quadratic computational complexity. Recent research has shown\nthat the state space model (SSM) represented by Mamba can efficiently model\nlong-range interactions while maintaining linear computational complexity.\nInspired by this, we propose Vision Mamba for medical image classification\n(MedMamba). More specifically, we introduce a novel Conv-SSM module. Conv-SSM\ncombines the local feature extraction ability of convolutional layers with the\nability of SSM to capture long-range dependency, thereby modeling medical\nimages with different modalities. To demonstrate the potential of MedMamba, we\nconducted extensive experiments using 14 publicly available medical datasets\nwith different imaging techniques and two private datasets built by ourselves.\nExtensive experimental results demonstrate that the proposed MedMamba performs\nwell in detecting lesions in various medical images. To the best of our\nknowledge, this is the first Vision Mamba tailored for medical image\nclassification. The purpose of this work is to establish a new baseline for\nmedical image classification tasks and provide valuable insights for the future\ndevelopment of more efficient and effective SSM-based artificial intelligence\nalgorithms and application systems in the medical. Source code has been\navailable at https://github.com/YubiaoYue/MedMamba.\n","authors":["Yubiao Yue","Zhenzhang Li"],"pdf_url":"https://arxiv.org/pdf/2403.03849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14547v1","updated":"2024-03-21T16:48:45Z","published":"2024-03-21T16:48:45Z","title":"Estimating Physical Information Consistency of Channel Data Augmentation\n  for Remote Sensing Images","summary":"  The application of data augmentation for deep learning (DL) methods plays an\nimportant role in achieving state-of-the-art results in supervised,\nsemi-supervised, and self-supervised image classification. In particular,\nchannel transformations (e.g., solarize, grayscale, brightness adjustments) are\nintegrated into data augmentation pipelines for remote sensing (RS) image\nclassification tasks. However, contradicting beliefs exist about their proper\napplications to RS images. A common point of critique is that the application\nof channel augmentation techniques may lead to physically inconsistent spectral\ndata (i.e., pixel signatures). To shed light on the open debate, we propose an\napproach to estimate whether a channel augmentation technique affects the\nphysical information of RS images. To this end, the proposed approach estimates\na score that measures the alignment of a pixel signature within a time series\nthat can be naturally subject to deviations caused by factors such as\nacquisition conditions or phenological states of vegetation. We compare the\nscores associated with original and augmented pixel signatures to evaluate the\nphysical consistency. Experimental results on a multi-label image\nclassification task show that channel augmentations yielding a score that\nexceeds the expected deviation of original pixel signatures can not improve the\nperformance of a baseline model trained without augmentation.\n","authors":["Tom Burgert","Begüm Demir"],"pdf_url":"https://arxiv.org/pdf/2403.14547v1.pdf","comment":"Accepted at the IEEE International Geoscience and Remote Sensing\n  Symposium"},{"id":"http://arxiv.org/abs/2403.14539v1","updated":"2024-03-21T16:40:10Z","published":"2024-03-21T16:40:10Z","title":"Object-Centric Domain Randomization for 3D Shape Reconstruction in the\n  Wild","summary":"  One of the biggest challenges in single-view 3D shape reconstruction in the\nwild is the scarcity of <3D shape, 2D image>-paired data from real-world\nenvironments. Inspired by remarkable achievements via domain randomization, we\npropose ObjectDR which synthesizes such paired data via a random simulation of\nvisual variations in object appearances and backgrounds. Our data synthesis\nframework exploits a conditional generative model (e.g., ControlNet) to\ngenerate images conforming to spatial conditions such as 2.5D sketches, which\nare obtainable through a rendering process of 3D shapes from object collections\n(e.g., Objaverse-XL). To simulate diverse variations while preserving object\nsilhouettes embedded in spatial conditions, we also introduce a disentangled\nframework which leverages an initial object guidance. After synthesizing a wide\nrange of data, we pre-train a model on them so that it learns to capture a\ndomain-invariant geometry prior which is consistent across various domains. We\nvalidate its effectiveness by substantially improving 3D shape reconstruction\nmodels on a real-world benchmark. In a scale-up evaluation, our pre-training\nachieves 23.6% superior results compared with the pre-training on high-quality\ncomputer graphics renderings.\n","authors":["Junhyeong Cho","Kim Youwang","Hunmin Yang","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2403.14539v1.pdf","comment":"Project Page: https://ObjectDR.github.io"},{"id":"http://arxiv.org/abs/2312.09234v3","updated":"2024-03-21T16:26:09Z","published":"2023-12-14T18:57:16Z","title":"Let's do the time-warp-attend: Learning topological invariants of\n  dynamical systems","summary":"  Dynamical systems across the sciences, from electrical circuits to ecological\nnetworks, undergo qualitative and often catastrophic changes in behavior,\ncalled bifurcations, when their underlying parameters cross a threshold.\nExisting methods predict oncoming catastrophes in individual systems but are\nprimarily time-series-based and struggle both to categorize qualitative\ndynamical regimes across diverse systems and to generalize to real data. To\naddress this challenge, we propose a data-driven, physically-informed\ndeep-learning framework for classifying dynamical regimes and characterizing\nbifurcation boundaries based on the extraction of topologically invariant\nfeatures. We focus on the paradigmatic case of the supercritical Hopf\nbifurcation, which is used to model periodic dynamics across a wide range of\napplications. Our convolutional attention method is trained with data\naugmentations that encourage the learning of topological invariants which can\nbe used to detect bifurcation boundaries in unseen systems and to design models\nof biological systems like oscillatory gene regulatory networks. We further\ndemonstrate our method's use in analyzing real data by recovering distinct\nproliferation and differentiation dynamics along pancreatic endocrinogenesis\ntrajectory in gene expression space based on single-cell data. Our method\nprovides valuable insights into the qualitative, long-term behavior of a wide\nrange of dynamical systems, and can detect bifurcations or catastrophic\ntransitions in large-scale physical and biological systems.\n","authors":["Noa Moriel","Matthew Ricci","Mor Nitzan"],"pdf_url":"https://arxiv.org/pdf/2312.09234v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03049v2","updated":"2024-03-21T16:21:45Z","published":"2023-10-04T02:18:28Z","title":"QuATON: Quantization Aware Training of Optical Neurons","summary":"  Optical processors, built with \"optical neurons\", can efficiently perform\nhigh-dimensional linear operations at the speed of light. Thus they are a\npromising avenue to accelerate large-scale linear computations. With the\ncurrent advances in micro-fabrication, such optical processors can now be 3D\nfabricated, but with a limited precision. This limitation translates to\nquantization of learnable parameters in optical neurons, and should be handled\nduring the design of the optical processor in order to avoid a model mismatch.\nSpecifically, optical neurons should be trained or designed within the\nphysical-constraints at a predefined quantized precision level. To address this\ncritical issues we propose a physics-informed quantization-aware training\nframework. Our approach accounts for physical constraints during the training\nprocess, leading to robust designs. We demonstrate that our approach can design\nstate of the art optical processors using diffractive networks for multiple\nphysics based tasks despite quantized learnable parameters. We thus lay the\nfoundation upon which improved optical processors may be 3D fabricated in the\nfuture.\n","authors":["Hasindu Kariyawasam","Ramith Hettiarachchi","Quansan Yang","Alex Matlock","Takahiro Nambara","Hiroyuki Kusaka","Yuichiro Kunai","Peter T C So","Edward S Boyden","Dushan Wadduwage"],"pdf_url":"https://arxiv.org/pdf/2310.03049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16584v3","updated":"2024-03-21T16:21:23Z","published":"2023-09-28T16:44:18Z","title":"Collaborative Distributed Machine Learning","summary":"  Various collaborative distributed machine learning (CDML) systems, including\nfederated learning systems and swarm learning systems, with different key\ntraits were developed to leverage resources for development and use of machine\nlearning (ML) models in a confidentiality-preserving way. To meet use case\nrequirements, suitable CDML systems need to be selected. However, comparison\nbetween CDML systems regarding their suitability for use cases is often\ndifficult. This work presents a CDML system conceptualization and CDML\narchetypes to support comparison of CDML systems and introduce scientific and\npractical audiences to the principal functioning and key traits of CDML\nsystems.\n","authors":["David Jin","Niclas Kannengießer","Sascha Rank","Ali Sunyaev"],"pdf_url":"https://arxiv.org/pdf/2309.16584v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11287v2","updated":"2024-03-21T16:11:17Z","published":"2023-10-17T14:09:45Z","title":"Assessing the Causal Impact of Humanitarian Aid on Food Security","summary":"  In the face of climate change-induced droughts, vulnerable regions encounter\nsevere threats to food security, demanding urgent humanitarian assistance. This\npaper introduces a causal inference framework for the Horn of Africa, aiming to\nassess the impact of cash-based interventions on food crises. Our contributions\ninclude identifying causal relationships within the food security system,\nharmonizing a comprehensive database including socio-economic, weather and\nremote sensing data, and estimating the causal effect of humanitarian\ninterventions on malnutrition. On a country level, our results revealed no\nsignificant effects, likely due to limited sample size, suboptimal data\nquality, and an imperfect causal graph resulting from our limited understanding\nof multidisciplinary systems like food security. Instead, on a district level,\nresults revealed significant effects, further implying the context-specific\nnature of the system. This underscores the need to enhance data collection and\nrefine causal models with domain experts for more effective future\ninterventions and policies, improving transparency and accountability in\nhumanitarian aid.\n","authors":["Jordi Cerdà-Bautista","José María Tárraga","Vasileios Sitokonstantinou","Gustau Camps-Valls"],"pdf_url":"https://arxiv.org/pdf/2310.11287v2.pdf","comment":"Accepted for publication and presentation at the International\n  Geoscience and Remote Sensing Symposium (IGARSS) 2024"},{"id":"http://arxiv.org/abs/2403.14514v1","updated":"2024-03-21T16:10:42Z","published":"2024-03-21T16:10:42Z","title":"Machine-learning invariant foliations in forced systems for reduced\n  order modelling","summary":"  We identify reduced order models (ROM) of forced systems from data using\ninvariant foliations. The forcing can be external, parametric, periodic or\nquasi-periodic. The process has four steps: 1. identify an approximate\ninvariant torus and the linear dynamics about the torus; 2. identify a globally\ndefined invariant foliation about the torus; 3. identify a local foliation\nabout an invariant manifold that complements the global foliation 4. extract\nthe invariant manifold as the leaf going through the torus and interpret the\nresult. We combine steps 2 and 3, so that we can track the location of the\ninvariant torus and scale the invariance equations appropriately. We highlight\nsome fundamental limitations of invariant manifolds and foliations when fitting\nthem to data, that require further mathematics to resolve.\n","authors":["Robert Szalai"],"pdf_url":"https://arxiv.org/pdf/2403.14514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12157v2","updated":"2024-03-21T16:09:57Z","published":"2023-03-21T19:34:20Z","title":"Learning a Depth Covariance Function","summary":"  We propose learning a depth covariance function with applications to\ngeometric vision tasks. Given RGB images as input, the covariance function can\nbe flexibly used to define priors over depth functions, predictive\ndistributions given observations, and methods for active point selection. We\nleverage these techniques for a selection of downstream tasks: depth\ncompletion, bundle adjustment, and monocular dense visual odometry.\n","authors":["Eric Dexheimer","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2303.12157v2.pdf","comment":"CVPR 2023. Project page: https://edexheim.github.io/DepthCov/"},{"id":"http://arxiv.org/abs/2403.14508v1","updated":"2024-03-21T16:02:52Z","published":"2024-03-21T16:02:52Z","title":"Constrained Reinforcement Learning with Smoothed Log Barrier Function","summary":"  Reinforcement Learning (RL) has been widely applied to many control tasks and\nsubstantially improved the performances compared to conventional control\nmethods in many domains where the reward function is well defined. However, for\nmany real-world problems, it is often more convenient to formulate optimization\nproblems in terms of rewards and constraints simultaneously. Optimizing such\nconstrained problems via reward shaping can be difficult as it requires tedious\nmanual tuning of reward functions with several interacting terms. Recent\nformulations which include constraints mostly require a pre-training phase,\nwhich often needs human expertise to collect data or assumes having a\nsub-optimal policy readily available. We propose a new constrained RL method\ncalled CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which\nachieves competitive performance without any pre-training by applying a linear\nsmoothed log barrier function to an additional safety critic. It implements an\nadaptive penalty for policy learning and alleviates the numerical issues that\nare known to complicate the application of the log barrier function method. As\na result, we show that with CSAC-LB, we achieve state-of-the-art performance on\nseveral constrained control tasks with different levels of difficulty and\nevaluate our methods in a locomotion task on a real quadruped robot platform.\n","authors":["Baohe Zhang","Yuan Zhang","Lilli Frison","Thomas Brox","Joschka Bödecker"],"pdf_url":"https://arxiv.org/pdf/2403.14508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01181v2","updated":"2024-03-21T15:57:29Z","published":"2023-06-01T22:29:28Z","title":"TMI! Finetuned Models Leak Private Information from their Pretraining\n  Data","summary":"  Transfer learning has become an increasingly popular technique in machine\nlearning as a way to leverage a pretrained model trained for one task to assist\nwith building a finetuned model for a related task. This paradigm has been\nespecially popular for $\\textit{privacy}$ in machine learning, where the\npretrained model is considered public, and only the data for finetuning is\nconsidered sensitive. However, there are reasons to believe that the data used\nfor pretraining is still sensitive, making it essential to understand how much\ninformation the finetuned model leaks about the pretraining data. In this work\nwe propose a new membership-inference threat model where the adversary only has\naccess to the finetuned model and would like to infer the membership of the\npretraining data. To realize this threat model, we implement a novel\nmetaclassifier-based attack, $\\textbf{TMI}$, that leverages the influence of\nmemorized pretraining samples on predictions in the downstream task. We\nevaluate $\\textbf{TMI}$ on both vision and natural language tasks across\nmultiple transfer learning settings, including finetuning with differential\nprivacy. Through our evaluation, we find that $\\textbf{TMI}$ can successfully\ninfer membership of pretraining examples using query access to the finetuned\nmodel. An open-source implementation of $\\textbf{TMI}$ can be found\n$\\href{https://github.com/johnmath/tmi-pets24}{\\text{on GitHub}}$.\n","authors":["John Abascal","Stanley Wu","Alina Oprea","Jonathan Ullman"],"pdf_url":"https://arxiv.org/pdf/2306.01181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14504v1","updated":"2024-03-21T15:56:15Z","published":"2024-03-21T15:56:15Z","title":"Soft Learning Probabilistic Circuits","summary":"  Probabilistic Circuits (PCs) are prominent tractable probabilistic models,\nallowing for a range of exact inferences. This paper focuses on the main\nalgorithm for training PCs, LearnSPN, a gold standard due to its efficiency,\nperformance, and ease of use, in particular for tabular data. We show that\nLearnSPN is a greedy likelihood maximizer under mild assumptions. While\ninferences in PCs may use the entire circuit structure for processing queries,\nLearnSPN applies a hard method for learning them, propagating at each sum node\na data point through one and only one of the children/edges as in a hard\nclustering process. We propose a new learning procedure named SoftLearn, that\ninduces a PC using a soft clustering process. We investigate the effect of this\nlearning-inference compatibility in PCs. Our experiments show that SoftLearn\noutperforms LearnSPN in many situations, yielding better likelihoods and\narguably better samples. We also analyze comparable tractable models to\nhighlight the differences between soft/hard learning and model querying.\n","authors":["Soroush Ghandi","Benjamin Quost","Cassio de Campos"],"pdf_url":"https://arxiv.org/pdf/2403.14504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10132v2","updated":"2024-03-21T15:42:06Z","published":"2023-12-15T17:02:19Z","title":"Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against\n  Query-Based Attacks","summary":"  Although promising, existing defenses against query-based attacks share a\ncommon limitation: they offer increased robustness against attacks at the price\nof a considerable accuracy drop on clean samples. In this work, we show how to\nefficiently establish, at test-time, a solid tradeoff between robustness and\naccuracy when mitigating query-based attacks. Given that these attacks\nnecessarily explore low-confidence regions, our insight is that activating\ndedicated defenses, such as random noise defense and random image\ntransformations, only for low-confidence inputs is sufficient to prevent them.\nOur approach is independent of training and supported by theory. We verify the\neffectiveness of our approach for various existing defenses by conducting\nextensive experiments on CIFAR-10, CIFAR-100, and ImageNet. Our results confirm\nthat our proposal can indeed enhance these defenses by providing better\ntradeoffs between robustness and accuracy when compared to state-of-the-art\napproaches while being completely training-free.\n","authors":["Pascal Zimmer","Sébastien Andreina","Giorgia Azzurra Marson","Ghassan Karame"],"pdf_url":"https://arxiv.org/pdf/2312.10132v2.pdf","comment":"To appear in the Proceedings of the AAAI Conference on Artificial\n  Intelligence (AAAI) 2024"},{"id":"http://arxiv.org/abs/2403.14488v1","updated":"2024-03-21T15:36:26Z","published":"2024-03-21T15:36:26Z","title":"Physics-Based Causal Reasoning for Safe & Robust Next-Best Action\n  Selection in Robot Manipulation Tasks","summary":"  Safe and efficient object manipulation is a key enabler of many real-world\nrobot applications. However, this is challenging because robot operation must\nbe robust to a range of sensor and actuator uncertainties. In this paper, we\npresent a physics-informed causal-inference-based framework for a robot to\nprobabilistically reason about candidate actions in a block stacking task in a\npartially observable setting. We integrate a physics-based simulation of the\nrigid-body system dynamics with a causal Bayesian network (CBN) formulation to\ndefine a causal generative probabilistic model of the robot decision-making\nprocess. Using simulation-based Monte Carlo experiments, we demonstrate our\nframework's ability to successfully: (1) predict block tower stability with\nhigh accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best\naction for the block stacking task, for execution by an integrated robot\nsystem, achieving 94.2% task success rate. We also demonstrate our framework's\nsuitability for real-world robot systems by demonstrating successful task\nexecutions with a domestic support robot, with perception and manipulation\nsub-system integration. Hence, we show that by embedding physics-based causal\nreasoning into robots' decision-making processes, we can make robot task\nexecution safer, more reliable, and more robust to various types of\nuncertainty.\n","authors":["Ricardo Cannizzaro","Michael Groom","Jonathan Routley","Robert Osazuwa Ness","Lars Kunze"],"pdf_url":"https://arxiv.org/pdf/2403.14488v1.pdf","comment":"8 pages, 9 figures, submitted to 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2402.03049v3","updated":"2024-03-21T15:33:34Z","published":"2024-02-05T14:33:56Z","title":"EasyInstruct: An Easy-to-use Instruction Processing Framework for Large\n  Language Models","summary":"  In recent years, instruction tuning has gained increasing attention and\nemerged as a crucial technique to enhance the capabilities of Large Language\nModels (LLMs). To construct high-quality instruction datasets, many instruction\nprocessing approaches have been proposed, aiming to achieve a delicate balance\nbetween data quantity and data quality. Nevertheless, due to inconsistencies\nthat persist among various instruction processing methods, there is no standard\nopen-source instruction processing implementation framework available for the\ncommunity, which hinders practitioners from further developing and advancing.\nTo facilitate instruction processing research and development, we present\nEasyInstruct, an easy-to-use instruction processing framework for LLMs, which\nmodularizes instruction generation, selection, and prompting, while also\nconsidering their combination and interaction. EasyInstruct is publicly\nreleased and actively maintained at https://github.com/zjunlp/EasyInstruct,\nalong with an online demo app and a demo video for quick-start, calling for\nbroader research centered on instruction data and synthetic data.\n","authors":["Yixin Ou","Ningyu Zhang","Honghao Gui","Ziwen Xu","Shuofei Qiao","Yida Xue","Runnan Fang","Kangwei Liu","Lei Li","Zhen Bi","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.03049v3.pdf","comment":"Project website: https://zjunlp.github.io/project/EasyInstruct Code:\n  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo\n  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct"},{"id":"http://arxiv.org/abs/2403.14484v1","updated":"2024-03-21T15:31:28Z","published":"2024-03-21T15:31:28Z","title":"HyperGALE: ASD Classification via Hypergraph Gated Attention with\n  Learnable Hyperedges","summary":"  Autism Spectrum Disorder (ASD) is a neurodevelopmental condition\ncharacterized by varied social cognitive challenges and repetitive behavioral\npatterns. Identifying reliable brain imaging-based biomarkers for ASD has been\na persistent challenge due to the spectrum's diverse symptomatology. Existing\nbaselines in the field have made significant strides in this direction, yet\nthere remains room for improvement in both performance and interpretability. We\npropose \\emph{HyperGALE}, which builds upon the hypergraph by incorporating\nlearned hyperedges and gated attention mechanisms. This approach has led to\nsubstantial improvements in the model's ability to interpret complex brain\ngraph data, offering deeper insights into ASD biomarker characterization.\nEvaluated on the extensive ABIDE II dataset, \\emph{HyperGALE} not only improves\ninterpretability but also demonstrates statistically significant enhancements\nin key performance metrics compared to both previous baselines and the\nfoundational hypergraph model. The advancement \\emph{HyperGALE} brings to ASD\nresearch highlights the potential of sophisticated graph-based techniques in\nneurodevelopmental studies. The source code and implementation instructions are\navailable at GitHub:https://github.com/mehular0ra/HyperGALE.\n","authors":["Mehul Arora","Chirag Shantilal Jain","Lalith Bharadwaj Baru","Kamalaker Dadi","Bapi Raju Surampudi"],"pdf_url":"https://arxiv.org/pdf/2403.14484v1.pdf","comment":"Accepted to IJCNN 2024"},{"id":"http://arxiv.org/abs/2403.14483v1","updated":"2024-03-21T15:29:24Z","published":"2024-03-21T15:29:24Z","title":"Utilizing the LightGBM Algorithm for Operator User Credit Assessment\n  Research","summary":"  Mobile Internet user credit assessment is an important way for communication\noperators to establish decisions and formulate measures, and it is also a\nguarantee for operators to obtain expected benefits. However, credit evaluation\nmethods have long been monopolized by financial industries such as banks and\ncredit. As supporters and providers of platform network technology and network\nresources, communication operators are also builders and maintainers of\ncommunication networks. Internet data improves the user's credit evaluation\nstrategy. This paper uses the massive data provided by communication operators\nto carry out research on the operator's user credit evaluation model based on\nthe fusion LightGBM algorithm. First, for the massive data related to user\nevaluation provided by operators, key features are extracted by data\npreprocessing and feature engineering methods, and a multi-dimensional feature\nset with statistical significance is constructed; then, linear regression,\ndecision tree, LightGBM, and other machine learning algorithms build multiple\nbasic models to find the best basic model; finally, integrates Averaging,\nVoting, Blending, Stacking and other integrated algorithms to refine multiple\nfusion models, and finally establish the most suitable fusion model for\noperator user evaluation.\n","authors":["Shaojie Li","Xinqi Dong","Danqing Ma","Bo Dang","Hengyi Zang","Yulu Gong"],"pdf_url":"https://arxiv.org/pdf/2403.14483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14472v1","updated":"2024-03-21T15:18:30Z","published":"2024-03-21T15:18:30Z","title":"Detoxifying Large Language Models via Knowledge Editing","summary":"  This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments to compare knowledge\nediting approaches with previous baselines, indicating that knowledge editing\nhas the potential to efficiently detoxify LLMs with limited impact on general\nperformance. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxify\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.\n","authors":["Mengru Wang","Ningyu Zhang","Ziwen Xu","Zekun Xi","Shumin Deng","Yunzhi Yao","Qishen Zhang","Linyi Yang","Jindong Wang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14472v1.pdf","comment":"Ongoing work. Project website:\n  https://zjunlp.github.io/project/SafeEdit Benchmark:\n  https://huggingface.co/datasets/zjunlp/SafeEdit Code:\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2403.14466v1","updated":"2024-03-21T15:13:54Z","published":"2024-03-21T15:13:54Z","title":"Universal Feature Selection for Simultaneous Interpretability of\n  Multitask Datasets","summary":"  Extracting meaningful features from complex, high-dimensional datasets across\nscientific domains remains challenging. Current methods often struggle with\nscalability, limiting their applicability to large datasets, or make\nrestrictive assumptions about feature-property relationships, hindering their\nability to capture complex interactions. BoUTS's general and scalable feature\nselection algorithm surpasses these limitations to identify both universal\nfeatures relevant to all datasets and task-specific features predictive for\nspecific subsets. Evaluated on seven diverse chemical regression datasets,\nBoUTS achieves state-of-the-art feature sparsity while maintaining prediction\naccuracy comparable to specialized methods. Notably, BoUTS's universal features\nenable domain-specific knowledge transfer between datasets, and suggest deep\nconnections in seemingly-disparate chemical datasets. We expect these results\nto have important repercussions in manually-guided inverse problems. Beyond its\ncurrent application, BoUTS holds immense potential for elucidating data-poor\nsystems by leveraging information from similar data-rich systems. BoUTS\nrepresents a significant leap in cross-domain feature selection, potentially\nleading to advancements in various scientific fields.\n","authors":["Matt Raymond","Jacob Charles Saldinger","Paolo Elvati","Clayton Scott","Angela Violi"],"pdf_url":"https://arxiv.org/pdf/2403.14466v1.pdf","comment":"Main text: 14 pages, 3 figures, 1 table; SI: 7 pages, 1 figure, 4\n  tables, 3 algorithms"},{"id":"http://arxiv.org/abs/2403.14457v1","updated":"2024-03-21T15:04:32Z","published":"2024-03-21T15:04:32Z","title":"gTBLS: Generating Tables from Text by Conditional Question Answering","summary":"  Distilling large, unstructured text into a structured, condensed form such as\ntables is an open research problem. One of the primary challenges in\nautomatically generating tables is ensuring their syntactic validity. Prior\napproaches address this challenge by including additional parameters in the\nTransformer's attention mechanism to attend to specific rows and column\nheaders. In contrast to this single-stage method, this paper presents a\ntwo-stage approach called Generative Tables (gTBLS). The first stage infers\ntable structure (row and column headers) from the text. The second stage\nformulates questions using these headers and fine-tunes a causal language model\nto answer them. Furthermore, the gTBLS approach is amenable to the utilization\nof pre-trained Large Language Models in a zero-shot configuration, presenting a\nsolution for table generation in situations where fine-tuning is not feasible.\ngTBLS improves prior approaches by up to 10% in BERTScore on the table\nconstruction task and up to 20% on the table content generation task of the\nE2E, WikiTableText, WikiBio, and RotoWire datasets.\n","authors":["Anirudh Sundar","Christopher Richardson","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2403.14457v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.14443v1","updated":"2024-03-21T14:48:37Z","published":"2024-03-21T14:48:37Z","title":"Language Models Can Reduce Asymmetry in Information Markets","summary":"  This work addresses the buyer's inspection paradox for information markets.\nThe paradox is that buyers need to access information to determine its value,\nwhile sellers need to limit access to prevent theft. To study this, we\nintroduce an open-source simulated digital marketplace where intelligent\nagents, powered by language models, buy and sell information on behalf of\nexternal participants. The central mechanism enabling this marketplace is the\nagents' dual capabilities: they not only have the capacity to assess the\nquality of privileged information but also come equipped with the ability to\nforget. This ability to induce amnesia allows vendors to grant temporary access\nto proprietary information, significantly reducing the risk of unauthorized\nretention while enabling agents to accurately gauge the information's relevance\nto specific queries or tasks. To perform well, agents must make rational\ndecisions, strategically explore the marketplace through generated sub-queries,\nand synthesize answers from purchased information. Concretely, our experiments\n(a) uncover biases in language models leading to irrational behavior and\nevaluate techniques to mitigate these biases, (b) investigate how price affects\ndemand in the context of informational goods, and (c) show that inspection and\nhigher budgets both lead to higher quality outcomes.\n","authors":["Nasim Rahaman","Martin Weiss","Manuel Wüthrich","Yoshua Bengio","Li Erran Li","Chris Pal","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2403.14443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14440v1","updated":"2024-03-21T14:45:54Z","published":"2024-03-21T14:45:54Z","title":"Analysing Diffusion Segmentation for Medical Images","summary":"  Denoising Diffusion Probabilistic models have become increasingly popular due\nto their ability to offer probabilistic modeling and generate diverse outputs.\nThis versatility inspired their adaptation for image segmentation, where\nmultiple predictions of the model can produce segmentation results that not\nonly achieve high quality but also capture the uncertainty inherent in the\nmodel. Here, powerful architectures were proposed for improving diffusion\nsegmentation performance. However, there is a notable lack of analysis and\ndiscussions on the differences between diffusion segmentation and image\ngeneration, and thorough evaluations are missing that distinguish the\nimprovements these architectures provide for segmentation in general from their\nbenefit for diffusion segmentation specifically. In this work, we critically\nanalyse and discuss how diffusion segmentation for medical images differs from\ndiffusion image generation, with a particular focus on the training behavior.\nFurthermore, we conduct an assessment how proposed diffusion segmentation\narchitectures perform when trained directly for segmentation. Lastly, we\nexplore how different medical segmentation tasks influence the diffusion\nsegmentation behavior and the diffusion process could be adapted accordingly.\nWith these analyses, we aim to provide in-depth insights into the behavior of\ndiffusion segmentation that allow for a better design and evaluation of\ndiffusion segmentation methods in the future.\n","authors":["Mathias Öttl","Siyuan Mei","Frauke Wilm","Jana Steenpass","Matthias Rübner","Arndt Hartmann","Matthias Beckmann","Peter Fasching","Andreas Maier","Ramona Erber","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2403.14440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14438v1","updated":"2024-03-21T14:44:03Z","published":"2024-03-21T14:44:03Z","title":"A Multimodal Approach to Device-Directed Speech Detection with Large\n  Language Models","summary":"  Interactions with virtual assistants typically start with a predefined\ntrigger phrase followed by the user command. To make interactions with the\nassistant more intuitive, we explore whether it is feasible to drop the\nrequirement that users must begin each command with a trigger phrase. We\nexplore this task in three ways: First, we train classifiers using only\nacoustic information obtained from the audio waveform. Second, we take the\ndecoder outputs of an automatic speech recognition (ASR) system, such as 1-best\nhypotheses, as input features to a large language model (LLM). Finally, we\nexplore a multimodal system that combines acoustic and lexical features, as\nwell as ASR decoder signals in an LLM. Using multimodal information yields\nrelative equal-error-rate improvements over text-only and audio-only models of\nup to 39% and 61%. Increasing the size of the LLM and training with low-rank\nadaption leads to further relative EER reductions of up to 18% on our dataset.\n","authors":["Dominik Wager","Alexander Churchill","Siddharth Sigtia","Panayiotis Georgiou","Matt Mirsamadi","Aarshee Mishra","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2403.14438v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03632"},{"id":"http://arxiv.org/abs/2403.14435v1","updated":"2024-03-21T14:41:58Z","published":"2024-03-21T14:41:58Z","title":"Biased Binary Attribute Classifiers Ignore the Majority Classes","summary":"  To visualize the regions of interest that classifiers base their decisions\non, different Class Activation Mapping (CAM) methods have been developed.\nHowever, all of these techniques target categorical classifiers only, though\nmost real-world tasks are binary classification. In this paper, we extend\ngradient-based CAM techniques to work with binary classifiers and visualize the\nactive regions for binary facial attribute classifiers. When training an\nunbalanced binary classifier on an imbalanced dataset, it is well-known that\nthe majority class, i.e. the class with many training samples, is mostly\npredicted much better than minority class with few training instances. In our\nexperiments on the CelebA dataset, we verify these results, when training an\nunbalanced classifier to extract 40 facial attributes simultaneously. One would\nexpect that the biased classifier has learned to extract features mainly for\nthe majority classes and that the proportional energy of the activations mainly\nreside in certain specific regions of the image where the attribute is located.\nHowever, we find very little regular activation for samples of majority\nclasses, while the active regions for minority classes seem mostly reasonable\nand overlap with our expectations. These results suggest that biased\nclassifiers mainly rely on bias activation for majority classes. When training\na balanced classifier on the imbalanced data by employing attribute-specific\nclass weights, majority and minority classes are classified similarly well and\nshow expected activations for almost all attributes\n","authors":["Xinyi Zhang","Johanna Sophie Bieri","Manuel Günther"],"pdf_url":"https://arxiv.org/pdf/2403.14435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14429v1","updated":"2024-03-21T14:36:59Z","published":"2024-03-21T14:36:59Z","title":"Style-Extracting Diffusion Models for Semi-Supervised Histopathology\n  Segmentation","summary":"  Deep learning-based image generation has seen significant advancements with\ndiffusion models, notably improving the quality of generated images. Despite\nthese developments, generating images with unseen characteristics beneficial\nfor downstream tasks has received limited attention. To bridge this gap, we\npropose Style-Extracting Diffusion Models, featuring two conditioning\nmechanisms. Specifically, we utilize 1) a style conditioning mechanism which\nallows to inject style information of previously unseen images during image\ngeneration and 2) a content conditioning which can be targeted to a downstream\ntask, e.g., layout for segmentation. We introduce a trainable style encoder to\nextract style information from images, and an aggregation block that merges\nstyle information from multiple style inputs. This architecture enables the\ngeneration of images with unseen styles in a zero-shot manner, by leveraging\nstyles from unseen images, resulting in more diverse generations. In this work,\nwe use the image layout as target condition and first show the capability of\nour method on a natural image dataset as a proof-of-concept. We further\ndemonstrate its versatility in histopathology, where we combine prior knowledge\nabout tissue composition and unannotated data to create diverse synthetic\nimages with known layouts. This allows us to generate additional synthetic data\nto train a segmentation network in a semi-supervised fashion. We verify the\nadded value of the generated images by showing improved segmentation results\nand lower performance variability between patients when synthetic images are\nincluded during segmentation training. Our code will be made publicly available\nat [LINK].\n","authors":["Mathias Öttl","Frauke Wilm","Jana Steenpass","Jingna Qiu","Matthias Rübner","Arndt Hartmann","Matthias Beckmann","Peter Fasching","Andreas Maier","Ramona Erber","Bernhard Kainz","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2403.14429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12032v2","updated":"2024-03-21T14:36:26Z","published":"2023-10-18T15:16:24Z","title":"Exact and general decoupled solutions of the LMC Multitask Gaussian\n  Process model","summary":"  The Linear Model of Co-regionalization (LMC) is a very general model of\nmultitask gaussian process for regression or classification. While its\nexpressivity and conceptual simplicity are appealing, naive implementations\nhave cubic complexity in the number of datapoints and number of tasks, making\napproximations mandatory for most applications. However, recent work has shown\nthat under some conditions the latent processes of the model can be decoupled,\nleading to a complexity that is only linear in the number of said processes. We\nhere extend these results, showing from the most general assumptions that the\nonly condition necessary to an efficient exact computation of the LMC is a mild\nhypothesis on the noise model. We introduce a full parametrization of the\nresulting \\emph{projected LMC} model, and an expression of the marginal\nlikelihood enabling efficient optimization. We perform a parametric study on\nsynthetic data to show the excellent performance of our approach, compared to\nan unrestricted exact LMC and approximations of the latter. Overall, the\nprojected LMC appears as a credible and simpler alternative to state-of-the art\nmodels, which greatly facilitates some computations such as leave-one-out\ncross-validation and fantasization.\n","authors":["Olivier Truffinet","Karim Ammar","Jean-Philippe Argaud","Bertrand Bouriquet"],"pdf_url":"https://arxiv.org/pdf/2310.12032v2.pdf","comment":"29 pages, 10 figures, submitted to UAI"},{"id":"http://arxiv.org/abs/2403.14425v1","updated":"2024-03-21T14:28:43Z","published":"2024-03-21T14:28:43Z","title":"Task-optimal data-driven surrogate models for eNMPC via differentiable\n  simulation and optimization","summary":"  We present a method for end-to-end learning of Koopman surrogate models for\noptimal performance in control. In contrast to previous contributions that\nemploy standard reinforcement learning (RL) algorithms, we use a training\nalgorithm that exploits the potential differentiability of environments based\non mechanistic simulation models. We evaluate the performance of our method by\ncomparing it to that of other controller type and training algorithm\ncombinations on a literature known eNMPC case study. Our method exhibits\nsuperior performance on this problem, thereby constituting a promising avenue\ntowards more capable controllers that employ dynamic surrogate models.\n","authors":["Daniel Mayfrank","Na Young Ahn","Alexander Mitsos","Manuel Dahmen"],"pdf_url":"https://arxiv.org/pdf/2403.14425v1.pdf","comment":"6 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.14421v1","updated":"2024-03-21T14:17:28Z","published":"2024-03-21T14:17:28Z","title":"DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning","summary":"  Text-to-image diffusion models have been shown to suffer from sample-level\nmemorization, possibly reproducing near-perfect replica of images that they are\ntrained on, which may be undesirable. To remedy this issue, we develop the\nfirst differentially private (DP) retrieval-augmented generation algorithm that\nis capable of generating high-quality image samples while providing provable\nprivacy guarantees. Specifically, we assume access to a text-to-image diffusion\nmodel trained on a small amount of public data, and design a DP retrieval\nmechanism to augment the text prompt with samples retrieved from a private\nretrieval dataset. Our \\emph{differentially private retrieval-augmented\ndiffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to\nadapt to another domain, and can use state-of-the-art generative models to\ngenerate high-quality image samples while satisfying rigorous DP guarantees.\nFor instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a\nprivacy budget of $\\epsilon=10$, while providing a $3.5$ point improvement in\nFID compared to public-only retrieval for up to $10,000$ queries.\n","authors":["Jonathan Lebensold","Maziar Sanjabi","Pietro Astolfi","Adriana Romero-Soriano","Kamalika Chaudhuri","Mike Rabbat","Chuan Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06137v2","updated":"2024-03-21T14:03:39Z","published":"2024-02-09T02:11:25Z","title":"On the Privacy of Selection Mechanisms with Gaussian Noise","summary":"  Report Noisy Max and Above Threshold are two classical differentially private\n(DP) selection mechanisms. Their output is obtained by adding noise to a\nsequence of low-sensitivity queries and reporting the identity of the query\nwhose (noisy) answer satisfies a certain condition. Pure DP guarantees for\nthese mechanisms are easy to obtain when Laplace noise is added to the queries.\nOn the other hand, when instantiated using Gaussian noise, standard analyses\nonly yield approximate DP guarantees despite the fact that the outputs of these\nmechanisms lie in a discrete space. In this work, we revisit the analysis of\nReport Noisy Max and Above Threshold with Gaussian noise and show that, under\nthe additional assumption that the underlying queries are bounded, it is\npossible to provide pure ex-ante DP bounds for Report Noisy Max and pure\nex-post DP bounds for Above Threshold. The resulting bounds are tight and\ndepend on closed-form expressions that can be numerically evaluated using\nstandard methods. Empirically we find these lead to tighter privacy accounting\nin the high privacy, low data regime. Further, we propose a simple privacy\nfilter for composing pure ex-post DP guarantees, and use it to derive a fully\nadaptive Gaussian Sparse Vector Technique mechanism. Finally, we provide\nexperiments on mobility and energy consumption datasets demonstrating that our\nSparse Vector Technique is practically competitive with previous approaches and\nrequires less hyper-parameter tuning.\n","authors":["Jonathan Lebensold","Doina Precup","Borja Balle"],"pdf_url":"https://arxiv.org/pdf/2402.06137v2.pdf","comment":"AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.14413v1","updated":"2024-03-21T13:59:19Z","published":"2024-03-21T13:59:19Z","title":"Model Uncertainty in Evolutionary Optimization and Bayesian\n  Optimization: A Comparative Analysis","summary":"  Black-box optimization problems, which are common in many real-world\napplications, require optimization through input-output interactions without\naccess to internal workings. This often leads to significant computational\nresources being consumed for simulations. Bayesian Optimization (BO) and\nSurrogate-Assisted Evolutionary Algorithm (SAEA) are two widely used\ngradient-free optimization techniques employed to address such challenges. Both\napproaches follow a similar iterative procedure that relies on surrogate models\nto guide the search process. This paper aims to elucidate the similarities and\ndifferences in the utilization of model uncertainty between these two methods,\nas well as the impact of model inaccuracies on algorithmic performance. A novel\nmodel-assisted strategy is introduced, which utilizes unevaluated solutions to\ngenerate offspring, leveraging the population-based search capabilities of\nevolutionary algorithm to enhance the effectiveness of model-assisted\noptimization. Experimental results demonstrate that the proposed approach\noutperforms mainstream Bayesian optimization algorithms in terms of accuracy\nand efficiency.\n","authors":["Hao Hao","Xiaoqun Zhang","Aimin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.14413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14410v1","updated":"2024-03-21T13:57:45Z","published":"2024-03-21T13:57:45Z","title":"GLC++: Source-Free Universal Domain Adaptation through Global-Local\n  Clustering and Contrastive Affinity Learning","summary":"  Deep neural networks often exhibit sub-optimal performance under covariate\nand category shifts. Source-Free Domain Adaptation (SFDA) presents a promising\nsolution to this dilemma, yet most SFDA approaches are restricted to closed-set\nscenarios. In this paper, we explore Source-Free Universal Domain Adaptation\n(SF-UniDA) aiming to accurately classify \"known\" data belonging to common\ncategories and segregate them from target-private \"unknown\" data. We propose a\nnovel Global and Local Clustering (GLC) technique, which comprises an adaptive\none-vs-all global clustering algorithm to discern between target classes,\ncomplemented by a local k-NN clustering strategy to mitigate negative transfer.\nDespite the effectiveness, the inherent closed-set source architecture leads to\nuniform treatment of \"unknown\" data, impeding the identification of distinct\n\"unknown\" categories. To address this, we evolve GLC to GLC++, integrating a\ncontrastive affinity learning strategy. We examine the superiority of GLC and\nGLC++ across multiple benchmarks and category shift scenarios. Remarkably, in\nthe most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by\n16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel\ncategory clustering accuracy of GLC by 4.3% in open-set scenarios on\nOffice-Home. Furthermore, the introduced contrastive learning strategy not only\nenhances GLC but also significantly facilitates existing methodologies.\n","authors":["Sanqing Qu","Tianpei Zou","Florian Röhrbein","Cewu Lu","Guang Chen","Dacheng Tao","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.14410v1.pdf","comment":"This is a substantial extension of the CVPR 2023 paper \"Upcycling\n  Models under Domain and Category Shift\""},{"id":"http://arxiv.org/abs/2312.02914v3","updated":"2024-03-21T13:53:48Z","published":"2023-12-05T17:39:19Z","title":"Unsupervised Video Domain Adaptation with Masked Pre-Training and\n  Collaborative Self-Training","summary":"  In this work, we tackle the problem of unsupervised domain adaptation (UDA)\nfor video action recognition. Our approach, which we call UNITE, uses an image\nteacher model to adapt a video student model to the target domain. UNITE first\nemploys self-supervised pre-training to promote discriminative feature learning\non target domain videos using a teacher-guided masked distillation objective.\nWe then perform self-training on masked target data, using the video student\nmodel and image teacher model together to generate improved pseudolabels for\nunlabeled target videos. Our self-training process successfully leverages the\nstrengths of both models to achieve strong transfer performance across domains.\nWe evaluate our approach on multiple video domain adaptation benchmarks and\nobserve significant improvements upon previously reported results.\n","authors":["Arun Reddy","William Paul","Corban Rivera","Ketul Shah","Celso M. de Melo","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2312.02914v3.pdf","comment":"Accepted at CVPR 2024. 13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.14404v1","updated":"2024-03-21T13:52:55Z","published":"2024-03-21T13:52:55Z","title":"Physics-Informed Diffusion Models","summary":"  Generative models such as denoising diffusion models are quickly advancing\ntheir ability to approximate highly complex data distributions. They are also\nincreasingly leveraged in scientific machine learning, where samples from the\nimplied data distribution are expected to adhere to specific governing\nequations. We present a framework to inform denoising diffusion models on\nunderlying constraints on such generated samples during model training. Our\napproach improves the alignment of the generated samples with the imposed\nconstraints and significantly outperforms existing methods without affecting\ninference speed. Additionally, our findings suggest that incorporating such\nconstraints during training provides a natural regularization against\noverfitting. Our framework is easy to implement and versatile in its\napplicability for imposing equality and inequality constraints as well as\nauxiliary optimization objectives.\n","authors":["Jan-Hendrik Bastek","WaiChing Sun","Dennis M. Kochmann"],"pdf_url":"https://arxiv.org/pdf/2403.14404v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2211.10938v2","updated":"2024-03-21T13:51:10Z","published":"2022-11-20T10:30:58Z","title":"AI-KD: Adversarial learning and Implicit regularization for\n  self-Knowledge Distillation","summary":"  We present a novel adversarial penalized self-knowledge distillation method,\nnamed adversarial learning and implicit regularization for self-knowledge\ndistillation (AI-KD), which regularizes the training procedure by adversarial\nlearning and implicit distillations. Our model not only distills the\ndeterministic and progressive knowledge which are from the pre-trained and\nprevious epoch predictive probabilities but also transfers the knowledge of the\ndeterministic predictive distributions using adversarial learning. The\nmotivation is that the self-knowledge distillation methods regularize the\npredictive probabilities with soft targets, but the exact distributions may be\nhard to predict. Our method deploys a discriminator to distinguish the\ndistributions between the pre-trained and student models while the student\nmodel is trained to fool the discriminator in the trained procedure. Thus, the\nstudent model not only can learn the pre-trained model's predictive\nprobabilities but also align the distributions between the pre-trained and\nstudent models. We demonstrate the effectiveness of the proposed method with\nnetwork architectures on multiple datasets and show the proposed method\nachieves better performance than state-of-the-art methods.\n","authors":["Hyungmin Kim","Sungho Suh","Sunghyun Baek","Daehwan Kim","Daun Jeong","Hansang Cho","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2211.10938v2.pdf","comment":"Accepted to KBS"},{"id":"http://arxiv.org/abs/2403.14398v1","updated":"2024-03-21T13:43:49Z","published":"2024-03-21T13:43:49Z","title":"Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact\n  Subproblem Solver for Training Structured Neural Network","summary":"  We propose a Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm\nfor training structured neural networks. Similar to existing regularized\nadaptive methods, the subproblem for computing the update direction of RAMDA\ninvolves a nonsmooth regularizer and a diagonal preconditioner, and therefore\ndoes not possess a closed-form solution in general. We thus also carefully\ndevise an implementable inexactness condition that retains convergence\nguarantees similar to the exact versions, and propose a companion efficient\nsolver for the subproblems of both RAMDA and existing methods to make them\npractically feasible. We leverage the theory of manifold identification in\nvariational analysis to show that, even in the presence of such inexactness,\nthe iterates of RAMDA attain the ideal structure induced by the regularizer at\nthe stationary point of asymptotic convergence. This structure is locally\noptimal near the point of convergence, so RAMDA is guaranteed to obtain the\nbest structure possible among all methods converging to the same point, making\nit the first regularized adaptive method outputting models that possess\noutstanding predictive performance while being (locally) optimally structured.\nExtensive numerical experiments in large-scale modern computer vision, language\nmodeling, and speech tasks show that the proposed RAMDA is efficient and\nconsistently outperforms state of the art for training structured neural\nnetwork. Implementation of our algorithm is available at\nhttp://www.github.com/ismoptgroup/RAMDA/.\n","authors":["Zih-Syuan Huang","Ching-pei Lee"],"pdf_url":"https://arxiv.org/pdf/2403.14398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11259v2","updated":"2024-03-21T13:41:35Z","published":"2023-09-20T12:35:19Z","title":"Sequence-to-Sequence Spanish Pre-trained Language Models","summary":"  In recent years, significant advancements in pre-trained language models have\ndriven the creation of numerous non-English language variants, with a\nparticular emphasis on encoder-only and decoder-only architectures. While\nSpanish language models based on BERT and GPT have demonstrated proficiency in\nnatural language understanding and generation, there remains a noticeable\nscarcity of encoder-decoder models explicitly designed for sequence-to-sequence\ntasks, which aim to map input sequences to generate output sequences\nconditionally. This paper breaks new ground by introducing the implementation\nand evaluation of renowned encoder-decoder architectures exclusively\npre-trained on Spanish corpora. Specifically, we present Spanish versions of\nBART, T5, and BERT2BERT-style models and subject them to a comprehensive\nassessment across various sequence-to-sequence tasks, including summarization,\nquestion answering, split-and-rephrase, dialogue, and translation. Our findings\nunderscore the competitive performance of all models, with the BART- and\nT5-based models emerging as top performers across all tasks. We have made all\nmodels publicly available to the research community to foster future\nexplorations and advancements in Spanish NLP:\nhttps://github.com/vgaraujov/Seq2Seq-Spanish-PLMs.\n","authors":["Vladimir Araujo","Maria Mihaela Trusca","Rodrigo Tufiño","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2309.11259v2.pdf","comment":"Accepted paper at LREC-Coling2024"},{"id":"http://arxiv.org/abs/2306.00618v2","updated":"2024-03-21T13:37:23Z","published":"2023-06-01T12:44:33Z","title":"Effective Structured Prompting by Meta-Learning and Representative\n  Verbalizer","summary":"  Prompt tuning for pre-trained masked language models (MLM) has shown\npromising performance in natural language processing tasks with few labeled\nexamples. It tunes a prompt for the downstream task, and a verbalizer is used\nto bridge the predicted token and label prediction. Due to the limited training\ndata, prompt initialization is crucial for prompt tuning. Recently,\nMetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared\ninitialization for all task-specific prompts. However, a single initialization\nis insufficient to obtain good prompts for all tasks and samples when the tasks\nare complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a\nheavy burden on computation and memory as the MLM is usually large. To address\nthese issues, we use a prompt pool to extract more task knowledge and construct\ninstance-dependent prompts via attention. We further propose a novel soft\nverbalizer (RepVerb) which constructs label embedding from feature embeddings\ndirectly. Combining meta-learning the prompt pool and RepVerb, we propose\nMetaPrompter for effective structured prompting. MetaPrompter is\nparameter-efficient as only the pool is required to be tuned. Experimental\nresults demonstrate that MetaPrompter performs better than the recent\nstate-of-the-arts and RepVerb outperforms existing soft verbalizers.\n","authors":["Weisen Jiang","Yu Zhang","James T. Kwok"],"pdf_url":"https://arxiv.org/pdf/2306.00618v2.pdf","comment":"Accepted at ICML 2023"},{"id":"http://arxiv.org/abs/2403.14392v1","updated":"2024-03-21T13:33:00Z","published":"2024-03-21T13:33:00Z","title":"A Bag of Tricks for Few-Shot Class-Incremental Learning","summary":"  We present a bag of tricks framework for few-shot class-incremental learning\n(FSCIL), which is a challenging form of continual learning that involves\ncontinuous adaptation to new tasks with limited samples. FSCIL requires both\nstability and adaptability, i.e., preserving proficiency in previously learned\ntasks while learning new ones. Our proposed bag of tricks brings together eight\nkey and highly influential techniques that improve stability, adaptability, and\noverall performance under a unified framework for FSCIL. We organize these\ntricks into three categories: stability tricks, adaptability tricks, and\ntraining tricks. Stability tricks aim to mitigate the forgetting of previously\nlearned classes by enhancing the separation between the embeddings of learned\nclasses and minimizing interference when learning new ones. On the other hand,\nadaptability tricks focus on the effective learning of new classes. Finally,\ntraining tricks improve the overall performance without compromising stability\nor adaptability. We perform extensive experiments on three benchmark datasets,\nCIFAR-100, CUB-200, and miniIMageNet, to evaluate the impact of our proposed\nframework. Our detailed analysis shows that our approach substantially improves\nboth stability and adaptability, establishing a new state-of-the-art by\noutperforming prior works in the area. We believe our method provides a go-to\nsolution and establishes a robust baseline for future research in this area.\n","authors":["Shuvendu Roy","Chunjong Park","Aldi Fahrezi","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2403.14392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03788v3","updated":"2024-03-21T13:30:22Z","published":"2023-02-07T22:56:58Z","title":"Toward a Theory of Causation for Interpreting Neural Code Models","summary":"  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly\nprogressing from research prototypes to commercial developer tools. As such,\nunderstanding the capabilities and limitations of such models is becoming\ncritical. However, the abilities of these models are typically measured using\nautomated metrics that often only reveal a portion of their real-world\nperformance. While, in general, the performance of NCMs appears promising,\ncurrently much is unknown about how such models arrive at decisions. To this\nend, this paper introduces $do_{code}$, a post hoc interpretability method\nspecific to NCMs that is capable of explaining model predictions. $do_{code}$\nis based upon causal inference to enable programming language-oriented\nexplanations. While the theoretical underpinnings of $do_{code}$ are extensible\nto exploring different model properties, we provide a concrete instantiation\nthat aims to mitigate the impact of spurious correlations by grounding\nexplanations of model behavior in properties of programming languages. To\ndemonstrate the practical benefit of $do_{code}$, we illustrate the insights\nthat our framework can provide by performing a case study on two popular deep\nlearning architectures and ten NCMs. The results of this case study illustrate\nthat our studied NCMs are sensitive to changes in code syntax. All our NCMs,\nexcept for the BERT-like model, statistically learn to predict tokens related\nto blocks of code (\\eg brackets, parenthesis, semicolon) with less confounding\nbias as compared to other programming language constructs. These insights\ndemonstrate the potential of $do_{code}$ as a useful method to detect and\nfacilitate the elimination of confounding bias in NCMs.\n","authors":["David N. Palacio","Alejandro Velasco","Nathan Cooper","Alvaro Rodriguez","Kevin Moran","Denys Poshyvanyk"],"pdf_url":"https://arxiv.org/pdf/2302.03788v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12648v3","updated":"2024-03-21T13:23:44Z","published":"2024-01-23T10:56:01Z","title":"Consistency Enhancement-Based Deep Multiview Clustering via Contrastive\n  Learning","summary":"  Multiview clustering (MVC) segregates data samples into meaningful clusters\nby synthesizing information across multiple views. Moreover, deep\nlearning-based methods have demonstrated their strong feature learning\ncapabilities in MVC scenarios. However, effectively generalizing feature\nrepresentations while maintaining consistency is still an intractable problem.\nIn addition, most existing deep clustering methods based on contrastive\nlearning overlook the consistency of the clustering representations during the\nclustering process. In this paper, we show how the above problems can be\novercome and propose a consistent enhancement-based deep MVC method via\ncontrastive learning (CCEC). Specifically, semantic connection blocks are\nincorporated into a feature representation to preserve the consistent\ninformation among multiple views. Furthermore, the representation process for\nclustering is enhanced through spectral clustering, and the consistency across\nmultiple views is improved. Experiments conducted on five datasets demonstrate\nthe effectiveness and superiority of our method in comparison with the\nstate-of-the-art (SOTA) methods. The code for this method can be accessed at\nhttps://anonymous.4open.science/r/CCEC-E84E/.\n","authors":["Hao Yang","Hua Mao","Wai Lok Woo","Jie Chen","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2401.12648v3.pdf","comment":"There are multiple errors that need to be corrected, including some\n  formulas and concept descriptions. We will re upload the paper after the\n  modifications are completed"},{"id":"http://arxiv.org/abs/2403.14385v1","updated":"2024-03-21T13:21:33Z","published":"2024-03-21T13:21:33Z","title":"Estimating Causal Effects with Double Machine Learning -- A Method\n  Evaluation","summary":"  The estimation of causal effects with observational data continues to be a\nvery active research area. In recent years, researchers have developed new\nframeworks which use machine learning to relax classical assumptions necessary\nfor the estimation of causal effects. In this paper, we review one of the most\nprominent methods - \"double/debiased machine learning\" (DML) - and empirically\nevaluate it by comparing its performance on simulated data relative to more\ntraditional statistical methods, before applying it to real-world data. Our\nfindings indicate that the application of a suitably flexible machine learning\nalgorithm within DML improves the adjustment for various nonlinear confounding\nrelationships. This advantage enables a departure from traditional functional\nform assumptions typically necessary in causal effect estimation. However, we\ndemonstrate that the method continues to critically depend on standard\nassumptions about causal structure and identification. When estimating the\neffects of air pollution on housing prices in our application, we find that DML\nestimates are consistently larger than estimates of less flexible methods. From\nour overall results, we provide actionable recommendations for specific choices\nresearchers must make when applying DML in practice.\n","authors":["Jonathan Fuhr","Philipp Berens","Dominik Papies"],"pdf_url":"https://arxiv.org/pdf/2403.14385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13502v2","updated":"2024-03-21T13:18:47Z","published":"2024-03-20T10:59:06Z","title":"Adversarial Attacks and Defenses in Automated Control Systems: A\n  Comprehensive Benchmark","summary":"  Integrating machine learning into Automated Control Systems (ACS) enhances\ndecision-making in industrial process management. One of the limitations to the\nwidespread adoption of these technologies in industry is the vulnerability of\nneural networks to adversarial attacks. This study explores the threats in\ndeploying deep learning models for fault diagnosis in ACS using the Tennessee\nEastman Process dataset. By evaluating three neural networks with different\narchitectures, we subject them to six types of adversarial attacks and explore\nfive different defense methods. Our results highlight the strong vulnerability\nof models to adversarial samples and the varying effectiveness of defense\nstrategies. We also propose a novel protection approach by combining multiple\ndefense methods and demonstrate it's efficacy. This research contributes\nseveral insights into securing machine learning within ACS, ensuring robust\nfault diagnosis in industrial processes.\n","authors":["Vitaliy Pozdnyakov","Aleksandr Kovalenko","Ilya Makarov","Mikhail Drobyshevskiy","Kirill Lukyanov"],"pdf_url":"https://arxiv.org/pdf/2403.13502v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13395v3","updated":"2024-03-21T13:16:27Z","published":"2023-01-31T04:03:28Z","title":"Learning to Solve Integer Linear Programs with Davis-Yin Splitting","summary":"  In many applications, a combinatorial problem must be repeatedly solved with\nsimilar, but distinct parameters. Yet, the parameters $w$ are not directly\nobserved; only contextual data $d$ that correlates with $w$ is available. It is\ntempting to use a neural network to predict $w$ given $d$. However, training\nsuch a model requires reconciling the discrete nature of combinatorial\noptimization with the gradient-based frameworks used to train neural networks.\nWhen the problem in question is an Integer Linear Program (ILP), one approach\nto overcome this training issue is to consider a continuous relaxation of the\ncombinatorial problem. While existing methods utilizing this approach have\nshown to be highly effective on small problems, they do not always scale well\nto large problems. In this work, we draw on ideas from modern convex\noptimization to design a network and training scheme which scales effortlessly\nto problems with thousands of variables. Our experiments verify the\ncomputational advantage our proposed method enjoys on two representative\nproblems, namely the shortest path problem and the knapsack problem.\n","authors":["Daniel McKenzie","Samy Wu Fung","Howard Heaton"],"pdf_url":"https://arxiv.org/pdf/2301.13395v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14379v1","updated":"2024-03-21T13:12:33Z","published":"2024-03-21T13:12:33Z","title":"Tensor network compressibility of convolutional models","summary":"  Convolutional neural networks (CNNs) represent one of the most widely used\nneural network architectures, showcasing state-of-the-art performance in\ncomputer vision tasks. Although larger CNNs generally exhibit higher accuracy,\ntheir size can be effectively reduced by \"tensorization\" while maintaining\naccuracy. Tensorization consists of replacing the convolution kernels with\ncompact decompositions such as Tucker, Canonical Polyadic decompositions, or\nquantum-inspired decompositions such as matrix product states, and directly\ntraining the factors in the decompositions to bias the learning towards\nlow-rank decompositions. But why doesn't tensorization seem to impact the\naccuracy adversely? We explore this by assessing how truncating the convolution\nkernels of dense (untensorized) CNNs impact their accuracy. Specifically, we\ntruncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50\npre-trained for image classification on CIFAR-10 and CIFAR-100 datasets. We\nfound that kernels (especially those inside deeper layers) could often be\ntruncated along several cuts resulting in significant loss in kernel norm but\nnot in classification accuracy. This suggests that such ``correlation\ncompression'' (underlying tensorization) is an intrinsic feature of how\ninformation is encoded in dense CNNs. We also found that aggressively truncated\nmodels could often recover the pre-truncation accuracy after only a few epochs\nof re-training, suggesting that compressing the internal correlations of\nconvolution layers does not often transport the model to a worse minimum. Our\nresults can be applied to tensorize and compress CNN models more effectively.\n","authors":["Sukhbinder Singh","Saeed S. Jahromi","Roman Orus"],"pdf_url":"https://arxiv.org/pdf/2403.14379v1.pdf","comment":"20 pages, 21 images"},{"id":"http://arxiv.org/abs/2403.14377v1","updated":"2024-03-21T13:09:23Z","published":"2024-03-21T13:09:23Z","title":"Knowledge-Enhanced Recommendation with User-Centric Subgraph Network","summary":"  Recommendation systems, as widely implemented nowadays on various platforms,\nrecommend relevant items to users based on their preferences. The classical\nmethods which rely on user-item interaction matrices has limitations,\nespecially in scenarios where there is a lack of interaction data for new\nitems. Knowledge graph (KG)-based recommendation systems have emerged as a\npromising solution. However, most KG-based methods adopt node embeddings, which\ndo not provide personalized recommendations for different users and cannot\ngeneralize well to the new items. To address these limitations, we propose\nKnowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning\napproach with graph neural network (GNN) for effective recommendation. KUCNet\nconstructs a U-I subgraph for each user-item pair that captures both the\nhistorical information of user-item interactions and the side information\nprovided in KG. An attention-based GNN is designed to encode the U-I subgraphs\nfor recommendation. Considering efficiency, the pruned user-centric computation\ngraph is further introduced such that multiple U-I subgraphs can be\nsimultaneously computed and that the size can be pruned by Personalized\nPageRank. Our proposed method achieves accurate, efficient, and interpretable\nrecommendations especially for new items. Experimental results demonstrate the\nsuperiority of KUCNet over state-of-the-art KG-based and collaborative\nfiltering (CF)-based methods.\n","authors":["Guangyi Liu","Quanming Yao","Yongqi Zhang","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14371v1","updated":"2024-03-21T12:59:24Z","published":"2024-03-21T12:59:24Z","title":"Loop Improvement: An Efficient Approach for Extracting Shared Features\n  from Heterogeneous Data without Central Server","summary":"  In federated learning, data heterogeneity significantly impacts performance.\nA typical solution involves segregating these parameters into shared and\npersonalized components, a concept also relevant in multi-task learning.\nAddressing this, we propose \"Loop Improvement\" (LI), a novel method enhancing\nthis separation and feature extraction without necessitating a central server\nor data interchange among participants. Our experiments reveal LI's superiority\nin several aspects: In personalized federated learning environments, LI\nconsistently outperforms the advanced FedALA algorithm in accuracy across\ndiverse scenarios. Additionally, LI's feature extractor closely matches the\nperformance achieved when aggregating data from all clients. In global model\ncontexts, employing LI with stacked personalized layers and an additional\nnetwork also yields comparable results to combined client data scenarios.\nFurthermore, LI's adaptability extends to multi-task learning, streamlining the\nextraction of common features across tasks and obviating the need for\nsimultaneous training. This approach not only enhances individual task\nperformance but also achieves accuracy levels on par with classic multi-task\nlearning methods where all tasks are trained simultaneously. LI integrates a\nloop topology with layer-wise and end-to-end training, compatible with various\nneural network models. This paper also delves into the theoretical\nunderpinnings of LI's effectiveness, offering insights into its potential\napplications. The code is on https://github.com/axedge1983/LI\n","authors":["Fei Li","Chu Kiong Loo","Wei Shiung Liew","Xiaofeng Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14371v1.pdf","comment":"11 pages, 11 figures"},{"id":"http://arxiv.org/abs/2310.10404v6","updated":"2024-03-21T12:59:04Z","published":"2023-10-16T13:49:46Z","title":"LLM4SGG: Large Language Model for Weakly Supervised Scene Graph\n  Generation","summary":"  Weakly-Supervised Scene Graph Generation (WSSGG) research has recently\nemerged as an alternative to the fully-supervised approach that heavily relies\non costly annotations. In this regard, studies on WSSGG have utilized image\ncaptions to obtain unlocalized triplets while primarily focusing on grounding\nthe unlocalized triplets over image regions. However, they have overlooked the\ntwo issues involved in the triplet formation process from the captions: 1)\nSemantic over-simplification issue arises when extracting triplets from\ncaptions, where fine-grained predicates in captions are undesirably converted\ninto coarse-grained predicates, resulting in a long-tailed predicate\ndistribution, and 2) Low-density scene graph issue arises when aligning the\ntriplets in the caption with entity/predicate classes of interest, where many\ntriplets are discarded and not used in training, leading to insufficient\nsupervision. To tackle the two issues, we propose a new approach, i.e., Large\nLanguage Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two\nissues by leveraging the LLM's in-depth understanding of language and reasoning\nability during the extraction of triplets from captions and alignment of\nentity/predicate classes with target data. To further engage the LLM in these\nprocesses, we adopt the idea of Chain-of-Thought and the in-context few-shot\nlearning strategy. To validate the effectiveness of LLM4SGG, we conduct\nextensive experiments on Visual Genome and GQA datasets, showing significant\nimprovements in both Recall@K and mean Recall@K compared to the\nstate-of-the-art WSSGG methods. A further appeal is that LLM4SGG is\ndata-efficient, enabling effective model training with a small amount of\ntraining images.\n","authors":["Kibum Kim","Kanghoon Yoon","Jaehyeong Jeon","Yeonjun In","Jinyoung Moon","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2310.10404v6.pdf","comment":"8 pages; CVPR 2024"},{"id":"http://arxiv.org/abs/2310.03054v3","updated":"2024-03-21T12:43:34Z","published":"2023-10-04T11:40:02Z","title":"Posterior Sampling Based on Gradient Flows of the MMD with Negative\n  Distance Kernel","summary":"  We propose conditional flows of the maximum mean discrepancy (MMD) with the\nnegative distance kernel for posterior sampling and conditional generative\nmodeling. This MMD, which is also known as energy distance, has several\nadvantageous properties like efficient computation via slicing and sorting. We\napproximate the joint distribution of the ground truth and the observations\nusing discrete Wasserstein gradient flows and establish an error bound for the\nposterior distributions. Further, we prove that our particle flow is indeed a\nWasserstein gradient flow of an appropriate functional. The power of our method\nis demonstrated by numerical examples including conditional image generation\nand inverse problems like superresolution, inpainting and computed tomography\nin low-dose and limited-angle settings.\n","authors":["Paul Hagemann","Johannes Hertrich","Fabian Altekrüger","Robert Beinert","Jannis Chemseddine","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2310.03054v3.pdf","comment":"Published as a conference paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2403.14359v1","updated":"2024-03-21T12:40:41Z","published":"2024-03-21T12:40:41Z","title":"Varroa destructor detection on honey bees using hyperspectral imagery","summary":"  Hyperspectral (HS) imagery in agriculture is becoming increasingly common.\nThese images have the advantage of higher spectral resolution. Advanced\nspectral processing techniques are required to unlock the information potential\nin these HS images. The present paper introduces a method rooted in\nmultivariate statistics designed to detect parasitic Varroa destructor mites on\nthe body of western honey bee Apis mellifera, enabling easier and continuous\nmonitoring of the bee hives. The methodology explores unsupervised (K-means++)\nand recently developed supervised (Kernel Flows - Partial Least-Squares,\nKF-PLS) methods for parasitic identification. Additionally, in light of the\nemergence of custom-band multispectral cameras, the present research outlines a\nstrategy for identifying the specific wavelengths necessary for effective\nbee-mite separation, suitable for implementation in a custom-band camera.\nIllustrated with a real-case dataset, our findings demonstrate that as few as\nfour spectral bands are sufficient for accurate parasite identification.\n","authors":["Zina-Sabrina Duma","Tomas Zemcik","Simon Bilik","Tuomas Sihvonen","Peter Honec","Satu-Pia Reinikainen","Karel Horak"],"pdf_url":"https://arxiv.org/pdf/2403.14359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14358v1","updated":"2024-03-21T12:37:54Z","published":"2024-03-21T12:37:54Z","title":"Exploring the Potential of Large Language Models in Graph Generation","summary":"  Large language models (LLMs) have achieved great success in many fields, and\nrecent works have studied exploring LLMs for graph discriminative tasks such as\nnode classification. However, the abilities of LLMs for graph generation remain\nunexplored in the literature. Graph generation requires the LLM to generate\ngraphs with given properties, which has valuable real-world applications such\nas drug discovery, while tends to be more challenging. In this paper, we\npropose LLM4GraphGen to explore the ability of LLMs for graph generation with\nsystematical task designs and extensive experiments. Specifically, we propose\nseveral tasks tailored with comprehensive experiments to address key questions\nregarding LLMs' understanding of different graph structure rules, their ability\nto capture structural type distributions, and their utilization of domain\nknowledge for property-based graph generation. Our evaluations demonstrate that\nLLMs, particularly GPT-4, exhibit preliminary abilities in graph generation\ntasks, including rule-based and distribution-based generation. We also observe\nthat popular prompting methods, such as few-shot and chain-of-thought\nprompting, do not consistently enhance performance. Besides, LLMs show\npotential in generating molecules with specific properties. These findings may\nserve as foundations for designing good LLMs based models for graph generation\nand provide valuable insights and further research.\n","authors":["Yang Yao","Xin Wang","Zeyang Zhang","Yijian Qin","Ziwei Zhang","Xu Chu","Yuekui Yang","Wenwu Zhu","Hong Mei"],"pdf_url":"https://arxiv.org/pdf/2403.14358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13927v2","updated":"2024-03-21T12:37:21Z","published":"2023-12-21T15:22:07Z","title":"On the convergence of loss and uncertainty-based active learning\n  algorithms","summary":"  We consider the convergence rates of loss and uncertainty-based active\nlearning algorithms under various assumptions. Firstly, we establish a set of\nconditions that ensure convergence rates when applied to linear classifiers and\nlinearly separable datasets. This includes demonstrating convergence rate\nguarantees for loss-based sampling with various loss functions. Secondly, we\nintroduce a framework that allows us to derive convergence rate bounds for\nloss-based sampling by leveraging known convergence rate bounds for stochastic\ngradient descent algorithms. Lastly, we propose a new algorithm that combines\npoint sampling and stochastic Polyak's step size. We establish a condition on\nthe sampling process, ensuring a convergence rate guarantee for this algorithm,\nparticularly in the case of smooth convex loss functions. Our numerical results\nshowcase the efficiency of the proposed algorithm.\n","authors":["Daniel Haimovich","Dima Karamshuk","Fridolin Linder","Niek Tax","Milan Vojnovic"],"pdf_url":"https://arxiv.org/pdf/2312.13927v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01753v2","updated":"2024-03-21T12:36:22Z","published":"2023-11-03T07:18:36Z","title":"RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value\n  Factorization","summary":"  Multi-agent systems are characterized by environmental uncertainty, varying\npolicies of agents, and partial observability, which result in significant\nrisks. In the context of Multi-Agent Reinforcement Learning (MARL), learning\ncoordinated and decentralized policies that are sensitive to risk is\nchallenging. To formulate the coordination requirements in risk-sensitive MARL,\nwe introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a\ngeneralization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM)\nprinciples. This principle requires that the collection of risk-sensitive\naction selections of each agent should be equivalent to the risk-sensitive\naction selection of the central policy. Current MARL value factorization\nmethods do not satisfy the RIGM principle for common risk metrics such as the\nValue at Risk (VaR) metric or distorted risk measurements. Therefore, we\npropose RiskQ to address this limitation, which models the joint return\ndistribution by modeling quantiles of it as weighted quantile mixtures of\nper-agent return distribution utilities. RiskQ satisfies the RIGM principle for\nthe VaR and distorted risk metrics. We show that RiskQ can obtain promising\nperformance through extensive experiments. The source code of RiskQ is\navailable in https://github.com/xmu-rl-3dv/RiskQ.\n","authors":["Siqi Shen","Chennan Ma","Chao Li","Weiquan Liu","Yongquan Fu","Songzhu Mei","Xinwang Liu","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2311.01753v2.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2403.14356v1","updated":"2024-03-21T12:35:46Z","published":"2024-03-21T12:35:46Z","title":"DomainLab: A modular Python package for domain generalization in deep\n  learning","summary":"  Poor generalization performance caused by distribution shifts in unseen\ndomains often hinders the trustworthy deployment of deep neural networks. Many\ndomain generalization techniques address this problem by adding a domain\ninvariant regularization loss terms during training. However, there is a lack\nof modular software that allows users to combine the advantages of different\nmethods with minimal effort for reproducibility. DomainLab is a modular Python\npackage for training user specified neural networks with composable\nregularization loss terms. Its decoupled design allows the separation of neural\nnetworks from regularization loss construction. Hierarchical combinations of\nneural networks, different domain generalization methods, and associated\nhyperparameters, can all be specified together with other experimental setup in\na single configuration file. Hierarchical combinations of neural networks,\ndifferent domain generalization methods, and associated hyperparameters, can\nall be specified together with other experimental setup in a single\nconfiguration file. In addition, DomainLab offers powerful benchmarking\nfunctionality to evaluate the generalization performance of neural networks in\nout-of-distribution data. The package supports running the specified benchmark\non an HPC cluster or on a standalone machine. The package is well tested with\nover 95 percent coverage and well documented. From the user perspective, it is\nclosed to modification but open to extension. The package is under the MIT\nlicense, and its source code, tutorial and documentation can be found at\nhttps://github.com/marrlab/DomainLab.\n","authors":["Xudong Sun","Carla Feistner","Alexej Gossmann","George Schwarz","Rao Muhammad Umer","Lisa Beer","Patrick Rockenschaub","Rahul Babu Shrestha","Armin Gruber","Nutan Chen","Sayedali Shetab Boushehri","Florian Buettner","Carsten Marr"],"pdf_url":"https://arxiv.org/pdf/2403.14356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11624v3","updated":"2024-03-21T12:34:14Z","published":"2023-01-27T09:57:36Z","title":"Neural Wasserstein Gradient Flows for Maximum Mean Discrepancies with\n  Riesz Kernels","summary":"  Wasserstein gradient flows of maximum mean discrepancy (MMD) functionals with\nnon-smooth Riesz kernels show a rich structure as singular measures can become\nabsolutely continuous ones and conversely. In this paper we contribute to the\nunderstanding of such flows. We propose to approximate the backward scheme of\nJordan, Kinderlehrer and Otto for computing such Wasserstein gradient flows as\nwell as a forward scheme for so-called Wasserstein steepest descent flows by\nneural networks (NNs). Since we cannot restrict ourselves to absolutely\ncontinuous measures, we have to deal with transport plans and velocity plans\ninstead of usual transport maps and velocity fields. Indeed, we approximate the\ndisintegration of both plans by generative NNs which are learned with respect\nto appropriate loss functions. In order to evaluate the quality of both neural\nschemes, we benchmark them on the interaction energy. Here we provide analytic\nformulas for Wasserstein schemes starting at a Dirac measure and show their\nconvergence as the time step size tends to zero. Finally, we illustrate our\nneural MMD flows by numerical examples.\n","authors":["Fabian Altekrüger","Johannes Hertrich","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2301.11624v3.pdf","comment":"Accepted at ICML 2023"},{"id":"http://arxiv.org/abs/2310.14525v2","updated":"2024-03-21T12:32:53Z","published":"2023-10-23T03:15:57Z","title":"Graph Ranking Contrastive Learning: A Extremely Simple yet Efficient\n  Method","summary":"  Graph contrastive learning (GCL) has emerged as a representative graph\nself-supervised method, achieving significant success. The currently prevalent\noptimization objective for GCL is InfoNCE. Typically, it employs augmentation\ntechniques to obtain two views, where a node in one view acts as the anchor,\nthe corresponding node in the other view serves as the positive sample, and all\nother nodes are regarded as negative samples. The goal is to minimize the\ndistance between the anchor node and positive samples and maximize the distance\nto negative samples. However, due to the lack of label information during\ntraining, InfoNCE inevitably treats samples from the same class as negative\nsamples, leading to the issue of false negative samples. This can impair the\nlearned node representations and subsequently hinder performance in downstream\ntasks. While numerous methods have been proposed to mitigate the impact of\nfalse negatives, they still face various challenges. For instance, while\nincreasing the number of negative samples can dilute the impact of false\nnegatives, it concurrently increases computational burden. Thus, we propose\nGraphRank, a simple yet efficient graph contrastive learning method that\naddresses the problem of false negative samples by redefining the concept of\nnegative samples to a certain extent, thereby avoiding the issue of false\nnegative samples. The effectiveness of GraphRank is empirically validated\nthrough experiments on the node, edge, and graph level tasks.\n","authors":["Yulan Hu","Sheng Ouyang","Jingyu Liu","Ge Chen","Zhirui Yang","Junchen Wan","Fuzheng Zhang","Zhongyuan Wang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2310.14525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06906v2","updated":"2024-03-21T12:30:16Z","published":"2024-03-11T16:57:20Z","title":"Cost-Sensitive Learning to Defer to Multiple Experts with Workload\n  Constraints","summary":"  Learning to defer (L2D) aims to improve human-AI collaboration systems by\nlearning how to defer decisions to humans when they are more likely to be\ncorrect than an ML classifier. Existing research in L2D overlooks key aspects\nof real-world systems that impede its practical adoption, namely: i) neglecting\ncost-sensitive scenarios, where type 1 and type 2 errors have different costs;\nii) requiring concurrent human predictions for every instance of the training\ndataset and iii) not dealing with human work capacity constraints. To address\nthese issues, we propose the deferral under cost and capacity constraints\nframework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised\nlearning to model the probability of human error under less restrictive data\nrequirements (only one expert prediction per instance) and using constraint\nprogramming to globally minimize the error cost subject to workload\nlimitations. We test DeCCaF in a series of cost-sensitive fraud detection\nscenarios with different teams of 9 synthetic fraud analysts, with individual\nwork capacity constraints. The results demonstrate that our approach performs\nsignificantly better than the baselines in a wide array of scenarios, achieving\nan average 8.4% reduction in the misclassification cost.\n","authors":["Jean V. Alves","Diogo Leitão","Sérgio Jesus","Marco O. P. Sampaio","Javier Liébana","Pedro Saleiro","Mário A. T. Figueiredo","Pedro Bizarro"],"pdf_url":"https://arxiv.org/pdf/2403.06906v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14353v1","updated":"2024-03-21T12:28:44Z","published":"2024-03-21T12:28:44Z","title":"DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video\n  Analytics","summary":"  Deep neural network (DNN) video analytics is crucial for autonomous systems\nsuch as self-driving vehicles, unmanned aerial vehicles (UAVs), and security\nrobots. However, real-world deployment faces challenges due to their limited\ncomputational resources and battery power. To tackle these challenges,\ncontinuous learning exploits a lightweight \"student\" model at deployment\n(inference), leverages a larger \"teacher\" model for labeling sampled data\n(labeling), and continuously retrains the student model to adapt to changing\nscenarios (retraining). This paper highlights the limitations in\nstate-of-the-art continuous learning systems: (1) they focus on computations\nfor retraining, while overlooking the compute needs for inference and labeling,\n(2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous\nsystems, and (3) they are located on a remote centralized server, intended for\nmulti-tenant scenarios, again unsuitable for autonomous systems due to privacy,\nnetwork availability, and latency concerns. We propose a hardware-algorithm\nco-designed solution for continuous learning, DaCapo, that enables autonomous\nsystems to perform concurrent executions of inference, labeling, and training\nin a performant and energy-efficient manner. DaCapo comprises (1) a\nspatially-partitionable and precision-flexible accelerator enabling parallel\nexecution of kernels on sub-accelerators at their respective precisions, and\n(2) a spatiotemporal resource allocation algorithm that strategically navigates\nthe resource-accuracy tradeoff space, facilitating optimal decisions for\nresource allocation to achieve maximal accuracy. Our evaluation shows that\nDaCapo achieves 6.5% and 5.5% higher accuracy than a state-of-the-art GPU-based\ncontinuous learning systems, Ekya and EOMU, respectively, while consuming 254x\nless power.\n","authors":["Yoonsung Kim","Changhun Oh","Jinwoo Hwang","Wonung Kim","Seongryong Oh","Yubin Lee","Hardik Sharma","Amir Yazdanbakhsh","Jongse Park"],"pdf_url":"https://arxiv.org/pdf/2403.14353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14340v1","updated":"2024-03-21T12:14:02Z","published":"2024-03-21T12:14:02Z","title":"Exploring Task Unification in Graph Representation Learning via\n  Generative Approach","summary":"  Graphs are ubiquitous in real-world scenarios and encompass a diverse range\nof tasks, from node-, edge-, and graph-level tasks to transfer learning.\nHowever, designing specific tasks for each type of graph data is often costly\nand lacks generalizability. Recent endeavors under the \"Pre-training +\nFine-tuning\" or \"Pre-training + Prompt\" paradigms aim to design a unified\nframework capable of generalizing across multiple graph tasks. Among these,\ngraph autoencoders (GAEs), generative self-supervised models, have demonstrated\ntheir potential in effectively addressing various graph tasks. Nevertheless,\nthese methods typically employ multi-stage training and require adaptive\ndesigns, which on one hand make it difficult to be seamlessly applied to\ndiverse graph tasks and on the other hand overlook the negative impact caused\nby discrepancies in task objectives between the different stages. To address\nthese challenges, we propose GA^2E, a unified adversarially masked autoencoder\ncapable of addressing the above challenges seamlessly. Specifically, GA^2E\nproposes to use the subgraph as the meta-structure, which remains consistent\nacross all graph tasks (ranging from node-, edge-, and graph-level to transfer\nlearning) and all stages (both during training and inference). Further, GA^2E\noperates in a \\textbf{\"Generate then Discriminate\"} manner. It leverages the\nmasked GAE to reconstruct the input subgraph whilst treating it as a generator\nto compel the reconstructed graphs resemble the input subgraph. Furthermore,\nGA^2E introduces an auxiliary discriminator to discern the authenticity between\nthe reconstructed (generated) subgraph and the input subgraph, thus ensuring\nthe robustness of the graph representation through adversarial training\nmechanisms. We validate GA^2E's capabilities through extensive experiments on\n21 datasets across four types of graph tasks.\n","authors":["Yulan Hu","Sheng Ouyang","Zhirui Yang","Ge Chen","Junchen Wan","Xiao Wang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2403.14340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14339v1","updated":"2024-03-21T12:11:26Z","published":"2024-03-21T12:11:26Z","title":"$\\nabla τ$: Gradient-based and Task-Agnostic machine Unlearning","summary":"  Machine Unlearning, the process of selectively eliminating the influence of\ncertain data examples used during a model's training, has gained significant\nattention as a means for practitioners to comply with recent data protection\nregulations. However, existing unlearning methods face critical drawbacks,\nincluding their prohibitively high cost, often associated with a large number\nof hyperparameters, and the limitation of forgetting only relatively small data\nportions. This often makes retraining the model from scratch a quicker and more\neffective solution. In this study, we introduce Gradient-based and\nTask-Agnostic machine Unlearning ($\\nabla \\tau$), an optimization framework\ndesigned to remove the influence of a subset of training data efficiently. It\napplies adaptive gradient ascent to the data to be forgotten while using\nstandard gradient descent for the remaining data. $\\nabla \\tau$ offers multiple\nbenefits over existing approaches. It enables the unlearning of large sections\nof the training dataset (up to 30%). It is versatile, supporting various\nunlearning tasks (such as subset forgetting or class removal) and applicable\nacross different domains (images, text, etc.). Importantly, $\\nabla \\tau$\nrequires no hyperparameter adjustments, making it a more appealing option than\nretraining the model from scratch. We evaluate our framework's effectiveness\nusing a set of well-established Membership Inference Attack metrics,\ndemonstrating up to 10% enhancements in performance compared to\nstate-of-the-art methods without compromising the original model's accuracy.\n","authors":["Daniel Trippa","Cesare Campagnano","Maria Sofia Bucarelli","Gabriele Tolomei","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2403.14339v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.09780v3","updated":"2024-03-21T12:05:47Z","published":"2023-03-17T05:27:16Z","title":"Mpox-AISM: AI-Mediated Super Monitoring for Mpox and Like-Mpox","summary":"  The key to preventing the spread of mpox (monkeypox) lies in timely,\nconvenient, and accurate diagnosis for earlier-stage infected individuals.\nUnfortunately, the resemblances between common skin diseases and mpox and the\nneed for professional diagnosis inevitably deteriorated the diagnosis of\nearlier-stage patients with Mpox and contributed to its widespread outbreak in\ncrowded areas. Here, we proposed a real-time visualization strategy called\n\"Super Monitoring\" using artificial intelligence and Internet technology,\nthereby performing a low-cost, convenient, timely, and unspecialized diagnosis\nfor earlier-stage mpox. Specifically, such AI-mediated \"super monitoring\"\n(Mpox-AISM) invokes a framework assembled by deep learning models, data\naugmentation, self-supervised learning, and cloud services. Verified by\npublicly available datasets, the Precision, Recall, Specificity, and F1-score\nof Mpox-AISM in diagnosing mpox achieved 99.3%, 94.1%, 99.9%, and 96.6%,\nrespectively. Furthermore, Mpox-AISM's overall accuracy reaches 94.51% in\ndiagnosing mpox, six like-mpox skin diseases, and normal skin. We also employed\ngradient-weighted class activation mapping to explain the decision-making\nprocess of Mpox-AISM, thus handily understanding the specific characteristics\nthat may indicate the mpox's onset and improving its reliability. With the help\nof the Internet and communication terminal, Mpox-AISM can perform a real-time,\nlow-cost, and convenient diagnosis for earlier-stage mpox in various real-world\nsettings, thereby effectively curbing the spread of mpox virus.\n","authors":["Yubiao Yue","Minghua Jiang","Xinyue Zhang","Jialong Xu","Huacong Ye","Fan Zhang","Zhenzhang Li","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2303.09780v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14332v1","updated":"2024-03-21T11:57:16Z","published":"2024-03-21T11:57:16Z","title":"A Differentially Private Clustering Algorithm for Well-Clustered Graphs","summary":"  We study differentially private (DP) algorithms for recovering clusters in\nwell-clustered graphs, which are graphs whose vertex set can be partitioned\ninto a small number of sets, each inducing a subgraph of high inner conductance\nand small outer conductance. Such graphs have widespread application as a\nbenchmark in the theoretical analysis of spectral clustering. We provide an\nefficient ($\\epsilon$,$\\delta$)-DP algorithm tailored specifically for such\ngraphs. Our algorithm draws inspiration from the recent work of Chen et al.,\nwho developed DP algorithms for recovery of stochastic block models in cases\nwhere the graph comprises exactly two nearly-balanced clusters. Our algorithm\nworks for well-clustered graphs with $k$ nearly-balanced clusters, and the\nmisclassification ratio almost matches the one of the best-known non-private\nalgorithms. We conduct experimental evaluations on datasets with known ground\ntruth clusters to substantiate the prowess of our algorithm. We also show that\nany (pure) $\\epsilon$-DP algorithm would result in substantial error.\n","authors":["Weiqiang He","Hendrik Fichtenberger","Pan Peng"],"pdf_url":"https://arxiv.org/pdf/2403.14332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14328v1","updated":"2024-03-21T11:54:45Z","published":"2024-03-21T11:54:45Z","title":"Distilling Reinforcement Learning Policies for Interpretable Robot\n  Locomotion: Gradient Boosting Machines and Symbolic Regression","summary":"  Recent advancements in reinforcement learning (RL) have led to remarkable\nachievements in robot locomotion capabilities. However, the complexity and\n``black-box'' nature of neural network-based RL policies hinder their\ninterpretability and broader acceptance, particularly in applications demanding\nhigh levels of safety and reliability. This paper introduces a novel approach\nto distill neural RL policies into more interpretable forms using Gradient\nBoosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic\nRegression. By leveraging the inherent interpretability of generalized additive\nmodels, decision trees, and analytical expressions, we transform opaque neural\nnetwork policies into more transparent ``glass-box'' models. We train expert\nneural network policies using RL and subsequently distill them into (i) GBMs,\n(ii) EBMs, and (iii) symbolic policies. To address the inherent distribution\nshift challenge of behavioral cloning, we propose to use the Dataset\nAggregation (DAgger) algorithm with a curriculum of episode-dependent\nalternation of actions between expert and distilled policies, to enable\nefficient distillation of feedback control policies. We evaluate our approach\non various robot locomotion gaits -- walking, trotting, bounding, and pacing --\nand study the importance of different observations in joint actions for\ndistilled policies using various methods. We train neural expert policies for\n205 hours of simulated experience and distill interpretable policies with only\n10 minutes of simulated interaction for each gait using the proposed method.\n","authors":["Fernando Acero","Zhibin Li"],"pdf_url":"https://arxiv.org/pdf/2403.14328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14327v1","updated":"2024-03-21T11:51:42Z","published":"2024-03-21T11:51:42Z","title":"Investigating the validity of structure learning algorithms in\n  identifying risk factors for intervention in patients with diabetes","summary":"  Diabetes, a pervasive and enduring health challenge, imposes significant\nglobal implications on health, financial healthcare systems, and societal\nwell-being. This study undertakes a comprehensive exploration of various\nstructural learning algorithms to discern causal pathways amongst potential\nrisk factors influencing diabetes progression. The methodology involves the\napplication of these algorithms to relevant diabetes data, followed by the\nconversion of their output graphs into Causal Bayesian Networks (CBNs),\nenabling predictive analysis and the evaluation of discrepancies in the effect\nof hypothetical interventions within our context-specific case study.\n  This study highlights the substantial impact of algorithm selection on\nintervention outcomes. To consolidate insights from diverse algorithms, we\nemploy a model-averaging technique that helps us obtain a unique causal model\nfor diabetes derived from a varied set of structural learning algorithms. We\nalso investigate how each of those individual graphs, as well as the average\ngraph, compare to the structures elicited by a domain expert who categorised\ngraph edges into high confidence, moderate, and low confidence types, leading\ninto three individual graphs corresponding to the three levels of confidence.\n  The resulting causal model and data are made available online, and serve as a\nvaluable resource and a guide for informed decision-making by healthcare\npractitioners, offering a comprehensive understanding of the interactions\nbetween relevant risk factors and the effect of hypothetical interventions.\nTherefore, this research not only contributes to the academic discussion on\ndiabetes, but also provides practical guidance for healthcare professionals in\ndeveloping efficient intervention and risk management strategies.\n","authors":["Sheresh Zahoor","Anthony C. Constantinou","Tim M Curtis","Mohammed Hasanuzzaman"],"pdf_url":"https://arxiv.org/pdf/2403.14327v1.pdf","comment":"20 pages, 17 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.14324v1","updated":"2024-03-21T11:44:25Z","published":"2024-03-21T11:44:25Z","title":"Neural Network-Based Processing and Reconstruction of Compromised\n  Biophotonic Image Data","summary":"  The integration of deep learning techniques with biophotonic setups has\nopened new horizons in bioimaging. A compelling trend in this field involves\ndeliberately compromising certain measurement metrics to engineer better\nbioimaging tools in terms of cost, speed, and form-factor, followed by\ncompensating for the resulting defects through the utilization of deep learning\nmodels trained on a large amount of ideal, superior or alternative data. This\nstrategic approach has found increasing popularity due to its potential to\nenhance various aspects of biophotonic imaging. One of the primary motivations\nfor employing this strategy is the pursuit of higher temporal resolution or\nincreased imaging speed, critical for capturing fine dynamic biological\nprocesses. This approach also offers the prospect of simplifying hardware\nrequirements/complexities, thereby making advanced imaging standards more\naccessible in terms of cost and/or size. This article provides an in-depth\nreview of the diverse measurement aspects that researchers intentionally impair\nin their biophotonic setups, including the point spread function,\nsignal-to-noise ratio, sampling density, and pixel resolution. By deliberately\ncompromising these metrics, researchers aim to not only recuperate them through\nthe application of deep learning networks, but also bolster in return other\ncrucial parameters, such as the field-of-view, depth-of-field, and\nspace-bandwidth product. Here, we discuss various biophotonic methods that have\nsuccessfully employed this strategic approach. These techniques span broad\napplications and showcase the versatility and effectiveness of deep learning in\nthe context of compromised biophotonic data. Finally, by offering our\nperspectives on the future possibilities of this rapidly evolving concept, we\nhope to motivate our readers to explore novel ways of balancing hardware\ncompromises with compensation via AI.\n","authors":["Michael John Fanous","Paloma Casteleiro Costa","Cagatay Isil","Luzhe Huang","Aydogan Ozcan"],"pdf_url":"https://arxiv.org/pdf/2403.14324v1.pdf","comment":"17 Pages, 4 Figures, 1 Table"},{"id":"http://arxiv.org/abs/2403.14302v1","updated":"2024-03-21T11:16:42Z","published":"2024-03-21T11:16:42Z","title":"SpikingResformer: Bridging ResNet and Vision Transformer in Spiking\n  Neural Networks","summary":"  The remarkable success of Vision Transformers in Artificial Neural Networks\n(ANNs) has led to a growing interest in incorporating the self-attention\nmechanism and transformer-based architecture into Spiking Neural Networks\n(SNNs). While existing methods propose spiking self-attention mechanisms that\nare compatible with SNNs, they lack reasonable scaling methods, and the overall\narchitectures proposed by these methods suffer from a bottleneck in effectively\nextracting local features. To address these challenges, we propose a novel\nspiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a\nreasonable scaling method. Based on DSSA, we propose a novel spiking Vision\nTransformer architecture called SpikingResformer, which combines the\nResNet-based multi-stage architecture with our proposed DSSA to improve both\nperformance and energy efficiency while reducing parameters. Experimental\nresults show that SpikingResformer achieves higher accuracy with fewer\nparameters and lower energy consumption than other spiking Vision Transformer\ncounterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on\nImageNet with 4 time-steps, which is the state-of-the-art result in the SNN\nfield.\n","authors":["Xinyu Shi","Zecheng Hao","Zhaofei Yu"],"pdf_url":"https://arxiv.org/pdf/2403.14302v1.pdf","comment":"To be published in the 2024 IEEE/CVF Conference on Computer Vision\n  and Pattern Recognition (CVPR)"},{"id":"http://arxiv.org/abs/2403.14297v1","updated":"2024-03-21T11:03:56Z","published":"2024-03-21T11:03:56Z","title":"Impact Assessment of Missing Data in Model Predictions for Earth\n  Observation Applications","summary":"  Earth observation (EO) applications involving complex and heterogeneous data\nsources are commonly approached with machine learning models. However, there is\na common assumption that data sources will be persistently available. Different\nsituations could affect the availability of EO sources, like noise, clouds, or\nsatellite mission failures. In this work, we assess the impact of missing\ntemporal and static EO sources in trained models across four datasets with\nclassification and regression tasks. We compare the predictive quality of\ndifferent methods and find that some are naturally more robust to missing data.\nThe Ensemble strategy, in particular, achieves a prediction robustness up to\n100%. We evidence that missing scenarios are significantly more challenging in\nregression than classification tasks. Finally, we find that the optical view is\nthe most critical view when it is missing individually.\n","authors":["Francisco Mena","Diego Arenas","Marcela Charfuelan","Marlon Nuske","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.14297v1.pdf","comment":"Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2024"},{"id":"http://arxiv.org/abs/2402.02622v2","updated":"2024-03-21T10:57:40Z","published":"2024-02-04T21:44:09Z","title":"DenseFormer: Enhancing Information Flow in Transformers via Depth\n  Weighted Averaging","summary":"  The transformer architecture by Vaswani et al. (2017) is now ubiquitous\nacross application domains, from natural language processing to speech\nprocessing and image understanding. We propose DenseFormer, a simple\nmodification to the standard architecture that improves the perplexity of the\nmodel without increasing its size -- adding a few thousand parameters for\nlarge-scale models in the 100B parameters range. Our approach relies on an\nadditional averaging step after each transformer block, which computes a\nweighted average of current and past representations -- we refer to this\noperation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit\ncoherent patterns of information flow, revealing the strong and structured\nreuse of activations from distant layers. Experiments demonstrate that\nDenseFormer is more data efficient, reaching the same perplexity of much deeper\ntransformer models, and that for the same perplexity, these new models\noutperform transformer baselines in terms of memory efficiency and inference\ntime.\n","authors":["Matteo Pagliardini","Amirkeivan Mohtashami","Francois Fleuret","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2402.02622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14290v1","updated":"2024-03-21T10:54:21Z","published":"2024-03-21T10:54:21Z","title":"Exploring Green AI for Audio Deepfake Detection","summary":"  The state-of-the-art audio deepfake detectors leveraging deep neural networks\nexhibit impressive recognition performance. Nonetheless, this advantage is\naccompanied by a significant carbon footprint. This is mainly due to the use of\nhigh-performance computing with accelerators and high training time. Studies\nshow that average deep NLP model produces around 626k lbs of\nCO\\textsubscript{2} which is equivalent to five times of average US car\nemission at its lifetime. This is certainly a massive threat to the\nenvironment. To tackle this challenge, this study presents a novel framework\nfor audio deepfake detection that can be seamlessly trained using standard CPU\nresources. Our proposed framework utilizes off-the-shelve self-supervised\nlearning (SSL) based models which are pre-trained and available in public\nrepositories. In contrast to existing methods that fine-tune SSL models and\nemploy additional deep neural networks for downstream tasks, we exploit\nclassical machine learning algorithms such as logistic regression and shallow\nneural networks using the SSL embeddings extracted using the pre-trained model.\nOur approach shows competitive results compared to the commonly used\nhigh-carbon footprint approaches. In experiments with the ASVspoof 2019 LA\ndataset, we achieve a 0.90\\% equal error rate (EER) with less than 1k trainable\nmodel parameters. To encourage further research in this direction and support\nreproducible results, the Python code will be made publicly accessible\nfollowing acceptance. Github: https://github.com/sahasubhajit/Speech-Spoofing-\n","authors":["Subhajit Saha","Md Sahidullah","Swagatam Das"],"pdf_url":"https://arxiv.org/pdf/2403.14290v1.pdf","comment":"This manuscript is under review in a conference"},{"id":"http://arxiv.org/abs/2403.14286v1","updated":"2024-03-21T10:49:54Z","published":"2024-03-21T10:49:54Z","title":"Assessing the Robustness of Spectral Clustering for Deep Speaker\n  Diarization","summary":"  Clustering speaker embeddings is crucial in speaker diarization but hasn't\nreceived as much focus as other components. Moreover, the robustness of speaker\ndiarization across various datasets hasn't been explored when the development\nand evaluation data are from different domains. To bridge this gap, this study\nthoroughly examines spectral clustering for both same-domain and cross-domain\nspeaker diarization. Our extensive experiments on two widely used corpora, AMI\nand DIHARD, reveal the performance trend of speaker diarization in the presence\nof domain mismatch. We observe that the performance difference between two\ndifferent domain conditions can be attributed to the role of spectral\nclustering. In particular, keeping other modules unchanged, we show that\ndifferences in optimal tuning parameters as well as speaker count estimation\noriginates due to the mismatch. This study opens several future directions for\nspeaker diarization research.\n","authors":["Nikhil Raghav","Md Sahidullah"],"pdf_url":"https://arxiv.org/pdf/2403.14286v1.pdf","comment":"Manuscript Under Review"},{"id":"http://arxiv.org/abs/2403.14282v1","updated":"2024-03-21T10:43:55Z","published":"2024-03-21T10:43:55Z","title":"How to be fair? A study of label and selection bias","summary":"  It is widely accepted that biased data leads to biased and thus potentially\nunfair models. Therefore, several measures for bias in data and model\npredictions have been proposed, as well as bias mitigation techniques whose aim\nis to learn models that are fair by design. Despite the myriad of mitigation\ntechniques developed in the past decade, however, it is still poorly understood\nunder what circumstances which methods work. Recently, Wick et al. showed, with\nexperiments on synthetic data, that there exist situations in which bias\nmitigation techniques lead to more accurate models when measured on unbiased\ndata. Nevertheless, in the absence of a thorough mathematical analysis, it\nremains unclear which techniques are effective under what circumstances. We\npropose to address this problem by establishing relationships between the type\nof bias and the effectiveness of a mitigation technique, where we categorize\nthe mitigation techniques by the bias measure they optimize. In this paper we\nillustrate this principle for label and selection bias on the one hand, and\ndemographic parity and ``We're All Equal'' on the other hand. Our theoretical\nanalysis allows to explain the results of Wick et al. and we also show that\nthere are situations where minimizing fairness measures does not result in the\nfairest possible distribution.\n","authors":["Marco Favier","Toon Calders","Sam Pinxteren","Jonathan Meyer"],"pdf_url":"https://arxiv.org/pdf/2403.14282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09478v2","updated":"2024-03-21T10:28:38Z","published":"2023-07-14T09:16:24Z","title":"The Role of Transparency in Repeated First-Price Auctions with Unknown\n  Valuations","summary":"  We study the problem of regret minimization for a single bidder in a sequence\nof first-price auctions where the bidder discovers the item's value only if the\nauction is won. Our main contribution is a complete characterization, up to\nlogarithmic factors, of the minimax regret in terms of the auction's\n\\emph{transparency}, which controls the amount of information on competing bids\ndisclosed by the auctioneer at the end of each auction. Our results hold under\ndifferent assumptions (stochastic, adversarial, and their smoothed variants) on\nthe environment generating the bidder's valuations and competing bids. These\nminimax rates reveal how the interplay between transparency and the nature of\nthe environment affects how fast one can learn to bid optimally in first-price\nauctions.\n","authors":["Nicolò Cesa-Bianchi","Tommaso Cesari","Roberto Colomboni","Federico Fusco","Stefano Leonardi"],"pdf_url":"https://arxiv.org/pdf/2307.09478v2.pdf","comment":"Accepted at STOC 2024"},{"id":"http://arxiv.org/abs/2402.00823v2","updated":"2024-03-21T10:21:37Z","published":"2024-02-01T18:07:33Z","title":"SLIM: Skill Learning with Multiple Critics","summary":"  Self-supervised skill learning aims to acquire useful behaviors that leverage\nthe underlying dynamics of the environment. Latent variable models, based on\nmutual information maximization, have been successful in this task but still\nstruggle in the context of robotic manipulation. As it requires impacting a\npossibly large set of degrees of freedom composing the environment, mutual\ninformation maximization fails alone in producing useful and safe manipulation\nbehaviors. Furthermore, tackling this by augmenting skill discovery rewards\nwith additional rewards through a naive combination might fail to produce\ndesired behaviors. To address this limitation, we introduce SLIM, a\nmulti-critic learning approach for skill discovery with a particular focus on\nrobotic manipulation. Our main insight is that utilizing multiple critics in an\nactor-critic framework to gracefully combine multiple reward functions leads to\na significant improvement in latent-variable skill discovery for robotic\nmanipulation while overcoming possible interference occurring among rewards\nwhich hinders convergence to useful skills. Furthermore, in the context of\ntabletop manipulation, we demonstrate the applicability of our novel skill\ndiscovery approach to acquire safe and efficient motor primitives in a\nhierarchical reinforcement learning fashion and leverage them through planning,\nsignificantly surpassing baseline approaches for skill discovery.\n","authors":["David Emukpere","Bingbing Wu","Julien Perez","Jean-Michel Renders"],"pdf_url":"https://arxiv.org/pdf/2402.00823v2.pdf","comment":"Accepted at IEEE ICRA 2024"},{"id":"http://arxiv.org/abs/2403.14270v1","updated":"2024-03-21T10:15:57Z","published":"2024-03-21T10:15:57Z","title":"Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship\n  Detection","summary":"  Visual relationship detection aims to identify objects and their\nrelationships in images. Prior methods approach this task by adding separate\nrelationship modules or decoders to existing object detection architectures.\nThis separation increases complexity and hinders end-to-end training, which\nlimits performance. We propose a simple and highly efficient decoder-free\narchitecture for open-vocabulary visual relationship detection. Our model\nconsists of a Transformer-based image encoder that represents objects as tokens\nand models their relationships implicitly. To extract relationship information,\nwe introduce an attention mechanism that selects object pairs likely to form a\nrelationship. We provide a single-stage recipe to train this model on a mixture\nof object and relationship detection data. Our approach achieves\nstate-of-the-art relationship detection performance on Visual Genome and on the\nlarge-vocabulary GQA benchmark at real-time inference speeds. We provide\nanalyses of zero-shot performance, ablations, and real-world qualitative\nexamples.\n","authors":["Tim Salzmann","Markus Ryll","Alex Bewley","Matthias Minderer"],"pdf_url":"https://arxiv.org/pdf/2403.14270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15141v3","updated":"2024-03-21T10:15:19Z","published":"2023-05-24T13:36:06Z","title":"From Tempered to Benign Overfitting in ReLU Neural Networks","summary":"  Overparameterized neural networks (NNs) are observed to generalize well even\nwhen trained to perfectly fit noisy data. This phenomenon motivated a large\nbody of work on \"benign overfitting\", where interpolating predictors achieve\nnear-optimal performance. Recently, it was conjectured and empirically observed\nthat the behavior of NNs is often better described as \"tempered overfitting\",\nwhere the performance is non-optimal yet also non-trivial, and degrades as a\nfunction of the noise level. However, a theoretical justification of this claim\nfor non-linear NNs has been lacking so far. In this work, we provide several\nresults that aim at bridging these complementing views. We study a simple\nclassification setting with 2-layer ReLU NNs, and prove that under various\nassumptions, the type of overfitting transitions from tempered in the extreme\ncase of one-dimensional data, to benign in high dimensions. Thus, we show that\nthe input dimension has a crucial role on the type of overfitting in this\nsetting, which we also validate empirically for intermediate dimensions.\nOverall, our results shed light on the intricate connections between the\ndimension, sample size, architecture and training algorithm on the one hand,\nand the type of resulting overfitting on the other hand.\n","authors":["Guy Kornowski","Gilad Yehudai","Ohad Shamir"],"pdf_url":"https://arxiv.org/pdf/2305.15141v3.pdf","comment":"NeurIPS 2023; fixed bug"},{"id":"http://arxiv.org/abs/2403.12821v2","updated":"2024-03-21T10:02:39Z","published":"2024-03-19T15:21:10Z","title":"FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware\n  Graph Transformer","summary":"  The success of a specific neural network architecture is closely tied to the\ndataset and task it tackles; there is no one-size-fits-all solution. Thus,\nconsiderable efforts have been made to quickly and accurately estimate the\nperformances of neural architectures, without full training or evaluation, for\ngiven tasks and datasets. Neural architecture encoding has played a crucial\nrole in the estimation, and graphbased methods, which treat an architecture as\na graph, have shown prominent performance. For enhanced representation learning\nof neural architectures, we introduce FlowerFormer, a powerful graph\ntransformer that incorporates the information flows within a neural\narchitecture. FlowerFormer consists of two key components: (a) bidirectional\nasynchronous message passing, inspired by the flows; (b) global attention built\non flow-based masking. Our extensive experiments demonstrate the superiority of\nFlowerFormer over existing neural encoding methods, and its effectiveness\nextends beyond computer vision models to include graph neural networks and auto\nspeech recognition models. Our code is available at\nhttp://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.\n","authors":["Dongyeong Hwang","Hyunju Kim","Sunwoo Kim","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2403.12821v2.pdf","comment":"CVPR 2024 Camera-Ready"},{"id":"http://arxiv.org/abs/2306.02090v3","updated":"2024-03-21T09:58:15Z","published":"2023-06-03T11:45:16Z","title":"Deep Classifier Mimicry without Data Access","summary":"  Access to pre-trained models has recently emerged as a standard across\nnumerous machine learning domains. Unfortunately, access to the original data\nthe models were trained on may not equally be granted. This makes it\ntremendously challenging to fine-tune, compress models, adapt continually, or\nto do any other type of data-driven update. We posit that original data access\nmay however not be required. Specifically, we propose Contrastive Abductive\nKnowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure\nthat mimics deep classifiers without access to the original data. To this end,\nCAKE generates pairs of noisy synthetic samples and diffuses them contrastively\ntoward a model's decision boundary. We empirically corroborate CAKE's\neffectiveness using several benchmark datasets and various architectural\nchoices, paving the way for broad application.\n","authors":["Steven Braun","Martin Mundt","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2306.02090v3.pdf","comment":"11 pages main, 4 figures, 2 tables, 4 pages appendix"},{"id":"http://arxiv.org/abs/2403.14262v1","updated":"2024-03-21T09:50:39Z","published":"2024-03-21T09:50:39Z","title":"Diffusion Models with Ensembled Structure-Based Anomaly Scoring for\n  Unsupervised Anomaly Detection","summary":"  Supervised deep learning techniques show promise in medical image analysis.\nHowever, they require comprehensive annotated data sets, which poses\nchallenges, particularly for rare diseases. Consequently, unsupervised anomaly\ndetection (UAD) emerges as a viable alternative for pathology segmentation, as\nonly healthy data is required for training. However, recent UAD anomaly scoring\nfunctions often focus on intensity only and neglect structural differences,\nwhich impedes the segmentation performance. This work investigates the\npotential of Structural Similarity (SSIM) to bridge this gap. SSIM captures\nboth intensity and structural disparities and can be advantageous over the\nclassical $l1$ error. However, we show that there is more than one optimal\nkernel size for the SSIM calculation for different pathologies. Therefore, we\ninvestigate an adaptive ensembling strategy for various kernel sizes to offer a\nmore pathology-agnostic scoring mechanism. We demonstrate that this ensembling\nstrategy can enhance the performance of DMs and mitigate the sensitivity to\ndifferent kernel sizes across varying pathologies, highlighting its promise for\nbrain MRI anomaly detection.\n","authors":["Finn Behrendt","Debayan Bhattacharya","Lennart Maack","Julia Krüger","Roland Opfer","Robin Mieling","Alexander Schlaefer"],"pdf_url":"https://arxiv.org/pdf/2403.14262v1.pdf","comment":"Accepted at IEEE ISBI 2024"},{"id":"http://arxiv.org/abs/2403.14255v1","updated":"2024-03-21T09:28:38Z","published":"2024-03-21T09:28:38Z","title":"ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion\n  Classification","summary":"  Improving the accessibility of psychotherapy with the aid of Large Language\nModels (LLMs) is garnering a significant attention in recent years. Recognizing\ncognitive distortions from the interviewee's utterances can be an essential\npart of psychotherapy, especially for cognitive behavioral therapy. In this\npaper, we propose ERD, which improves LLM-based cognitive distortion\nclassification performance with the aid of additional modules of (1) extracting\nthe parts related to cognitive distortion, and (2) debating the reasoning steps\nby multiple agents. Our experimental results on a public dataset show that ERD\nimproves the multi-class F1 score as well as binary specificity score.\nRegarding the latter score, it turns out that our method is effective in\ndebiasing the baseline method which has high false positive rate, especially\nwhen the summary of multi-agent debate is provided to LLMs.\n","authors":["Sehee Lim","Yejin Kim","Chi-Hyun Choi","Jy-yong Sohn","Byung-Hoon Kim"],"pdf_url":"https://arxiv.org/pdf/2403.14255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14252v1","updated":"2024-03-21T09:25:24Z","published":"2024-03-21T09:25:24Z","title":"LayoutLLM: Large Language Model Instruction Tuning for Visually Rich\n  Document Understanding","summary":"  This paper proposes LayoutLLM, a more flexible document analysis method for\nunderstanding imaged documents. Visually Rich Document Understanding tasks,\nsuch as document image classification and information extraction, have gained\nsignificant attention due to their importance. Existing methods have been\ndeveloped to enhance document comprehension by incorporating pre-training\nawareness of images, text, and layout structure. However, these methods require\nfine-tuning for each task and dataset, and the models are expensive to train\nand operate. To overcome this limitation, we propose a new LayoutLLM that\nintegrates these with large-scale language models (LLMs). By leveraging the\nstrengths of existing research in document image understanding and LLMs'\nsuperior language understanding capabilities, the proposed model, fine-tuned\nwith multimodal instruction datasets, performs an understanding of document\nimages in a single model. Our experiments demonstrate improvement over the\nbaseline model in various document analysis tasks.\n","authors":["Masato Fujitake"],"pdf_url":"https://arxiv.org/pdf/2403.14252v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2308.03574v2","updated":"2024-03-21T09:13:17Z","published":"2023-08-07T13:25:48Z","title":"Generalized Early Stopping in Evolutionary Direct Policy Search","summary":"  Lengthy evaluation times are common in many optimization problems such as\ndirect policy search tasks, especially when they involve conducting evaluations\nin the physical world, e.g. in robotics applications. Often when evaluating\nsolution over a fixed time period it becomes clear that the objective value\nwill not increase with additional computation time (for example when a two\nwheeled robot continuously spins on the spot). In such cases, it makes sense to\nstop the evaluation early to save computation time. However, most approaches to\nstop the evaluation are problem specific and need to be specifically designed\nfor the task at hand. Therefore, we propose an early stopping method for direct\npolicy search. The proposed method only looks at the objective value at each\ntime step and requires no problem specific knowledge. We test the introduced\nstopping criterion in five direct policy search environments drawn from games,\nrobotics and classic control domains, and show that it can save up to 75% of\nthe computation time. We also compare it with problem specific stopping\ncriteria and show that it performs comparably, while being more generally\napplicable.\n","authors":["Etor Arza","Leni K. Le Goff","Emma Hart"],"pdf_url":"https://arxiv.org/pdf/2308.03574v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02094v3","updated":"2024-03-21T09:03:48Z","published":"2023-09-05T10:00:33Z","title":"TensorBank: Tensor Lakehouse for Foundation Model Training","summary":"  Storing and streaming high dimensional data for foundation model training\nbecame a critical requirement with the rise of foundation models beyond natural\nlanguage. In this paper we introduce TensorBank, a petabyte scale tensor\nlakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU\nmemory at wire speed based on complex relational queries. We use Hierarchical\nStatistical Indices (HSI) for query acceleration. Our architecture allows to\ndirectly address tensors on block level using HTTP range reads. Once in GPU\nmemory, data can be transformed using PyTorch transforms. We provide a generic\nPyTorch dataset type with a corresponding dataset factory translating\nrelational queries and requested transformations as an instance. By making use\nof the HSI, irrelevant blocks can be skipped without reading them as those\nindices contain statistics on their content at different hierarchical\nresolution levels. This is an opinionated architecture powered by open\nstandards and making heavy use of open-source technology. Although, hardened\nfor production use using geospatial-temporal data, this architecture\ngeneralizes to other use case like computer vision, computational neuroscience,\nbiological sequence analysis and more.\n","authors":["Romeo Kienzler","Leonardo Pondian Tizzei","Benedikt Blumenstiel","Zoltan Arnold Nagy","S. Karthik Mukkavilli","Johannes Schmude","Marcus Freitag","Michael Behrendt","Daniel Salles Civitarese","Naomi Simumba","Daiki Kimura","Hendrik Hamann"],"pdf_url":"https://arxiv.org/pdf/2309.02094v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14244v1","updated":"2024-03-21T09:02:31Z","published":"2024-03-21T09:02:31Z","title":"Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering","summary":"  The 3D Gaussian splatting method has drawn a lot of attention, thanks to its\nhigh performance in training and high quality of the rendered image. However,\nit uses anisotropic Gaussian kernels to represent the scene. Although such\nanisotropic kernels have advantages in representing the geometry, they lead to\ndifficulties in terms of computation, such as splitting or merging two kernels.\nIn this paper, we propose to use isotropic Gaussian kernels to avoid such\ndifficulties in the computation, leading to a higher performance method. The\nexperiments confirm that the proposed method is about {\\bf 100X} faster without\nlosing the geometry representation accuracy. The proposed method can be applied\nin a large range applications where the radiance field is needed, such as 3D\nreconstruction, view synthesis, and dynamic object modeling.\n","authors":["Yuanhao Gong","Lantao Yu","Guanghui Yue"],"pdf_url":"https://arxiv.org/pdf/2403.14244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16828v2","updated":"2024-03-21T17:56:19Z","published":"2023-10-25T17:57:07Z","title":"TD-MPC2: Scalable, Robust World Models for Continuous Control","summary":"  TD-MPC is a model-based reinforcement learning (RL) algorithm that performs\nlocal trajectory optimization in the latent space of a learned implicit\n(decoder-free) world model. In this work, we present TD-MPC2: a series of\nimprovements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves\nsignificantly over baselines across 104 online RL tasks spanning 4 diverse task\ndomains, achieving consistently strong results with a single set of\nhyperparameters. We further show that agent capabilities increase with model\nand data size, and successfully train a single 317M parameter agent to perform\n80 tasks across multiple task domains, embodiments, and action spaces. We\nconclude with an account of lessons, opportunities, and risks associated with\nlarge TD-MPC2 agents. Explore videos, models, data, code, and more at\nhttps://tdmpc2.com\n","authors":["Nicklas Hansen","Hao Su","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16828v2.pdf","comment":"ICLR 2024. Explore videos, models, data, code, and more at\n  https://tdmpc2.com"},{"id":"http://arxiv.org/abs/2009.09213v6","updated":"2024-03-21T16:24:05Z","published":"2020-09-19T11:26:01Z","title":"Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering","summary":"  The current high-fidelity generation and high-precision detection of DeepFake\nimages are at an arms race. We believe that producing DeepFakes that are highly\nrealistic and 'detection evasive' can serve the ultimate goal of improving\nfuture generation DeepFake detection capabilities. In this paper, we propose a\nsimple yet powerful pipeline to reduce the artifact patterns of fake images\nwithout hurting image quality by performing implicit spatial-domain notch\nfiltering. We first demonstrate that frequency-domain notch filtering, although\nfamously shown to be effective in removing periodic noise in the spatial\ndomain, is infeasible for our task at hand due to the manual designs required\nfor the notch filters. We, therefore, resort to a learning-based approach to\nreproduce the notch filtering effects, but solely in the spatial domain. We\nadopt a combination of adding overwhelming spatial noise for breaking the\nperiodic noise pattern and deep image filtering to reconstruct the noise-free\nfake images, and we name our method DeepNotch. Deep image filtering provides a\nspecialized filter for each pixel in the noisy image, producing filtered images\nwith high fidelity compared to their DeepFake counterparts. Moreover, we also\nuse the semantic information of the image to generate an adversarial guidance\nmap to add noise intelligently. Our large-scale evaluation on 3 representative\nstate-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)\nhas demonstrated that our technique significantly reduces the accuracy of these\n3 fake image detection methods, 36.79% on average and up to 97.02% in the best\ncase.\n","authors":["Yihao Huang","Felix Juefei-Xu","Qing Guo","Yang Liu","Geguang Pu"],"pdf_url":"https://arxiv.org/pdf/2009.09213v6.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2205.14116v3","updated":"2024-03-21T16:14:01Z","published":"2022-05-27T17:28:54Z","title":"Don't Explain Noise: Robust Counterfactuals for Randomized Ensembles","summary":"  Counterfactual explanations describe how to modify a feature vector in order\nto flip the outcome of a trained classifier. Obtaining robust counterfactual\nexplanations is essential to provide valid algorithmic recourse and meaningful\nexplanations. We study the robustness of explanations of randomized ensembles,\nwhich are always subject to algorithmic uncertainty even when the training data\nis fixed. We formalize the generation of robust counterfactual explanations as\na probabilistic problem and show the link between the robustness of ensemble\nmodels and the robustness of base learners. We develop a practical method with\ngood empirical performance and support it with theoretical guarantees for\nensembles of convex base learners. Our results show that existing methods give\nsurprisingly low robustness: the validity of naive counterfactuals is below\n$50\\%$ on most data sets and can fall to $20\\%$ on problems with many features.\nIn contrast, our method achieves high robustness with only a small increase in\nthe distance from counterfactual explanations to their initial observations.\n","authors":["Alexandre Forel","Axel Parmentier","Thibaut Vidal"],"pdf_url":"https://arxiv.org/pdf/2205.14116v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.14468v1","updated":"2024-03-21T15:15:00Z","published":"2024-03-21T15:15:00Z","title":"AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks","summary":"  Video-to-video editing involves editing a source video along with additional\ncontrol (such as text prompts, subjects, or styles) to generate a new video\nthat aligns with the source video and the provided control. Traditional methods\nhave been constrained to certain editing types, limiting their ability to meet\nthe wide range of user demands. In this paper, we introduce AnyV2V, a novel\ntraining-free framework designed to simplify video editing into two primary\nsteps: (1) employing an off-the-shelf image editing model (e.g.\nInstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an\nexisting image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion\nand feature injection. In the first stage, AnyV2V can plug in any existing\nimage editing tools to support an extensive array of video editing tasks.\nBeyond the traditional prompt-based editing methods, AnyV2V also can support\nnovel video editing tasks, including reference-based style transfer,\nsubject-driven editing, and identity manipulation, which were unattainable by\nprevious methods. In the second stage, AnyV2V can plug in any existing\nimage-to-video models to perform DDIM inversion and intermediate feature\ninjection to maintain the appearance and motion consistency with the source\nvideo. On the prompt-based editing, we show that AnyV2V can outperform the\nprevious best approach by 35\\% on prompt alignment, and 25\\% on human\npreference. On the three novel tasks, we show that AnyV2V also achieves a high\nsuccess rate. We believe AnyV2V will continue to thrive due to its ability to\nseamlessly integrate the fast-evolving image editing methods. Such\ncompatibility can help AnyV2V to increase its versatility to cater to diverse\nuser demands.\n","authors":["Max Ku","Cong Wei","Weiming Ren","Huan Yang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14468v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.14449v1","updated":"2024-03-21T14:56:46Z","published":"2024-03-21T14:56:46Z","title":"Bringing Robots Home: The Rise of AI Robots in Consumer Electronics","summary":"  On March 18, 2024, NVIDIA unveiled Project GR00T, a general-purpose\nmultimodal generative AI model designed specifically for training humanoid\nrobots. Preceding this event, Tesla's unveiling of the Optimus Gen 2 humanoid\nrobot on December 12, 2023, underscored the profound impact robotics is poised\nto have on reshaping various facets of our daily lives. While robots have long\ndominated industrial settings, their presence within our homes is a burgeoning\nphenomenon. This can be attributed, in part, to the complexities of domestic\nenvironments and the challenges of creating robots that can seamlessly\nintegrate into our daily routines.\n","authors":["Haiwei Dong","Yang Liu","Ted Chu","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2403.14449v1.pdf","comment":"Accepted by IEEE Consumer Electronics Magazine"},{"id":"http://arxiv.org/abs/2301.12831v3","updated":"2024-03-21T05:39:44Z","published":"2023-01-30T12:37:04Z","title":"M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing\n  System","summary":"  Face presentation attacks (FPA), also known as face spoofing, have brought\nincreasing concerns to the public through various malicious applications, such\nas financial fraud and privacy leakage. Therefore, safeguarding face\nrecognition systems against FPA is of utmost importance. Although existing\nlearning-based face anti-spoofing (FAS) models can achieve outstanding\ndetection performance, they lack generalization capability and suffer\nsignificant performance drops in unforeseen environments. Many methodologies\nseek to use auxiliary modality data (e.g., depth and infrared maps) during the\npresentation attack detection (PAD) to address this limitation. However, these\nmethods can be limited since (1) they require specific sensors such as depth\nand infrared cameras for data capture, which are rarely available on commodity\nmobile devices, and (2) they cannot work properly in practical scenarios when\neither modality is missing or of poor quality. In this paper, we devise an\naccurate and robust MultiModal Mobile Face Anti-Spoofing system named M3FAS to\novercome the issues above. The primary innovation of this work lies in the\nfollowing aspects: (1) To achieve robust PAD, our system combines visual and\nauditory modalities using three commonly available sensors: camera, speaker,\nand microphone; (2) We design a novel two-branch neural network with three\nhierarchical feature aggregation modules to perform cross-modal feature fusion;\n(3). We propose a multi-head training strategy, allowing the model to output\npredictions from the vision, acoustic, and fusion heads, resulting in a more\nflexible PAD. Extensive experiments have demonstrated the accuracy, robustness,\nand flexibility of M3FAS under various challenging experimental settings. The\nsource code and dataset are available at: https://github.com/ChenqiKONG/M3FAS/\n","authors":["Chenqi Kong","Kexin Zheng","Yibing Liu","Shiqi Wang","Anderson Rocha","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2301.12831v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06255v3","updated":"2024-03-21T03:21:24Z","published":"2023-09-12T14:16:34Z","title":"Enhancing Multimodal Cooperation via Fine-grained Modality Valuation","summary":"  One primary topic of multimodal learning is to jointly incorporate\nheterogeneous information from different modalities. However, most models often\nsuffer from unsatisfactory multimodal cooperation, which cannot jointly utilize\nall modalities well. Some methods are proposed to identify and enhance the\nworse learnt modality, but they are often hard to provide the fine-grained\nobservation of multimodal cooperation at sample-level with theoretical support.\nHence, it is essential to reasonably observe and improve the fine-grained\ncooperation between modalities, especially when facing realistic scenarios\nwhere the modality discrepancy could vary across different samples. To this\nend, we introduce a sample-level modality valuation metric to evaluate the\ncontribution of each modality for each sample. Via modality valuation, we\nobserve that modality discrepancy indeed could be different at sample-level,\nbeyond the global contribution discrepancy at dataset-level. We further analyze\nthis issue and improve cooperation between modalities at sample-level by\nenhancing the discriminative ability of low-contributing modalities in a\ntargeted manner. Overall, our methods reasonably observe the fine-grained\nuni-modal contribution and achieve considerable improvement. The source code\nand dataset are available at\n\\url{https://github.com/GeWu-Lab/Valuate-and-Enhance-Multimodal-Cooperation}.\n","authors":["Yake Wei","Ruoxuan Feng","Zihe Wang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2309.06255v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.10406v2","updated":"2024-03-21T01:45:43Z","published":"2024-03-15T15:38:30Z","title":"Deep Bi-directional Attention Network for Image Super-Resolution Quality\n  Assessment","summary":"  There has emerged a growing interest in exploring efficient quality\nassessment algorithms for image super-resolution (SR). However, employing deep\nlearning techniques, especially dual-branch algorithms, to automatically\nevaluate the visual quality of SR images remains challenging. Existing SR image\nquality assessment (IQA) metrics based on two-stream networks lack interactions\nbetween branches. To address this, we propose a novel full-reference IQA\n(FR-IQA) method for SR images. Specifically, producing SR images and evaluating\nhow close the SR images are to the corresponding HR references are separate\nprocesses. Based on this consideration, we construct a deep Bi-directional\nAttention Network (BiAtten-Net) that dynamically deepens visual attention to\ndistortions in both processes, which aligns well with the human visual system\n(HVS). Experiments on public SR quality databases demonstrate the superiority\nof our proposed BiAtten-Net over state-of-the-art quality assessment methods.\nIn addition, the visualization results and ablation study show the\neffectiveness of bi-directional attention.\n","authors":["Yixiao Li","Xiaoyuan Yang","Jun Fu","Guanghui Yue","Wei Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.10406v2.pdf","comment":"7 pages, 3 figures, published to 2024 IEEE International Conference\n  on Multimedia and Expo (ICME)"},{"id":"http://arxiv.org/abs/2403.14773v1","updated":"2024-03-21T18:27:29Z","published":"2024-03-21T18:27:29Z","title":"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation\n  from Text","summary":"  Text-to-video diffusion models enable the generation of high-quality videos\nthat follow text instructions, making it easy to create diverse and individual\ncontent. However, existing approaches mostly focus on high-quality short video\ngeneration (typically 16 or 24 frames), ending up with hard-cuts when naively\nextended to the case of long video synthesis. To overcome these limitations, we\nintroduce StreamingT2V, an autoregressive approach for long video generation of\n80, 240, 600, 1200 or more frames with smooth transitions. The key components\nare:(i) a short-term memory block called conditional attention module (CAM),\nwhich conditions the current generation on the features extracted from the\nprevious chunk via an attentional mechanism, leading to consistent chunk\ntransitions, (ii) a long-term memory block called appearance preservation\nmodule, which extracts high-level scene and object features from the first\nvideo chunk to prevent the model from forgetting the initial scene, and (iii) a\nrandomized blending approach that enables to apply a video enhancer\nautoregressively for infinitely long videos without inconsistencies between\nchunks. Experiments show that StreamingT2V generates high motion amount. In\ncontrast, all competing image-to-video methods are prone to video stagnation\nwhen applied naively in an autoregressive manner. Thus, we propose with\nStreamingT2V a high-quality seamless text-to-long video generator that\noutperforms competitors with consistency and motion. Our code will be available\nat: https://github.com/Picsart-AI-Research/StreamingT2V\n","authors":["Roberto Henschel","Levon Khachatryan","Daniil Hayrapetyan","Hayk Poghosyan","Vahram Tadevosyan","Zhangyang Wang","Shant Navasardyan","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.14773v1.pdf","comment":"https://github.com/Picsart-AI-Research/StreamingT2V"}]},"2024-03-22T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.15388v1","updated":"2024-03-22T17:59:52Z","published":"2024-03-22T17:59:52Z","title":"LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal\n  Models","summary":"  Large Multimodal Models (LMMs) have shown significant reasoning capabilities\nby connecting a visual encoder and a large language model. LMMs typically use a\nfixed amount of visual tokens, such as the penultimate layer features in the\nCLIP visual encoder, as the prefix content. Recent LMMs incorporate more\ncomplex visual inputs, such as high-resolution images and videos, which\nincrease the number of visual tokens significantly. However, due to the design\nof the Transformer architecture, computational costs associated with these\nmodels tend to increase quadratically with the number of input tokens. To\ntackle this problem, we explore a token reduction mechanism and find, similar\nto prior work, that many visual tokens are spatially redundant. Based on this,\nwe propose PruMerge, a novel adaptive visual token reduction approach, which\nlargely reduces the number of visual tokens while maintaining comparable model\nperformance. We first select the unpruned visual tokens based on their\nsimilarity to class tokens and spatial tokens. We then cluster the pruned\ntokens based on key similarity and merge the clustered tokens with the unpruned\ntokens to supplement their information. Empirically, when applied to LLaVA-1.5,\nour approach can compress the visual tokens by 14.4 times on average, and\nachieve comparable performance across diverse visual question-answering and\nreasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.\n","authors":["Yuzhang Shang","Mu Cai","Bingxin Xu","Yong Jae Lee","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2403.15388v1.pdf","comment":"Project page: https://llava-prumerge.github.io/"},{"id":"http://arxiv.org/abs/2403.08763v2","updated":"2024-03-22T17:56:38Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15371v1","updated":"2024-03-22T17:50:43Z","published":"2024-03-22T17:50:43Z","title":"Can large language models explore in-context?","summary":"  We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.\n","authors":["Akshay Krishnamurthy","Keegan Harris","Dylan J. Foster","Cyril Zhang","Aleksandrs Slivkins"],"pdf_url":"https://arxiv.org/pdf/2403.15371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15365v1","updated":"2024-03-22T17:33:11Z","published":"2024-03-22T17:33:11Z","title":"A Transfer Attack to Image Watermarks","summary":"  Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.\n","authors":["Yuepeng Hu","Zhengyuan Jiang","Moyang Guo","Neil Gong"],"pdf_url":"https://arxiv.org/pdf/2403.15365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15364v1","updated":"2024-03-22T17:32:43Z","published":"2024-03-22T17:32:43Z","title":"Towards Knowledge-Grounded Natural Language Understanding and Generation","summary":"  This thesis investigates how natural language understanding and generation\nwith transformer models can benefit from grounding the models with knowledge\nrepresentations and addresses the following key research questions: (i) Can\nknowledge of entities extend its benefits beyond entity-centric tasks, such as\nentity linking? (ii) How can we faithfully and effectively extract such\nstructured knowledge from raw text, especially noisy web text? (iii) How do\nother types of knowledge, beyond structured knowledge, contribute to improving\nNLP tasks?\n  Studies in this thesis find that incorporating relevant and up-to-date\nknowledge of entities benefits fake news detection, and entity-focused\ncode-switching significantly enhances zero-shot cross-lingual transfer on\nentity-centric tasks. In terms of effective and faithful approaches to\nextracting structured knowledge, it is observed that integrating negative\nexamples and training with entity planning significantly improves performance.\nAdditionally, it is established that other general forms of knowledge, such as\nparametric and distilled knowledge, enhance multimodal and multilingual\nknowledge-intensive tasks. This research shows the tangible benefits of diverse\nknowledge integration and motivates further exploration in this direction.\n","authors":["Chenxi Whitehouse"],"pdf_url":"https://arxiv.org/pdf/2403.15364v1.pdf","comment":"PhD Thesis"},{"id":"http://arxiv.org/abs/2403.04639v2","updated":"2024-03-22T17:28:42Z","published":"2024-03-07T16:29:19Z","title":"MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis","summary":"  The present paper introduces new sentiment data, MaCMS, for\nMagahi-Hindi-English (MHE) code-mixed language, where Magahi is a\nless-resourced minority language. This dataset is the first\nMagahi-Hindi-English code-mixed dataset for sentiment analysis tasks. Further,\nwe also provide a linguistics analysis of the dataset to understand the\nstructure of code-mixing and a statistical study to understand the language\npreferences of speakers with different polarities. With these analyses, we also\ntrain baseline models to evaluate the dataset's quality.\n","authors":["Priya Rani","Gaurav Negi","Theodorus Fransen","John P. McCrae"],"pdf_url":"https://arxiv.org/pdf/2403.04639v2.pdf","comment":"Lrec-Colin 2024"},{"id":"http://arxiv.org/abs/2309.12276v3","updated":"2024-03-22T17:28:17Z","published":"2023-09-21T17:37:01Z","title":"LLMR: Real-time Prompting of Interactive Worlds using Large Language\n  Models","summary":"  We present Large Language Model for Mixed Reality (LLMR), a framework for the\nreal-time creation and modification of interactive Mixed Reality experiences\nusing LLMs. LLMR leverages novel strategies to tackle difficult cases where\nideal training data is scarce, or where the design goal requires the synthesis\nof internal dynamics, intuitive analysis, or advanced interactivity. Our\nframework relies on text interaction and the Unity game engine. By\nincorporating techniques for scene understanding, task planning,\nself-debugging, and memory management, LLMR outperforms the standard GPT-4 by\n4x in average error rate. We demonstrate LLMR's cross-platform interoperability\nwith several example worlds, and evaluate it on a variety of creation and\nmodification tasks to show that it can produce and edit diverse objects, tools,\nand scenes. Finally, we conducted a usability study (N=11) with a diverse set\nthat revealed participants had positive experiences with the system and would\nuse it again.\n","authors":["Fernanda De La Torre","Cathy Mengying Fang","Han Huang","Andrzej Banburski-Fahey","Judith Amores Fernandez","Jaron Lanier"],"pdf_url":"https://arxiv.org/pdf/2309.12276v3.pdf","comment":"46 pages, 18 figures; Matching version accepted at CHI 2024"},{"id":"http://arxiv.org/abs/2403.15362v1","updated":"2024-03-22T17:26:05Z","published":"2024-03-22T17:26:05Z","title":"CoLLEGe: Concept Embedding Generation for Large Language Models","summary":"  Current language models are unable to quickly learn new concepts on the fly,\noften requiring a more involved finetuning process to learn robustly. Prompting\nin-context is not robust to context distractions, and often fails to confer\nmuch information about the new concepts. Classic methods for few-shot word\nlearning in NLP, relying on global word vectors, are less applicable to large\nlanguage models. In this paper, we introduce a novel approach named CoLLEGe\n(Concept Learning with Language Embedding Generation) to modernize few-shot\nconcept learning. CoLLEGe is a meta-learning framework capable of generating\nflexible embeddings for new concepts using a small number of example sentences\nor definitions. Our primary meta-learning objective is simply to facilitate a\nlanguage model to make next word predictions in forthcoming sentences, making\nit compatible with language model pretraining. We design a series of tasks to\ntest new concept learning in challenging real-world scenarios, including new\nword acquisition, definition inference, and verbal reasoning, and demonstrate\nthat our method succeeds in each setting without task-specific training.\n","authors":["Ryan Teehan","Brenden Lake","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2403.15362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17543v2","updated":"2024-03-22T17:12:49Z","published":"2023-12-29T10:18:36Z","title":"Building Efficient Universal Classifiers with Natural Language Inference","summary":"  Generative Large Language Models (LLMs) have become the mainstream choice for\nfewshot and zeroshot learning thanks to the universality of text generation.\nMany users, however, do not need the broad capabilities of generative LLMs when\nthey only want to automate a classification task. Smaller BERT-like models can\nalso learn universal tasks, which allow them to do any text classification task\nwithout requiring fine-tuning (zeroshot classification) or to learn new tasks\nwith only a few examples (fewshot), while being significantly more efficient\nthan generative LLMs. This paper (1) explains how Natural Language Inference\n(NLI) can be used as a universal classification task that follows similar\nprinciples as instruction fine-tuning of generative LLMs, (2) provides a\nstep-by-step guide with reusable Jupyter notebooks for building a universal\nclassifier, and (3) shares the resulting universal classifier that is trained\non 33 datasets with 389 diverse classes. Parts of the code we share has been\nused to train our older zeroshot classifiers that have been downloaded more\nthan 55 million times via the Hugging Face Hub as of December 2023. Our new\nclassifier improves zeroshot performance by 9.4%.\n","authors":["Moritz Laurer","Wouter van Atteveldt","Andreu Casas","Kasper Welbers"],"pdf_url":"https://arxiv.org/pdf/2312.17543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15351v1","updated":"2024-03-22T17:06:05Z","published":"2024-03-22T17:06:05Z","title":"Multi-Review Fusion-in-Context","summary":"  Grounded text generation, encompassing tasks such as long-form\nquestion-answering and summarization, necessitates both content selection and\ncontent consolidation. Current end-to-end methods are difficult to control and\ninterpret due to their opaqueness. Accordingly, recent works have proposed a\nmodular approach, with separate components for each step. Specifically, we\nfocus on the second subtask, of generating coherent text given pre-selected\ncontent in a multi-document setting. Concretely, we formalize\n\\textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists of\nsource texts with highlighted spans of targeted content. A model then needs to\ngenerate a coherent passage that includes all and only the target information.\nOur work includes the development of a curated dataset of 1000 instances in the\nreviews domain, alongside a novel evaluation framework for assessing the\nfaithfulness and coverage of highlights, which strongly correlate to human\njudgment. Several baseline models exhibit promising outcomes and provide\ninsightful analyses. This study lays the groundwork for further exploration of\nmodular text generation in the multi-document setting, offering potential\nimprovements in the quality and reliability of generated content. \\footnote{Our\nbenchmark, FuseReviews, including the dataset, evaluation framework and\ndesignated leaderboard, can be found at \\url{https://fusereviews.github.io/}.}\n","authors":["Aviv Slobodkin","Ori Shapira","Ran Levy","Ido Dagan"],"pdf_url":"https://arxiv.org/pdf/2403.15351v1.pdf","comment":"NAACL 2024, findings"},{"id":"http://arxiv.org/abs/2403.09611v3","updated":"2024-03-22T17:03:16Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Ankur Jain","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Guoli Yin","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15322v1","updated":"2024-03-22T16:17:55Z","published":"2024-03-22T16:17:55Z","title":"CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for\n  Named Entity Recognition and Relation Extraction","summary":"  The process of cyber mapping gives insights in relationships among financial\nentities and service providers. Centered around the outsourcing practices of\ncompanies within fund prospectuses in Germany, we introduce a dataset\nspecifically designed for named entity recognition and relation extraction\ntasks. The labeling process on 948 sentences was carried out by three experts\nwhich yields to 5,969 annotations for four entity types (Outsourcing, Company,\nLocation and Software) and 4,102 relation annotations (Outsourcing-Company,\nCompany-Location). State-of-the-art deep learning models were trained to\nrecognize entities and extract relations showing first promising results. An\nanonymized version of the dataset, along with guidelines and the code used for\nmodel training, are publicly available at\nhttps://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip.\n","authors":["Neda Foroutan","Markus Schröder","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.15322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10311v5","updated":"2024-03-22T16:10:14Z","published":"2024-02-15T20:24:39Z","title":"The optimal placement of the head in the noun phrase. The case of\n  demonstrative, numeral, adjective and noun","summary":"  The word order of a sentence is shaped by multiple principles. The principle\nof syntactic dependency distance minimization is in conflict with the principle\nof surprisal minimization (or predictability maximization) in single head\nsyntactic dependency structures: while the former predicts that the head should\nbe placed at the center of the linear arrangement, the latter predicts that the\nhead should be placed at one of the ends (either first or last). A critical\nquestion is when surprisal minimization (or predictability maximization) should\nsurpass syntactic dependency distance minimization. In the context of single\nhead structures, it has been predicted that this is more likely to happen when\ntwo conditions are met, i.e. (a) fewer words are involved and (b) words are\nshorter. Here we test the prediction on the noun phrase when it is composed of\na demonstrative, a numeral, an adjective and a noun. We find that, across\npreferred orders in languages, the noun tends to be placed at one of the ends,\nconfirming the theoretical prediction. We also show evidence of anti locality\neffects: syntactic dependency distances in preferred orders are longer than\nexpected by chance.\n","authors":["Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2402.10311v5.pdf","comment":"Typos corrected"},{"id":"http://arxiv.org/abs/2403.09919v2","updated":"2024-03-22T16:06:42Z","published":"2024-03-14T23:40:56Z","title":"Recurrent Drafter for Fast Speculative Decoding in Large Language Models","summary":"  In this paper, we introduce an improved approach of speculative decoding\naimed at enhancing the efficiency of serving large language models. Our method\ncapitalizes on the strengths of two established techniques: the classic\ntwo-model speculative decoding approach, and the more recent single-model\napproach, Medusa. Drawing inspiration from Medusa, our approach adopts a\nsingle-model strategy for speculative decoding. However, our method\ndistinguishes itself by employing a single, lightweight draft head with a\nrecurrent dependency design, akin in essence to the small, draft model uses in\nclassic speculative decoding, but without the complexities of the full\ntransformer architecture. And because of the recurrent dependency, we can use\nbeam search to swiftly filter out undesired candidates with the draft head. The\noutcome is a method that combines the simplicity of single-model design and\navoids the need to create a data-dependent tree attention structure only for\ninference in Medusa. We empirically demonstrate the effectiveness of the\nproposed method on several popular open source language models, along with a\ncomprehensive analysis of the trade-offs involved in adopting this approach.\n","authors":["Aonan Zhang","Chong Wang","Yi Wang","Xuanyu Zhang","Yunfei Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.09919v2.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.10581v2","updated":"2024-03-22T16:00:24Z","published":"2024-03-15T13:25:09Z","title":"Large Language Model-informed ECG Dual Attention Network for Heart\n  Failure Risk Prediction","summary":"  Heart failure (HF) poses a significant public health challenge, with a rising\nglobal mortality rate. Early detection and prevention of HF could significantly\nreduce its impact. We introduce a novel methodology for predicting HF risk\nusing 12-lead electrocardiograms (ECGs). We present a novel, lightweight\ndual-attention ECG network designed to capture complex ECG features essential\nfor early HF risk prediction, despite the notable imbalance between low and\nhigh-risk groups. This network incorporates a cross-lead attention module and\ntwelve lead-specific temporal attention modules, focusing on cross-lead\ninteractions and each lead's local dynamics. To further alleviate model\noverfitting, we leverage a large language model (LLM) with a public ECG-Report\ndataset for pretraining on an ECG-report alignment task. The network is then\nfine-tuned for HF risk prediction using two specific cohorts from the UK\nBiobank study, focusing on patients with hypertension (UKB-HYP) and those who\nhave had a myocardial infarction (UKB-MI).The results reveal that LLM-informed\npre-training substantially enhances HF risk prediction in these cohorts. The\ndual-attention design not only improves interpretability but also predictive\naccuracy, outperforming existing competitive methods with C-index scores of\n0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method's\npotential in advancing HF risk assessment with clinical complex ECG data.\n","authors":["Chen Chen","Lei Li","Marcel Beetz","Abhirup Banerjee","Ramneek Gupta","Vicente Grau"],"pdf_url":"https://arxiv.org/pdf/2403.10581v2.pdf","comment":"Under journal revision"},{"id":"http://arxiv.org/abs/2403.15309v1","updated":"2024-03-22T15:59:24Z","published":"2024-03-22T15:59:24Z","title":"Controlled Training Data Generation with Diffusion Models","summary":"  In this work, we present a method to control a text-to-image generative model\nto produce training data specifically \"useful\" for supervised learning. Unlike\nprevious works that employ an open-loop approach and pre-define prompts to\ngenerate new data using either a language model or human expertise, we develop\nan automated closed-loop system which involves two feedback mechanisms. The\nfirst mechanism uses feedback from a given supervised model and finds\nadversarial prompts that result in image generations that maximize the model\nloss. While these adversarial prompts result in diverse data informed by the\nmodel, they are not informed of the target distribution, which can be\ninefficient. Therefore, we introduce the second feedback mechanism that guides\nthe generation process towards a certain target distribution. We call the\nmethod combining these two mechanisms Guided Adversarial Prompts. We perform\nour evaluations on different tasks, datasets and architectures, with different\ntypes of distribution shifts (spuriously correlated data, unseen domains) and\ndemonstrate the efficiency of the proposed feedback mechanisms compared to\nopen-loop approaches.\n","authors":["Teresa Yeo","Andrei Atanov","Harold Benoit","Aleksandr Alekseev","Ruchira Ray","Pooya Esmaeil Akhoondi","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2403.15309v1.pdf","comment":"Project page at https://adversarial-prompts.epfl.ch/"},{"id":"http://arxiv.org/abs/2403.00411v2","updated":"2024-03-22T15:54:03Z","published":"2024-03-01T09:57:46Z","title":"Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with\n  Fact-Checking in Turkish","summary":"  The rapid spread of misinformation through social media platforms has raised\nconcerns regarding its impact on public opinion. While misinformation is\nprevalent in other languages, the majority of research in this field has\nconcentrated on the English language. Hence, there is a scarcity of datasets\nfor other languages, including Turkish. To address this concern, we have\nintroduced the FCTR dataset, consisting of 3238 real-world claims. This dataset\nspans multiple domains and incorporates evidence collected from three Turkish\nfact-checking organizations. Additionally, we aim to assess the effectiveness\nof cross-lingual transfer learning for low-resource languages, with a\nparticular focus on Turkish. We demonstrate in-context learning (zero-shot and\nfew-shot) performance of large language models in this context. The\nexperimental results indicate that the dataset has the potential to advance\nresearch in the Turkish language.\n","authors":["Recep Firat Cekinel","Pinar Karagoz","Cagri Coltekin"],"pdf_url":"https://arxiv.org/pdf/2403.00411v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.15293v1","updated":"2024-03-22T15:40:11Z","published":"2024-03-22T15:40:11Z","title":"Human behaviour through a LENS: How Linguistic content triggers Emotions\n  and Norms and determines Strategy choices","summary":"  Over the last two decades, a growing body of experimental research has\nprovided evidence that linguistic frames influence human behaviour in economic\ngames, beyond the economic consequences of the available actions. This article\nproposes a novel framework that transcends the traditional confines of\noutcome-based preference models. According to the LENS model, the Linguistic\ndescription of the decision problem triggers Emotional responses and suggests\npotential Norms of behaviour, which then interact to shape an individual's\nStrategic choice. The article reviews experimental evidence that supports each\npath of the LENS model. Furthermore, it identifies and discusses several\ncritical research questions that arise from this model, pointing towards\navenues for future inquiry.\n","authors":["Valerio Capraro"],"pdf_url":"https://arxiv.org/pdf/2403.15293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14913v2","updated":"2024-03-22T15:39:24Z","published":"2023-09-26T13:14:35Z","title":"Robustness of the Random Language Model","summary":"  The Random Language Model (De Giuli 2019) is an ensemble of stochastic\ncontext-free grammars, quantifying the syntax of human and computer languages.\nThe model suggests a simple picture of first language learning as a type of\nannealing in the vast space of potential languages. In its simplest\nformulation, it implies a single continuous transition to grammatical syntax,\nat which the symmetry among potential words and categories is spontaneously\nbroken. Here this picture is scrutinized by considering its robustness against\nextensions of the original model, and trajectories through parameter space\ndifferent from those originally considered. It is shown here that (i) the\nscenario is robust to explicit symmetry breaking, an inevitable component of\nlearning in the real world; and (ii) the transition to grammatical syntax can\nbe encountered by fixing the deep (hidden) structure while varying the surface\n(observable) properties. It is also argued that the transition becomes a sharp\nthermodynamic transition in an idealized limit. Moreover, comparison with human\ndata on the clustering coefficient of syntax networks suggests that the\nobserved transition is equivalent to that normally experienced by children at\nage 24 months. The results are discussed in light of theory of first-language\nacquisition in linguistics, and recent successes in machine learning.\n","authors":["Fatemeh Lalegani","Eric De Giuli"],"pdf_url":"https://arxiv.org/pdf/2309.14913v2.pdf","comment":"11 pages; v2: expanded discussion throughout"},{"id":"http://arxiv.org/abs/2403.09530v2","updated":"2024-03-22T15:26:05Z","published":"2024-03-14T16:13:00Z","title":"VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding","summary":"  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n","authors":["Chris Kelly","Luhui Hu","Jiayin Hu","Yu Tian","Deshun Yang","Bang Yang","Cindy Yang","Zihao Li","Zaoshan Huang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2403.09530v2.pdf","comment":"12 pages, 7 figures, pending conference"},{"id":"http://arxiv.org/abs/2403.15279v1","updated":"2024-03-22T15:22:06Z","published":"2024-03-22T15:22:06Z","title":"Fundus: A Simple-to-Use News Scraper Optimized for High Quality\n  Extractions","summary":"  This paper introduces Fundus, a user-friendly news scraper that enables users\nto obtain millions of high-quality news articles with just a few lines of code.\nUnlike existing news scrapers, we use manually crafted, bespoke content\nextractors that are specifically tailored to the formatting guidelines of each\nsupported online newspaper. This allows us to optimize our scraping for quality\nsuch that retrieved news articles are textually complete and without HTML\nartifacts. Further, our framework combines both crawling (retrieving HTML from\nthe web or large web archives) and content extraction into a single pipeline.\nBy providing a unified interface for a predefined collection of newspapers, we\naim to make Fundus broadly usable even for non-technical users. This paper\ngives an overview of the framework, discusses our design choices, and presents\na comparative evaluation against other popular news scrapers. Our evaluation\nshows that Fundus yields significantly higher quality extractions (complete and\nartifact-free news articles) than prior work. The framework is available on\nGitHub under https://github.com/flairNLP/fundus and can be simply installed\nusing pip.\n","authors":["Max Dallabetta","Conrad Dobberstein","Adrian Breiding","Alan Akbik"],"pdf_url":"https://arxiv.org/pdf/2403.15279v1.pdf","comment":"10 pages, 4 figures, submitted to ACL 2024, for a screencast see\n  https://www.youtube.com/watch?v=9GJExMelhdI"},{"id":"http://arxiv.org/abs/2403.15278v1","updated":"2024-03-22T15:21:07Z","published":"2024-03-22T15:21:07Z","title":"Specifying Genericity through Inclusiveness and Abstractness Continuous\n  Scales","summary":"  This paper introduces a novel annotation framework for the fine-grained\nmodeling of Noun Phrases' (NPs) genericity in natural language. The framework\nis designed to be simple and intuitive, making it accessible to non-expert\nannotators and suitable for crowd-sourced tasks. Drawing from theoretical and\ncognitive literature on genericity, this framework is grounded in established\nlinguistic theory. Through a pilot study, we created a small but crucial\nannotated dataset of 324 sentences, serving as a foundation for future\nresearch. To validate our approach, we conducted an evaluation comparing our\ncontinuous annotations with existing binary annotations on the same dataset,\ndemonstrating the framework's effectiveness in capturing nuanced aspects of\ngenericity. Our work offers a practical resource for linguists, providing a\nfirst annotated dataset and an annotation scheme designed to build\nreal-language datasets that can be used in studies on the semantics of\ngenericity, and NLP practitioners, contributing to the development of\ncommonsense knowledge repositories valuable in enhancing various NLP\napplications.\n","authors":["Claudia Collacciani","Andrea Amelio Ravelli","Marianna Marcella Bolognesi"],"pdf_url":"https://arxiv.org/pdf/2403.15278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15273v1","updated":"2024-03-22T15:16:10Z","published":"2024-03-22T15:16:10Z","title":"Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs","summary":"  Event temporal relation (TempRel) is a primary subject of the event relation\nextraction task. However, the inherent ambiguity of TempRel increases the\ndifficulty of the task. With the rise of prompt engineering, it is important to\ndesign effective prompt templates and verbalizers to extract relevant\nknowledge. The traditional manually designed templates struggle to extract\nprecise temporal knowledge. This paper introduces a novel retrieval-augmented\nTempRel extraction approach, leveraging knowledge retrieved from large language\nmodels (LLMs) to enhance prompt templates and verbalizers. Our method\ncapitalizes on the diverse capabilities of various LLMs to generate a wide\narray of ideas for template and verbalizer design. Our proposed method fully\nexploits the potential of LLMs for generation tasks and contributes more\nknowledge to our design. Empirical evaluations across three widely recognized\ndatasets demonstrate the efficacy of our method in improving the performance of\nevent temporal relation extraction tasks.\n","authors":["Xiaobin Zhang","Liangjun Zang","Qianwen Liu","Shuchong Wei","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2403.15273v1.pdf","comment":"8 pages,6 figures.Accepted to the International Joint Conference on\n  Neural Networks (IJCNN2024)"},{"id":"http://arxiv.org/abs/2403.15268v1","updated":"2024-03-22T15:06:45Z","published":"2024-03-22T15:06:45Z","title":"Imagination Augmented Generation: Learning to Imagine Richer Context for\n  Question Answering over Large Language Models","summary":"  Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been\nproposed to enhance the knowledge required for question answering over Large\nLanguage Models (LLMs). However, the former depends on external resources, and\nboth require incorporating the explicit documents into the context, which\nresults in longer contexts that lead to more resource consumption. Recent works\nindicate that LLMs have modeled rich knowledge, albeit not effectively\ntriggered or activated. Inspired by this, we propose a novel\nknowledge-augmented framework, Imagination-Augmented-Generation (IAG), which\nsimulates the human capacity to compensate for knowledge deficits while\nanswering questions solely through imagination, without relying on external\nresources. Guided by IAG, we propose an imagine richer context method for\nquestion answering (IMcQA), which obtains richer context through the following\ntwo modules: explicit imagination by generating a short dummy document with\nlong context compress and implicit imagination with HyperNetwork for generating\nadapter weights. Experimental results on three datasets demonstrate that IMcQA\nexhibits significant advantages in both open-domain and closed-book settings,\nas well as in both in-distribution performance and out-of-distribution\ngeneralizations. Our code will be available at\nhttps://github.com/Xnhyacinth/IAG.\n","authors":["Huanxuan Liao","Shizhu He","Yao Xu","Yuanzhe Zhang","Kang Liu","Shengping Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.15268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15250v1","updated":"2024-03-22T14:47:35Z","published":"2024-03-22T14:47:35Z","title":"Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A\n  Multifaceted Statistical Approach","summary":"  Amidst the rapid evolution of LLMs, the significance of evaluation in\ncomprehending and propelling these models forward is increasingly paramount.\nEvaluations have revealed that factors such as scaling, training types,\narchitectures and other factors profoundly impact the performance of LLMs.\nHowever, the extent and nature of these impacts continue to be subjects of\ndebate because most assessments have been restricted to a limited number of\nmodels and data points. Clarifying the effects of these factors on performance\nscores can be more effectively achieved through a statistical lens. Our study\nembarks on a thorough re-examination of these LLMs, targeting the inadequacies\nin current evaluation methods. With the advent of a uniform evaluation\nframework, our research leverages an expansive dataset of evaluation results,\nintroducing a comprehensive statistical methodology. This includes the\napplication of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering\na robust and transparent approach to deciphering LLM performance data. Contrary\nto prevailing findings, our results challenge assumptions about emergent\nabilities and the influence of given training types and architectures in LLMs.\nThese findings furnish new perspectives on the characteristics, intrinsic\nnature, and developmental trajectories of LLMs. By providing straightforward\nand reliable methods to scrutinize and reassess LLM performance data, this\nstudy contributes a nuanced perspective on LLM efficiency and potentials.\n","authors":["Kun Sun","Rong Wang","Haitao Liu","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2403.15250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15246v1","updated":"2024-03-22T14:42:29Z","published":"2024-03-22T14:42:29Z","title":"FollowIR: Evaluating and Teaching Information Retrieval Models to Follow\n  Instructions","summary":"  Modern Large Language Models (LLMs) are capable of following long and complex\ninstructions that enable a diverse amount of user tasks. However, despite\nInformation Retrieval (IR) models using LLMs as the backbone of their\narchitectures, nearly all of them still only take queries as input, with no\ninstructions. For the handful of recent models that do take instructions, it's\nunclear how they use them. We introduce our dataset FollowIR, which contains a\nrigorous instruction evaluation benchmark as well as a training set for helping\nIR models learn to better follow real-world instructions. FollowIR builds off\nthe long history of the TREC conferences: as TREC provides human annotators\nwith instructions (also known as narratives) to determine document relevance,\nso should IR models be able to understand and decide relevance based on these\ndetailed instructions. Our evaluation benchmark starts with three deeply judged\nTREC collections and alters the annotator instructions, re-annotating relevant\ndocuments. Through this process, we can measure how well IR models follow\ninstructions, through a new pairwise evaluation framework. Our results indicate\nthat existing retrieval models fail to correctly use instructions, using them\nfor basic keywords and struggling to understand long-form information. However,\nwe show that it is possible for IR models to learn to follow complex\ninstructions: our new FollowIR-7B model has significant improvements (over 13%)\nafter fine-tuning on our training set.\n","authors":["Orion Weller","Benjamin Chang","Sean MacAvaney","Kyle Lo","Arman Cohan","Benjamin Van Durme","Dawn Lawrie","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2403.15246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15226v1","updated":"2024-03-22T14:20:34Z","published":"2024-03-22T14:20:34Z","title":"Not All Attention is Needed: Parameter and Computation Efficient\n  Transfer Learning for Multi-modal Large Language Models","summary":"  In this paper, we propose a novel parameter and computation efficient tuning\nmethod for Multi-modal Large Language Models (MLLMs), termed Efficient\nAttention Skipping (EAS). Concretely, we first reveal that multi-head\nattentions (MHAs), the main computational overhead of MLLMs, are often\nredundant to downstream tasks. Based on this observation, EAS evaluates the\nattention redundancy and skips the less important MHAs to speed up inference.\nBesides, we also propose a novel propagation-of-information adapter (PIA) to\nserve the attention skipping of EAS and keep parameter efficiency, which can be\nfurther re-parameterized into feed-forward networks (FFNs) for zero-extra\nlatency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN\nand a classic VL pre-trained model called METER, and conduct extensive\nexperiments on a set of benchmarks. The experiments show that EAS not only\nretains high performance and parameter efficiency, but also greatly speeds up\ninference speed. For instance, LaVIN-EAS can obtain 89.98\\% accuracy on\nScineceQA while speeding up inference by 2.2 times to LaVIN\n","authors":["Qiong Wu","Weihao Ye","Yiyi Zhou","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2403.15226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15214v1","updated":"2024-03-22T13:58:42Z","published":"2024-03-22T13:58:42Z","title":"InstaSynth: Opportunities and Challenges in Generating Synthetic\n  Instagram Data with ChatGPT for Sponsored Content Detection","summary":"  Large Language Models (LLMs) raise concerns about lowering the cost of\ngenerating texts that could be used for unethical or illegal purposes,\nespecially on social media. This paper investigates the promise of such models\nto help enforce legal requirements related to the disclosure of sponsored\ncontent online. We investigate the use of LLMs for generating synthetic\nInstagram captions with two objectives: The first objective (fidelity) is to\nproduce realistic synthetic datasets. For this, we implement content-level and\nnetwork-level metrics to assess whether synthetic captions are realistic. The\nsecond objective (utility) is to create synthetic data that is useful for\nsponsored content detection. For this, we evaluate the effectiveness of the\ngenerated synthetic data for training classifiers to identify undisclosed\nadvertisements on Instagram. Our investigations show that the objectives of\nfidelity and utility may conflict and that prompt engineering is a useful but\ninsufficient strategy. Additionally, we find that while individual synthetic\nposts may appear realistic, collectively they lack diversity, topic\nconnectivity, and realistic user interaction patterns.\n","authors":["Thales Bertaglia","Lily Heisig","Rishabh Kaushal","Adriana Iamnitchi"],"pdf_url":"https://arxiv.org/pdf/2403.15214v1.pdf","comment":"To appear at the 18th International AAAI Conference on Web and Social\n  Media (ICWSM 2024) -- please cite accordingly"},{"id":"http://arxiv.org/abs/2403.13592v2","updated":"2024-03-22T13:37:28Z","published":"2024-03-20T13:42:57Z","title":"Llama meets EU: Investigating the European Political Spectrum through\n  the Lens of LLMs","summary":"  Instruction-finetuned Large Language Models inherit clear political leanings\nthat have been shown to influence downstream task performance. We expand this\nline of research beyond the two-party system in the US and audit Llama Chat in\nthe context of EU politics in various settings to analyze the model's political\nknowledge and its ability to reason in context. We adapt, i.e., further\nfine-tune, Llama Chat on speeches of individual euro-parties from debates in\nthe European Parliament to reevaluate its political leaning based on the EUandI\nquestionnaire. Llama Chat shows considerable knowledge of national parties'\npositions and is capable of reasoning in context. The adapted, party-specific,\nmodels are substantially re-aligned towards respective positions which we see\nas a starting point for using chat-based LLMs as data-driven conversational\nengines to assist research in political science.\n","authors":["Ilias Chalkidis","Stephanie Brandl"],"pdf_url":"https://arxiv.org/pdf/2403.13592v2.pdf","comment":"accepted to NAACL 2024 as a short paper"},{"id":"http://arxiv.org/abs/2306.14899v2","updated":"2024-03-22T13:24:35Z","published":"2023-06-26T17:59:55Z","title":"FunQA: Towards Surprising Video Comprehension","summary":"  Surprising videos, such as funny clips, creative performances, or visual\nillusions, attract significant attention. Enjoyment of these videos is not\nsimply a response to visual stimuli; rather, it hinges on the human capacity to\nunderstand (and appreciate) commonsense violations depicted in these videos. We\nintroduce FunQA, a challenging video question-answering (QA) dataset\nspecifically designed to evaluate and enhance the depth of video reasoning\nbased on counter-intuitive and fun videos. Unlike most video QA benchmarks\nwhich focus on less surprising contexts, e.g., cooking or instructional videos,\nFunQA covers three previously unexplored types of surprising videos: 1)\nHumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous\nQA tasks designed to assess the model's capability in counter-intuitive\ntimestamp localization, detailed video description, and reasoning around\ncounter-intuitiveness. We also pose higher-level tasks, such as attributing a\nfitting and vivid title to the video and scoring the video creativity. In\ntotal, the FunQA benchmark consists of 312K free-text QA pairs derived from\n4.3K video clips, spanning a total of 24 video hours. Moreover, we propose\nFunMentor, an agent designed for Vision-Language Models (VLMs) that uses\nmulti-turn dialogues to enhance models' understanding of counter-intuitiveness.\nExtensive experiments with existing VLMs demonstrate the effectiveness of\nFunMentor and reveal significant performance gaps for the FunQA videos across\nspatial-temporal reasoning, visual-centered reasoning, and free-text\ngeneration.\n","authors":["Binzhu Xie","Sicheng Zhang","Zitang Zhou","Bo Li","Yuanhan Zhang","Jack Hessel","Jingkang Yang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2306.14899v2.pdf","comment":"Project Page: https://funqa-benchmark.github.io/ Codebase:\n  https://github.com/Jingkang50/FunQA"},{"id":"http://arxiv.org/abs/2403.15185v1","updated":"2024-03-22T13:13:13Z","published":"2024-03-22T13:13:13Z","title":"Investigating the Performance of Language Models for Completing Code in\n  Functional Programming Languages: a Haskell Case Study","summary":"  Language model-based code completion models have quickly grown in use,\nhelping thousands of developers write code in many different programming\nlanguages. However, research on code completion models typically focuses on\nimperative languages such as Python and JavaScript, which results in a lack of\nrepresentation for functional programming languages. Consequently, these models\noften perform poorly on functional languages such as Haskell. To investigate\nwhether this can be alleviated, we evaluate the performance of two language\nmodels for code, CodeGPT and UniXcoder, on the functional programming language\nHaskell. We fine-tune and evaluate the models on Haskell functions sourced from\na publicly accessible Haskell dataset on HuggingFace. Additionally, we manually\nevaluate the models using our novel translated HumanEval dataset. Our automatic\nevaluation shows that knowledge of imperative programming languages in the\npre-training of LLMs may not transfer well to functional languages, but that\ncode completion on functional languages is feasible. Consequently, this shows\nthe need for more high-quality Haskell datasets. A manual evaluation on\nHumanEval-Haskell indicates CodeGPT frequently generates empty predictions and\nextra comments, while UniXcoder more often produces incomplete or incorrect\npredictions. Finally, we release HumanEval-Haskell, along with the fine-tuned\nmodels and all code required to reproduce our experiments on GitHub\n(https://github.com/AISE-TUDelft/HaskellCCEval).\n","authors":["Tim van Dam","Frank van der Heijden","Philippe de Bekker","Berend Nieuwschepen","Marc Otten","Maliheh Izadi"],"pdf_url":"https://arxiv.org/pdf/2403.15185v1.pdf","comment":"To appear in the First Special Event on AI Foundation Models and\n  Software Engineering (FORGE 2024)"},{"id":"http://arxiv.org/abs/2403.14221v2","updated":"2024-03-22T12:34:47Z","published":"2024-03-21T08:21:12Z","title":"Improving the Robustness of Large Language Models via Consistency\n  Alignment","summary":"  Large language models (LLMs) have shown tremendous success in following user\ninstructions and generating helpful responses. Nevertheless, their robustness\nis still far from optimal, as they may generate significantly inconsistent\nresponses due to minor changes in the verbalized instructions. Recent\nliterature has explored this inconsistency issue, highlighting the importance\nof continued improvement in the robustness of response generation. However,\nsystematic analysis and solutions are still lacking. In this paper, we\nquantitatively define the inconsistency problem and propose a two-stage\ntraining framework consisting of instruction-augmented supervised fine-tuning\nand consistency alignment training. The first stage helps a model generalize on\nfollowing instructions via similar instruction augmentations. In the second\nstage, we improve the diversity and help the model understand which responses\nare more aligned with human expectations by differentiating subtle differences\nin similar responses. The training process is accomplished by self-rewards\ninferred from the trained model at the first stage without referring to\nexternal human preference resources. We conduct extensive experiments on recent\npublicly available LLMs on instruction-following tasks and demonstrate the\neffectiveness of our training framework.\n","authors":["Yukun Zhao","Lingyong Yan","Weiwei Sun","Guoliang Xing","Shuaiqiang Wang","Chong Meng","Zhicong Cheng","Zhaochun Ren","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2403.14221v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2311.02945v3","updated":"2024-03-22T12:18:51Z","published":"2023-11-06T08:26:14Z","title":"PhoGPT: Generative Pre-training for Vietnamese","summary":"  We open-source a state-of-the-art 4B-parameter generative model series for\nVietnamese, which includes the base pre-trained monolingual model PhoGPT-4B and\nits chat variant, PhoGPT-4B-Chat. The base model, PhoGPT-4B, with exactly 3.7B\nparameters, is pre-trained from scratch on a Vietnamese corpus of 102B tokens,\nwith an 8192 context length, employing a vocabulary of 20480 token types. The\nchat variant, PhoGPT-4B-Chat, is the modeling output obtained by fine-tuning\nPhoGPT-4B on a dataset of 70K instructional prompts and their responses, along\nwith an additional 290K conversations. In addition, we also demonstrate its\nsuperior performance compared to previous open-source models. Our PhoGPT models\nare available at: https://github.com/VinAIResearch/PhoGPT\n","authors":["Dat Quoc Nguyen","Linh The Nguyen","Chi Tran","Dung Ngoc Nguyen","Dinh Phung","Hung Bui"],"pdf_url":"https://arxiv.org/pdf/2311.02945v3.pdf","comment":"PhoGPT-4B Technical Report - 5 pages"},{"id":"http://arxiv.org/abs/2403.15137v1","updated":"2024-03-22T11:42:47Z","published":"2024-03-22T11:42:47Z","title":"CACA Agent: Capability Collaboration based AI Agent","summary":"  As AI Agents based on Large Language Models (LLMs) have shown potential in\npractical applications across various fields, how to quickly deploy an AI agent\nand how to conveniently expand the application scenario of AI agents has become\na challenge. Previous studies mainly focused on implementing all the reasoning\ncapabilities of AI agents within a single LLM, which often makes the model more\ncomplex and also reduces the extensibility of AI agent functionality. In this\npaper, we propose CACA Agent (Capability Collaboration based AI Agent), using\nan open architecture inspired by service computing. CACA Agent integrates a set\nof collaborative capabilities to implement AI Agents, not only reducing the\ndependence on a single LLM, but also enhancing the extensibility of both the\nplanning abilities and the tools available to AI agents. Utilizing the proposed\nsystem, we present a demo to illustrate the operation and the application\nscenario extension of CACA Agent.\n","authors":["Peng Xu","Haoran Wang","Chuang Wang","Xu Liu"],"pdf_url":"https://arxiv.org/pdf/2403.15137v1.pdf","comment":"4 pages,5 figures"},{"id":"http://arxiv.org/abs/2403.15115v1","updated":"2024-03-22T11:16:43Z","published":"2024-03-22T11:16:43Z","title":"Language Models in Dialogue: Conversational Maxims for Human-AI\n  Interactions","summary":"  Modern language models, while sophisticated, exhibit some inherent\nshortcomings, particularly in conversational settings. We claim that many of\nthe observed shortcomings can be attributed to violation of one or more\nconversational principles. By drawing upon extensive research from both the\nsocial science and AI communities, we propose a set of maxims -- quantity,\nquality, relevance, manner, benevolence, and transparency -- for describing\neffective human-AI conversation. We first justify the applicability of the\nfirst four maxims (from Grice) in the context of human-AI interactions. We then\nargue that two new maxims, benevolence (concerning the generation of, and\nengagement with, harmful content) and transparency (concerning recognition of\none's knowledge boundaries, operational constraints, and intents), are\nnecessary for addressing behavior unique to modern human-AI interactions. The\nproposed maxims offer prescriptive guidance on how to assess conversational\nquality between humans and LLM-driven conversational agents, informing both\ntheir evaluation and improved design.\n","authors":["Erik Miehling","Manish Nagireddy","Prasanna Sattigeri","Elizabeth M. Daly","David Piorkowski","John T. Richards"],"pdf_url":"https://arxiv.org/pdf/2403.15115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15112v1","updated":"2024-03-22T11:08:48Z","published":"2024-03-22T11:08:48Z","title":"Text clustering with LLM embeddings","summary":"  Text clustering is an important approach for organising the growing amount of\ndigital content, helping to structure and find hidden patterns in uncategorised\ndata. In this research, we investigated how different textual embeddings -\nparticularly those used in large language models (LLMs) - and clustering\nalgorithms affect how text datasets are clustered. A series of experiments were\nconducted to assess how embeddings influence clustering results, the role\nplayed by dimensionality reduction through summarisation, and embedding size\nadjustment. Results reveal that LLM embeddings excel at capturing the nuances\nof structured language, while BERT leads the lightweight options in\nperformance. In addition, we find that increasing embedding dimensionality and\nsummarisation techniques do not uniformly improve clustering efficiency,\nsuggesting that these strategies require careful analysis to use in real-life\nmodels. These results highlight a complex balance between the need for nuanced\ntext representation and computational feasibility in text clustering\napplications. This study extends traditional text clustering frameworks by\nincorporating embeddings from LLMs, thereby paving the way for improved\nmethodologies and opening new avenues for future research in various types of\ntextual analysis.\n","authors":["Alina Petukhova","Joao P. Matos-Carvalho","Nuno Fachada"],"pdf_url":"https://arxiv.org/pdf/2403.15112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15097v1","updated":"2024-03-22T10:32:43Z","published":"2024-03-22T10:32:43Z","title":"Argument-Aware Approach To Event Linking","summary":"  Event linking connects event mentions in text with relevant nodes in a\nknowledge base (KB). Prior research in event linking has mainly borrowed\nmethods from entity linking, overlooking the distinct features of events.\nCompared to the extensively explored entity linking task, events have more\ncomplex structures and can be more effectively distinguished by examining their\nassociated arguments. Moreover, the information-rich nature of events leads to\nthe scarcity of event KBs. This emphasizes the need for event linking models to\nidentify and classify event mentions not in the KB as ``out-of-KB,'' an area\nthat has received limited attention. In this work, we tackle these challenges\nby introducing an argument-aware approach. First, we improve event linking\nmodels by augmenting input text with tagged event argument information,\nfacilitating the recognition of key information about event mentions.\nSubsequently, to help the model handle ``out-of-KB'' scenarios, we synthesize\nout-of-KB training examples from in-KB instances through controlled\nmanipulation of event arguments. Our experiment across two test datasets showed\nsignificant enhancements in both in-KB and out-of-KB scenarios, with a notable\n22% improvement in out-of-KB evaluations.\n","authors":["I-Hung Hsu","Zihan Xue","Nilay Pochh","Sahil Bansal","Premkumar Natarajan","Jayanth Srinivasa","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2403.15097v1.pdf","comment":"Work In Progress"},{"id":"http://arxiv.org/abs/2403.15088v1","updated":"2024-03-22T10:12:10Z","published":"2024-03-22T10:12:10Z","title":"CHisIEC: An Information Extraction Corpus for Ancient Chinese History","summary":"  Natural Language Processing (NLP) plays a pivotal role in the realm of\nDigital Humanities (DH) and serves as the cornerstone for advancing the\nstructural analysis of historical and cultural heritage texts. This is\nparticularly true for the domains of named entity recognition (NER) and\nrelation extraction (RE). In our commitment to expediting ancient history and\nculture, we present the ``Chinese Historical Information Extraction\nCorpus''(CHisIEC). CHisIEC is a meticulously curated dataset designed to\ndevelop and evaluate NER and RE tasks, offering a resource to facilitate\nresearch in the field. Spanning a remarkable historical timeline encompassing\ndata from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the\nextensive temporal range and text heterogeneity inherent in Chinese historical\ndocuments. The dataset encompasses four distinct entity types and twelve\nrelation types, resulting in a meticulously labeled dataset comprising 14,194\nentities and 8,609 relations. To establish the robustness and versatility of\nour dataset, we have undertaken comprehensive experimentation involving models\nof various sizes and paradigms. Additionally, we have evaluated the\ncapabilities of Large Language Models (LLMs) in the context of tasks related to\nancient Chinese history. The dataset and code are available at\n\\url{https://github.com/tangxuemei1995/CHisIEC}.\n","authors":["Xuemei Tang","Zekun Deng","Qi Su","Hao Yang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15088v1.pdf","comment":"11 pages, 6 tables, 3 figures"},{"id":"http://arxiv.org/abs/2310.15851v2","updated":"2024-03-22T10:02:11Z","published":"2023-10-24T14:08:26Z","title":"Self-Guard: Empower the LLM to Safeguard Itself","summary":"  The jailbreak attack can bypass the safety measures of a Large Language Model\n(LLM), generating harmful content. This misuse of LLM has led to negative\nsocietal consequences. Currently, there are two main approaches to address\njailbreak attacks: safety training and safeguards. Safety training focuses on\nfurther training LLM to enhance its safety. On the other hand, safeguards\ninvolve implementing external models or filters to prevent harmful outputs.\nHowever, safety training has constraints in its ability to adapt to new attack\ntypes and often leads to a drop in model performance. Safeguards have proven to\nbe of limited help. To tackle these issues, we propose a novel approach called\nSelf-Guard, which combines the strengths of both safety methods. Self-Guard\nincludes two stages. In the first stage, we enhance the model's ability to\nassess harmful content, and in the second stage, we instruct the model to\nconsistently perform harmful content detection on its own responses. The\nexperiment has demonstrated that Self-Guard is robust against jailbreak\nattacks. In the bad case analysis, we find that LLM occasionally provides\nharmless responses to harmful queries. Additionally, we evaluated the general\ncapabilities of the LLM before and after safety training, providing evidence\nthat Self-Guard does not result in the LLM's performance degradation. In\nsensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM\nbut also can even mitigate this issue.\n","authors":["Zezhong Wang","Fangkai Yang","Lu Wang","Pu Zhao","Hongru Wang","Liang Chen","Qingwei Lin","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2310.15851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15062v1","updated":"2024-03-22T09:40:27Z","published":"2024-03-22T09:40:27Z","title":"Construction of a Japanese Financial Benchmark for Large Language Models","summary":"  With the recent development of large language models (LLMs), models that\nfocus on certain domains and languages have been discussed for their necessity.\nThere is also a growing need for benchmarks to evaluate the performance of\ncurrent LLMs in each domain. Therefore, in this study, we constructed a\nbenchmark comprising multiple tasks specific to the Japanese and financial\ndomains and performed benchmark measurements on some models. Consequently, we\nconfirmed that GPT-4 is currently outstanding, and that the constructed\nbenchmarks function effectively. According to our analysis, our benchmark can\ndifferentiate benchmark scores among models in all performance ranges by\ncombining tasks with different difficulties.\n","authors":["Masanori Hirano"],"pdf_url":"https://arxiv.org/pdf/2403.15062v1.pdf","comment":"9 pages, Joint Workshop of the 7th Financial Technology and Natural\n  Language Processing (FinNLP), the 5th Knowledge Discovery from Unstructured\n  Data in Financial Services (KDF), and The 4th Workshop on Economics and\n  Natural Language Processing (ECONLP) In conjunction with LREC-COLING-2024"},{"id":"http://arxiv.org/abs/2310.15929v2","updated":"2024-03-22T09:18:24Z","published":"2023-10-24T15:27:15Z","title":"E-Sparse: Boosting the Large Language Model Inference through\n  Entropy-based N:M Sparsity","summary":"  Traditional pruning methods are known to be challenging to work in Large\nLanguage Models (LLMs) for Generative AI because of their unaffordable training\nprocess and large computational demands. For the first time, we introduce the\ninformation entropy of hidden state features into a pruning metric design,\nnamely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse\nemploys the information richness to leverage the channel importance, and\nfurther incorporates several novel techniques to put it into effect: (1) it\nintroduces information entropy to enhance the significance of parameter weights\nand input feature norms as a novel pruning metric, and performs N:M sparsity\nwithout modifying the remaining weights. (2) it designs global naive shuffle\nand local block shuffle to quickly optimize the information distribution and\nadequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is\nimplemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA Ampere\nGPUs. Extensive experiments on the LLaMA family and OPT models show that\nE-Sparse can significantly speed up the model inference over the dense model\n(up to 1.53X) and obtain significant memory saving (up to 43.52%), with\nacceptable accuracy loss.\n","authors":["Yun Li","Lin Niu","Xipeng Zhang","Kai Liu","Jianchen Zhu","Zhanhui Kang"],"pdf_url":"https://arxiv.org/pdf/2310.15929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01479v2","updated":"2024-03-22T09:14:48Z","published":"2024-03-03T11:13:44Z","title":"Align-to-Distill: Trainable Attention Alignment for Knowledge\n  Distillation in Neural Machine Translation","summary":"  The advent of scalable deep models and large datasets has improved the\nperformance of Neural Machine Translation. Knowledge Distillation (KD) enhances\nefficiency by transferring knowledge from a teacher model to a more compact\nstudent model. However, KD approaches to Transformer architecture often rely on\nheuristics, particularly when deciding which teacher layers to distill from. In\nthis paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to\naddress the feature mapping problem by adaptively aligning student attention\nheads with their teacher counterparts during training. The Attention Alignment\nModule in A2D performs a dense head-by-head comparison between student and\nteacher attention heads across layers, turning the combinatorial mapping\nheuristics into a learning problem. Our experiments show the efficacy of A2D,\ndemonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb\nand WMT-2014 En->De, respectively, compared to Transformer baselines.\n","authors":["Heegon Jin","Seonil Son","Jemin Park","Youngseok Kim","Hyungjong Noh","Yeonsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2403.01479v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.15220v2","updated":"2024-03-22T09:03:10Z","published":"2024-02-23T09:29:19Z","title":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition","summary":"  Self-attention is an essential component of large language models(LLMs) but a\nsignificant source of inference latency for long sequences. In multi-tenant\nLLMs serving scenarios, the compute and memory operation cost of self-attention\ncan be optimized by using the probability that multiple LLM requests have\nshared system prompts in prefixes. In this paper, we introduce ChunkAttention,\na prefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the start-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.\n","authors":["Lu Ye","Ze Tao","Yong Huang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2402.15220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15042v1","updated":"2024-03-22T08:57:07Z","published":"2024-03-22T08:57:07Z","title":"LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement","summary":"  Pretrained large language models (LLMs) are currently state-of-the-art for\nsolving the vast majority of natural language processing tasks. While many\nreal-world applications still require fine-tuning to reach satisfactory levels\nof performance, many of them are in the low-data regime, making fine-tuning\nchallenging. To address this, we propose LLM2LLM, a targeted and iterative data\naugmentation strategy that uses a teacher LLM to enhance a small seed dataset\nby augmenting additional data that can be used for fine-tuning on a specific\ntask. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data,\n(2) evaluates and extracts data points that the model gets wrong, and (3) uses\na teacher LLM to generate synthetic data based on these incorrect data points,\nwhich are then added back into the training data. This approach amplifies the\nsignal from incorrectly predicted data points by the LLM during training and\nreintegrates them into the dataset to focus on more challenging examples for\nthe LLM. Our results show that LLM2LLM significantly enhances the performance\nof LLMs in the low-data regime, outperforming both traditional fine-tuning and\nother data augmentation baselines. LLM2LLM reduces the dependence on\nlabor-intensive data curation and paves the way for more scalable and\nperformant LLM solutions, allowing us to tackle data-constrained domains and\ntasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on\nCaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular\nfine-tuning in the low-data regime using a LLaMA2-7B student model.\n","authors":["Nicholas Lee","Thanakul Wattanawong","Sehoon Kim","Karttikeya Mangalam","Sheng Shen","Gopala Anumanchipali","Michael W. Mahoney","Kurt Keutzer","Amir Gholami"],"pdf_url":"https://arxiv.org/pdf/2403.15042v1.pdf","comment":"Our code is available at https://github.com/SqueezeAILab/LLM2LLM"},{"id":"http://arxiv.org/abs/2403.15040v1","updated":"2024-03-22T08:45:30Z","published":"2024-03-22T08:45:30Z","title":"ESG Classification by Implicit Rule Learning via GPT-4","summary":"  Environmental, social, and governance (ESG) factors are widely adopted as\nhigher investment return indicators. Accordingly, ongoing efforts are being\nmade to automate ESG evaluation with language models to extract signals from\nmassive web text easily. However, recent approaches suffer from a lack of\ntraining data, as rating agencies keep their evaluation metrics confidential.\nThis paper investigates whether state-of-the-art language models like GPT-4 can\nbe guided to align with unknown ESG evaluation criteria through strategies such\nas prompting, chain-of-thought reasoning, and dynamic in-context learning. We\ndemonstrate the efficacy of these approaches by ranking 2nd in the Shared-Task\nML-ESG-3 Impact Type track for Korean without updating the model on the\nprovided training data. We also explore how adjusting prompts impacts the\nability of language models to address financial tasks leveraging smaller models\nwith openly available weights. We observe longer general pre-training to\ncorrelate with enhanced performance in financial downstream tasks. Our findings\nshowcase the potential of language models to navigate complex, subjective\nevaluation guidelines despite lacking explicit training examples, revealing\nopportunities for training-free solutions for financial downstream tasks.\n","authors":["Hyo Jeong Yun","Chanyoung Kim","Moonjeong Hahm","Kyuri Kim","Guijin Son"],"pdf_url":"https://arxiv.org/pdf/2403.15040v1.pdf","comment":"Accepted as Shared Track Paper at 7th FinNLP Workshop @ LREC-COLING\n  2024"},{"id":"http://arxiv.org/abs/2403.02893v2","updated":"2024-03-22T07:44:32Z","published":"2024-03-05T11:57:21Z","title":"Zero-Shot Cross-Lingual Document-Level Event Causality Identification\n  with Heterogeneous Graph Contrastive Transfer Learning","summary":"  Event Causality Identification (ECI) refers to the detection of causal\nrelations between events in texts. However, most existing studies focus on\nsentence-level ECI with high-resource languages, leaving more challenging\ndocument-level ECI (DECI) with low-resource languages under-explored. In this\npaper, we propose a Heterogeneous Graph Interaction Model with\nMulti-granularity Contrastive Transfer Learning (GIMC) for zero-shot\ncross-lingual document-level ECI. Specifically, we introduce a heterogeneous\ngraph interaction network to model the long-distance dependencies between\nevents that are scattered over a document. Then, to improve cross-lingual\ntransferability of causal knowledge learned from the source language, we\npropose a multi-granularity contrastive transfer learning module to align the\ncausal representations across languages. Extensive experiments show our\nframework outperforms the previous state-of-the-art model by 9.4% and 8.2% of\naverage F1 score on monolingual and multilingual scenarios respectively.\nNotably, in the multilingual scenario, our zero-shot framework even exceeds\nGPT-3.5 with few-shot learning by 24.3% in overall performance.\n","authors":["Zhitao He","Pengfei Cao","Zhuoran Jin","Yubo Chen","Kang Liu","Zhiqiang Zhang","Mengshu Sun","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.02893v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.05574v2","updated":"2024-03-22T07:34:38Z","published":"2024-02-26T09:10:34Z","title":"HealMe: Harnessing Cognitive Reframing in Large Language Models for\n  Psychotherapy","summary":"  Large Language Models (LLMs) can play a vital role in psychotherapy by\nadeptly handling the crucial task of cognitive reframing and overcoming\nchallenges such as shame, distrust, therapist skill variability, and resource\nscarcity. Previous LLMs in cognitive reframing mainly converted negative\nemotions to positive ones, but these approaches have limited efficacy, often\nnot promoting clients' self-discovery of alternative perspectives. In this\npaper, we unveil the Helping and Empowering through Adaptive Language in Mental\nEnhancement (HealMe) model. This novel cognitive reframing therapy method\neffectively addresses deep-rooted negative thoughts and fosters rational,\nbalanced perspectives. Diverging from traditional LLM methods, HealMe employs\nempathetic dialogue based on psychotherapeutic frameworks. It systematically\nguides clients through distinguishing circumstances from feelings,\nbrainstorming alternative viewpoints, and developing empathetic, actionable\nsuggestions. Moreover, we adopt the first comprehensive and expertly crafted\npsychological evaluation metrics, specifically designed to rigorously assess\nthe performance of cognitive reframing, in both AI-simulated dialogues and\nreal-world therapeutic conversations. Experimental results show that our model\noutperforms others in terms of empathy, guidance, and logical coherence,\ndemonstrating its effectiveness and potential positive impact on psychotherapy.\n","authors":["Mengxi Xiao","Qianqian Xie","Ziyan Kuang","Zhicheng Liu","Kailai Yang","Min Peng","Weiguang Han","Jimin Huang"],"pdf_url":"https://arxiv.org/pdf/2403.05574v2.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2304.14178v2","updated":"2024-03-22T07:23:22Z","published":"2023-04-27T13:27:01Z","title":"mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality","summary":"  Large language models (LLMs) have demonstrated impressive zero-shot abilities\non a variety of open-ended tasks, while recent research has also explored the\nuse of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,\na novel training paradigm that equips LLMs with multi-modal abilities through\nmodularized learning of foundation LLM, a visual knowledge module, and a visual\nabstractor module. This approach can support multiple modalities and facilitate\ndiverse unimodal and multimodal abilities through modality collaboration. The\ntraining paradigm of mPLUG-Owl involves a two-stage method for aligning image\nand text, which learns visual knowledge with the assistance of LLM while\nmaintaining and even improving the generation abilities of LLM. In the first\nstage, the visual knowledge module and abstractor module are trained with a\nfrozen LLM module to align the image and text. In the second stage,\nlanguage-only and multi-modal supervised datasets are used to jointly fine-tune\na low-rank adaption (LoRA) module on LLM and the abstractor module by freezing\nthe visual knowledge module. We carefully build a visually-related instruction\nevaluation set OwlEval. Experimental results show that our model outperforms\nexisting multi-modal models, demonstrating mPLUG-Owl's impressive instruction\nand visual understanding ability, multi-turn conversation ability, and\nknowledge reasoning ability. Besides, we observe some unexpected and exciting\nabilities such as multi-image correlation and scene text understanding, which\nmakes it possible to leverage it for harder real scenarios, such as vision-only\ndocument comprehension. Our code, pre-trained model, instruction-tuned models,\nand evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The\nonline demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.\n","authors":["Qinghao Ye","Haiyang Xu","Guohai Xu","Jiabo Ye","Ming Yan","Yiyang Zhou","Junyang Wang","Anwen Hu","Pengcheng Shi","Yaya Shi","Chenliang Li","Yuanhong Xu","Hehong Chen","Junfeng Tian","Qi Qian","Ji Zhang","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2304.14178v2.pdf","comment":"Working in Process"},{"id":"http://arxiv.org/abs/2403.14990v1","updated":"2024-03-22T06:47:42Z","published":"2024-03-22T06:47:42Z","title":"MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic\n  Textual Relatedness","summary":"  This paper presents the MasonTigers entry to the SemEval-2024 Task 1 -\nSemantic Textual Relatedness. The task encompasses supervised (Track A),\nunsupervised (Track B), and cross-lingual (Track C) approaches across 14\ndifferent languages. MasonTigers stands out as one of the two teams who\nparticipated in all languages across the three tracks. Our approaches achieved\nrankings ranging from 11th to 21st in Track A, from 1st to 8th in Track B, and\nfrom 5th to 12th in Track C. Adhering to the task-specific constraints, our\nbest performing approaches utilize ensemble of statistical machine learning\napproaches combined with language-specific BERT based models and sentence\ntransformers.\n","authors":["Dhiman Goswami","Sadiya Sayara Chowdhury Puspo","Md Nishat Raihan","Al Nahian Bin Emran","Amrita Ganguly","Marcos Zampieri"],"pdf_url":"https://arxiv.org/pdf/2403.14990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14989v1","updated":"2024-03-22T06:47:28Z","published":"2024-03-22T06:47:28Z","title":"MasonTigers at SemEval-2024 Task 8: Performance Analysis of\n  Transformer-based Models on Machine-Generated Text Detection","summary":"  This paper presents the MasonTigers entry to the SemEval-2024 Task 8 -\nMultigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text\nDetection. The task encompasses Binary Human-Written vs. Machine-Generated Text\nClassification (Track A), Multi-Way Machine-Generated Text Classification\n(Track B), and Human-Machine Mixed Text Detection (Track C). Our best\nperforming approaches utilize mainly the ensemble of discriminator transformer\nmodels along with sentence transformer and statistical machine learning\napproaches in specific cases. Moreover, zero-shot prompting and fine-tuning of\nFLAN-T5 are used for Track A and B.\n","authors":["Sadiya Sayara Chowdhury Puspo","Md Nishat Raihan","Dhiman Goswami","Al Nahian Bin Emran","Amrita Ganguly","Ozlem Uzuner"],"pdf_url":"https://arxiv.org/pdf/2403.14989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14988v1","updated":"2024-03-22T06:46:40Z","published":"2024-03-22T06:46:40Z","title":"Risk and Response in Large Language Models: Evaluating Key Threat\n  Categories","summary":"  This paper explores the pressing issue of risk assessment in Large Language\nModels (LLMs) as they become increasingly prevalent in various applications.\nFocusing on how reward models, which are designed to fine-tune pretrained LLMs\nto align with human values, perceive and categorize different types of risks,\nwe delve into the challenges posed by the subjective nature of preference-based\ntraining data. By utilizing the Anthropic Red-team dataset, we analyze major\nrisk categories, including Information Hazards, Malicious Uses, and\nDiscrimination/Hateful content. Our findings indicate that LLMs tend to\nconsider Information Hazards less harmful, a finding confirmed by a specially\ndeveloped regression model. Additionally, our analysis shows that LLMs respond\nless stringently to Information Hazards compared to other risks. The study\nfurther reveals a significant vulnerability of LLMs to jailbreaking attacks in\nInformation Hazard scenarios, highlighting a critical security concern in LLM\nrisk assessment and emphasizing the need for improved AI safety measures.\n","authors":["Bahareh Harandizadeh","Abel Salinas","Fred Morstatter"],"pdf_url":"https://arxiv.org/pdf/2403.14988v1.pdf","comment":"19 pages, 14 figures"},{"id":"http://arxiv.org/abs/2402.14704v3","updated":"2024-03-22T06:45:51Z","published":"2024-02-22T17:04:30Z","title":"An LLM-Enhanced Adversarial Editing System for Lexical Simplification","summary":"  Lexical Simplification (LS) aims to simplify text at the lexical level.\nExisting methods rely heavily on annotated data, making it challenging to apply\nin low-resource scenarios. In this paper, we propose a novel LS method without\nparallel corpora. This method employs an Adversarial Editing System with\nguidance from a confusion loss and an invariance loss to predict lexical edits\nin the original sentences. Meanwhile, we introduce an innovative LLM-enhanced\nloss to enable the distillation of knowledge from Large Language Models (LLMs)\ninto a small-size LS system. From that, complex words within sentences are\nmasked and a Difficulty-aware Filling module is crafted to replace masked\npositions with simpler words. At last, extensive experimental results and\nanalyses on three benchmark LS datasets demonstrate the effectiveness of our\nproposed method.\n","authors":["Keren Tan","Kangyang Luo","Yunshi Lan","Zheng Yuan","Jinlong Shu"],"pdf_url":"https://arxiv.org/pdf/2402.14704v3.pdf","comment":"Accepted by COLING 2024 main conference"},{"id":"http://arxiv.org/abs/2403.14982v1","updated":"2024-03-22T06:31:49Z","published":"2024-03-22T06:31:49Z","title":"MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of\n  Chain-of-Thoughts","summary":"  Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 -\nwhich provides a dataset of puzzles for testing natural language understanding.\nWe employ large language models (LLMs) to solve this task through several\nprompting techniques. Zero-shot and few-shot prompting generate reasonably good\nresults when tested with proprietary LLMs, compared to the open-source models.\nWe obtain further improved results with chain-of-thought prompting, an\niterative prompting method that breaks down the reasoning process step-by-step.\nWe obtain our best results by utilizing an ensemble of chain-of-thought\nprompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle\nsubtask. The strong performance of prompted LLMs demonstrates their capability\nfor complex reasoning when provided with a decomposition of the thought\nprocess. Our work sheds light on how step-wise explanatory prompts can unlock\nmore of the knowledge encoded in the parameters of large models.\n","authors":["Md Nishat Raihan","Dhiman Goswami","Al Nahian Bin Emran","Sadiya Sayara Chowdhury Puspo","Amrita Ganguly","Marcos Zampieri"],"pdf_url":"https://arxiv.org/pdf/2403.14982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14428v2","updated":"2024-03-22T06:29:26Z","published":"2024-02-22T10:17:57Z","title":"KoCoSa: Korean Context-aware Sarcasm Detection Dataset","summary":"  Sarcasm is a way of verbal irony where someone says the opposite of what they\nmean, often to ridicule a person, situation, or idea. It is often difficult to\ndetect sarcasm in the dialogue since detecting sarcasm should reflect the\ncontext (i.e., dialogue history). In this paper, we introduce a new dataset for\nthe Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware\nSarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and\nthe labels for this task on the last response. To build the dataset, we propose\nan efficient sarcasm detection dataset generation pipeline: 1) generating new\nsarcastic dialogues from source dialogues with large language models, 2)\nautomatic and manual filtering of abnormal and toxic dialogues, and 3) human\nannotation for the sarcasm detection task. We also provide a simple but\neffective baseline for the Korean sarcasm detection task trained on our\ndataset. Experimental results on the dataset show that our baseline system\noutperforms strong baselines like large language models, such as GPT-3.5, in\nthe Korean sarcasm detection task. We show that the sarcasm detection task\nrelies deeply on the existence of sufficient context. We will release the\ndataset at https://github.com/Yu-billie/KoCoSa_sarcasm_detection.\n","authors":["Yumin Kim","Heejae Suh","Mingi Kim","Dongyeon Won","Hwanhee Lee"],"pdf_url":"https://arxiv.org/pdf/2402.14428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12373v3","updated":"2024-03-22T06:18:54Z","published":"2024-03-19T02:34:18Z","title":"RankPrompt: Step-by-Step Comparisons Make Language Models Better\n  Reasoners","summary":"  Large Language Models (LLMs) have achieved impressive performance across\nvarious reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT\nare prone to logical errors during their reasoning processes. Existing\nsolutions, such as deploying task-specific verifiers or voting over multiple\nreasoning paths, either require extensive human annotations or fail in\nscenarios with inconsistent responses. To address these challenges, we\nintroduce RankPrompt, a new prompting method that enables LLMs to self-rank\ntheir responses without additional resources. RankPrompt breaks down the\nranking problem into a series of comparisons among diverse responses,\nleveraging the inherent capabilities of LLMs to generate chains of comparison\nas contextual exemplars. Our experiments across 11 arithmetic and commonsense\nreasoning tasks show that RankPrompt significantly enhances the reasoning\nperformance of ChatGPT and GPT-4, with improvements of up to 13%. Moreover,\nRankPrompt excels in LLM-based automatic evaluations for open-ended tasks,\naligning with human judgments 74% of the time in the AlpacaEval dataset. It\nalso exhibits robustness to variations in response order and consistency.\nCollectively, our results validate RankPrompt as an effective method for\neliciting high-quality feedback from language models.\n","authors":["Chi Hu","Yuan Ge","Xiangnan Ma","Hang Cao","Qiang Li","Yonghua Yang","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.12373v3.pdf","comment":"LREC-Coling 2024 Long Paper"},{"id":"http://arxiv.org/abs/2403.14972v1","updated":"2024-03-22T06:03:07Z","published":"2024-03-22T06:03:07Z","title":"A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal\n  Reasoning","summary":"  This paper presents a pilot study aimed at introducing multi-agent debate\ninto multimodal reasoning. The study addresses two key challenges: the\ntrivialization of opinions resulting from excessive summarization and the\ndiversion of focus caused by distractor concepts introduced from images. These\nchallenges stem from the inductive (bottom-up) nature of existing debating\nschemes. To address the issue, we propose a deductive (top-down) debating\napproach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are\nconfined to a blueprint graph to prevent opinion trivialization through\nworld-level summarization. Moreover, by storing evidence in branches within the\ngraph, BDoG mitigates distractions caused by frequent but irrelevant concepts.\nExtensive experiments validate BDoG, achieving state-of-the-art results in\nScience QA and MMBench with significant improvements over previous methods.\n","authors":["Changmeng Zheng","Dayong Liang","Wengyu Zhang","Xiao-Yong Wei","Tat-Seng Chua","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.14972v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2309.10435v3","updated":"2024-03-22T05:57:48Z","published":"2023-09-19T08:54:47Z","title":"Language Modeling for Content-enriched Recommendation","summary":"  Recommender systems are indispensable in the realm of online applications,\nand sequential recommendation has enjoyed considerable prevalence due to its\ncapacity to encapsulate the dynamic shifts in user interests. However, previous\nsequential modeling methods still have limitations in capturing contextual\ninformation. The primary reason is the lack of understanding of domain-specific\nknowledge and item-related textual content by language models. Fortunately, the\nemergence of powerful language models has unlocked the potential to incorporate\nextensive world knowledge into recommendation algorithms, enabling them to go\nbeyond simple item attributes and truly understand the world surrounding user\npreferences. To achieve this, we propose LANCER, which leverages the semantic\nunderstanding capabilities of pre-trained language models to generate\npersonalized recommendations. Our approach bridges the gap between language\nmodels and recommender systems, resulting in more human-like recommendations.\nWe demonstrate the effectiveness of our approach through a series of\nexperiments conducted on multiple benchmark datasets, showing promising results\nand providing valuable insights into the influence of our model on sequential\nrecommendation tasks. Furthermore, our experimental codes are publicly\navailable.\n","authors":["Junzhe Jiang","Shang Qu","Mingyue Cheng","Qi Liu","Zhiding Liu","Hao Zhang","Rujiao Zhang","Kai Zhang","Rui Li","Jiatong Li","Min Gao"],"pdf_url":"https://arxiv.org/pdf/2309.10435v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14958v1","updated":"2024-03-22T05:23:31Z","published":"2024-03-22T05:23:31Z","title":"Adapprox: Adaptive Approximation in Adam Optimization via Randomized\n  Low-Rank Matrices","summary":"  As deep learning models exponentially increase in size, optimizers such as\nAdam encounter significant memory consumption challenges due to the storage of\nfirst and second moment data. Current memory-efficient methods like Adafactor\nand CAME often compromise accuracy with their matrix factorization techniques.\nAddressing this, we introduce Adapprox, a novel approach that employs\nrandomized low-rank matrix approximation for a more effective and accurate\napproximation of Adam's second moment. Adapprox features an adaptive rank\nselection mechanism, finely balancing accuracy and memory efficiency, and\nincludes an optional cosine similarity guidance strategy to enhance stability\nand expedite convergence. In GPT-2 training and downstream tasks, Adapprox\nsurpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings\nfor the 117M and 345M models, respectively, with the first moment enabled, and\nfurther increases these savings without the first moment. Besides, it enhances\nconvergence speed and improves downstream task performance relative to its\ncounterparts.\n","authors":["Pengxiang Zhao","Ping Li","Yingjie Gu","Yi Zheng","Stephan Ludger Kölker","Zhefeng Wang","Xiaoming Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.14958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14952v1","updated":"2024-03-22T05:05:45Z","published":"2024-03-22T05:05:45Z","title":"Evidence-Driven Retrieval Augmented Response Generation for Online\n  Misinformation","summary":"  The proliferation of online misinformation has posed significant threats to\npublic interest. While numerous online users actively participate in the combat\nagainst misinformation, many of such responses can be characterized by the lack\nof politeness and supporting facts. As a solution, text generation approaches\nare proposed to automatically produce counter-misinformation responses.\nNevertheless, existing methods are often trained end-to-end without leveraging\nexternal knowledge, resulting in subpar text quality and excessively repetitive\nresponses. In this paper, we propose retrieval augmented response generation\nfor online misinformation (RARG), which collects supporting evidence from\nscientific sources and generates counter-misinformation responses based on the\nevidences. In particular, our RARG consists of two stages: (1) evidence\ncollection, where we design a retrieval pipeline to retrieve and rerank\nevidence documents using a database comprising over 1M academic articles; (2)\nresponse generation, in which we align large language models (LLMs) to generate\nevidence-based responses via reinforcement learning from human feedback (RLHF).\nWe propose a reward function to maximize the utilization of the retrieved\nevidence while maintaining the quality of the generated text, which yields\npolite and factual responses that clearly refutes misinformation. To\ndemonstrate the effectiveness of our method, we study the case of COVID-19 and\nperform extensive experiments with both in- and cross-domain datasets, where\nRARG consistently outperforms baselines by generating high-quality\ncounter-misinformation responses.\n","authors":["Zhenrui Yue","Huimin Zeng","Yimeng Lu","Lanyu Shang","Yang Zhang","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14952v1.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14950v1","updated":"2024-03-22T04:48:41Z","published":"2024-03-22T04:48:41Z","title":"KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable\n  Adaptation","summary":"  Parameter-efficient finetuning (PEFT) is a key technique for adapting large\nlanguage models (LLMs) to downstream tasks. In this paper, we study leveraging\nknowledge graph embeddings to improve the effectiveness of PEFT. We propose a\nknowledgeable adaptation method called KnowLA. It inserts an adaptation layer\ninto an LLM to integrate the embeddings of entities appearing in the input\ntext. The adaptation layer is trained in combination with LoRA on instruction\ndata. Experiments on six benchmarks with two popular LLMs and three knowledge\ngraphs demonstrate the effectiveness and robustness of KnowLA. We show that\n\\modelname can help activate the relevant parameterized knowledge in an LLM to\nanswer a question without changing its parameters or input prompts.\n","authors":["Xindi Luo","Zequn Sun","Jing Zhao","Zhe Zhao","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2403.14950v1.pdf","comment":"Accepted in the 2024 Annual Conference of the North American Chapter\n  of the Association for Computational Linguistics (NAACL 2024)"},{"id":"http://arxiv.org/abs/2403.14946v1","updated":"2024-03-22T04:38:42Z","published":"2024-03-22T04:38:42Z","title":"A Single Linear Layer Yields Task-Adapted Low-Rank Matrices","summary":"  Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning\n(PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix\n$\\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study\nsuggested that there is correlation between $W_0$ and $\\Delta W$. In this\nstudy, we aim to delve deeper into relationships between $W_0$ and low-rank\nmatrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular,\nwe analyze a conversion matrix that transform $W_0$ into low-rank matrices,\nwhich encapsulates information about the relationships. Our analysis reveals\nthat the conversion matrices are similar across each layer. Inspired by these\nfindings, we hypothesize that a single linear layer, which takes each layer's\n$W_0$ as input, can yield task-adapted low-rank matrices. To confirm this\nhypothesis, we devise a method named Conditionally Parameterized LoRA\n(CondLoRA) that updates initial weight matrices with low-rank matrices derived\nfrom a single linear layer. Our empirical results show that CondLoRA maintains\na performance on par with LoRA, despite the fact that the trainable parameters\nof CondLoRA are fewer than those of LoRA. Therefore, we conclude that \"a single\nlinear layer yields task-adapted low-rank matrices.\"\n","authors":["Hwichan Kim","Shota Sasaki","Sho Hoshino","Ukyo Honda"],"pdf_url":"https://arxiv.org/pdf/2403.14946v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.14938v1","updated":"2024-03-22T04:13:10Z","published":"2024-03-22T04:13:10Z","title":"On Zero-Shot Counterspeech Generation by LLMs","summary":"  With the emergence of numerous Large Language Models (LLM), the usage of such\nmodels in various Natural Language Processing (NLP) applications is increasing\nextensively. Counterspeech generation is one such key task where efforts are\nmade to develop generative models by fine-tuning LLMs with hatespeech -\ncounterspeech pairs, but none of these attempts explores the intrinsic\nproperties of large language models in zero-shot settings. In this work, we\npresent a comprehensive analysis of the performances of four LLMs namely GPT-2,\nDialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech\ngeneration, which is the first of its kind. For GPT-2 and DialoGPT, we further\ninvestigate the deviation in performance with respect to the sizes (small,\nmedium, large) of the models. On the other hand, we propose three different\nprompting strategies for generating different types of counterspeech and\nanalyse the impact of such strategies on the performance of the models. Our\nanalysis shows that there is an improvement in generation quality for two\ndatasets (17%), however the toxicity increase (25%) with increase in model\nsize. Considering type of model, GPT-2 and FlanT5 models are significantly\nbetter in terms of counterspeech quality but also have high toxicity as\ncompared to DialoGPT. ChatGPT are much better at generating counter speech than\nother models across all metrics. In terms of prompting, we find that our\nproposed strategies help in improving counter speech generation across all the\nmodels.\n","authors":["Punyajoy Saha","Aalok Agrawal","Abhik Jana","Chris Biemann","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2403.14938v1.pdf","comment":"12 pages, 7 tables, accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2305.09955v3","updated":"2024-03-22T04:04:41Z","published":"2023-05-17T05:25:27Z","title":"Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized\n  Language Models","summary":"  By design, large language models (LLMs) are static general-purpose models,\nexpensive to retrain or update frequently. As they are increasingly adopted for\nknowledge-intensive tasks, it becomes evident that these design choices lead to\nfailures to generate factual, relevant, and up-to-date knowledge. To this end,\nwe propose Knowledge Card, a modular framework to plug in new factual and\nrelevant knowledge into general-purpose LLMs. We first introduce knowledge\ncards -- specialized language models trained on corpora from specific domains\nand sources. Knowledge cards serve as parametric repositories that are selected\nat inference time to generate background knowledge for the base LLM. We then\npropose three content selectors to dynamically select and retain information in\ndocuments generated by knowledge cards, specifically controlling for relevance,\nbrevity, and factuality of outputs. Finally, we propose two complementary\nintegration approaches to augment the base LLM with the (relevant, factual)\nknowledge curated from the specialized LMs. Through extensive experiments, we\ndemonstrate that Knowledge Card achieves state-of-the-art performance on six\nbenchmark datasets. Ultimately, Knowledge Card framework enables dynamic\nsynthesis and updates of knowledge from diverse domains. Its modularity will\nensure that relevant knowledge can be continuously updated through the\ncollective efforts of the research community.\n","authors":["Shangbin Feng","Weijia Shi","Yuyang Bai","Vidhisha Balachandran","Tianxing He","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2305.09955v3.pdf","comment":"ICLR 2024, oral"},{"id":"http://arxiv.org/abs/2403.14932v1","updated":"2024-03-22T03:23:58Z","published":"2024-03-22T03:23:58Z","title":"Attention-Driven Reasoning: Unlocking the Potential of Large Language\n  Models","summary":"  Large Language Models (LLMs) have shown remarkable capabilities, but their\nreasoning abilities and underlying mechanisms remain poorly understood. We\npresent a novel approach to enhance LLMs' reasoning through attention mechanism\noptimization, without additional training data. We identify inefficiencies in\nthe attention distribution caused by non-semantic tokens and propose an\nalgorithm to re-balance the skewed distribution, enabling the model to abstract\nmore nuanced knowledge. Our experiments demonstrate significantly improved\nreasoning capabilities, particularly for non-STEM questions. We provide\ninsights into the role of attention patterns in LLMs' reasoning and propose a\nmethod to enhance these abilities, paving the way for more powerful and\nversatile language models.\n","authors":["Bingli Liao","Danilo Vasconcellos Vargas"],"pdf_url":"https://arxiv.org/pdf/2403.14932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14919v1","updated":"2024-03-22T02:44:05Z","published":"2024-03-22T02:44:05Z","title":"Hierarchical Skip Decoding for Efficient Autoregressive Text Generation","summary":"  Autoregressive decoding strategy is a commonly used method for text\ngeneration tasks with pre-trained language models, while early-exiting is an\neffective approach to speedup the inference stage. In this work, we propose a\nnovel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient\nautoregressive text generation. Different from existing methods that require\nadditional trainable components, HSD is a plug-and-play method applicable to\nautoregressive text generation models, it adaptively skips decoding layers in a\nhierarchical manner based on the current sequence length, thereby reducing\ncomputational workload and allocating computation resources. Comprehensive\nexperiments on five text generation datasets with pre-trained language models\ndemonstrate HSD's advantages in balancing efficiency and text quality. With\nalmost half of the layers skipped, HSD can sustain 90% of the text quality\ncompared to vanilla autoregressive decoding, outperforming the competitive\napproaches.\n","authors":["Yunqi Zhu","Xuebing Yang","Yuanyuan Wu","Wensheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12038v3","updated":"2024-03-22T02:24:57Z","published":"2023-08-23T09:55:41Z","title":"Large Multilingual Models Pivot Zero-Shot Multimodal Learning across\n  Languages","summary":"  Recently there has been a significant surge in multimodal learning in terms\nof both image-to-text and text-to-image generation. However, the success is\ntypically limited to English, leaving other languages largely behind. Building\na competitive counterpart in other languages is highly challenging due to the\nlow-resource nature of non-English multimodal data (i.e., lack of large-scale,\nhigh-quality image-text data). In this work, we propose MPM, an effective\ntraining paradigm for training large multimodal models in non-English\nlanguages. MPM demonstrates that Multilingual language models can Pivot\nzero-shot Multimodal learning across languages. Specifically, based on a strong\nmultilingual large language model, multimodal models pretrained on English-only\nimage-text data can well generalize to other languages in a (quasi)-zero-shot\nmanner, even surpassing models trained on image-text data in native languages.\nTaking Chinese as a practice of MPM, we build large multimodal models VisCPM in\nimage-to-text and text-to-image generation, which achieve state-of-the-art\n(open-source) performance in Chinese. To facilitate future research, we\nopen-source codes and model weights at https://github.com/OpenBMB/VisCPM.git.\n","authors":["Jinyi Hu","Yuan Yao","Chongyi Wang","Shan Wang","Yinxu Pan","Qianyu Chen","Tianyu Yu","Hanghao Wu","Yue Zhao","Haoye Zhang","Xu Han","Yankai Lin","Jiao Xue","Dahai Li","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2308.12038v3.pdf","comment":"https://github.com/OpenBMB/VisCPM.git"},{"id":"http://arxiv.org/abs/2305.14521v2","updated":"2024-03-22T01:20:41Z","published":"2023-05-23T20:49:45Z","title":"Few-shot Adaption to Distribution Shifts By Mixing Source and Target\n  Embeddings","summary":"  Pretrained machine learning models need to be adapted to distribution shifts\nwhen deployed in new target environments. When obtaining labeled data from the\ntarget distribution is expensive, few-shot adaptation with only a few examples\nfrom the target distribution becomes essential. In this work, we propose\nMixPro, a lightweight and highly data-efficient approach for few-shot\nadaptation. MixPro first generates a relatively large dataset by mixing\n(linearly combining) pre-trained embeddings of large source data with those of\nthe few target examples. This process preserves important features of both\nsource and target distributions, while mitigating the specific noise in the\nsmall target data. Then, it trains a linear classifier on the mixed embeddings\nto effectively adapts the model to the target distribution without overfitting\nthe small target data. Theoretically, we demonstrate the advantages of MixPro\nover previous methods. Our experiments, conducted across various model\narchitectures on 8 datasets featuring different types of distribution shifts,\nreveal that MixPro can outperform baselines by up to 7\\%, with only 2-4 target\nexamples.\n","authors":["Yihao Xue","Ali Payani","Yu Yang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2305.14521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09738v3","updated":"2024-03-22T01:08:42Z","published":"2024-03-13T18:16:21Z","title":"Evaluating Large Language Models as Generative User Simulators for\n  Conversational Recommendation","summary":"  Synthetic users are cost-effective proxies for real users in the evaluation\nof conversational recommender systems. Large language models show promise in\nsimulating human-like behavior, raising the question of their ability to\nrepresent a diverse population of users. We introduce a new protocol to measure\nthe degree to which language models can accurately emulate human behavior in\nconversational recommendation. This protocol is comprised of five tasks, each\ndesigned to evaluate a key property that a synthetic user should exhibit:\nchoosing which items to talk about, expressing binary preferences, expressing\nopen-ended preferences, requesting recommendations, and giving feedback.\nThrough evaluation of baseline simulators, we demonstrate these tasks\neffectively reveal deviations of language models from human behavior, and offer\ninsights on how to reduce the deviations with model selection and prompting\nstrategies.\n","authors":["Se-eun Yoon","Zhankui He","Jessica Maria Echterhoff","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2403.09738v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.14895v1","updated":"2024-03-22T00:58:28Z","published":"2024-03-22T00:58:28Z","title":"Stance Reasoner: Zero-Shot Stance Detection on Social Media with\n  Explicit Reasoning","summary":"  Social media platforms are rich sources of opinionated content. Stance\ndetection allows the automatic extraction of users' opinions on various topics\nfrom such content. We focus on zero-shot stance detection, where the model's\nsuccess relies on (a) having knowledge about the target topic; and (b) learning\ngeneral reasoning strategies that can be employed for new topics. We present\nStance Reasoner, an approach to zero-shot stance detection on social media that\nleverages explicit reasoning over background knowledge to guide the model's\ninference about the document's stance on a target. Specifically, our method\nuses a pre-trained language model as a source of world knowledge, with the\nchain-of-thought in-context learning approach to generate intermediate\nreasoning steps. Stance Reasoner outperforms the current state-of-the-art\nmodels on 3 Twitter datasets, including fully supervised models. It can better\ngeneralize across targets, while at the same time providing explicit and\ninterpretable explanations for its predictions.\n","authors":["Maksym Taranukhin","Vered Shwartz","Evangelos Milios"],"pdf_url":"https://arxiv.org/pdf/2403.14895v1.pdf","comment":"Accepted to COLING 2024"},{"id":"http://arxiv.org/abs/2403.13679v3","updated":"2024-03-22T00:49:59Z","published":"2024-03-20T15:38:36Z","title":"RoleInteract: Evaluating the Social Interaction of Role-Playing Agents","summary":"  Large language models (LLMs) have advanced the development of various AI\nconversational agents, including role-playing conversational agents that mimic\ndiverse characters and human behaviors. While prior research has predominantly\nfocused on enhancing the conversational capability, role-specific knowledge,\nand stylistic attributes of these agents, there has been a noticeable gap in\nassessing their social intelligence. In this paper, we introduce RoleInteract,\nthe first benchmark designed to systematically evaluate the sociality of\nrole-playing conversational agents at both individual and group levels of\nsocial interactions. The benchmark is constructed from a variety of sources and\ncovers a wide range of 500 characters and over 6,000 question prompts and\n30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations\non this benchmark using mainstream open-source and closed-source LLMs. We find\nthat agents excelling in individual level does not imply their proficiency in\ngroup level. Moreover, the behavior of individuals may drift as a result of the\ninfluence exerted by other agents within the group. Experimental results on\nRoleInteract confirm its significance as a testbed for assessing the social\ninteraction of role-playing conversational agents. The benchmark is publicly\naccessible at https://github.com/X-PLUG/RoleInteract.\n","authors":["Hongzhan Chen","Hehong Chen","Ming Yan","Wenshen Xu","Xing Gao","Weizhou Shen","Xiaojun Quan","Chenliang Li","Ji Zhang","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.13679v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.07766v2","updated":"2024-03-22T00:28:51Z","published":"2023-05-12T21:22:08Z","title":"NL2TL: Transforming Natural Languages to Temporal Logics using Large\n  Language Models","summary":"  Temporal Logic (TL) can be used to rigorously specify complex high-level\nspecification for systems in many engineering applications. The translation\nbetween natural language (NL) and TL has been under-explored due to the lack of\ndataset and generalizable model across different application domains. In this\npaper, we propose an accurate and generalizable transformation framework of\nEnglish instructions from NL to TL, exploring the use of Large Language Models\n(LLMs) at multiple stages. Our contributions are twofold. First, we develop a\nframework to create a dataset of NL-TL pairs combining LLMs and human\nannotation. We publish a dataset with 28K NL-TL pairs. Then, we finetune T5\nmodels on the lifted versions (i.e., the specific Atomic Propositions (AP) are\nhidden) of the NL and TL. The enhanced generalizability originates from two\naspects: 1) Usage of lifted NL-TL characterizes common logical structures,\nwithout constraints of specific domains. 2) Application of LLMs in dataset\ncreation largely enhances corpus richness. We test the generalization of\ntrained models on five varied domains. To achieve full NL-TL transformation, we\neither combine the lifted model with AP recognition task or do the further\nfinetuning on each specific domain. During the further finetuning, our model\nachieves higher accuracy (>95%) using only <10% training data, compared with\nthe baseline sequence to sequence (Seq2Seq) model.\n","authors":["Yongchao Chen","Rujul Gandhi","Yang Zhang","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2305.07766v2.pdf","comment":"25 pages, 18 figures"},{"id":"http://arxiv.org/abs/2306.06531v3","updated":"2024-03-22T00:21:04Z","published":"2023-06-10T21:58:29Z","title":"AutoTAMP: Autoregressive Task and Motion Planning with LLMs as\n  Translators and Checkers","summary":"  For effective human-robot interaction, robots need to understand, plan, and\nexecute complex, long-horizon tasks described by natural language. Recent\nadvances in large language models (LLMs) have shown promise for translating\nnatural language into robot action sequences for complex tasks. However,\nexisting approaches either translate the natural language directly into robot\ntrajectories or factor the inference process by decomposing language into task\nsub-goals and relying on a motion planner to execute each sub-goal. When\ncomplex environmental and temporal constraints are involved, inference over\nplanning tasks must be performed jointly with motion plans using traditional\ntask-and-motion planning (TAMP) algorithms, making factorization into subgoals\nuntenable. Rather than using LLMs to directly plan task sub-goals, we instead\nperform few-shot translation from natural language task descriptions to an\nintermediate task representation that can then be consumed by a TAMP algorithm\nto jointly solve the task and motion plan. To improve translation, we\nautomatically detect and correct both syntactic and semantic errors via\nautoregressive re-prompting, resulting in significant improvements in task\ncompletion. We show that our approach outperforms several methods using LLMs as\nplanners in complex task domains. See our project website\nhttps://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.\n","authors":["Yongchao Chen","Jacob Arkin","Charles Dawson","Yang Zhang","Nicholas Roy","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2306.06531v3.pdf","comment":"8 pages, 4 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.15389v1","updated":"2024-03-22T17:59:58Z","published":"2024-03-22T17:59:58Z","title":"DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from\n  Partially Annotated Data","summary":"  Recently, there has been an increased interest in the practical problem of\nlearning multiple dense scene understanding tasks from partially annotated\ndata, where each training sample is only labeled for a subset of the tasks. The\nmissing of task labels in training leads to low-quality and noisy predictions,\nas can be observed from state-of-the-art methods. To tackle this issue, we\nreformulate the partially-labeled multi-task dense prediction as a pixel-level\ndenoising problem, and propose a novel multi-task denoising diffusion framework\ncoined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to\nmodel a potential noisy distribution in the task prediction or feature maps and\ngenerate rectified outputs for different tasks. To exploit multi-task\nconsistency in denoising, we further introduce a Multi-Task Conditioning\nstrategy, which can implicitly utilize the complementary nature of the tasks to\nhelp learn the unlabeled tasks, leading to an improvement in the denoising\nperformance of the different tasks. Extensive quantitative and qualitative\nexperiments demonstrate that the proposed multi-task denoising diffusion model\ncan significantly improve multi-task prediction maps, and outperform the\nstate-of-the-art methods on three challenging multi-task benchmarks, under two\ndifferent partial-labeling evaluation settings. The code is available at\nhttps://prismformore.github.io/diffusionmtl/.\n","authors":["Hanrong Ye","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.15389v1.pdf","comment":"The paper is accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15388v1","updated":"2024-03-22T17:59:52Z","published":"2024-03-22T17:59:52Z","title":"LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal\n  Models","summary":"  Large Multimodal Models (LMMs) have shown significant reasoning capabilities\nby connecting a visual encoder and a large language model. LMMs typically use a\nfixed amount of visual tokens, such as the penultimate layer features in the\nCLIP visual encoder, as the prefix content. Recent LMMs incorporate more\ncomplex visual inputs, such as high-resolution images and videos, which\nincrease the number of visual tokens significantly. However, due to the design\nof the Transformer architecture, computational costs associated with these\nmodels tend to increase quadratically with the number of input tokens. To\ntackle this problem, we explore a token reduction mechanism and find, similar\nto prior work, that many visual tokens are spatially redundant. Based on this,\nwe propose PruMerge, a novel adaptive visual token reduction approach, which\nlargely reduces the number of visual tokens while maintaining comparable model\nperformance. We first select the unpruned visual tokens based on their\nsimilarity to class tokens and spatial tokens. We then cluster the pruned\ntokens based on key similarity and merge the clustered tokens with the unpruned\ntokens to supplement their information. Empirically, when applied to LLaVA-1.5,\nour approach can compress the visual tokens by 14.4 times on average, and\nachieve comparable performance across diverse visual question-answering and\nreasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.\n","authors":["Yuzhang Shang","Mu Cai","Bingxin Xu","Yong Jae Lee","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2403.15388v1.pdf","comment":"Project page: https://llava-prumerge.github.io/"},{"id":"http://arxiv.org/abs/2403.15385v1","updated":"2024-03-22T17:59:37Z","published":"2024-03-22T17:59:37Z","title":"LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis","summary":"  Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.\n","authors":["Kevin Xie","Jonathan Lorraine","Tianshi Cao","Jun Gao","James Lucas","Antonio Torralba","Sanja Fidler","Xiaohui Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.15385v1.pdf","comment":"See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LATTE3D/"},{"id":"http://arxiv.org/abs/2312.10070v2","updated":"2024-03-22T17:59:09Z","published":"2023-12-06T10:47:53Z","title":"Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting","summary":"  We present a dense simultaneous localization and mapping (SLAM) method that\nuses 3D Gaussians as a scene representation. Our approach enables\ninteractive-time reconstruction and photo-realistic rendering from real-world\nsingle-camera RGBD videos. To this end, we propose a novel effective strategy\nfor seeding new Gaussians for newly explored areas and their effective online\noptimization that is independent of the scene size and thus scalable to larger\nscenes. This is achieved by organizing the scene into sub-maps which are\nindependently optimized and do not need to be kept in memory. We further\naccomplish frame-to-model camera tracking by minimizing photometric and\ngeometric losses between the input and rendered frames. The Gaussian\nrepresentation allows for high-quality photo-realistic real-time rendering of\nreal-world scenes. Evaluation on synthetic and real-world datasets demonstrates\ncompetitive or superior performance in mapping, tracking, and rendering\ncompared to existing neural dense SLAM methods.\n","authors":["Vladimir Yugay","Yue Li","Theo Gevers","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2312.10070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15383v1","updated":"2024-03-22T17:59:01Z","published":"2024-03-22T17:59:01Z","title":"ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars","summary":"  Real-world applications often require a large gallery of 3D assets that share\na consistent theme. While remarkable advances have been made in general 3D\ncontent creation from text or image, synthesizing customized 3D assets\nfollowing the shared theme of input 3D exemplars remains an open and\nchallenging problem. In this work, we present ThemeStation, a novel approach\nfor theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D\nassets based on given few exemplars with two goals: 1) unity for generating 3D\nassets that thematically align with the given exemplars and 2) diversity for\ngenerating 3D assets with a high degree of variations. To this end, we design a\ntwo-stage framework that draws a concept image first, followed by a\nreference-informed 3D modeling stage. We propose a novel dual score\ndistillation (DSD) loss to jointly leverage priors from both the input\nexemplars and the synthesized concept image. Extensive experiments and user\nstudies confirm that ThemeStation surpasses prior works in producing diverse\ntheme-aware 3D models with impressive quality. ThemeStation also enables\nvarious applications such as controllable 3D-to-3D generation.\n","authors":["Zhenwei Wang","Tengfei Wang","Gerhard Hancke","Ziwei Liu","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2403.15383v1.pdf","comment":"Project page: https://3dthemestation.github.io/"},{"id":"http://arxiv.org/abs/2403.15382v1","updated":"2024-03-22T17:58:59Z","published":"2024-03-22T17:58:59Z","title":"DragAPart: Learning a Part-Level Motion Prior for Articulated Objects","summary":"  We introduce DragAPart, a method that, given an image and a set of drags as\ninput, can generate a new image of the same object in a new state, compatible\nwith the action of the drags. Differently from prior works that focused on\nrepositioning objects, DragAPart predicts part-level interactions, such as\nopening and closing a drawer. We study this problem as a proxy for learning a\ngeneralist motion model, not restricted to a specific kinematic structure or\nobject category. To this end, we start from a pre-trained image generator and\nfine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce.\nCombined with a new encoding for the drags and dataset randomization, the new\nmodel generalizes well to real images and different categories. Compared to\nprior motion-controlled generators, we demonstrate much better part-level\nmotion understanding.\n","authors":["Ruining Li","Chuanxia Zheng","Christian Rupprecht","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2403.15382v1.pdf","comment":"Project page: https://dragapart.github.io/"},{"id":"http://arxiv.org/abs/2403.15378v1","updated":"2024-03-22T17:58:16Z","published":"2024-03-22T17:58:16Z","title":"Long-CLIP: Unlocking the Long-Text Capability of CLIP","summary":"  Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for\nzero-shot classification, text-image retrieval, and text-image generation by\naligning image and text modalities. Despite its widespread adoption, a\nsignificant limitation of CLIP lies in the inadequate length of text input. The\nlength of the text token is restricted to 77, and an empirical study shows the\nactual effective length is even less than 20. This prevents CLIP from handling\ndetailed descriptions, limiting its applications for image retrieval and\ntext-to-image generation with extensive prerequisites. To this end, we propose\nLong-CLIP as a plug-and-play alternative to CLIP that supports long-text input,\nretains or even surpasses its zero-shot generalizability, and aligns the CLIP\nlatent space, making it readily replace CLIP without any further adaptation in\ndownstream frameworks. Nevertheless, achieving this goal is far from\nstraightforward, as simplistic fine-tuning can result in a significant\ndegradation of CLIP's performance. Moreover, substituting the text encoder with\na language model supporting longer contexts necessitates pretraining with vast\namounts of data, incurring significant expenses. Accordingly, Long-CLIP\nintroduces an efficient fine-tuning solution on CLIP with two novel strategies\ndesigned to maintain the original capabilities, including (1) a\nknowledge-preserved stretching of positional embedding and (2) a primary\ncomponent matching of CLIP features. With leveraging just one million extra\nlong text-image pairs, Long-CLIP has shown the superiority to CLIP for about\n20% in long caption text-image retrieval and 6% in traditional text-image\nretrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers\nenhanced capabilities for generating images from detailed text descriptions by\nreplacing CLIP in a plug-and-play manner.\n","authors":["Beichen Zhang","Pan Zhang","Xiaoyi Dong","Yuhang Zang","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15378v1.pdf","comment":"All codes and models are publicly available at\n  https://github.com/beichenzbc/Long-CLIP"},{"id":"http://arxiv.org/abs/2403.15377v1","updated":"2024-03-22T17:57:42Z","published":"2024-03-22T17:57:42Z","title":"InternVideo2: Scaling Video Foundation Models for Multimodal Video\n  Understanding","summary":"  We introduce InternVideo2, a new video foundation model (ViFM) that achieves\nthe state-of-the-art performance in action recognition, video-text tasks, and\nvideo-centric dialogue. Our approach employs a progressive training paradigm\nthat unifies the different self- or weakly-supervised learning frameworks of\nmasked video token reconstruction, cross-modal contrastive learning, and next\ntoken prediction. Different training stages would guide our model to capture\ndifferent levels of structure and semantic information through different\npretext tasks. At the data level, we prioritize the spatiotemporal consistency\nby semantically segmenting videos and generating video-audio-speech captions.\nThis improves the alignment between video and text. We scale both data and\nmodel size for our InternVideo2. Through extensive experiments, we validate our\ndesigns and demonstrate the state-of-the-art performance on over 60 video and\naudio tasks. Notably, our model outperforms others on various video-related\ncaptioning, dialogue, and long video understanding benchmarks, highlighting its\nability to reason and comprehend long temporal contexts. Code and models are\navailable at https://github.com/OpenGVLab/InternVideo2/.\n","authors":["Yi Wang","Kunchang Li","Xinhao Li","Jiashuo Yu","Yinan He","Guo Chen","Baoqi Pei","Rongkun Zheng","Jilan Xu","Zun Wang","Yansong Shi","Tianxiang Jiang","Songze Li","Hongjie Zhang","Yifei Huang","Yu Qiao","Yali Wang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15377v1.pdf","comment":"a technical report about video understanding"},{"id":"http://arxiv.org/abs/2403.15370v1","updated":"2024-03-22T17:49:11Z","published":"2024-03-22T17:49:11Z","title":"Augmented Reality based Simulated Data (ARSim) with multi-view\n  consistency for AV perception networks","summary":"  Detecting a diverse range of objects under various driving scenarios is\nessential for the effectiveness of autonomous driving systems. However, the\nreal-world data collected often lacks the necessary diversity presenting a\nlong-tail distribution. Although synthetic data has been utilized to overcome\nthis issue by generating virtual scenes, it faces hurdles such as a significant\ndomain gap and the substantial efforts required from 3D artists to create\nrealistic environments. To overcome these challenges, we present ARSim, a fully\nautomated, comprehensive, modular framework designed to enhance real multi-view\nimage data with 3D synthetic objects of interest. The proposed method\nintegrates domain adaptation and randomization strategies to address covariate\nshift between real and simulated data by inferring essential domain attributes\nfrom real data and employing simulation-based randomization for other\nattributes. We construct a simplified virtual scene using real data and\nstrategically place 3D synthetic assets within it. Illumination is achieved by\nestimating light distribution from multiple images capturing the surroundings\nof the vehicle. Camera parameters from real data are employed to render\nsynthetic assets in each frame. The resulting augmented multi-view consistent\ndataset is used to train a multi-camera perception network for autonomous\nvehicles. Experimental results on various AV perception tasks demonstrate the\nsuperior performance of networks trained on the augmented dataset.\n","authors":["Aqeel Anwar","Tae Eun Choe","Zian Wang","Sanja Fidler","Minwoo Park"],"pdf_url":"https://arxiv.org/pdf/2403.15370v1.pdf","comment":"17 pages, 15 figures, 7 tables"},{"id":"http://arxiv.org/abs/2403.14617v2","updated":"2024-03-22T17:45:52Z","published":"2024-03-21T17:59:03Z","title":"Videoshop: Localized Semantic Video Editing with Noise-Extrapolated\n  Diffusion Inversion","summary":"  We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.\n","authors":["Xiang Fan","Anand Bhattad","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.14617v2.pdf","comment":"Project page at https://videoshop-editing.github.io/"},{"id":"http://arxiv.org/abs/2403.15361v1","updated":"2024-03-22T17:23:37Z","published":"2024-03-22T17:23:37Z","title":"Learning Topological Representations for Deep Image Understanding","summary":"  In many scenarios, especially biomedical applications, the correct\ndelineation of complex fine-scaled structures such as neurons, tissues, and\nvessels is critical for downstream analysis. Despite the strong predictive\npower of deep learning methods, they do not provide a satisfactory\nrepresentation of these structures, thus creating significant barriers in\nscalable annotation and downstream analysis. In this dissertation, we tackle\nsuch challenges by proposing novel representations of these topological\nstructures in a deep learning framework. We leverage the mathematical tools\nfrom topological data analysis, i.e., persistent homology and discrete Morse\ntheory, to develop principled methods for better segmentation and uncertainty\nestimation, which will become powerful tools for scalable annotation.\n","authors":["Xiaoling Hu"],"pdf_url":"https://arxiv.org/pdf/2403.15361v1.pdf","comment":"Ph.D. thesis from Stony Brook University. This thesis includes works\n  arXiv:1906.05404, arXiv:2110.08335, arXiv:2112.07812, arXiv:2103.09992,\n  arXiv:2206.01742"},{"id":"http://arxiv.org/abs/2403.15360v1","updated":"2024-03-22T17:22:56Z","published":"2024-03-22T17:22:56Z","title":"SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate\n  Time series","summary":"  Transformers have widely adopted attention networks for sequence mixing and\nMLPs for channel mixing, playing a pivotal role in achieving breakthroughs\nacross domains. However, recent literature highlights issues with attention\nnetworks, including low inductive bias and quadratic complexity concerning\ninput sequence length. State Space Models (SSMs) like S4 and others (Hippo,\nGlobal Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address\nthe above issues to help handle longer sequence lengths. Mamba, while being the\nstate-of-the-art SSM, has a stability issue when scaled to large networks for\ncomputer vision datasets. We propose SiMBA, a new architecture that introduces\nEinstein FFT (EinFFT) for channel modeling by specific eigenvalue computations\nand uses the Mamba block for sequence modeling. Extensive performance studies\nacross image and time-series benchmarks demonstrate that SiMBA outperforms\nexisting SSMs, bridging the performance gap with state-of-the-art transformers.\nNotably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet\nand transfer learning benchmarks such as Stanford Car and Flower as well as\ntask learning benchmarks as well as seven time series benchmark datasets. The\nproject page is available on this website\n~\\url{https://github.com/badripatro/Simba}.\n","authors":["Badri N. Patro","Vijay S. Agneeswaran"],"pdf_url":"https://arxiv.org/pdf/2403.15360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15356v1","updated":"2024-03-22T17:11:47Z","published":"2024-03-22T17:11:47Z","title":"Neural Plasticity-Inspired Foundation Model for Observing the Earth\n  Crossing Modalities","summary":"  The development of foundation models has revolutionized our ability to\ninterpret the Earth's surface using satellite observational data. Traditional\nmodels have been siloed, tailored to specific sensors or data types like\noptical, radar, and hyperspectral, each with its own unique characteristics.\nThis specialization hinders the potential for a holistic analysis that could\nbenefit from the combined strengths of these diverse data sources. Our novel\napproach introduces the Dynamic One-For-All (DOFA) model, leveraging the\nconcept of neural plasticity in brain science to integrate various data\nmodalities into a single framework adaptively. This dynamic hypernetwork,\nadjusting to different wavelengths, enables a single versatile Transformer\njointly trained on data from five sensors to excel across 12 distinct Earth\nobservation tasks, including sensors never seen during pretraining. DOFA's\ninnovative design offers a promising leap towards more accurate, efficient, and\nunified Earth observation analysis, showcasing remarkable adaptability and\nperformance in harnessing the potential of multimodal Earth observation data.\n","authors":["Zhitong Xiong","Yi Wang","Fahong Zhang","Adam J. Stewart","Joëlle Hanna","Damian Borth","Ioannis Papoutsis","Bertrand Le Saux","Gustau Camps-Valls","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.15356v1.pdf","comment":"33 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.15353v1","updated":"2024-03-22T17:08:03Z","published":"2024-03-22T17:08:03Z","title":"Fully automated workflow for the design of patient-specific orthopaedic\n  implants: application to total knee arthroplasty","summary":"  Arthroplasty is commonly performed to treat joint osteoarthritis, reducing\npain and improving mobility. While arthroplasty has known several technical\nimprovements, a significant share of patients are still unsatisfied with their\nsurgery. Personalised arthroplasty improves surgical outcomes however current\nsolutions require delays, making it difficult to integrate in clinical routine.\nWe propose a fully automated workflow to design patient-specific implants,\npresented for total knee arthroplasty, the most widely performed arthroplasty\nin the world nowadays.\n  The proposed pipeline first uses artificial neural networks to segment the\nproximal and distal extremities of the femur and tibia. Then the full bones are\nreconstructed using augmented statistical shape models, combining shape and\nlandmarks information. Finally, 77 morphological parameters are computed to\ndesign patient-specific implants. The developed workflow has been trained using\n91 CT scans of lower limb and evaluated on 41 CT scans manually segmented, in\nterms of accuracy and execution time.\n  The workflow accuracy was $0.4\\pm0.2mm$ for the segmentation, $1.2\\pm0.4mm$\nfor the full bones reconstruction, and $2.8\\pm2.2mm$ for the anatomical\nlandmarks determination. The custom implants fitted the patients' anatomy with\n$0.6\\pm0.2mm$ accuracy. The whole process from segmentation to implants' design\nlasted about 5 minutes.\n  The proposed workflow allows for a fast and reliable personalisation of knee\nimplants, directly from the patient CT image without requiring any manual\nintervention. It establishes a patient-specific pre-operative planning for TKA\nin a very short time making it easily available for all patients. Combined with\nefficient implant manufacturing techniques, this solution could help answer the\ngrowing number of arthroplasties while reducing complications and improving the\npatients' satisfaction.\n","authors":["Aziliz Guezou-Philippe","Arnaud Clavé","Ehouarn Maguet","Ludivine Maintier","Charles Garraud","Jean-Rassaire Fouefack","Valérie Burdin","Eric Stindel","Guillaume Dardenne"],"pdf_url":"https://arxiv.org/pdf/2403.15353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14125v3","updated":"2024-03-22T17:06:53Z","published":"2023-12-21T18:46:41Z","title":"VideoPoet: A Large Language Model for Zero-Shot Video Generation","summary":"  We present VideoPoet, a language model capable of synthesizing high-quality\nvideo, with matching audio, from a large variety of conditioning signals.\nVideoPoet employs a decoder-only transformer architecture that processes\nmultimodal inputs -- including images, videos, text, and audio. The training\nprotocol follows that of Large Language Models (LLMs), consisting of two\nstages: pretraining and task-specific adaptation. During pretraining, VideoPoet\nincorporates a mixture of multimodal generative objectives within an\nautoregressive Transformer framework. The pretrained LLM serves as a foundation\nthat can be adapted for a range of video generation tasks. We present empirical\nresults demonstrating the model's state-of-the-art capabilities in zero-shot\nvideo generation, specifically highlighting VideoPoet's ability to generate\nhigh-fidelity motions. Project page: http://sites.research.google/videopoet/\n","authors":["Dan Kondratyuk","Lijun Yu","Xiuye Gu","José Lezama","Jonathan Huang","Grant Schindler","Rachel Hornung","Vighnesh Birodkar","Jimmy Yan","Ming-Chang Chiu","Krishna Somandepalli","Hassan Akbari","Yair Alon","Yong Cheng","Josh Dillon","Agrim Gupta","Meera Hahn","Anja Hauth","David Hendon","Alonso Martinez","David Minnen","Mikhail Sirotenko","Kihyuk Sohn","Xuan Yang","Hartwig Adam","Ming-Hsuan Yang","Irfan Essa","Huisheng Wang","David A. Ross","Bryan Seybold","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.14125v3.pdf","comment":"Project page: http://sites.research.google/videopoet/"},{"id":"http://arxiv.org/abs/2403.09611v3","updated":"2024-03-22T17:03:16Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Ankur Jain","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Guoli Yin","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10115v2","updated":"2024-03-22T16:46:36Z","published":"2023-12-15T09:57:21Z","title":"SkySense: A Multi-Modal Remote Sensing Foundation Model Towards\n  Universal Interpretation for Earth Observation Imagery","summary":"  Prior studies on Remote Sensing Foundation Model (RSFM) reveal immense\npotential towards a generic model for Earth Observation. Nevertheless, these\nworks primarily focus on a single modality without temporal and geo-context\nmodeling, hampering their capabilities for diverse tasks. In this study, we\npresent SkySense, a generic billion-scale model, pre-trained on a curated\nmulti-modal Remote Sensing Imagery (RSI) dataset with 21.5 million temporal\nsequences. SkySense incorporates a factorized multi-modal spatiotemporal\nencoder taking temporal sequences of optical and Synthetic Aperture Radar (SAR)\ndata as input. This encoder is pre-trained by our proposed Multi-Granularity\nContrastive Learning to learn representations across different modal and\nspatial granularities. To further enhance the RSI representations by the\ngeo-context clue, we introduce Geo-Context Prototype Learning to learn\nregion-aware prototypes upon RSI's multi-modal spatiotemporal features. To our\nbest knowledge, SkySense is the largest Multi-Modal RSFM to date, whose modules\ncan be flexibly combined or used individually to accommodate various tasks. It\ndemonstrates remarkable generalization capabilities on a thorough evaluation\nencompassing 16 datasets over 7 tasks, from single- to multi-modal, static to\ntemporal, and classification to localization. SkySense surpasses 18 recent\nRSFMs in all test scenarios. Specifically, it outperforms the latest models\nsuch as GFM, SatLas and Scale-MAE by a large margin, i.e., 2.76%, 3.67% and\n3.61% on average respectively. We will release the pre-trained weights to\nfacilitate future research and Earth Observation applications.\n","authors":["Xin Guo","Jiangwei Lao","Bo Dang","Yingying Zhang","Lei Yu","Lixiang Ru","Liheng Zhong","Ziyuan Huang","Kang Wu","Dingxiang Hu","Huimei He","Jian Wang","Jingdong Chen","Ming Yang","Yongjun Zhang","Yansheng Li"],"pdf_url":"https://arxiv.org/pdf/2312.10115v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2312.00094v2","updated":"2024-03-22T16:38:34Z","published":"2023-11-30T13:07:19Z","title":"Fast ODE-based Sampling for Diffusion Models in Around 5 Steps","summary":"  Sampling from diffusion models can be treated as solving the corresponding\nordinary differential equations (ODEs), with the aim of obtaining an accurate\nsolution with as few number of function evaluations (NFE) as possible.\nRecently, various fast samplers utilizing higher-order ODE solvers have emerged\nand achieved better performance than the initial first-order one. However,\nthese numerical methods inherently result in certain approximation errors,\nwhich significantly degrades sample quality with extremely small NFE (e.g.,\naround 5). In contrast, based on the geometric observation that each sampling\ntrajectory almost lies in a two-dimensional subspace embedded in the ambient\nspace, we propose Approximate MEan-Direction Solver (AMED-Solver) that\neliminates truncation errors by directly learning the mean direction for fast\ndiffusion sampling. Besides, our method can be easily used as a plugin to\nfurther improve existing ODE-based samplers. Extensive experiments on image\nsynthesis with the resolution ranging from 32 to 512 demonstrate the\neffectiveness of our method. With only 5 NFE, we achieve 6.61 FID on CIFAR-10,\n10.74 FID on ImageNet 64$\\times$64, and 13.20 FID on LSUN Bedroom. Our code is\navailable at https://github.com/zju-pi/diff-sampler.\n","authors":["Zhenyu Zhou","Defang Chen","Can Wang","Chun Chen"],"pdf_url":"https://arxiv.org/pdf/2312.00094v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.14520v2","updated":"2024-03-22T16:35:49Z","published":"2024-03-21T16:17:57Z","title":"Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient\n  Inference","summary":"  In recent years, the application of multimodal large language models (MLLM)\nin various fields has achieved remarkable success. However, as the foundation\nmodel for many downstream tasks, current MLLMs are composed of the well-known\nTransformer network, which has a less efficient quadratic computation\ncomplexity. To improve the efficiency of such basic models, we propose Cobra, a\nlinear computational complexity MLLM. Specifically, Cobra integrates the\nefficient Mamba language model into the visual modality. Moreover, we explore\nand study various modal fusion schemes to create an effective multi-modal\nMamba. Extensive experiments demonstrate that (1) Cobra achieves extremely\ncompetitive performance with current computationally efficient state-of-the-art\nmethods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due\nto Cobra's linear sequential modeling. (2) Interestingly, the results of\nclosed-set challenging prediction benchmarks show that Cobra performs well in\novercoming visual illusions and spatial relationship judgments. (3) Notably,\nCobra even achieves comparable performance to LLaVA with about 43% of the\nnumber of parameters. We will make all codes of Cobra open-source and hope that\nthe proposed method can facilitate future research on complexity problems in\nMLLM. Our project page is available at: https://sites.google.com/view/cobravlm.\n","authors":["Han Zhao","Min Zhang","Wei Zhao","Pengxiang Ding","Siteng Huang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.14520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15330v1","updated":"2024-03-22T16:35:38Z","published":"2024-03-22T16:35:38Z","title":"Selectively Informative Description can Reduce Undesired Embedding\n  Entanglements in Text-to-Image Personalization","summary":"  In text-to-image personalization, a timely and crucial challenge is the\ntendency of generated images overfitting to the biases present in the reference\nimages. We initiate our study with a comprehensive categorization of the biases\ninto background, nearby-object, tied-object, substance (in style\nre-contextualization), and pose biases. These biases manifest in the generated\nimages due to their entanglement into the subject embedding. This undesired\nembedding entanglement not only results in the reflection of biases from the\nreference images into the generated images but also notably diminishes the\nalignment of the generated images with the given generation prompt. To address\nthis challenge, we propose SID~(Selectively Informative Description), a text\ndescription strategy that deviates from the prevalent approach of only\ncharacterizing the subject's class identification. SID is generated utilizing\nmultimodal GPT-4 and can be seamlessly integrated into optimization-based\nmodels. We present comprehensive experimental results along with analyses of\ncross-attention maps, subject-alignment, non-subject-disentanglement, and\ntext-alignment.\n","authors":["Jimyeong Kim","Jungwon Park","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.15330v1.pdf","comment":"Published at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.04690v2","updated":"2024-03-22T16:26:40Z","published":"2024-03-07T17:35:58Z","title":"Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self\n  Attention at the Threadblock Level","summary":"  Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\nfirst show that neighborhood attention can be represented as a batched GEMM\nproblem, similar to standard attention, and implement it for 1-D and 2-D\nneighborhood attention. These kernels on average provide 895% and 272%\nimprovement in full precision latency compared to existing naive kernels for\n1-D and 2-D neighborhood attention respectively. We find certain inherent\ninefficiencies in all unfused neighborhood attention kernels that bound their\nperformance and lower-precision scalability. We also developed fused\nneighborhood attention; an adaptation of fused dot-product attention kernels\nthat allow fine-grained control over attention across different spatial axes.\nKnown for reducing the quadratic time complexity of self attention to a linear\ncomplexity, neighborhood attention can now enjoy a reduced and constant memory\nfootprint, and record-breaking half precision latency. We observe that our\nfused kernels successfully circumvent some of the unavoidable inefficiencies in\nunfused implementations. While our unfused GEMM-based kernels only improve half\nprecision performance compared to naive kernels by an average of 496% and 113%\nin 1-D and 2-D problems respectively, our fused kernels improve naive kernels\nby an average of 1607% and 581% in 1-D and 2-D problems respectively.\n","authors":["Ali Hassani","Wen-Mei Hwu","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.04690v2.pdf","comment":"Project page: https://github.com/SHI-Labs/NATTEN"},{"id":"http://arxiv.org/abs/2403.15317v1","updated":"2024-03-22T16:11:29Z","published":"2024-03-22T16:11:29Z","title":"Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for\n  Weakly Semi-supervised 3D Object Detection","summary":"  Training high-accuracy 3D detectors necessitates massive labeled 3D\nannotations with 7 degree-of-freedom, which is laborious and time-consuming.\nTherefore, the form of point annotations is proposed to offer significant\nprospects for practical applications in 3D detection, which is not only more\naccessible and less expensive but also provides strong spatial information for\nobject localization.In this paper, we empirically discover that it is\nnon-trivial to merely adapt Point-DETR to its 3D form, encountering two main\nbottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it\ngenerates low-quality pseudo labels in distant regions due to the extreme\nsparsity of LiDAR points. To overcome these challenges, we introduce\nPoint-DETR3D, a teacher-student framework for weakly semi-supervised 3D\ndetection, designed to fully capitalize on point-wise supervision within a\nconstrained instance-wise annotation budget.Different from Point-DETR which\nencodes 3D positional information solely through a point encoder, we propose an\nexplicit positional query initialization strategy to enhance the positional\nprior. Considering the low quality of pseudo labels at distant regions produced\nby the teacher model, we enhance the detector's perception by incorporating\ndense imagery data through a novel Cross-Modal Deformable RoI Fusion\n(D-RoI).Moreover, an innovative point-guided self-supervised learning technique\nis proposed to allow for fully exploiting point priors, even in student\nmodels.Extensive experiments on representative nuScenes dataset demonstrate our\nPoint-DETR3D obtains significant improvements compared to previous works.\nNotably, with only 5% of labeled data, Point-DETR3D achieves over 90%\nperformance of its fully supervised counterpart.\n","authors":["Hongzhi Gao","Zheng Chen","Zehui Chen","Lin Chen","Jiaming Liu","Shanghang Zhang","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.15317v1.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.15316v1","updated":"2024-03-22T16:10:38Z","published":"2024-03-22T16:10:38Z","title":"Ultrasound Imaging based on the Variance of a Diffusion Restoration\n  Model","summary":"  Despite today's prevalence of ultrasound imaging in medicine, ultrasound\nsignal-to-noise ratio is still affected by several sources of noise and\nartefacts. Moreover, enhancing ultrasound image quality involves balancing\nconcurrent factors like contrast, resolution, and speckle preservation.\nRecently, there has been progress in both model-based and learning-based\napproaches addressing the problem of ultrasound image reconstruction. Bringing\nthe best from both worlds, we propose a hybrid reconstruction method combining\nan ultrasound linear direct model with a learning-based prior coming from a\ngenerative Denoising Diffusion model. More specifically, we rely on the\nunsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model\n(DDRM). Given the nature of multiplicative noise inherent to ultrasound, this\npaper proposes an empirical model to characterize the stochasticity of\ndiffusion reconstruction of ultrasound images, and shows the interest of its\nvariance as an echogenicity map estimator. We conduct experiments on synthetic,\nin-vitro, and in-vivo data, demonstrating the efficacy of our variance imaging\napproach in achieving high-quality image reconstructions from single plane-wave\nacquisitions and in comparison to state-of-the-art methods.\n","authors":["Yuxin Zhang","Clément Huneau","Jérôme Idier","Diana Mateus"],"pdf_url":"https://arxiv.org/pdf/2403.15316v1.pdf","comment":"5 pages; submitted to EUSIPCO 2024. arXiv admin note: text overlap\n  with arXiv:2310.20618"},{"id":"http://arxiv.org/abs/2403.15314v1","updated":"2024-03-22T16:06:43Z","published":"2024-03-22T16:06:43Z","title":"Global Control for Local SO(3)-Equivariant Scale-Invariant Vessel\n  Segmentation","summary":"  Personalized 3D vascular models can aid in a range of diagnostic, prognostic,\nand treatment-planning tasks relevant to cardiovascular disease management.\nDeep learning provides a means to automatically obtain such models. Ideally, a\nuser should have control over the exact region of interest (ROI) to be included\nin a vascular model, and the model should be watertight and highly accurate. To\nthis end, we propose a combination of a global controller leveraging voxel mask\nsegmentations to provide boundary conditions for vessels of interest to a\nlocal, iterative vessel segmentation model. We introduce the preservation of\nscale- and rotational symmetries in the local segmentation model, leading to\ngeneralisation to vessels of unseen sizes and orientations. Combined with the\nglobal controller, this enables flexible 3D vascular model building, without\nadditional retraining. We demonstrate the potential of our method on a dataset\ncontaining abdominal aortic aneurysms (AAAs). Our method performs on par with a\nstate-of-the-art segmentation model in the segmentation of AAAs, iliac arteries\nand renal arteries, while providing a watertight, smooth surface segmentation.\nMoreover, we demonstrate that by adapting the global controller, we can easily\nextend vessel sections in the 3D model.\n","authors":["Patryk Rygiel","Dieuwertje Alblas","Christoph Brune","Kak Khee Yeung","Jelmer M. Wolterink"],"pdf_url":"https://arxiv.org/pdf/2403.15314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15313v1","updated":"2024-03-22T16:06:05Z","published":"2024-03-22T16:06:05Z","title":"CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking","summary":"  Accurate detection and tracking of surrounding objects is essential to enable\nself-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have\nset the benchmark for high performance, the appeal of camera-only solutions\nlies in their cost-effectiveness. Notably, despite the prevalent use of Radio\nDetection and Ranging (RADAR) sensors in automotive systems, their potential in\n3D detection and tracking has been largely disregarded due to data sparsity and\nmeasurement noise. As a recent development, the combination of RADARs and\ncameras is emerging as a promising solution. This paper presents Camera-RADAR\n3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object\ndetection, and Multi-Object Tracking (MOT). Building upon the foundations of\nthe State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates\nsubstantial improvements in both detection and tracking capabilities, by\nincorporating the spatial and velocity information of the RADAR sensor.\nExperimental results demonstrate an absolute improvement in detection\nperformance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in\nAverage Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when\nleveraging both modalities. CR3DT bridges the gap between high-performance and\ncost-effective perception systems in autonomous driving, by capitalizing on the\nubiquitous presence of RADAR in automotive applications.\n","authors":["Nicolas Baumann","Michael Baumgartner","Edoardo Ghignone","Jonas Kühne","Tobias Fischer","Yung-Hsu Yang","Marc Pollefeys","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2403.15313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15309v1","updated":"2024-03-22T15:59:24Z","published":"2024-03-22T15:59:24Z","title":"Controlled Training Data Generation with Diffusion Models","summary":"  In this work, we present a method to control a text-to-image generative model\nto produce training data specifically \"useful\" for supervised learning. Unlike\nprevious works that employ an open-loop approach and pre-define prompts to\ngenerate new data using either a language model or human expertise, we develop\nan automated closed-loop system which involves two feedback mechanisms. The\nfirst mechanism uses feedback from a given supervised model and finds\nadversarial prompts that result in image generations that maximize the model\nloss. While these adversarial prompts result in diverse data informed by the\nmodel, they are not informed of the target distribution, which can be\ninefficient. Therefore, we introduce the second feedback mechanism that guides\nthe generation process towards a certain target distribution. We call the\nmethod combining these two mechanisms Guided Adversarial Prompts. We perform\nour evaluations on different tasks, datasets and architectures, with different\ntypes of distribution shifts (spuriously correlated data, unseen domains) and\ndemonstrate the efficiency of the proposed feedback mechanisms compared to\nopen-loop approaches.\n","authors":["Teresa Yeo","Andrei Atanov","Harold Benoit","Aleksandr Alekseev","Ruchira Ray","Pooya Esmaeil Akhoondi","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2403.15309v1.pdf","comment":"Project page at https://adversarial-prompts.epfl.ch/"},{"id":"http://arxiv.org/abs/2403.12505v2","updated":"2024-03-22T15:41:20Z","published":"2024-03-19T07:11:53Z","title":"Semantics, Distortion, and Style Matter: Towards Source-free UDA for\n  Panoramic Segmentation","summary":"  This paper addresses an interesting yet challenging problem -- source-free\nunsupervised domain adaptation (SFUDA) for pinhole-to-panoramic semantic\nsegmentation -- given only a pinhole image-trained model (i.e., source) and\nunlabeled panoramic images (i.e., target). Tackling this problem is nontrivial\ndue to the semantic mismatches, style discrepancies, and inevitable distortion\nof panoramic images. To this end, we propose a novel method that utilizes\nTangent Projection (TP) as it has less distortion and meanwhile slits the\nequirectangular projection (ERP) with a fixed FoV to mimic the pinhole images.\nBoth projections are shown effective in extracting knowledge from the source\nmodel. However, the distinct projection discrepancies between source and target\ndomains impede the direct knowledge transfer; thus, we propose a panoramic\nprototype adaptation module (PPAM) to integrate panoramic prototypes from the\nextracted knowledge for adaptation. We then impose the loss constraints on both\npredictions and prototypes and propose a cross-dual attention module (CDAM) at\nthe feature level to better align the spatial and channel characteristics\nacross the domains and projections. Both knowledge extraction and transfer\nprocesses are synchronously updated to reach the best performance. Extensive\nexperiments on the synthetic and real-world benchmarks, including outdoor and\nindoor scenarios, demonstrate that our method achieves significantly better\nperformance than prior SFUDA methods for pinhole-to-panoramic adaptation.\n","authors":["Xu Zheng","Pengyuan Zhou","Athanasios V. Vasilakos","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.12505v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2310.00632v2","updated":"2024-03-22T15:38:53Z","published":"2023-10-01T10:06:01Z","title":"Win-Win: Training High-Resolution Vision Transformers from Two Windows","summary":"  Transformers have become the standard in state-of-the-art vision\narchitectures, achieving impressive performance on both image-level and dense\npixelwise tasks. However, training vision transformers for high-resolution\npixelwise tasks has a prohibitive cost. Typical solutions boil down to\nhierarchical architectures, fast and approximate attention, or training on\nlow-resolution crops. This latter solution does not constrain architectural\nchoices, but it leads to a clear performance drop when testing at resolutions\nsignificantly higher than that used for training, thus requiring ad-hoc and\nslow post-processing schemes. In this paper, we propose a novel strategy for\nefficient training and inference of high-resolution vision transformers. The\nkey principle is to mask out most of the high-resolution inputs during\ntraining, keeping only N random windows. This allows the model to learn local\ninteractions between tokens inside each window, and global interactions between\ntokens from different windows. As a result, the model can directly process the\nhigh-resolution input at test time without any special trick. We show that this\nstrategy is effective when using relative positional embedding such as rotary\nembeddings. It is 4 times faster to train than a full-resolution network, and\nit is straightforward to use at test time compared to existing approaches. We\napply this strategy to three dense prediction tasks with high-resolution data.\nFirst, we show on the task of semantic segmentation that a simple setting with\n2 windows performs best, hence the name of our method: Win-Win. Second, we\nconfirm this result on the task of monocular depth prediction. Third, we\nfurther extend it to the binocular task of optical flow, reaching\nstate-of-the-art performance on the Spring benchmark that contains Full-HD\nimages with an order of magnitude faster inference than the best competitor.\n","authors":["Vincent Leroy","Jerome Revaud","Thomas Lucas","Philippe Weinzaepfel"],"pdf_url":"https://arxiv.org/pdf/2310.00632v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2401.11170v2","updated":"2024-03-22T15:31:39Z","published":"2024-01-20T08:46:06Z","title":"Inducing High Energy-Latency of Large Vision-Language Models with\n  Verbose Images","summary":"  Large vision-language models (VLMs) such as GPT-4 have achieved exceptional\nperformance across various multi-modal tasks. However, the deployment of VLMs\nnecessitates substantial energy consumption and computational resources. Once\nattackers maliciously induce high energy consumption and latency time\n(energy-latency cost) during inference of VLMs, it will exhaust computational\nresources. In this paper, we explore this attack surface about availability of\nVLMs and aim to induce high energy-latency cost during inference of VLMs. We\nfind that high energy-latency cost during inference of VLMs can be manipulated\nby maximizing the length of generated sequences. To this end, we propose\nverbose images, with the goal of crafting an imperceptible perturbation to\ninduce VLMs to generate long sentences during inference. Concretely, we design\nthree loss objectives. First, a loss is proposed to delay the occurrence of\nend-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop\ngenerating further tokens. Moreover, an uncertainty loss and a token diversity\nloss are proposed to increase the uncertainty over each generated token and the\ndiversity among all tokens of the whole generated sequence, respectively, which\ncan break output dependency at token-level and sequence-level. Furthermore, a\ntemporal weight adjustment algorithm is proposed, which can effectively balance\nthese losses. Extensive experiments demonstrate that our verbose images can\nincrease the length of generated sequences by 7.87 times and 8.56 times\ncompared to original images on MS-COCO and ImageNet datasets, which presents\npotential challenges for various applications. Our code is available at\nhttps://github.com/KuofengGao/Verbose_Images.\n","authors":["Kuofeng Gao","Yang Bai","Jindong Gu","Shu-Tao Xia","Philip Torr","Zhifeng Li","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2401.11170v2.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2308.13712v3","updated":"2024-03-22T15:30:57Z","published":"2023-08-25T23:54:15Z","title":"Residual Denoising Diffusion Models","summary":"  We propose residual denoising diffusion models (RDDM), a novel dual diffusion\nprocess that decouples the traditional single denoising diffusion process into\nresidual diffusion and noise diffusion. This dual diffusion framework expands\nthe denoising-based diffusion models, initially uninterpretable for image\nrestoration, into a unified and interpretable model for both image generation\nand restoration by introducing residuals. Specifically, our residual diffusion\nrepresents directional diffusion from the target image to the degraded input\nimage and explicitly guides the reverse generation process for image\nrestoration, while noise diffusion represents random perturbations in the\ndiffusion process. The residual prioritizes certainty, while the noise\nemphasizes diversity, enabling RDDM to effectively unify tasks with varying\ncertainty or diversity requirements, such as image generation and restoration.\nWe demonstrate that our sampling process is consistent with that of DDPM and\nDDIM through coefficient transformation, and propose a partially\npath-independent generation process to better understand the reverse process.\nNotably, our RDDM enables a generic UNet, trained with only an L1 loss and a\nbatch size of 1, to compete with state-of-the-art image restoration methods. We\nprovide code and pre-trained models to encourage further exploration,\napplication, and development of our innovative framework\n(https://github.com/nachifur/RDDM).\n","authors":["Jiawei Liu","Qiang Wang","Huijie Fan","Yinong Wang","Yandong Tang","Liangqiong Qu"],"pdf_url":"https://arxiv.org/pdf/2308.13712v3.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.09530v2","updated":"2024-03-22T15:26:05Z","published":"2024-03-14T16:13:00Z","title":"VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding","summary":"  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n","authors":["Chris Kelly","Luhui Hu","Jiayin Hu","Yu Tian","Deshun Yang","Bang Yang","Cindy Yang","Zihao Li","Zaoshan Huang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2403.09530v2.pdf","comment":"12 pages, 7 figures, pending conference"},{"id":"http://arxiv.org/abs/2403.15272v1","updated":"2024-03-22T15:15:44Z","published":"2024-03-22T15:15:44Z","title":"WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization","summary":"  Despite the advancements in deep learning for camera relocalization tasks,\nobtaining ground truth pose labels required for the training process remains a\ncostly endeavor. While current weakly supervised methods excel in lightweight\nlabel generation, their performance notably declines in scenarios with sparse\nviews. In response to this challenge, we introduce WSCLoc, a system capable of\nbeing customized to various deep learning-based relocalization models to\nenhance their performance under weakly-supervised and sparse view conditions.\nThis is realized with two stages. In the initial stage, WSCLoc employs a\nmultilayer perceptron-based structure called WFT-NeRF to co-optimize image\nreconstruction quality and initial pose information. To ensure a stable\nlearning process, we incorporate temporal information as input. Furthermore,\ninstead of optimizing SE(3), we opt for $\\mathfrak{sim}(3)$ optimization to\nexplicitly enforce a scale constraint. In the second stage, we co-optimize the\npre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by\nTime-Encoding based Random View Synthesis and supervised by inter-frame\ngeometric constraints that consider pose, depth, and RGB information. We\nvalidate our approaches on two publicly available datasets, one outdoor and one\nindoor. Our experimental results demonstrate that our weakly-supervised\nrelocalization solutions achieve superior pose estimation accuracy in\nsparse-view scenarios, comparable to state-of-the-art camera relocalization\nmethods. We will make our code publicly available.\n","authors":["Jialu Wang","Kaichen Zhou","Andrew Markham","Niki Trigoni"],"pdf_url":"https://arxiv.org/pdf/2403.15272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15260v1","updated":"2024-03-22T15:00:29Z","published":"2024-03-22T15:00:29Z","title":"Hyperbolic Metric Learning for Visual Outlier Detection","summary":"  Out-Of-Distribution (OOD) detection is critical to deploy deep learning\nmodels in safety-critical applications. However, the inherent hierarchical\nconcept structure of visual data, which is instrumental to OOD detection, is\noften poorly captured by conventional methods based on Euclidean geometry. This\nwork proposes a metric framework that leverages the strengths of Hyperbolic\ngeometry for OOD detection. Inspired by previous works that refine the decision\nboundary for OOD data with synthetic outliers, we extend this method to\nHyperbolic space. Interestingly, we find that synthetic outliers do not benefit\nOOD detection in Hyperbolic space as they do in Euclidean space. Furthermore we\nexplore the relationship between OOD detection performance and Hyperbolic\nembedding dimension, addressing practical concerns in resource-constrained\nenvironments. Extensive experiments show that our framework improves the FPR95\nfor OOD detection from 22\\% to 15\\% and from 49% to 28% on CIFAR-10 and\nCIFAR-100 respectively compared to Euclidean methods.\n","authors":["Alvaro Gonzalez-Jimenez","Simone Lionetti","Dena Bazazian","Philippe Gottfrois","Fabian Gröger","Marc Pouly","Alexander Navarini"],"pdf_url":"https://arxiv.org/pdf/2403.15260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15249v1","updated":"2024-03-22T14:47:18Z","published":"2024-03-22T14:47:18Z","title":"Spectral Motion Alignment for Video Motion Transfer using Diffusion\n  Models","summary":"  The evolution of diffusion models has greatly impacted video generation and\nunderstanding. Particularly, text-to-video diffusion models (VDMs) have\nsignificantly facilitated the customization of input video with target\nappearance, motion, etc. Despite these advances, challenges persist in\naccurately distilling motion information from video frames. While existing\nworks leverage the consecutive frame residual as the target motion vector, they\ninherently lack global motion context and are vulnerable to frame-wise\ndistortions. To address this, we present Spectral Motion Alignment (SMA), a\nnovel framework that refines and aligns motion vectors using Fourier and\nwavelet transforms. SMA learns motion patterns by incorporating\nfrequency-domain regularization, facilitating the learning of whole-frame\nglobal motion dynamics, and mitigating spatial artifacts. Extensive experiments\ndemonstrate SMA's efficacy in improving motion transfer while maintaining\ncomputational efficiency and compatibility across various video customization\nframeworks.\n","authors":["Geon Yeong Park","Hyeonho Jeong","Sang Wan Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.15249v1.pdf","comment":"Project page:\n  https://geonyeong-park.github.io/spectral-motion-alignment/"},{"id":"http://arxiv.org/abs/2403.15248v1","updated":"2024-03-22T14:46:51Z","published":"2024-03-22T14:46:51Z","title":"Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks","summary":"  Computer vision in agriculture is game-changing with its ability to transform\nfarming into a data-driven, precise, and sustainable industry. Deep learning\nhas empowered agriculture vision to analyze vast, complex visual data, but\nheavily rely on the availability of large annotated datasets. This remains a\nbottleneck as manual labeling is error-prone, time-consuming, and expensive.\nThe lack of efficient labeling approaches inspired us to consider\nself-supervised learning as a paradigm shift, learning meaningful feature\nrepresentations from raw agricultural image data. In this work, we explore how\nself-supervised representation learning unlocks the potential applicability to\ndiverse agriculture vision tasks by eliminating the need for large-scale\nannotated datasets. We propose a lightweight framework utilizing SimCLR, a\ncontrastive learning approach, to pre-train a ResNet-50 backbone on a large,\nunannotated dataset of real-world agriculture field images. Our experimental\nanalysis and results indicate that the model learns robust features applicable\nto a broad range of downstream agriculture tasks discussed in the paper.\nAdditionally, the reduced reliance on annotated data makes our approach more\ncost-effective and accessible, paving the way for broader adoption of computer\nvision in agriculture.\n","authors":["Sudhir Sornapudi","Rajhans Singh"],"pdf_url":"https://arxiv.org/pdf/2403.15248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08863v2","updated":"2024-03-22T14:46:05Z","published":"2023-11-15T10:49:15Z","title":"Toulouse Hyperspectral Data Set: a benchmark data set to assess\n  semi-supervised spectral representation learning and pixel-wise\n  classification techniques","summary":"  Airborne hyperspectral images can be used to map the land cover in large\nurban areas, thanks to their very high spatial and spectral resolutions on a\nwide spectral domain. While the spectral dimension of hyperspectral images is\nhighly informative of the chemical composition of the land surface, the use of\nstate-of-the-art machine learning algorithms to map the land cover has been\ndramatically limited by the availability of training data. To cope with the\nscarcity of annotations, semi-supervised and self-supervised techniques have\nlately raised a lot of interest in the community. Yet, the publicly available\nhyperspectral data sets commonly used to benchmark machine learning models are\nnot totally suited to evaluate their generalization performances due to one or\nseveral of the following properties: a limited geographical coverage (which\ndoes not reflect the spectral diversity in metropolitan areas), a small number\nof land cover classes and a lack of appropriate standard train / test splits\nfor semi-supervised and self-supervised learning. Therefore, we release in this\npaper the Toulouse Hyperspectral Data Set that stands out from other data sets\nin the above-mentioned respects in order to meet key issues in spectral\nrepresentation learning and classification over large-scale hyperspectral\nimages with very few labeled pixels. Besides, we discuss and experiment\nself-supervised techniques for spectral representation learning, including the\nMasked Autoencoder, and establish a baseline for pixel-wise classification\nachieving 85% overall accuracy and 77% F1 score. The Toulouse Hyperspectral\nData Set and our code are publicly available at\nhttps://www.toulouse-hyperspectral-data-set.com and\nhttps://www.github.com/Romain3Ch216/tlse-experiments, respectively.\n","authors":["Romain Thoreau","Laurent Risser","Véronique Achard","Béatrice Berthelot","Xavier Briottet"],"pdf_url":"https://arxiv.org/pdf/2311.08863v2.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.15245v1","updated":"2024-03-22T14:41:55Z","published":"2024-03-22T14:41:55Z","title":"Reasoning-Enhanced Object-Centric Learning for Videos","summary":"  Object-centric learning aims to break down complex visual scenes into more\nmanageable object representations, enhancing the understanding and reasoning\nabilities of machine learning systems toward the physical world. Recently,\nslot-based video models have demonstrated remarkable proficiency in segmenting\nand tracking objects, but they overlook the importance of the effective\nreasoning module. In the real world, reasoning and predictive abilities play a\ncrucial role in human perception and object tracking; in particular, these\nabilities are closely related to human intuitive physics. Inspired by this, we\ndesigned a novel reasoning module called the Slot-based Time-Space Transformer\nwith Memory buffer (STATM) to enhance the model's perception ability in complex\nscenes. The memory buffer primarily serves as storage for slot information from\nupstream modules, the Slot-based Time-Space Transformer makes predictions\nthrough slot-based spatiotemporal attention computations and fusion. Our\nexperiment results on various datasets show that STATM can significantly\nenhance object-centric learning capabilities of slot-based video models.\n","authors":["Jian Li","Pu Ren","Yang Liu","Hao Sun"],"pdf_url":"https://arxiv.org/pdf/2403.15245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15241v1","updated":"2024-03-22T14:34:17Z","published":"2024-03-22T14:34:17Z","title":"IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object\n  Detection","summary":"  Bird's eye view (BEV) representation has emerged as a dominant solution for\ndescribing 3D space in autonomous driving scenarios. However, objects in the\nBEV representation typically exhibit small sizes, and the associated point\ncloud context is inherently sparse, which leads to great challenges for\nreliable 3D perception. In this paper, we propose IS-Fusion, an innovative\nmultimodal fusion framework that jointly captures the Instance- and Scene-level\ncontextual information. IS-Fusion essentially differs from existing approaches\nthat only focus on the BEV scene-level fusion by explicitly incorporating\ninstance-level multimodal information, thus facilitating the instance-centric\ntasks like 3D object detection. It comprises a Hierarchical Scene Fusion (HSF)\nmodule and an Instance-Guided Fusion (IGF) module. HSF applies Point-to-Grid\nand Grid-to-Region transformers to capture the multimodal scene context at\ndifferent granularities. IGF mines instance candidates, explores their\nrelationships, and aggregates the local multimodal context for each instance.\nThese instances then serve as guidance to enhance the scene feature and yield\nan instance-aware BEV representation. On the challenging nuScenes benchmark,\nIS-Fusion outperforms all the published multimodal works to date. Code is\navailable at: https://github.com/yinjunbo/IS-Fusion.\n","authors":["Junbo Yin","Jianbing Shen","Runnan Chen","Wei Li","Ruigang Yang","Pascal Frossard","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15241v1.pdf","comment":"Accepted to CVPR 2024; Code: https://github.com/yinjunbo/IS-Fusion"},{"id":"http://arxiv.org/abs/2403.15238v1","updated":"2024-03-22T14:32:02Z","published":"2024-03-22T14:32:02Z","title":"WEEP: A method for spatial interpretation of weakly supervised CNN\n  models in computational pathology","summary":"  Deep learning enables the modelling of high-resolution histopathology\nwhole-slide images (WSI). Weakly supervised learning of tile-level data is\ntypically applied for tasks where labels only exist on the patient or WSI level\n(e.g. patient outcomes or histological grading). In this context, there is a\nneed for improved spatial interpretability of predictions from such models. We\npropose a novel method, Wsi rEgion sElection aPproach (WEEP), for model\ninterpretation. It provides a principled yet straightforward way to establish\nthe spatial area of WSI required for assigning a particular prediction label.\nWe demonstrate WEEP on a binary classification task in the area of breast\ncancer computational pathology. WEEP is easy to implement, is directly\nconnected to the model-based decision process, and offers information relevant\nto both research and diagnostic applications.\n","authors":["Abhinav Sharma","Bojing Liu","Mattias Rantalainen"],"pdf_url":"https://arxiv.org/pdf/2403.15238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15234v1","updated":"2024-03-22T14:27:58Z","published":"2024-03-22T14:27:58Z","title":"Shadow Generation for Composite Image Using Diffusion model","summary":"  In the realm of image composition, generating realistic shadow for the\ninserted foreground remains a formidable challenge. Previous works have\ndeveloped image-to-image translation models which are trained on paired\ntraining data. However, they are struggling to generate shadows with accurate\nshapes and intensities, hindered by data scarcity and inherent task complexity.\nIn this paper, we resort to foundation model with rich prior knowledge of\nnatural shadow images. Specifically, we first adapt ControlNet to our task and\nthen propose intensity modulation modules to improve the shadow intensity.\nMoreover, we extend the small-scale DESOBA dataset to DESOBAv2 using a novel\ndata acquisition pipeline. Experimental results on both DESOBA and DESOBAv2\ndatasets as well as real composite images demonstrate the superior capability\nof our model for shadow generation task. The dataset, code, and model are\nreleased at https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2.\n","authors":["Qingyang Liu","Junqi You","Jianting Wang","Xinhao Tao","Bo Zhang","Li Niu"],"pdf_url":"https://arxiv.org/pdf/2403.15234v1.pdf","comment":"accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.11376v2","updated":"2024-03-22T14:25:14Z","published":"2024-03-18T00:03:48Z","title":"ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal\n  Instance Segmentation","summary":"  Amodal Instance Segmentation (AIS) presents a challenging task as it involves\npredicting both visible and occluded parts of objects within images. Existing\nAIS methods rely on a bidirectional approach, encompassing both the transition\nfrom amodal features to visible features (amodal-to-visible) and from visible\nfeatures to amodal features (visible-to-amodal). Our observation shows that the\nutilization of amodal features through the amodal-to-visible can confuse the\nvisible features due to the extra information of occluded/hidden segments not\npresented in visible display. Consequently, this compromised quality of visible\nfeatures during the subsequent visible-to-amodal transition. To tackle this\nissue, we introduce ShapeFormer, a decoupled Transformer-based model with a\nvisible-to-amodal transition. It facilitates the explicit relationship between\noutput segmentations and avoids the need for amodal-to-visible transitions.\nShapeFormer comprises three key modules: (i) Visible-Occluding Mask Head for\npredicting visible segmentation with occlusion awareness, (ii) Shape-Prior\nAmodal Mask Head for predicting amodal and occluded masks, and (iii)\nCategory-Specific Shape Prior Retriever aims to provide shape prior knowledge.\nComprehensive experiments and extensive ablation studies across various AIS\nbenchmarks demonstrate the effectiveness of our ShapeFormer. The code is\navailable at: https://github.com/UARK-AICV/ShapeFormer\n","authors":["Minh Tran","Winston Bounsavy","Khoa Vo","Anh Nguyen","Tri Nguyen","Ngan Le"],"pdf_url":"https://arxiv.org/pdf/2403.11376v2.pdf","comment":"Accepted to IJCNN 2024"},{"id":"http://arxiv.org/abs/2403.15227v1","updated":"2024-03-22T14:20:54Z","published":"2024-03-22T14:20:54Z","title":"LeGO: Leveraging a Surface Deformation Network for Animatable Stylized\n  Face Generation with One Example","summary":"  Recent advances in 3D face stylization have made significant strides in few\nto zero-shot settings. However, the degree of stylization achieved by existing\nmethods is often not sufficient for practical applications because they are\nmostly based on statistical 3D Morphable Models (3DMM) with limited variations.\nTo this end, we propose a method that can produce a highly stylized 3D face\nmodel with desired topology. Our methods train a surface deformation network\nwith 3DMM and translate its domain to the target style using a paired exemplar.\nThe network achieves stylization of the 3D face mesh by mimicking the style of\nthe target using a differentiable renderer and directional CLIP losses.\nAdditionally, during the inference process, we utilize a Mesh Agnostic Encoder\n(MAGE) that takes deformation target, a mesh of diverse topologies as input to\nthe stylization process and encodes its shape into our latent space. The\nresulting stylized face model can be animated by commonly used 3DMM blend\nshapes. A set of quantitative and qualitative evaluations demonstrate that our\nmethod can produce highly stylized face meshes according to a given style and\noutput them in a desired topology. We also demonstrate example applications of\nour method including image-based stylized avatar generation, linear\ninterpolation of geometric styles, and facial animation of stylized avatars.\n","authors":["Soyeon Yoon","Kwan Yun","Kwanggyoon Seo","Sihun Cha","Jung Eun Yoo","Junyong Noh"],"pdf_url":"https://arxiv.org/pdf/2403.15227v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.15218v1","updated":"2024-03-22T14:07:07Z","published":"2024-03-22T14:07:07Z","title":"Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment\n  Anything Model for Crowd-Sourcing Medical Image Annotations","summary":"  Curating annotations for medical image segmentation is a labor-intensive and\ntime-consuming task that requires domain expertise, resulting in \"narrowly\"\nfocused deep learning (DL) models with limited translational utility. Recently,\nfoundation models like the Segment Anything Model (SAM) have revolutionized\nsemantic segmentation with exceptional zero-shot generalizability across\nvarious domains, including medical imaging, and hold a lot of promise for\nstreamlining the annotation process. However, SAM has yet to be evaluated in a\ncrowd-sourced setting to curate annotations for training 3D DL segmentation\nmodels. In this work, we explore the potential of SAM for crowd-sourcing\n\"sparse\" annotations from non-experts to generate \"dense\" segmentation masks\nfor training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our\nresults indicate that while SAM-generated annotations exhibit high mean Dice\nscores compared to ground-truth annotations, nnU-Net models trained on\nSAM-generated annotations perform significantly worse than nnU-Net models\ntrained on ground-truth annotations ($p<0.001$, all).\n","authors":["Pranav Kulkarni","Adway Kanhere","Dharmam Savani","Andrew Chan","Devina Chatterjee","Paul H. Yi","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2403.15218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06205v3","updated":"2024-03-22T14:05:33Z","published":"2024-03-10T13:04:01Z","title":"S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes","summary":"  Current 3D stylization methods often assume static scenes, which violates the\ndynamic nature of our real world. To address this limitation, we present\nS-DyRF, a reference-based spatio-temporal stylization method for dynamic neural\nradiance fields. However, stylizing dynamic 3D scenes is inherently challenging\ndue to the limited availability of stylized reference images along the temporal\naxis. Our key insight lies in introducing additional temporal cues besides the\nprovided reference. To this end, we generate temporal pseudo-references from\nthe given stylized reference. These pseudo-references facilitate the\npropagation of style information from the reference to the entire dynamic 3D\nscene. For coarse style transfer, we enforce novel views and times to mimic the\nstyle details present in pseudo-references at the feature level. To preserve\nhigh-frequency details, we create a collection of stylized temporal pseudo-rays\nfrom temporal pseudo-references. These pseudo-rays serve as detailed and\nexplicit stylization guidance for achieving fine style transfer. Experiments on\nboth synthetic and real-world datasets demonstrate that our method yields\nplausible stylized results of space-time view synthesis on dynamic 3D scenes.\n","authors":["Xingyi Li","Zhiguo Cao","Yizheng Wu","Kewei Wang","Ke Xian","Zhe Wang","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2403.06205v3.pdf","comment":"Accepted by CVPR 2024. Project page:\n  https://xingyi-li.github.io/s-dyrf/"},{"id":"http://arxiv.org/abs/2402.00631v2","updated":"2024-03-22T14:00:37Z","published":"2024-01-31T11:52:33Z","title":"Beyond Inserting: Learning Identity Embedding for Semantic-Fidelity\n  Personalized Diffusion Generation","summary":"  Advanced diffusion-based Text-to-Image (T2I) models, such as the Stable\nDiffusion Model, have made significant progress in generating diverse and\nhigh-quality images using text prompts alone. However, when non-famous users\nrequire personalized image generation for their identities (IDs), the T2I\nmodels fail to accurately generate their ID-related images. The main problem is\nthat pre-trained T2I models do not learn the mapping between the new ID prompts\nand their corresponding visual content. The previous methods either failed to\naccurately fit the face region or lost the interactive generative ability with\nother existing concepts in T2I models. In other words, they are unable to\ngenerate T2I-aligned and semantic-fidelity images for the given prompts with\nother concepts such as scenes (``Eiffel Tower''), actions (``holding a\nbasketball''), and facial attributes (``eyes closed''). In this paper, we focus\non inserting accurate and interactive ID embedding into the Stable Diffusion\nModel for semantic-fidelity personalized generation. We address this challenge\nfrom two perspectives: face-wise region fitting and semantic-fidelity token\noptimization. Specifically, we first visualize the attention overfit problem\nand propose a face-wise attention loss to fit the face region instead of\nentangling ID-unrelated information, such as face layout and background. This\nkey trick significantly enhances the ID accuracy and interactive generative\nability with other existing concepts. Then, we optimize one ID representation\nas multiple per-stage tokens where each token contains two disentangled\nfeatures. This expansion of the textual conditioning space improves\nsemantic-fidelity control. Extensive experiments validate that our results\nexhibit superior ID accuracy, text-based manipulation ability, and\ngeneralization compared to previous methods.\n","authors":["Yang Li","Songlin Yang","Wei Wang","Jing Dong"],"pdf_url":"https://arxiv.org/pdf/2402.00631v2.pdf","comment":"14 pages, 16 figures"},{"id":"http://arxiv.org/abs/2403.15212v1","updated":"2024-03-22T13:55:52Z","published":"2024-03-22T13:55:52Z","title":"GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition","summary":"  Skeleton-based action recognition (SAR) in videos is an important but\nchallenging task in computer vision. The recent state-of-the-art models for SAR\nare primarily based on graph convolutional neural networks (GCNs), which are\npowerful in extracting the spatial information of skeleton data. However, it is\nyet clear that such GCN-based models can effectively capture the temporal\ndynamics of human action sequences. To this end, we propose the DevLSTM module,\nwhich exploits the path development -- a principled and parsimonious\nrepresentation for sequential data by leveraging the Lie group structure. The\npath development, originated from Rough path theory, can effectively capture\nthe order of events in high-dimensional stream data with massive dimension\nreduction and consequently enhance the LSTM module substantially. Our proposed\nG-DevLSTM module can be conveniently plugged into the temporal graph,\ncomplementing existing advanced GCN-based models. Our empirical studies on the\nNTU60, NTU120 and Chalearn2013 datasets demonstrate that our proposed hybrid\nmodel significantly outperforms the current best-performing methods in SAR\ntasks. The code is available at https://github.com/DeepIntoStreams/GCN-DevLSTM.\n","authors":["Lei Jiang","Weixin Yang","Xin Zhang","Hao Ni"],"pdf_url":"https://arxiv.org/pdf/2403.15212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06020v2","updated":"2024-03-22T13:51:55Z","published":"2024-03-09T21:45:31Z","title":"Multi-conditioned Graph Diffusion for Neural Architecture Search","summary":"  Neural architecture search automates the design of neural network\narchitectures usually by exploring a large and thus complex architecture search\nspace. To advance the architecture search, we present a graph diffusion-based\nNAS approach that uses discrete conditional graph diffusion processes to\ngenerate high-performing neural network architectures. We then propose a\nmulti-conditioned classifier-free guidance approach applied to graph diffusion\nnetworks to jointly impose constraints such as high accuracy and low hardware\nlatency. Unlike the related work, our method is completely differentiable and\nrequires only a single model training. In our evaluations, we show promising\nresults on six standard benchmarks, yielding novel and unique architectures at\na fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we\ndemonstrate the generalisability and efficiency of our method through\nexperiments on ImageNet dataset.\n","authors":["Rohan Asthana","Joschua Conrad","Youssef Dawoud","Maurits Ortmanns","Vasileios Belagiannis"],"pdf_url":"https://arxiv.org/pdf/2403.06020v2.pdf","comment":"Accepted at Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2403.15209v1","updated":"2024-03-22T13:50:27Z","published":"2024-03-22T13:50:27Z","title":"MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral\n  Pedestrian Detection","summary":"  Multispectral pedestrian detection is attractive for around-the-clock\napplications due to the complementary information between RGB and thermal\nmodalities. However, current models often fail to detect pedestrians in obvious\ncases, especially due to the modality bias learned from statistically biased\ndatasets. From these problems, we anticipate that maybe understanding the\ncomplementary information itself is difficult to achieve from vision-only\nmodels. Accordingly, we propose a novel Multispectral Chain-of-Thought\nDetection (MSCoTDet) framework, which incorporates Large Language Models (LLMs)\nto understand the complementary information at the semantic level and further\nenhance the fusion process. Specifically, we generate text descriptions of the\npedestrian in each RGB and thermal modality and design a Multispectral\nChain-of-Thought (MSCoT) prompting, which models a step-by-step process to\nfacilitate cross-modal reasoning at the semantic level and perform accurate\ndetection. Moreover, we design a Language-driven Multi-modal Fusion (LMF)\nstrategy that enables fusing vision-driven and language-driven detections.\nExtensive experiments validate that MSCoTDet improves multispectral pedestrian\ndetection.\n","authors":["Taeheon Kim","Sangyun Chung","Damin Yeom","Youngjoon Yu","Hak Gu Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2403.15209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15203v1","updated":"2024-03-22T13:46:51Z","published":"2024-03-22T13:46:51Z","title":"DITTO: Demonstration Imitation by Trajectory Transformation","summary":"  Teaching robots new skills quickly and conveniently is crucial for the\nbroader adoption of robotic systems. In this work, we address the problem of\none-shot imitation from a single human demonstration, given by an RGB-D video\nrecording through a two-stage process. In the first stage which is offline, we\nextract the trajectory of the demonstration. This entails segmenting\nmanipulated objects and determining their relative motion in relation to\nsecondary objects such as containers. Subsequently, in the live online\ntrajectory generation stage, we first \\mbox{re-detect} all objects, then we\nwarp the demonstration trajectory to the current scene, and finally, we trace\nthe trajectory with the robot. To complete these steps, our method makes\nleverages several ancillary models, including those for segmentation, relative\nobject pose estimation, and grasp prediction. We systematically evaluate\ndifferent combinations of correspondence and re-detection methods to validate\nour design decision across a diverse range of tasks. Specifically, we collect\ndemonstrations of ten different tasks including pick-and-place tasks as well as\narticulated object manipulation. Finally, we perform extensive evaluations on a\nreal robot system to demonstrate the effectiveness and utility of our approach\nin real-world scenarios. We make the code publicly available at\nhttp://ditto.cs.uni-freiburg.de.\n","authors":["Nick Heppert","Max Argus","Tim Welschehold","Thomas Brox","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.15203v1.pdf","comment":"8 pages, 4 figures, 3 tables, submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.15194v1","updated":"2024-03-22T13:27:57Z","published":"2024-03-22T13:27:57Z","title":"Your Image is My Video: Reshaping the Receptive Field via Image-To-Video\n  Differentiable AutoAugmentation and Fusion","summary":"  The landscape of deep learning research is moving towards innovative\nstrategies to harness the true potential of data. Traditionally, emphasis has\nbeen on scaling model architectures, resulting in large and complex neural\nnetworks, which can be difficult to train with limited computational resources.\nHowever, independently of the model size, data quality (i.e. amount and\nvariability) is still a major factor that affects model generalization. In this\nwork, we propose a novel technique to exploit available data through the use of\nautomatic data augmentation for the tasks of image classification and semantic\nsegmentation. We introduce the first Differentiable Augmentation Search method\n(DAS) to generate variations of images that can be processed as videos.\nCompared to previous approaches, DAS is extremely fast and flexible, allowing\nthe search on very large search spaces in less than a GPU day. Our intuition is\nthat the increased receptive field in the temporal dimension provided by DAS\ncould lead to benefits also to the spatial receptive field. More specifically,\nwe leverage DAS to guide the reshaping of the spatial receptive field by\nselecting task-dependant transformations. As a result, compared to standard\naugmentation alternatives, we improve in terms of accuracy on ImageNet,\nCifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when\nplugging-in our DAS over different light-weight video backbones.\n","authors":["Sofia Casarin","Cynthia I. Ugwu","Sergio Escalera","Oswald Lanz"],"pdf_url":"https://arxiv.org/pdf/2403.15194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13964v2","updated":"2024-03-22T13:25:53Z","published":"2023-12-21T15:51:12Z","title":"PIA: Your Personalized Image Animator via Plug-and-Play Modules in\n  Text-to-Image Models","summary":"  Recent advancements in personalized text-to-image (T2I) models have\nrevolutionized content creation, empowering non-experts to generate stunning\nimages with unique styles. While promising, adding realistic motions into these\npersonalized images by text poses significant challenges in preserving distinct\nstyles, high-fidelity details, and achieving motion controllability by text. In\nthis paper, we present PIA, a Personalized Image Animator that excels in\naligning with condition images, achieving motion controllability by text, and\nthe compatibility with various personalized T2I models without specific tuning.\nTo achieve these goals, PIA builds upon a base T2I model with well-trained\ntemporal alignment layers, allowing for the seamless transformation of any\npersonalized T2I model into an image animation model. A key component of PIA is\nthe introduction of the condition module, which utilizes the condition frame\nand inter-frame affinity as input to transfer appearance information guided by\nthe affinity hint for individual frame synthesis in the latent space. This\ndesign mitigates the challenges of appearance-related image alignment within\nand allows for a stronger focus on aligning with motion-related guidance.\n","authors":["Yiming Zhang","Zhening Xing","Yanhong Zeng","Youqing Fang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2312.13964v2.pdf","comment":"Project page: https://pi-animator.github.io/"},{"id":"http://arxiv.org/abs/2403.15192v1","updated":"2024-03-22T13:24:50Z","published":"2024-03-22T13:24:50Z","title":"SFOD: Spiking Fusion Object Detector","summary":"  Event cameras, characterized by high temporal resolution, high dynamic range,\nlow power consumption, and high pixel bandwidth, offer unique capabilities for\nobject detection in specialized contexts. Despite these advantages, the\ninherent sparsity and asynchrony of event data pose challenges to existing\nobject detection algorithms. Spiking Neural Networks (SNNs), inspired by the\nway the human brain codes and processes information, offer a potential solution\nto these difficulties. However, their performance in object detection using\nevent cameras is limited in current implementations. In this paper, we propose\nthe Spiking Fusion Object Detector (SFOD), a simple and efficient approach to\nSNN-based object detection. Specifically, we design a Spiking Fusion Module,\nachieving the first-time fusion of feature maps from different scales in SNNs\napplied to event cameras. Additionally, through integrating our analysis and\nexperiments conducted during the pretraining of the backbone network on the\nNCAR dataset, we delve deeply into the impact of spiking decoding strategies\nand loss functions on model performance. Thereby, we establish state-of-the-art\nclassification results based on SNNs, achieving 93.7\\% accuracy on the NCAR\ndataset. Experimental results on the GEN1 detection dataset demonstrate that\nthe SFOD achieves a state-of-the-art mAP of 32.1\\%, outperforming existing\nSNN-based approaches. Our research not only underscores the potential of SNNs\nin object detection with event cameras but also propels the advancement of\nSNNs. Code is available at https://github.com/yimeng-fan/SFOD.\n","authors":["Yimeng Fan","Wei Zhang","Changsong Liu","Mingyang Li","Wenrui Lu"],"pdf_url":"https://arxiv.org/pdf/2403.15192v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2306.14899v2","updated":"2024-03-22T13:24:35Z","published":"2023-06-26T17:59:55Z","title":"FunQA: Towards Surprising Video Comprehension","summary":"  Surprising videos, such as funny clips, creative performances, or visual\nillusions, attract significant attention. Enjoyment of these videos is not\nsimply a response to visual stimuli; rather, it hinges on the human capacity to\nunderstand (and appreciate) commonsense violations depicted in these videos. We\nintroduce FunQA, a challenging video question-answering (QA) dataset\nspecifically designed to evaluate and enhance the depth of video reasoning\nbased on counter-intuitive and fun videos. Unlike most video QA benchmarks\nwhich focus on less surprising contexts, e.g., cooking or instructional videos,\nFunQA covers three previously unexplored types of surprising videos: 1)\nHumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous\nQA tasks designed to assess the model's capability in counter-intuitive\ntimestamp localization, detailed video description, and reasoning around\ncounter-intuitiveness. We also pose higher-level tasks, such as attributing a\nfitting and vivid title to the video and scoring the video creativity. In\ntotal, the FunQA benchmark consists of 312K free-text QA pairs derived from\n4.3K video clips, spanning a total of 24 video hours. Moreover, we propose\nFunMentor, an agent designed for Vision-Language Models (VLMs) that uses\nmulti-turn dialogues to enhance models' understanding of counter-intuitiveness.\nExtensive experiments with existing VLMs demonstrate the effectiveness of\nFunMentor and reveal significant performance gaps for the FunQA videos across\nspatial-temporal reasoning, visual-centered reasoning, and free-text\ngeneration.\n","authors":["Binzhu Xie","Sicheng Zhang","Zitang Zhou","Bo Li","Yuanhan Zhang","Jack Hessel","Jingkang Yang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2306.14899v2.pdf","comment":"Project Page: https://funqa-benchmark.github.io/ Codebase:\n  https://github.com/Jingkang50/FunQA"},{"id":"http://arxiv.org/abs/2403.15182v1","updated":"2024-03-22T13:11:26Z","published":"2024-03-22T13:11:26Z","title":"PDE-CNNs: Axiomatic Derivations and Applications","summary":"  PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of\ngeometrically meaningful evolution PDEs as substitutes for the conventional\ncomponents in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer\nparameters, inherent equivariance, better performance, data efficiency, and\ngeometric interpretability. In this article we focus on Euclidean equivariant\nPDE-G-CNNs where the feature maps are two dimensional throughout. We call this\nvariant of the framework a PDE-CNN. We list several practically desirable\naxioms and derive from these which PDEs should be used in a PDE-CNN. Here our\napproach to geometric learning via PDEs is inspired by the axioms of classical\nlinear and morphological scale-space theory, which we generalize by introducing\nsemifield-valued signals. Furthermore, we experimentally confirm for small\nnetworks that PDE-CNNs offer fewer parameters, better performance, and data\nefficiency in comparison to CNNs. We also investigate what effect the use of\ndifferent semifields has on the performance of the models.\n","authors":["Gijs Bellaard","Sei Sakata","Bart M. N. Smets","Remco Duits"],"pdf_url":"https://arxiv.org/pdf/2403.15182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08709v2","updated":"2024-03-22T12:55:14Z","published":"2023-04-18T02:45:18Z","title":"You Only Need Two Detectors to Achieve Multi-Modal 3D Multi-Object\n  Tracking","summary":"  In the classical tracking-by-detection (TBD) paradigm, detection and tracking\nare separately and sequentially conducted, and data association must be\nproperly performed to achieve satisfactory tracking performance. In this paper,\na new end-to-end multi-object tracking framework is proposed, which integrates\nobject detection and multi-object tracking into a single model. The proposed\ntracking framework eliminates the complex data association process in the\nclassical TBD paradigm, and requires no additional training. Secondly, the\nregression confidence of historical trajectories is investigated, and the\npossible states of a trajectory (weak object or strong object) in the current\nframe are predicted. Then, a confidence fusion module is designed to guide\nnon-maximum suppression for trajectories and detections to achieve ordered and\nrobust tracking. Thirdly, by integrating historical trajectory features, the\nregression performance of the detector is enhanced, which better reflects the\nocclusion and disappearance patterns of objects in real world. Lastly,\nextensive experiments are conducted on the commonly used KITTI and Waymo\ndatasets. The results show that the proposed framework can achieve robust\ntracking by using only a 2D detector and a 3D detector, and it is proven more\naccurate than many of the state-of-the-art TBD-based multi-modal tracking\nmethods. The source codes of the proposed method are available at\nhttps://github.com/wangxiyang2022/YONTD-MOT.\n","authors":["Xiyang Wang","Chunyun Fu","Jiawei He","Mingguang Huang","Ting Meng","Siyu Zhang","Hangning Zhou","Ziyao Xu","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.08709v2.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.15173v1","updated":"2024-03-22T12:54:33Z","published":"2024-03-22T12:54:33Z","title":"LSK3DNet: Towards Effective and Efficient 3D Perception with Large\n  Sparse Kernels","summary":"  Autonomous systems need to process large-scale, sparse, and irregular point\nclouds with limited compute resources. Consequently, it is essential to develop\nLiDAR perception methods that are both efficient and effective. Although\nnaively enlarging 3D kernel size can enhance performance, it will also lead to\na cubically-increasing overhead. Therefore, it is crucial to develop\nstreamlined 3D large kernel designs that eliminate redundant weights and work\neffectively with larger kernels. In this paper, we propose an efficient and\neffective Large Sparse Kernel 3D Neural Network (LSK3DNet) that leverages\ndynamic pruning to amplify the 3D kernel size. Our method comprises two core\ncomponents: Spatial-wise Dynamic Sparsity (SDS) and Channel-wise Weight\nSelection (CWS). SDS dynamically prunes and regrows volumetric weights from the\nbeginning to learn a large sparse 3D kernel. It not only boosts performance but\nalso significantly reduces model size and computational cost. Moreover, CWS\nselects the most important channels for 3D convolution during training and\nsubsequently prunes the redundant channels to accelerate inference for 3D\nvision tasks. We demonstrate the effectiveness of LSK3DNet on three benchmark\ndatasets and five tracks compared with classical models and large kernel\ndesigns. Notably, LSK3DNet achieves the state-of-the-art performance on\nSemanticKITTI (i.e., 75.6% on single-scan and 63.4% on multi-scan), with\nroughly 40% model size reduction and 60% computing operations reduction\ncompared to the naive large 3D kernel model.\n","authors":["Tuo Feng","Wenguan Wang","Fan Ma","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.15173v1.pdf","comment":"Accepted at CVPR 2024; Project page:\n  https://github.com/FengZicai/LSK3DNet"},{"id":"http://arxiv.org/abs/2403.13248v2","updated":"2024-03-22T12:43:56Z","published":"2024-03-20T02:19:21Z","title":"Mora: Enabling Generalist Video Generation via A Multi-Agent Framework","summary":"  Sora is the first large-scale generalist video generation model that garnered\nsignificant attention across society. Since its launch by OpenAI in February\n2024, no other video generation models have paralleled {Sora}'s performance or\nits capacity to support a broad spectrum of video generation tasks.\nAdditionally, there are only a few fully published video generation models,\nwith the majority being closed-source. To address this gap, this paper proposes\na new multi-agent framework Mora, which incorporates several advanced visual AI\nagents to replicate generalist video generation demonstrated by Sora. In\nparticular, Mora can utilize multiple visual agents and successfully mimic\nSora's video generation capabilities in various tasks, such as (1)\ntext-to-video generation, (2) text-conditional image-to-video generation, (3)\nextend generated videos, (4) video-to-video editing, (5) connect videos and (6)\nsimulate digital worlds. Our extensive experimental results show that Mora\nachieves performance that is proximate to that of Sora in various tasks.\nHowever, there exists an obvious performance gap between our work and Sora when\nassessed holistically. In summary, we hope this project can guide the future\ntrajectory of video generation through collaborative AI agents.\n","authors":["Zhengqing Yuan","Ruoxi Chen","Zhaoxu Li","Haolong Jia","Lifang He","Chi Wang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2403.13248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07846v3","updated":"2024-03-22T12:41:50Z","published":"2023-09-14T16:40:44Z","title":"MC-NeRF: Multi-Camera Neural Radiance Fields for Multi-Camera Image\n  Acquisition Systems","summary":"  Neural Radiance Fields (NeRF) use multi-view images for 3D scene\nrepresentation, demonstrating remarkable performance. As one of the primary\nsources of multi-view images, multi-camera systems encounter challenges such as\nvarying intrinsic parameters and frequent pose changes. Most previous\nNeRF-based methods assume a unique camera and rarely consider multi-camera\nscenarios. Besides, some NeRF methods that can optimize intrinsic and extrinsic\nparameters still remain susceptible to suboptimal solutions when these\nparameters are poor initialized. In this paper, we propose MC-NeRF, a method\nthat enables joint optimization of both intrinsic and extrinsic parameters\nalongside NeRF. The method also supports each image corresponding to\nindependent camera parameters. First, we tackle coupling issue and the\ndegenerate case that arise from the joint optimization between intrinsic and\nextrinsic parameters. Second, based on the proposed solutions, we introduce an\nefficient calibration image acquisition scheme for multi-camera systems,\nincluding the design of calibration object. Finally, we present an end-to-end\nnetwork with training sequence that enables the estimation of intrinsic and\nextrinsic parameters, along with the rendering network. Furthermore,\nrecognizing that most existing datasets are designed for a unique camera, we\nconstruct a real multi-camera image acquisition system and create a\ncorresponding new dataset, which includes both simulated data and real-world\ncaptured images. Experiments confirm the effectiveness of our method when each\nimage corresponds to different camera parameters. Specifically, we use\nmulti-cameras, each with different intrinsic and extrinsic parameters in\nreal-world system, to achieve 3D scene representation without providing initial\nposes.\n","authors":["Yu Gao","Lutong Su","Hao Liang","Yufeng Yue","Yi Yang","Mengyin Fu"],"pdf_url":"https://arxiv.org/pdf/2309.07846v3.pdf","comment":"This manuscript is currently under review"},{"id":"http://arxiv.org/abs/2312.04964v2","updated":"2024-03-22T12:34:13Z","published":"2023-12-07T12:09:56Z","title":"ZePT: Zero-Shot Pan-Tumor Segmentation via Query-Disentangling and\n  Self-Prompting","summary":"  The long-tailed distribution problem in medical image analysis reflects a\nhigh prevalence of common conditions and a low prevalence of rare ones, which\nposes a significant challenge in developing a unified model capable of\nidentifying rare or novel tumor categories not encountered during training. In\nthis paper, we propose a new zero-shot pan-tumor segmentation framework (ZePT)\nbased on query-disentangling and self-prompting to segment unseen tumor\ncategories beyond the training set. ZePT disentangles the object queries into\ntwo subsets and trains them in two stages. Initially, it learns a set of\nfundamental queries for organ segmentation through an object-aware feature\ngrouping strategy, which gathers organ-level visual features. Subsequently, it\nrefines the other set of advanced queries that focus on the auto-generated\nvisual prompts for unseen tumor segmentation. Moreover, we introduce\nquery-knowledge alignment at the feature level to enhance each query's\ndiscriminative representation and generalizability. Extensive experiments on\nvarious tumor segmentation tasks demonstrate the performance superiority of\nZePT, which surpasses the previous counterparts and evidence the promising\nability for zero-shot tumor segmentation in real-world settings.\n","authors":["Yankai Jiang","Zhongzhen Huang","Rongzhao Zhang","Xiaofan Zhang","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.04964v2.pdf","comment":"This paper has been accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07359v3","updated":"2024-03-22T12:33:51Z","published":"2024-03-12T06:45:34Z","title":"FSC: Few-point Shape Completion","summary":"  While previous studies have demonstrated successful 3D object shape\ncompletion with a sufficient number of points, they often fail in scenarios\nwhen a few points, e.g. tens of points, are observed. Surprisingly, via entropy\nanalysis, we find that even a few points, e.g. 64 points, could retain\nsubstantial information to help recover the 3D shape of the object. To address\nthe challenge of shape completion with very sparse point clouds, we then\npropose Few-point Shape Completion (FSC) model, which contains a novel\ndual-branch feature extractor for handling extremely sparse inputs, coupled\nwith an extensive branch for maximal point utilization with a saliency branch\nfor dynamic importance assignment. This model is further bolstered by a\ntwo-stage revision network that refines both the extracted features and the\ndecoder output, enhancing the detail and authenticity of the completed point\ncloud. Our experiments demonstrate the feasibility of recovering 3D shapes from\na few points. The proposed Few-point Shape Completion (FSC) model outperforms\nprevious methods on both few-point inputs and many-point inputs, and shows good\ngeneralizability to different object categories.\n","authors":["Xianzu Wu","Xianfeng Wu","Tianyu Luan","Yajing Bai","Zhongyuan Lai","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.07359v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15161v1","updated":"2024-03-22T12:20:23Z","published":"2024-03-22T12:20:23Z","title":"FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos","summary":"  Digitising the 3D world into a clean, CAD model-based representation has\nimportant applications for augmented reality and robotics. Current\nstate-of-the-art methods are computationally intensive as they individually\nencode each detected object and optimise CAD alignments in a second stage. In\nthis work, we propose FastCAD, a real-time method that simultaneously retrieves\nand aligns CAD models for all objects in a given scene. In contrast to previous\nworks, we directly predict alignment parameters and shape embeddings. We\nachieve high-quality shape retrievals by learning CAD embeddings in a\ncontrastive learning framework and distilling those into FastCAD. Our\nsingle-stage method accelerates the inference time by a factor of 50 compared\nto other methods operating on RGB-D scans while outperforming them on the\nchallenging Scan2CAD alignment benchmark. Further, our approach collaborates\nseamlessly with online 3D reconstruction techniques. This enables the real-time\ngeneration of precise CAD model-based reconstructions from videos at 10 FPS.\nDoing so, we significantly improve the Scan2CAD alignment accuracy in the video\nsetting from 43.0% to 48.2% and the reconstruction accuracy from 22.9% to\n29.6%.\n","authors":["Florian Langer","Jihong Ju","Georgi Dikov","Gerhard Reitmayr","Mohsen Ghafoorian"],"pdf_url":"https://arxiv.org/pdf/2403.15161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15156v1","updated":"2024-03-22T12:11:06Z","published":"2024-03-22T12:11:06Z","title":"Infrastructure-Assisted Collaborative Perception in Automated Valet\n  Parking: A Safety Perspective","summary":"  Environmental perception in Automated Valet Parking (AVP) has been a\nchallenging task due to severe occlusions in parking garages. Although\nCollaborative Perception (CP) can be applied to broaden the field of view of\nconnected vehicles, the limited bandwidth of vehicular communications restricts\nits application. In this work, we propose a BEV feature-based CP network\narchitecture for infrastructure-assisted AVP systems. The model takes the\nroadside camera and LiDAR as optional inputs and adaptively fuses them with\nonboard sensors in a unified BEV representation. Autoencoder and downsampling\nare applied for channel-wise and spatial-wise dimension reduction, while\nsparsification and quantization further compress the feature map with little\nloss in data precision. Combining these techniques, the size of a BEV feature\nmap is effectively compressed to fit in the feasible data rate of the NR-V2X\nnetwork. With the synthetic AVP dataset, we observe that CP can effectively\nincrease perception performance, especially for pedestrians. Moreover, the\nadvantage of infrastructure-assisted CP is demonstrated in two typical\nsafety-critical scenarios in the AVP setting, increasing the maximum safe\ncruising speed by up to 3m/s in both scenarios.\n","authors":["Yukuan Jia","Jiawen Zhang","Shimeng Lu","Baokang Fan","Ruiqing Mao","Sheng Zhou","Zhisheng Niu"],"pdf_url":"https://arxiv.org/pdf/2403.15156v1.pdf","comment":"7 pages, 7 figures, 4 tables, accepted by IEEE VTC2024-Spring"},{"id":"http://arxiv.org/abs/2403.15152v1","updated":"2024-03-22T12:08:16Z","published":"2024-03-22T12:08:16Z","title":"A Multimodal Approach for Cross-Domain Image Retrieval","summary":"  Image generators are gaining vast amount of popularity and have rapidly\nchanged how digital content is created. With the latest AI technology, millions\nof high quality images are being generated by the public, which are constantly\nmotivating the research community to push the limits of generative models to\ncreate more complex and realistic images. This paper focuses on Cross-Domain\nImage Retrieval (CDIR) which can be used as an additional tool to inspect\ncollections of generated images by determining the level of similarity between\nimages in a dataset. An ideal retrieval system would be able to generalize to\nunseen complex images from multiple domains (e.g., photos, drawings and\npaintings). To address this goal, we propose a novel caption-matching approach\nthat leverages multimodal language-vision architectures pre-trained on large\ndatasets. The method is tested on DomainNet and Office-Home datasets and\nconsistently achieves state-of-the-art performance over the latest approaches\nin the literature for cross-domain image retrieval. In order to verify the\neffectiveness with AI-generated images, the method was also put to test with a\ndatabase composed by samples collected from Midjourney, which is a widely used\ngenerative platform for content creation.\n","authors":["Lucas Iijima","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2403.15152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15150v1","updated":"2024-03-22T12:06:40Z","published":"2024-03-22T12:06:40Z","title":"An In-Depth Analysis of Data Reduction Methods for Sustainable Deep\n  Learning","summary":"  In recent years, Deep Learning has gained popularity for its ability to solve\ncomplex classification tasks, increasingly delivering better results thanks to\nthe development of more accurate models, the availability of huge volumes of\ndata and the improved computational capabilities of modern computers. However,\nthese improvements in performance also bring efficiency problems, related to\nthe storage of datasets and models, and to the waste of energy and time\ninvolved in both the training and inference processes. In this context, data\nreduction can help reduce energy consumption when training a deep learning\nmodel. In this paper, we present up to eight different methods to reduce the\nsize of a tabular training dataset, and we develop a Python package to apply\nthem. We also introduce a representativeness metric based on topology to\nmeasure how similar are the reduced datasets and the full training dataset.\nAdditionally, we develop a methodology to apply these data reduction methods to\nimage datasets for object detection tasks. Finally, we experimentally compare\nhow these data reduction methods affect the representativeness of the reduced\ndataset, the energy consumption and the predictive performance of the model.\n","authors":["Víctor Toscano-Durán","Javier Perera-Lago","Eduardo Paluzo-Hidalgo","Rocío Gonzalez-Diaz","Miguel Ángel Gutierrez-Naranjo","Matteo Rucco"],"pdf_url":"https://arxiv.org/pdf/2403.15150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12550v2","updated":"2024-03-22T12:05:53Z","published":"2024-03-19T08:49:48Z","title":"RGBD GS-ICP SLAM","summary":"  Simultaneous Localization and Mapping (SLAM) with dense representation plays\na key role in robotics, Virtual Reality (VR), and Augmented Reality (AR)\napplications. Recent advancements in dense representation SLAM have highlighted\nthe potential of leveraging neural scene representation and 3D Gaussian\nrepresentation for high-fidelity spatial representation. In this paper, we\npropose a novel dense representation SLAM approach with a fusion of Generalized\nIterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast\nto existing methods, we utilize a single Gaussian map for both tracking and\nmapping, resulting in mutual benefits. Through the exchange of covariances\nbetween tracking and mapping processes with scale alignment techniques, we\nminimize redundant computations and achieve an efficient system. Additionally,\nwe enhance tracking accuracy and mapping quality through our keyframe selection\nmethods. Experimental results demonstrate the effectiveness of our approach,\nshowing an incredibly fast speed up to 107 FPS (for the entire system) and\nsuperior quality of the reconstructed map.\n","authors":["Seongbo Ha","Jiung Yeon","Hyeonwoo Yu"],"pdf_url":"https://arxiv.org/pdf/2403.12550v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15143v1","updated":"2024-03-22T11:53:03Z","published":"2024-03-22T11:53:03Z","title":"Modular Deep Active Learning Framework for Image Annotation: A Technical\n  Report for the Ophthalmo-AI Project","summary":"  Image annotation is one of the most essential tasks for guaranteeing proper\ntreatment for patients and tracking progress over the course of therapy in the\nfield of medical imaging and disease diagnosis. However, manually annotating a\nlot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL)\nbased segmentation algorithms have completely transformed this process and made\nit possible to automate image segmentation. By accurately segmenting medical\nimages, these algorithms can greatly minimize the time and effort necessary for\nmanual annotation. Additionally, by incorporating Active Learning (AL) methods,\nthese segmentation algorithms can perform far more effectively with a smaller\namount of ground truth data. We introduce MedDeepCyleAL, an end-to-end\nframework implementing the complete AL cycle. It provides researchers with the\nflexibility to choose the type of deep learning model they wish to employ and\nincludes an annotation tool that supports the classification and segmentation\nof medical images. The user-friendly interface allows for easy alteration of\nthe AL and DL model settings through a configuration file, requiring no prior\nprogramming experience. While MedDeepCyleAL can be applied to any kind of image\ndata, we have specifically applied it to ophthalmology data in this project.\n","authors":["Md Abdul Kadir","Hasan Md Tusfiqur Alam","Pascale Maul","Hans-Jürgen Profitlich","Moritz Wolf","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2403.15143v1.pdf","comment":"DFKI Technical Report"},{"id":"http://arxiv.org/abs/2403.15139v1","updated":"2024-03-22T11:48:09Z","published":"2024-03-22T11:48:09Z","title":"Deep Generative Model based Rate-Distortion for Image Downscaling\n  Assessment","summary":"  In this paper, we propose Image Downscaling Assessment by Rate-Distortion\n(IDA-RD), a novel measure to quantitatively evaluate image downscaling\nalgorithms. In contrast to image-based methods that measure the quality of\ndownscaled images, ours is process-based that draws ideas from rate-distortion\ntheory to measure the distortion incurred during downscaling. Our main idea is\nthat downscaling and super-resolution (SR) can be viewed as the encoding and\ndecoding processes in the rate-distortion model, respectively, and that a\ndownscaling algorithm that preserves more details in the resulting\nlow-resolution (LR) images should lead to less distorted high-resolution (HR)\nimages in SR. In other words, the distortion should increase as the downscaling\nalgorithm deteriorates. However, it is non-trivial to measure this distortion\nas it requires the SR algorithm to be blind and stochastic. Our key insight is\nthat such requirements can be met by recent SR algorithms based on deep\ngenerative models that can find all matching HR images for a given LR image on\ntheir learned image manifolds. Extensive experimental results show the\neffectiveness of our IDA-RD measure.\n","authors":["Yuanbang Liang","Bhavesh Garg","Paul L Rosin","Yipeng Qin"],"pdf_url":"https://arxiv.org/pdf/2403.15139v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.11220v3","updated":"2024-03-22T11:42:40Z","published":"2024-03-17T13:43:10Z","title":"CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object\n  Detection under Unknown Degradations","summary":"  Object detection methods under known single degradations have been\nextensively investigated. However, existing approaches require prior knowledge\nof the degradation type and train a separate model for each, limiting their\npractical applications in unpredictable environments. To address this\nchallenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer,\nCPA-Enhancer, for object detection under unknown degradations. Specifically,\nCPA-Enhancer progressively adapts its enhancement strategy under the\nstep-by-step guidance of CoT prompts, that encode degradation-related\ninformation. To the best of our knowledge, it's the first work that exploits\nCoT prompting for object detection tasks. Overall, CPA-Enhancer is a\nplug-and-play enhancement model that can be integrated into any generic\ndetectors to achieve substantial gains on degraded images, without knowing the\ndegradation type priorly. Experimental results demonstrate that CPA-Enhancer\nnot only sets the new state of the art for object detection but also boosts the\nperformance of other downstream vision tasks under unknown degradations.\n","authors":["Yuwei Zhang","Yan Wu","Yanming Liu","Xinyue Peng"],"pdf_url":"https://arxiv.org/pdf/2403.11220v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13408v2","updated":"2024-03-22T11:41:38Z","published":"2024-03-20T08:50:15Z","title":"S2DM: Sector-Shaped Diffusion Models for Video Generation","summary":"  Diffusion models have achieved great success in image generation. However,\nwhen leveraging this idea for video generation, we face significant challenges\nin maintaining the consistency and continuity across video frames. This is\nmainly caused by the lack of an effective framework to align frames of videos\nwith desired temporal features while preserving consistent semantic and\nstochastic features. In this work, we propose a novel Sector-Shaped Diffusion\nModel (S2DM) whose sector-shaped diffusion region is formed by a set of\nray-shaped reverse diffusion processes starting at the same noise point. S2DM\ncan generate a group of intrinsically related data sharing the same semantic\nand stochastic features while varying on temporal features with appropriate\nguided conditions. We apply S2DM to video generation tasks, and explore the use\nof optical flow as temporal conditions. Our experimental results show that S2DM\noutperforms many existing methods in the task of video generation without any\ntemporal-feature modelling modules. For text-to-video generation tasks where\ntemporal conditions are not explicitly given, we propose a two-stage generation\nstrategy which can decouple the generation of temporal features from\nsemantic-content features. We show that, without additional training, our model\nintegrated with another temporal conditions generative model can still achieve\ncomparable performance with existing works. Our results can be viewd at\nhttps://s2dm.github.io/S2DM/.\n","authors":["Haoran Lang","Yuxuan Ge","Zheng Tian"],"pdf_url":"https://arxiv.org/pdf/2403.13408v2.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.15132v1","updated":"2024-03-22T11:33:04Z","published":"2024-03-22T11:33:04Z","title":"Transfer CLIP for Generalizable Image Denoising","summary":"  Image denoising is a fundamental task in computer vision. While prevailing\ndeep learning-based supervised and self-supervised methods have excelled in\neliminating in-distribution noise, their susceptibility to out-of-distribution\n(OOD) noise remains a significant challenge. The recent emergence of\ncontrastive language-image pre-training (CLIP) model has showcased exceptional\ncapabilities in open-world image recognition and segmentation. Yet, the\npotential for leveraging CLIP to enhance the robustness of low-level tasks\nremains largely unexplored. This paper uncovers that certain dense features\nextracted from the frozen ResNet image encoder of CLIP exhibit\ndistortion-invariant and content-related properties, which are highly desirable\nfor generalizable denoising. Leveraging these properties, we devise an\nasymmetrical encoder-decoder denoising network, which incorporates dense\nfeatures including the noisy image and its multi-scale features from the frozen\nResNet encoder of CLIP into a learnable image decoder to achieve generalizable\ndenoising. The progressive feature augmentation strategy is further proposed to\nmitigate feature overfitting and improve the robustness of the learnable\ndecoder. Extensive experiments and comparisons conducted across diverse OOD\nnoises, including synthetic noise, real-world sRGB noise, and low-dose CT image\nnoise, demonstrate the superior generalization ability of our method.\n","authors":["Jun Cheng","Dong Liang","Shan Tan"],"pdf_url":"https://arxiv.org/pdf/2403.15132v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.15127v1","updated":"2024-03-22T11:30:10Z","published":"2024-03-22T11:30:10Z","title":"Gradient-based Sampling for Class Imbalanced Semi-supervised Object\n  Detection","summary":"  Current semi-supervised object detection (SSOD) algorithms typically assume\nclass balanced datasets (PASCAL VOC etc.) or slightly class imbalanced datasets\n(MS-COCO, etc). This assumption can be easily violated since real world\ndatasets can be extremely class imbalanced in nature, thus making the\nperformance of semi-supervised object detectors far from satisfactory. Besides,\nthe research for this problem in SSOD is severely under-explored. To bridge\nthis research gap, we comprehensively study the class imbalance problem for\nSSOD under more challenging scenarios, thus forming the first experimental\nsetting for class imbalanced SSOD (CI-SSOD). Moreover, we propose a simple yet\neffective gradient-based sampling framework that tackles the class imbalance\nproblem from the perspective of two types of confirmation biases. To tackle\nconfirmation bias towards majority classes, the gradient-based reweighting and\ngradient-based thresholding modules leverage the gradients from each class to\nfully balance the influence of the majority and minority classes. To tackle the\nconfirmation bias from incorrect pseudo labels of minority classes, the\nclass-rebalancing sampling module resamples unlabeled data following the\nguidance of the gradient-based reweighting module. Experiments on three\nproposed sub-tasks, namely MS-COCO, MS-COCO to Object365 and LVIS, suggest that\nour method outperforms current class imbalanced object detectors by clear\nmargins, serving as a baseline for future research in CI-SSOD. Code will be\navailable at https://github.com/nightkeepers/CI-SSOD.\n","authors":["Jiaming Li","Xiangru Lin","Wei Zhang","Xiao Tan","Yingying Li","Junyu Han","Errui Ding","Jingdong Wang","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2403.15127v1.pdf","comment":"Accepted by ICCV2023"},{"id":"http://arxiv.org/abs/2403.15124v1","updated":"2024-03-22T11:27:43Z","published":"2024-03-22T11:27:43Z","title":"EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic\n  Surgeries using Gaussian Splatting","summary":"  Precise camera tracking, high-fidelity 3D tissue reconstruction, and\nreal-time online visualization are critical for intrabody medical imaging\ndevices such as endoscopes and capsule robots. However, existing SLAM\n(Simultaneous Localization and Mapping) methods often struggle to achieve both\ncomplete high-quality surgical field reconstruction and efficient computation,\nrestricting their intraoperative applications among endoscopic surgeries. In\nthis paper, we introduce EndoGSLAM, an efficient SLAM approach for endoscopic\nsurgeries, which integrates streamlined Gaussian representation and\ndifferentiable rasterization to facilitate over 100 fps rendering speed during\nonline camera tracking and tissue reconstructing. Extensive experiments show\nthat EndoGSLAM achieves a better trade-off between intraoperative availability\nand reconstruction quality than traditional or neural SLAM approaches, showing\ntremendous potential for endoscopic surgeries. The project page is at\nhttps://EndoGSLAM.loping151.com\n","authors":["Kailing Wang","Chen Yang","Yuehao Wang","Sikuang Li","Yan Wang","Qi Dou","Xiaokang Yang","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2403.15124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15121v1","updated":"2024-03-22T11:24:31Z","published":"2024-03-22T11:24:31Z","title":"SYNCS: Synthetic Data and Contrastive Self-Supervised Training for\n  Central Sulcus Segmentation","summary":"  Bipolar disorder (BD) and schizophrenia (SZ) are severe mental disorders with\nprofound societal impact. Identifying risk markers early is crucial for\nunderstanding disease progression and enabling preventive measures. The Danish\nHigh Risk and Resilience Study (VIA) focuses on understanding early disease\nprocesses, particularly in children with familial high risk (FHR).\nUnderstanding structural brain changes associated with these diseases during\nearly stages is essential for effective interventions. The central sulcus (CS)\nis a prominent brain landmark related to brain regions involved in motor and\nsensory processing. Analyzing CS morphology can provide valuable insights into\nneurodevelopmental abnormalities in the FHR group. However, segmenting the\ncentral sulcus (CS) presents challenges due to its variability, especially in\nadolescents. This study introduces two novel approaches to improve CS\nsegmentation: synthetic data generation to model CS variability and\nself-supervised pre-training with multi-task learning to adapt models to new\ncohorts. These methods aim to enhance segmentation performance across diverse\npopulations, eliminating the need for extensive preprocessing.\n","authors":["Vladyslav Zalevskyi","Kristoffer Hougaard Madsen"],"pdf_url":"https://arxiv.org/pdf/2403.15121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15119v1","updated":"2024-03-22T11:21:51Z","published":"2024-03-22T11:21:51Z","title":"An Open-World, Diverse, Cross-Spatial-Temporal Benchmark for Dynamic\n  Wild Person Re-Identification","summary":"  Person re-identification (ReID) has made great strides thanks to the\ndata-driven deep learning techniques. However, the existing benchmark datasets\nlack diversity, and models trained on these data cannot generalize well to\ndynamic wild scenarios. To meet the goal of improving the explicit\ngeneralization of ReID models, we develop a new Open-World, Diverse,\nCross-Spatial-Temporal dataset named OWD with several distinct features. 1)\nDiverse collection scenes: multiple independent open-world and highly dynamic\ncollecting scenes, including streets, intersections, shopping malls, etc. 2)\nDiverse lighting variations: long time spans from daytime to nighttime with\nabundant illumination changes. 3) Diverse person status: multiple camera\nnetworks in all seasons with normal/adverse weather conditions and diverse\npedestrian appearances (e.g., clothes, personal belongings, poses, etc.). 4)\nProtected privacy: invisible faces for privacy critical applications. To\nimprove the implicit generalization of ReID, we further propose a Latent Domain\nExpansion (LDE) method to develop the potential of source data, which decouples\ndiscriminative identity-relevant and trustworthy domain-relevant features and\nimplicitly enforces domain-randomized identity feature space expansion with\nricher domain diversity to facilitate domain invariant representations. Our\ncomprehensive evaluations with most benchmark datasets in the community are\ncrucial for progress, although this work is far from the grand goal toward\nopen-world and dynamic wild applications.\n","authors":["Lei Zhang","Xiaowei Fu","Fuxiang Huang","Yi Yang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2403.15119v1.pdf","comment":"Accepted by IJCV in 2024"},{"id":"http://arxiv.org/abs/2403.15107v1","updated":"2024-03-22T10:51:31Z","published":"2024-03-22T10:51:31Z","title":"PseudoTouch: Efficiently Imaging the Surface Feel of Objects for Robotic\n  Manipulation","summary":"  Humans seemingly incorporate potential touch signals in their perception. Our\ngoal is to equip robots with a similar capability, which we term \\ourmodel.\n\\ourmodel aims to predict the expected touch signal based on a visual patch\nrepresenting the touched area. We frame this problem as the task of learning a\nlow-dimensional visual-tactile embedding, wherein we encode a depth patch from\nwhich we decode the tactile signal. To accomplish this task, we employ ReSkin,\nan inexpensive and replaceable magnetic-based tactile sensor. Using ReSkin, we\ncollect and train PseudoTouch on a dataset comprising aligned tactile and\nvisual data pairs obtained through random touching of eight basic geometric\nshapes. We demonstrate the efficacy of PseudoTouch through its application to\ntwo downstream tasks: object recognition and grasp stability prediction. In the\nobject recognition task, we evaluate the learned embedding's performance on a\nset of five basic geometric shapes and five household objects. Using\nPseudoTouch, we achieve an object recognition accuracy 84% after just ten\ntouches, surpassing a proprioception baseline. For the grasp stability task, we\nuse ACRONYM labels to train and evaluate a grasp success predictor using\nPseudoTouch's predictions derived from virtual depth information. Our approach\nyields an impressive 32% absolute improvement in accuracy compared to the\nbaseline relying on partial point cloud data. We make the data, code, and\ntrained models publicly available at http://pseudotouch.cs.uni-freiburg.de.\n","authors":["Adrian Röfer","Nick Heppert","Abdallah Ayman","Eugenio Chisari","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.15107v1.pdf","comment":"8 pages, 7 figures, 2 tables, submitted to IROS2024"},{"id":"http://arxiv.org/abs/2403.15103v1","updated":"2024-03-22T10:42:25Z","published":"2024-03-22T10:42:25Z","title":"Improving cross-domain brain tissue segmentation in fetal MRI with\n  synthetic data","summary":"  Segmentation of fetal brain tissue from magnetic resonance imaging (MRI)\nplays a crucial role in the study of in utero neurodevelopment. However,\nautomated tools face substantial domain shift challenges as they must be robust\nto highly heterogeneous clinical data, often limited in numbers and lacking\nannotations. Indeed, high variability of the fetal brain morphology, MRI\nacquisition parameters, and superresolution reconstruction (SR) algorithms\nadversely affect the model's performance when evaluated out-of-domain. In this\nwork, we introduce FetalSynthSeg, a domain randomization method to segment\nfetal brain MRI, inspired by SynthSeg. Our results show that models trained\nsolely on synthetic data outperform models trained on real data in out-ofdomain\nsettings, validated on a 120-subject cross-domain dataset. Furthermore, we\nextend our evaluation to 40 subjects acquired using lowfield (0.55T) MRI and\nreconstructed with novel SR models, showcasing robustness across different\nmagnetic field strengths and SR algorithms. Leveraging a generative synthetic\napproach, we tackle the domain shift problem in fetal brain MRI and offer\ncompelling prospects for applications in fields with limited and highly\nheterogeneous data.\n","authors":["Vladyslav Zalevskyi","Thomas Sanchez","Margaux Roulet","Jordina Aviles Verddera","Jana Hutter","Hamza Kebiri","Meritxell Bach Cuadra"],"pdf_url":"https://arxiv.org/pdf/2403.15103v1.pdf","comment":"10 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.15098v1","updated":"2024-03-22T10:36:50Z","published":"2024-03-22T10:36:50Z","title":"UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction","summary":"  Vehicle trajectory prediction has increasingly relied on data-driven\nsolutions, but their ability to scale to different data domains and the impact\nof larger dataset sizes on their generalization remain under-explored. While\nthese questions can be studied by employing multiple datasets, it is\nchallenging due to several discrepancies, \\textit{e.g.,} in data formats, map\nresolution, and semantic annotation types. To address these challenges, we\nintroduce UniTraj, a comprehensive framework that unifies various datasets,\nmodels, and evaluation criteria, presenting new opportunities for the vehicle\ntrajectory prediction field. In particular, using UniTraj, we conduct extensive\nexperiments and find that model performance significantly drops when\ntransferred to other datasets. However, enlarging data size and diversity can\nsubstantially improve performance, leading to a new state-of-the-art result for\nthe nuScenes dataset. We provide insights into dataset characteristics to\nexplain these findings. The code can be found here:\n\\hyperlink{https://github.com/vita-epfl/UniTraj}{https://github.com/vita-epfl/UniTraj}.\n","authors":["Lan Feng","Mohammadhossein Bahari","Kaouther Messaoud Ben Amor","Éloi Zablocki","Matthieu Cord","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2403.15098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00354v3","updated":"2024-03-22T10:36:47Z","published":"2023-09-30T12:17:36Z","title":"AI-Dentify: Deep learning for proximal caries detection on bitewing\n  x-ray -- HUNT4 Oral Health Study","summary":"  Background: Dental caries diagnosis requires the manual inspection of\ndiagnostic bitewing images of the patient, followed by a visual inspection and\nprobing of the identified dental pieces with potential lesions. Yet the use of\nartificial intelligence, and in particular deep-learning, has the potential to\naid in the diagnosis by providing a quick and informative analysis of the\nbitewing images.\n  Methods: A dataset of 13,887 bitewings from the HUNT4 Oral Health Study were\nannotated individually by six different experts, and used to train three\ndifferent object detection deep-learning architectures: RetinaNet (ResNet50),\nYOLOv5 (M size), and EfficientDet (D0 and D1 sizes). A consensus dataset of 197\nimages, annotated jointly by the same six dentist, was used for evaluation. A\nfive-fold cross validation scheme was used to evaluate the performance of the\nAI models.\n  Results: he trained models show an increase in average precision and\nF1-score, and decrease of false negative rate, with respect to the dental\nclinicians. When compared against the dental clinicians, the YOLOv5 model shows\nthe largest improvement, reporting 0.647 mean average precision, 0.548 mean\nF1-score, and 0.149 mean false negative rate. Whereas the best annotators on\neach of these metrics reported 0.299, 0.495, and 0.164 respectively.\n  Conclusion: Deep-learning models have shown the potential to assist dental\nprofessionals in the diagnosis of caries. Yet, the task remains challenging due\nto the artifacts natural to the bitewing images.\n","authors":["Javier Pérez de Frutos","Ragnhild Holden Helland","Shreya Desai","Line Cathrine Nymoen","Thomas Langø","Theodor Remman","Abhijit Sen"],"pdf_url":"https://arxiv.org/pdf/2310.00354v3.pdf","comment":"24 pages, 5 figure, 7 tables"},{"id":"http://arxiv.org/abs/2304.09793v2","updated":"2024-03-22T10:36:32Z","published":"2023-04-19T16:21:14Z","title":"Event-based Simultaneous Localization and Mapping: A Comprehensive\n  Survey","summary":"  In recent decades, visual simultaneous localization and mapping (vSLAM) has\ngained significant interest in both academia and industry. It estimates camera\nmotion and reconstructs the environment concurrently using visual sensors on a\nmoving robot. However, conventional cameras are limited by hardware, including\nmotion blur and low dynamic range, which can negatively impact performance in\nchallenging scenarios like high-speed motion and high dynamic range\nillumination. Recent studies have demonstrated that event cameras, a new type\nof bio-inspired visual sensor, offer advantages such as high temporal\nresolution, dynamic range, low power consumption, and low latency. This paper\npresents a timely and comprehensive review of event-based vSLAM algorithms that\nexploit the benefits of asynchronous and irregular event streams for\nlocalization and mapping tasks. The review covers the working principle of\nevent cameras and various event representations for preprocessing event data.\nIt also categorizes event-based vSLAM methods into four main categories:\nfeature-based, direct, motion-compensation, and deep learning methods, with\ndetailed discussions and practical guidance for each approach. Furthermore, the\npaper evaluates the state-of-the-art methods on various benchmarks,\nhighlighting current challenges and future opportunities in this emerging\nresearch area. A public repository will be maintained to keep track of the\nrapid developments in this field at\n{\\url{https://github.com/kun150kun/ESLAM-survey}}.\n","authors":["Kunping Huang","Sen Zhang","Jing Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2304.09793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14370v2","updated":"2024-03-22T10:26:33Z","published":"2024-03-21T12:57:30Z","title":"SyncTweedies: A General Generative Framework Based on Synchronized\n  Diffusions","summary":"  We introduce a general framework for generating diverse visual content,\nincluding ambiguous images, panorama images, mesh textures, and Gaussian splat\ntextures, by synchronizing multiple diffusion processes. We present exhaustive\ninvestigation into all possible scenarios for synchronizing multiple diffusion\nprocesses through a canonical space and analyze their characteristics across\napplications. In doing so, we reveal a previously unexplored case: averaging\nthe outputs of Tweedie's formula while conducting denoising in multiple\ninstance spaces. This case also provides the best quality with the widest\napplicability to downstream tasks. We name this case SyncTweedies. In our\nexperiments generating visual content aforementioned, we demonstrate the\nsuperior quality of generation by SyncTweedies compared to other\nsynchronization methods, optimization-based and iterative-update-based methods.\n","authors":["Jaihoon Kim","Juil Koo","Kyeongmin Yeo","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2403.14370v2.pdf","comment":"Project page: https://synctweedies.github.io/"},{"id":"http://arxiv.org/abs/2402.15756v2","updated":"2024-03-22T10:19:06Z","published":"2024-02-24T08:07:48Z","title":"Detection Is Tracking: Point Cloud Multi-Sweep Deep Learning Models\n  Revisited","summary":"  Conventional tracking paradigm takes in instantaneous measurements such as\nrange and bearing, and produces object tracks across time. In applications such\nas autonomous driving, lidar measurements in the form of point clouds are\nusually passed through a \"virtual sensor\" realized by a deep learning model, to\nproduce \"measurements\" such as bounding boxes, which are in turn ingested by a\ntracking module to produce object tracks. Very often multiple lidar sweeps are\naccumulated in a buffer to merge and become the input to the virtual sensor. We\nargue in this paper that such an input already contains temporal information,\nand therefore the virtual sensor output should also contain temporal\ninformation, not just instantaneous values for the time corresponding to the\nend of the buffer. In particular, we present the deep learning model called\nMULti-Sweep PAired Detector (MULSPAD) that produces, for each detected object,\na pair of bounding boxes at both the end time and the beginning time of the\ninput buffer. This is achieved with fairly straightforward changes in commonly\nused lidar detection models, and with only marginal extra processing, but the\nresulting symmetry is satisfying. Such paired detections make it possible not\nonly to construct rudimentary trackers fairly easily, but also to construct\nmore sophisticated trackers that can exploit the extra information conveyed by\nthe pair and be robust to choices of motion models and object birth/death\nmodels. We have conducted preliminary training and experimentation using Waymo\nOpen Dataset, which shows the efficacy of our proposed method.\n","authors":["Lingji Chen"],"pdf_url":"https://arxiv.org/pdf/2402.15756v2.pdf","comment":"My previous employer Motional is requiring a review and approval\n  process before I can publish this paper"},{"id":"http://arxiv.org/abs/2403.15089v1","updated":"2024-03-22T10:15:53Z","published":"2024-03-22T10:15:53Z","title":"IFSENet : Harnessing Sparse Iterations for Interactive Few-shot\n  Segmentation Excellence","summary":"  Training a computer vision system to segment a novel class typically requires\ncollecting and painstakingly annotating lots of images with objects from that\nclass. Few-shot segmentation techniques reduce the required number of images to\nlearn to segment a new class, but careful annotations of object boundaries are\nstill required. On the other hand, interactive segmentation techniques only\nfocus on incrementally improving the segmentation of one object at a time\n(typically, using clicks given by an expert) in a class-agnostic manner. We\ncombine the two concepts to drastically reduce the effort required to train\nsegmentation models for novel classes. Instead of trivially feeding interactive\nsegmentation masks as ground truth to a few-shot segmentation model, we propose\nIFSENet, which can accept sparse supervision on a single or few support images\nin the form of clicks to generate masks on support (training, at least clicked\nupon once) as well as query (test, never clicked upon) images. To trade-off\neffort for accuracy flexibly, the number of images and clicks can be\nincrementally added to the support set to further improve the segmentation of\nsupport as well as query images. The proposed model approaches the accuracy of\nprevious state-of-the-art few-shot segmentation models with considerably lower\nannotation effort (clicks instead of maps), when tested on Pascal and SBD\ndatasets on query images. It also works well as an interactive segmentation\nmethod on support images.\n","authors":["Shreyas Chandgothia","Ardhendu Sekhar","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2403.15089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15082v1","updated":"2024-03-22T10:06:31Z","published":"2024-03-22T10:06:31Z","title":"Cell Variational Information Bottleneck Network","summary":"  In this work, we propose Cell Variational Information Bottleneck Network\n(cellVIB), a convolutional neural network using information bottleneck\nmechanism, which can be combined with the latest feedforward network\narchitecture in an end-to-end training method. Our Cell Variational Information\nBottleneck Network is constructed by stacking VIB cells, which generate feature\nmaps with uncertainty. As layers going deeper, the regularization effect will\ngradually increase, instead of directly adding excessive regular constraints to\nthe output layer of the model as in Deep VIB. Under each VIB cell, the\nfeedforward process learns an independent mean term and an standard deviation\nterm, and predicts the Gaussian distribution based on them. The feedback\nprocess is based on reparameterization trick for effective training. This work\nperforms an extensive analysis on MNIST dataset to verify the effectiveness of\neach VIB cells, and provides an insightful analysis on how the VIB cells affect\nmutual information. Experiments conducted on CIFAR-10 also prove that our\ncellVIB is robust against noisy labels during training and against corrupted\nimages during testing. Then, we validate our method on PACS dataset, whose\nresults show that the VIB cells can significantly improve the generalization\nperformance of the basic model. Finally, in a more complex representation\nlearning task, face recognition, our network structure has also achieved very\ncompetitive results.\n","authors":["Zhonghua Zhai","Chen Ju","Jinsong Lan","Shuai Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.15082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15068v1","updated":"2024-03-22T09:48:50Z","published":"2024-03-22T09:48:50Z","title":"Integrating multiscale topology in digital pathology with pyramidal\n  graph convolutional networks","summary":"  Graph convolutional networks (GCNs) have emerged as a powerful alternative to\nmultiple instance learning with convolutional neural networks in digital\npathology, offering superior handling of structural information across various\nspatial ranges - a crucial aspect of learning from gigapixel H&E-stained whole\nslide images (WSI). However, graph message-passing algorithms often suffer from\noversmoothing when aggregating a large neighborhood. Hence, effective modeling\nof multi-range interactions relies on the careful construction of the graph.\nOur proposed multi-scale GCN (MS-GCN) tackles this issue by leveraging\ninformation across multiple magnification levels in WSIs. MS-GCN enables the\nsimultaneous modeling of long-range structural dependencies at lower\nmagnifications and high-resolution cellular details at higher magnifications,\nakin to analysis pipelines usually conducted by pathologists. The\narchitecture's unique configuration allows for the concurrent modeling of\nstructural patterns at lower magnifications and detailed cellular features at\nhigher ones, while also quantifying the contribution of each magnification\nlevel to the prediction. Through testing on different datasets, MS-GCN\ndemonstrates superior performance over existing single-magnification GCN\nmethods. The enhancement in performance and interpretability afforded by our\nmethod holds promise for advancing computational pathology models, especially\nin tasks requiring extensive spatial context.\n","authors":["Victor Ibañez","Przemyslaw Szostak","Quincy Wong","Konstanty Korski","Samaneh Abbasi-Sureshjani","Alvaro Gomariz"],"pdf_url":"https://arxiv.org/pdf/2403.15068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15064v1","updated":"2024-03-22T09:46:11Z","published":"2024-03-22T09:46:11Z","title":"Recent Trends in 3D Reconstruction of General Non-Rigid Scenes","summary":"  Reconstructing models of the real world, including 3D geometry, appearance,\nand motion of real scenes, is essential for computer graphics and computer\nvision. It enables the synthesizing of photorealistic novel views, useful for\nthe movie industry and AR/VR applications. It also facilitates the content\ncreation necessary in computer games and AR/VR by avoiding laborious manual\ndesign processes. Further, such models are fundamental for intelligent\ncomputing systems that need to interpret real-world scenes and actions to act\nand interact safely with the human world. Notably, the world surrounding us is\ndynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a\nseverely underconstrained and challenging problem. This state-of-the-art report\n(STAR) offers the reader a comprehensive summary of state-of-the-art techniques\nwith monocular and multi-view inputs such as data from RGB and RGB-D sensors,\namong others, conveying an understanding of different approaches, their\npotential applications, and promising further research directions. The report\ncovers 3D reconstruction of general non-rigid scenes and further addresses the\ntechniques for scene decomposition, editing and controlling, and generalizable\nand generative modeling. More specifically, we first review the common and\nfundamental concepts necessary to understand and navigate the field and then\ndiscuss the state-of-the-art techniques by reviewing recent approaches that use\ntraditional and machine-learning-based neural representations, including a\ndiscussion on the newly enabled applications. The STAR is concluded with a\ndiscussion of the remaining limitations and open challenges.\n","authors":["Raza Yunus","Jan Eric Lenssen","Michael Niemeyer","Yiyi Liao","Christian Rupprecht","Christian Theobalt","Gerard Pons-Moll","Jia-Bin Huang","Vladislav Golyanik","Eddy Ilg"],"pdf_url":"https://arxiv.org/pdf/2403.15064v1.pdf","comment":"42 pages, 18 figures, 5 tables; State-of-the-Art Report at\n  EUROGRAPHICS 2024"},{"id":"http://arxiv.org/abs/2403.15063v1","updated":"2024-03-22T09:40:52Z","published":"2024-03-22T09:40:52Z","title":"Towards a Comprehensive, Efficient and Promptable Anatomic Structure\n  Segmentation Model using 3D Whole-body CT Scans","summary":"  Segment anything model (SAM) demonstrates strong generalization ability on\nnatural image segmentation. However, its direct adaption in medical image\nsegmentation tasks shows significant performance drops with inferior accuracy\nand unstable results. It may also requires an excessive number of prompt points\nto obtain a reasonable accuracy. For segmenting 3D radiological CT or MRI\nscans, a 2D SAM model has to separately handle hundreds of 2D slices. Although\nquite a few studies explore adapting SAM into medical image volumes, the\nefficiency of 2D adaption methods is unsatisfactory and 3D adaptation methods\nonly capable of segmenting specific organs/tumors. In this work, we propose a\ncomprehensive and scalable 3D SAM model for whole-body CT segmentation, named\nCT-SAM3D. Instead of adapting SAM, we propose a 3D promptable segmentation\nmodel using a (nearly) fully labeled CT dataset. To train CT-SAM3D effectively,\nensuring the model's accurate responses to higher-dimensional spatial prompts\nis crucial, and 3D patch-wise training is required due to GPU memory\nconstraints. For this purpose, we propose two key technical developments: 1) a\nprogressively and spatially aligned prompt encoding method to effectively\nencode click prompts in local 3D space; and 2) a cross-patch prompt learning\nscheme to capture more 3D spatial context, which is beneficial for reducing the\nediting workloads when interactively prompting on large organs. CT-SAM3D is\ntrained and validated using a curated dataset of 1204 CT scans containing 107\nwhole-body anatomies, reporting significantly better quantitative performance\nagainst all previous SAM-derived models by a large margin with much fewer click\nprompts. Our model can handle segmenting unseen organ as well. Code, data, and\nour 3D interactive segmentation tool with quasi-real-time responses will be\nmade publicly available.\n","authors":["Heng Guo","Jianfeng Zhang","Jiaxing Huang","Tony C. W. Mok","Dazhou Guo","Ke Yan","Le Lu","Dakai Jin","Minfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.15063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15061v1","updated":"2024-03-22T09:38:16Z","published":"2024-03-22T09:38:16Z","title":"Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic\n  Range Videos","summary":"  High Dynamic Range (HDR) videos are able to represent wider ranges of\ncontrasts and colors than Standard Dynamic Range (SDR) videos, giving more\nvivid experiences. Due to this, HDR videos are expected to grow into the\ndominant video modality of the future. However, HDR videos are incompatible\nwith existing SDR displays, which form the majority of affordable consumer\ndisplays on the market. Because of this, HDR videos must be processed by\ntone-mapping them to reduced bit-depths to service a broad swath of SDR-limited\nvideo consumers. Here, we analyze the impact of tone-mapping operators on the\nvisual quality of streaming HDR videos. To this end, we built the first\nlarge-scale subjectively annotated open-source database of compressed\ntone-mapped HDR videos, containing 15,000 tone-mapped sequences derived from 40\nunique HDR source contents. The videos in the database were labeled with more\nthan 750,000 subjective quality annotations, collected from more than 1,600\nunique human observers. We demonstrate the usefulness of the new subjective\ndatabase by benchmarking objective models of visual quality on it. We envision\nthat the new LIVE Tone-Mapped HDR (LIVE-TMHDR) database will enable significant\nprogress on HDR video tone mapping and quality assessment in the future. To\nthis end, we make the database freely available to the community at\nhttps://live.ece.utexas.edu/research/LIVE_TMHDR/index.html\n","authors":["Abhinau K. Venkataramanan","Alan C. Bovik"],"pdf_url":"https://arxiv.org/pdf/2403.15061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12379v4","updated":"2024-03-22T09:36:53Z","published":"2023-12-19T18:11:19Z","title":"Mixture of Cluster-conditional LoRA Experts for Vision-language\n  Instruction Tuning","summary":"  Instruction tuning of Large Vision-language Models (LVLMs) has revolutionized\nthe development of versatile models with zero-shot generalization across a wide\nrange of downstream vision-language tasks. However, the diversity of training\ntasks of different sources and formats would lead to inevitable task conflicts,\nwhere different tasks conflict for the same set of model parameters, resulting\nin sub-optimal instructionfollowing abilities. To address that, we propose the\nMixture of Clusterconditional LoRA Experts (MoCLE), a novel Mixture of Experts\n(MoE) architecture designed to activate the task-customized model parameters\nbased on the instruction clusters. A separate universal expert is further\nincorporated to improve generalization capabilities of MoCLE for novel\ninstructions. Extensive experiments on 11 zero-shot tasks demonstrate the\neffectiveness of MoCLE.\n","authors":["Yunhao Gou","Zhili Liu","Kai Chen","Lanqing Hong","Hang Xu","Aoxue Li","Dit-Yan Yeung","James T. Kwok","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.12379v4.pdf","comment":"Project website: https://gyhdog99.github.io/projects/mocle/"},{"id":"http://arxiv.org/abs/2403.15059v1","updated":"2024-03-22T09:32:31Z","published":"2024-03-22T09:32:31Z","title":"MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition\n  Integration","summary":"  Recent advances in tuning-free personalized image generation based on\ndiffusion models are impressive. However, to improve subject fidelity, existing\nmethods either retrain the diffusion model or infuse it with dense visual\nembeddings, both of which suffer from poor generalization and efficiency. Also,\nthese methods falter in multi-subject image generation due to the unconstrained\ncross-attention mechanism. In this paper, we propose MM-Diff, a unified and\ntuning-free image personalization framework capable of generating high-fidelity\nimages of both single and multiple subjects in seconds. Specifically, to\nsimultaneously enhance text consistency and subject fidelity, MM-Diff employs a\nvision encoder to transform the input image into CLS and patch embeddings. CLS\nembeddings are used on the one hand to augment the text embeddings, and on the\nother hand together with patch embeddings to derive a small number of\ndetail-rich subject embeddings, both of which are efficiently integrated into\nthe diffusion model through the well-designed multimodal cross-attention\nmechanism. Additionally, MM-Diff introduces cross-attention map constraints\nduring the training phase, ensuring flexible multi-subject image sampling\nduring inference without any predefined inputs (e.g., layout). Extensive\nexperiments demonstrate the superior performance of MM-Diff over other leading\nmethods.\n","authors":["Zhichao Wei","Qingkun Su","Long Qin","Weizhi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02733v3","updated":"2024-03-22T09:17:24Z","published":"2024-02-05T05:25:33Z","title":"ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer","summary":"  Face re-aging is a prominent field in computer vision and graphics, with\nsignificant applications in photorealistic domains such as movies, advertising,\nand live streaming. Recently, the need to apply face re-aging to\nnon-photorealistic images, like comics, illustrations, and animations, has\nemerged as an extension in various entertainment sectors. However, the lack of\na network that can seamlessly edit the apparent age in NPR images has limited\nthese tasks to a naive, sequential approach. This often results in unpleasant\nartifacts and a loss of facial attributes due to domain discrepancies. In this\npaper, we introduce a novel one-stage method for face re-aging combined with\nportrait style transfer, executed in a single generative step. We leverage\nexisting face re-aging and style transfer networks, both trained within the\nsame PR domain. Our method uniquely fuses distinct latent vectors, each\nresponsible for managing aging-related attributes and NPR appearance. By\nadopting an exemplar-based approach, our method offers greater flexibility\ncompared to domain-level fine-tuning approaches, which typically require\nseparate training or fine-tuning for each domain. This effectively addresses\nthe limitation of requiring paired datasets for re-aging and domain-level,\ndata-driven approaches for stylization. Our experiments show that our model can\neffortlessly generate re-aged images while simultaneously transferring the\nstyle of examples, maintaining both natural appearance and controllability.\n","authors":["Bumsoo Kim","Abdul Muqeet","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2402.02733v3.pdf","comment":"14 pages, 15 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.15049v1","updated":"2024-03-22T09:15:36Z","published":"2024-03-22T09:15:36Z","title":"Continual Vision-and-Language Navigation","summary":"  Vision-and-Language Navigation (VLN) agents navigate to a destination using\nnatural language instructions and the visual information they observe. Existing\nmethods for training VLN agents presuppose fixed datasets, leading to a\nsignificant limitation: the introduction of new environments necessitates\nretraining with previously encountered environments to preserve their\nknowledge. This makes it difficult to train VLN agents that operate in the\never-changing real world. To address this limitation, we present the Continual\nVision-and-Language Navigation (CVLN) paradigm, designed to evaluate agents\ntrained through a continual learning process. For the training and evaluation\nof CVLN agents, we re-arrange existing VLN datasets to propose two datasets:\nCVLN-I, focused on navigation via initial-instruction interpretation, and\nCVLN-D, aimed at navigation through dialogue with other agents. Furthermore, we\npropose two novel rehearsal-based methods for CVLN, Perplexity Replay (PerpR)\nand Episodic Self-Replay (ESR). PerpR prioritizes replaying challenging\nepisodes based on action perplexity, while ESR replays previously predicted\naction logits to preserve learned behaviors. We demonstrate the effectiveness\nof the proposed methods on CVLN through extensive experiments.\n","authors":["Seongjun Jeong","Gi-Cheon Kang","Seongho Choi","Joochan Kim","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.15049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15048v1","updated":"2024-03-22T09:13:09Z","published":"2024-03-22T09:13:09Z","title":"Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning","summary":"  Large-scale Text-to-Image (TTI) models have become a common approach for\ngenerating training data in various generative fields. However, visual\nhallucinations, which contain perceptually critical defects, remain a concern,\nespecially in non-photorealistic styles like cartoon characters. We propose a\nnovel visual hallucination detection system for cartoon character images\ngenerated by TTI models. Our approach leverages pose-aware in-context visual\nlearning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB\nimages and pose information. By incorporating pose guidance from a fine-tuned\npose estimator, we enable VLMs to make more accurate decisions. Experimental\nresults demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images. This\nresearch advances TTI models by mitigating visual hallucinations, expanding\ntheir potential in non-photorealistic domains.\n","authors":["Bumsoo Kim","Wonseop Shin","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2403.15048v1.pdf","comment":"11 pages, 12 figures, 1 table, Project page:\n  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/"},{"id":"http://arxiv.org/abs/2403.09572v2","updated":"2024-03-22T09:07:06Z","published":"2024-03-14T17:03:04Z","title":"Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text\n  Transformation","summary":"  Multimodal large language models (MLLMs) have shown impressive reasoning\nabilities, which, however, are also more vulnerable to jailbreak attacks than\ntheir LLM predecessors. Although still capable of detecting unsafe responses,\nwe observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be\neasily bypassed due to the introduction of image features. To construct robust\nMLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-free\nprotecting approach that exploits the inherent safety awareness of MLLMs, and\ngenerates safer responses via adaptively transforming unsafe images into texts\nto activate intrinsic safety mechanism of pre-aligned LLMs in MLLMs.\nExperiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSO\nenhances model safety significantly (e.g., a 37.6% improvement on the\nMM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), while\nconsistently maintaining utility results on common MLLM benchmarks.\nFurthermore, we show that ECSO can be used as a data engine to generate\nsupervised-finetuning (SFT) data for MLLM alignment without extra human\nintervention.\n","authors":["Yunhao Gou","Kai Chen","Zhili Liu","Lanqing Hong","Hang Xu","Zhenguo Li","Dit-Yan Yeung","James T. Kwok","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.09572v2.pdf","comment":"Project Page: https://gyhdog99.github.io/projects/ecso/"},{"id":"http://arxiv.org/abs/2403.15044v1","updated":"2024-03-22T09:00:24Z","published":"2024-03-22T09:00:24Z","title":"Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour\n  Analysis In-the-wild","summary":"  Multimodal fusion is a significant method for most multimodal tasks. With the\nrecent surge in the number of large pre-trained models, combining both\nmultimodal fusion methods and pre-trained model features can achieve\noutstanding performance in many multimodal tasks. In this paper, we present our\napproach, which leverages both advantages for addressing the task of Expression\n(Expr) Recognition and Valence-Arousal (VA) Estimation. We evaluate the\nAff-Wild2 database using pre-trained models, then extract the final hidden\nlayers of the models as features. Following preprocessing and interpolation or\nconvolution to align the extracted features, different models are employed for\nmodal fusion. Our code is available at GitHub - FulgenceWen/ABAW6th.\n","authors":["Zhuofan Wen","Fengyu Zhang","Siyuan Zhang","Haiyang Sun","Mingyu Xu","Licai Sun","Zheng Lian","Bin Liu","Jianhua Tao"],"pdf_url":"https://arxiv.org/pdf/2403.15044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.08856v3","updated":"2024-03-22T08:51:55Z","published":"2023-08-17T08:29:54Z","title":"MV-ROPE: Multi-view Constraints for Robust Category-level Object Pose\n  and Size Estimation","summary":"  Recently there has been a growing interest in category-level object pose and\nsize estimation, and prevailing methods commonly rely on single view RGB-D\nimages. However, one disadvantage of such methods is that they require accurate\ndepth maps which cannot be produced by consumer-grade sensors. Furthermore,\nmany practical real-world situations involve a moving camera that continuously\nobserves its surroundings, and the temporal information of the input video\nstreams is simply overlooked by single-view methods. We propose a novel\nsolution that makes use of RGB video streams. Our framework consists of three\nmodules: a scale-aware monocular dense SLAM solution, a lightweight object pose\npredictor, and an object-level pose graph optimizer. The SLAM module utilizes a\nvideo stream and additional scale-sensitive readings to estimate camera poses\nand metric depth. The object pose predictor then generates canonical object\nrepresentations from RGB images. The object pose is estimated through geometric\nregistration of these canonical object representations with estimated object\ndepth points. All per-view estimates finally undergo optimization within a pose\ngraph, culminating in the output of robust and accurate canonical object poses.\nOur experimental results demonstrate that when utilizing public dataset\nsequences with high-quality depth information, the proposed method exhibits\ncomparable performance to state-of-the-art RGB-D methods. We also collect and\nevaluate on new datasets containing depth maps of varying quality to further\nquantitatively benchmark the proposed method alongside previous RGB-D based\nmethods. We demonstrate a significant advantage in scenarios where depth input\nis absent or the quality of depth sensing is limited.\n","authors":["Jiaqi Yang","Yucong Chen","Xiangting Meng","Chenxin Yan","Min Li","Ran Cheng","Lige Liu","Tao Sun","Laurent Kneip"],"pdf_url":"https://arxiv.org/pdf/2308.08856v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14189v3","updated":"2024-03-22T08:45:52Z","published":"2023-11-23T20:14:50Z","title":"D-SCo: Dual-Stream Conditional Diffusion for Monocular Hand-Held Object\n  Reconstruction","summary":"  Reconstructing hand-held objects from a single RGB image is a challenging\ntask in computer vision. In contrast to prior works that utilize deterministic\nmodeling paradigms, we employ a point cloud denoising diffusion model to\naccount for the probabilistic nature of this problem. In the core, we introduce\ncentroid-fixed dual-stream conditional diffusion for monocular hand-held object\nreconstruction (D-SCo), tackling two predominant challenges. First, to avoid\nthe object centroid from deviating, we utilize a novel hand-constrained\ncentroid fixing paradigm, enhancing the stability of diffusion and reverse\nprocesses and the precision of feature projection. Second, we introduce a\ndual-stream denoiser to semantically and geometrically model hand-object\ninteractions with a novel unified hand-object semantic embedding, enhancing the\nreconstruction performance of the hand-occluded region of the object.\nExperiments on the synthetic ObMan dataset and three real-world datasets HO3D,\nMOW and DexYCB demonstrate that our approach can surpass all other\nstate-of-the-art methods. Codes will be released.\n","authors":["Bowen Fu","Gu Wang","Chenyangguang Zhang","Yan Di","Ziqin Huang","Zhiying Leng","Fabian Manhardt","Xiangyang Ji","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2311.14189v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15033v1","updated":"2024-03-22T08:32:30Z","published":"2024-03-22T08:32:30Z","title":"Toward Tiny and High-quality Facial Makeup with Data Amplify Learning","summary":"  Contemporary makeup approaches primarily hinge on unpaired learning\nparadigms, yet they grapple with the challenges of inaccurate supervision\n(e.g., face misalignment) and sophisticated facial prompts (including face\nparsing, and landmark detection). These challenges prohibit low-cost deployment\nof facial makeup models, especially on mobile devices. To solve above problems,\nwe propose a brand-new learning paradigm, termed \"Data Amplify Learning (DAL),\"\nalongside a compact makeup model named \"TinyBeauty.\" The core idea of DAL lies\nin employing a Diffusion-based Data Amplifier (DDA) to \"amplify\" limited images\nfor the model training, thereby enabling accurate pixel-to-pixel supervision\nwith merely a handful of annotations. Two pivotal innovations in DDA facilitate\nthe above training approach: (1) A Residual Diffusion Model (RDM) is designed\nto generate high-fidelity detail and circumvent the detail vanishing problem in\nthe vanilla diffusion models; (2) A Fine-Grained Makeup Module (FGMM) is\nproposed to achieve precise makeup control and combination while retaining face\nidentity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to\nachieve a state-of-the-art performance without intricate face prompts.\nMeanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on\nthe iPhone 13. Extensive experiments show that DAL can produce highly\ncompetitive makeup models using only 5 image pairs.\n","authors":["Qiaoqiao Jin","Xuanhong Chen","Meiguang Jin","Ying Cheng","Rui Shi","Yucheng Zheng","Yupeng Zhu","Bingbing Ni"],"pdf_url":"https://arxiv.org/pdf/2403.15033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15032v1","updated":"2024-03-22T08:27:25Z","published":"2024-03-22T08:27:25Z","title":"An Integrated Neighborhood and Scale Information Network for Open-Pit\n  Mine Change Detection in High-Resolution Remote Sensing Images","summary":"  Open-pit mine change detection (CD) in high-resolution (HR) remote sensing\nimages plays a crucial role in mineral development and environmental\nprotection. Significant progress has been made in this field in recent years,\nlargely due to the advancement of deep learning techniques. However, existing\ndeep-learning-based CD methods encounter challenges in effectively integrating\nneighborhood and scale information, resulting in suboptimal performance.\nTherefore, by exploring the influence patterns of neighborhood and scale\ninformation, this paper proposes an Integrated Neighborhood and Scale\nInformation Network (INSINet) for open-pit mine CD in HR remote sensing images.\nSpecifically, INSINet introduces 8-neighborhood-image information to acquire a\nlarger receptive field, improving the recognition of center image boundary\nregions. Drawing on techniques of skip connection, deep supervision, and\nattention mechanism, the multi-path deep supervised attention (MDSA) module is\ndesigned to enhance multi-scale information fusion and change feature\nextraction. Experimental analysis reveals that incorporating neighborhood and\nscale information enhances the F1 score of INSINet by 6.40%, with improvements\nof 3.08% and 3.32% respectively. INSINet outperforms existing methods with an\nOverall Accuracy of 97.69%, Intersection over Union of 71.26%, and F1 score of\n83.22%. INSINet shows significance for open-pit mine CD in HR remote sensing\nimages.\n","authors":["Zilin Xie","Kangning Li","Jinbao Jiang","Jinzhong Yang","Xiaojun Qiao","Deshuai Yuan","Cheng Nie"],"pdf_url":"https://arxiv.org/pdf/2403.15032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15031v1","updated":"2024-03-22T08:26:31Z","published":"2024-03-22T08:26:31Z","title":"Image Classification with Rotation-Invariant Variational Quantum\n  Circuits","summary":"  Variational quantum algorithms are gaining attention as an early application\nof Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of\nvariational methods lies in the phenomenon of Barren Plateaus, present in the\noptimization of variational parameters. Adding geometric inductive bias to the\nquantum models has been proposed as a potential solution to mitigate this\nproblem, leading to a new field called Geometric Quantum Machine Learning. In\nthis work, an equivariant architecture for variational quantum classifiers is\nintroduced to create a label-invariant model for image classification with\n$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against\ntwo different architectures, and it is experimentally observed that the\ngeometric approach boosts the model's performance. Finally, a classical\nequivariant convolution operation is proposed to extend the quantum model for\nthe processing of larger images, employing the resources available in NISQ\ndevices.\n","authors":["Paul San Sebastian","Mikel Cañizo","Román Orús"],"pdf_url":"https://arxiv.org/pdf/2403.15031v1.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.15026v1","updated":"2024-03-22T08:16:59Z","published":"2024-03-22T08:16:59Z","title":"VRSO: Visual-Centric Reconstruction for Static Object Annotation","summary":"  As a part of the perception results of intelligent driving systems, static\nobject detection (SOD) in 3D space provides crucial cues for driving\nenvironment understanding. With the rapid deployment of deep neural networks\nfor SOD tasks, the demand for high-quality training samples soars. The\ntraditional, also reliable, way is manual labeling over the dense LiDAR point\nclouds and reference images. Though most public driving datasets adopt this\nstrategy to provide SOD ground truth (GT), it is still expensive (requires\nLiDAR scanners) and low-efficient (time-consuming and unscalable) in practice.\nThis paper introduces VRSO, a visual-centric approach for static object\nannotation. VRSO is distinguished in low cost, high efficiency, and high\nquality: (1) It recovers static objects in 3D space with only camera images as\ninput, and (2) manual labeling is barely involved since GT for SOD tasks is\ngenerated based on an automatic reconstruction and annotation pipeline. (3)\nExperiments on the Waymo Open Dataset show that the mean reprojection error\nfrom VRSO annotation is only 2.6 pixels, around four times lower than the Waymo\nlabeling (10.6 pixels). Source code is available at:\nhttps://github.com/CaiYingFeng/VRSO.\n","authors":["Chenyao Yu","Yingfeng Cai","Jiaxin Zhang","Hui Kong","Wei Sui","Cong Yang"],"pdf_url":"https://arxiv.org/pdf/2403.15026v1.pdf","comment":"submitted to iros 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.15279v1","updated":"2024-03-22T15:22:06Z","published":"2024-03-22T15:22:06Z","title":"Fundus: A Simple-to-Use News Scraper Optimized for High Quality\n  Extractions","summary":"  This paper introduces Fundus, a user-friendly news scraper that enables users\nto obtain millions of high-quality news articles with just a few lines of code.\nUnlike existing news scrapers, we use manually crafted, bespoke content\nextractors that are specifically tailored to the formatting guidelines of each\nsupported online newspaper. This allows us to optimize our scraping for quality\nsuch that retrieved news articles are textually complete and without HTML\nartifacts. Further, our framework combines both crawling (retrieving HTML from\nthe web or large web archives) and content extraction into a single pipeline.\nBy providing a unified interface for a predefined collection of newspapers, we\naim to make Fundus broadly usable even for non-technical users. This paper\ngives an overview of the framework, discusses our design choices, and presents\na comparative evaluation against other popular news scrapers. Our evaluation\nshows that Fundus yields significantly higher quality extractions (complete and\nartifact-free news articles) than prior work. The framework is available on\nGitHub under https://github.com/flairNLP/fundus and can be simply installed\nusing pip.\n","authors":["Max Dallabetta","Conrad Dobberstein","Adrian Breiding","Alan Akbik"],"pdf_url":"https://arxiv.org/pdf/2403.15279v1.pdf","comment":"10 pages, 4 figures, submitted to ACL 2024, for a screencast see\n  https://www.youtube.com/watch?v=9GJExMelhdI"},{"id":"http://arxiv.org/abs/2403.15246v1","updated":"2024-03-22T14:42:29Z","published":"2024-03-22T14:42:29Z","title":"FollowIR: Evaluating and Teaching Information Retrieval Models to Follow\n  Instructions","summary":"  Modern Large Language Models (LLMs) are capable of following long and complex\ninstructions that enable a diverse amount of user tasks. However, despite\nInformation Retrieval (IR) models using LLMs as the backbone of their\narchitectures, nearly all of them still only take queries as input, with no\ninstructions. For the handful of recent models that do take instructions, it's\nunclear how they use them. We introduce our dataset FollowIR, which contains a\nrigorous instruction evaluation benchmark as well as a training set for helping\nIR models learn to better follow real-world instructions. FollowIR builds off\nthe long history of the TREC conferences: as TREC provides human annotators\nwith instructions (also known as narratives) to determine document relevance,\nso should IR models be able to understand and decide relevance based on these\ndetailed instructions. Our evaluation benchmark starts with three deeply judged\nTREC collections and alters the annotator instructions, re-annotating relevant\ndocuments. Through this process, we can measure how well IR models follow\ninstructions, through a new pairwise evaluation framework. Our results indicate\nthat existing retrieval models fail to correctly use instructions, using them\nfor basic keywords and struggling to understand long-form information. However,\nwe show that it is possible for IR models to learn to follow complex\ninstructions: our new FollowIR-7B model has significant improvements (over 13%)\nafter fine-tuning on our training set.\n","authors":["Orion Weller","Benjamin Chang","Sean MacAvaney","Kyle Lo","Arman Cohan","Benjamin Van Durme","Dawn Lawrie","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2403.15246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15075v1","updated":"2024-03-22T09:58:33Z","published":"2024-03-22T09:58:33Z","title":"Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation","summary":"  Recent methods utilize graph contrastive Learning within graph-structured\nuser-item interaction data for collaborative filtering and have demonstrated\ntheir efficacy in recommendation tasks. However, they ignore that the\ndifference relation density of nodes between the user- and item-side causes the\nadaptability of graphs on bilateral nodes to be different after multi-hop graph\ninteraction calculation, which limits existing models to achieve ideal results.\nTo solve this issue, we propose a novel framework for recommendation tasks\ncalled Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that\nconsider the bilateral unsymmetry on user-item node relation density for sliced\nuser and item graph reasoning better with bilateral slicing contrastive\ntraining. Especially, taking into account the aggregation ability of\nhypergraph-based graph convolutional network (GCN) in digging implicit\nsimilarities is more suitable for user nodes, embeddings generated from three\ndifferent modules: hypergraph-based GCN, GCN and perturbed GCN, are sliced into\ntwo subviews by the user- and item-side respectively, and selectively combined\ninto subview pairs bilaterally based on the characteristics of inter-node\nrelation structure. Furthermore, to align the distribution of user and item\nembeddings after aggregation, a dispersing loss is leveraged to adjust the\nmutual distance between all embeddings for maintaining learning ability.\nComprehensive experiments on two public datasets have proved the superiority of\nBusGCL in comparison to various recommendation methods. Other models can simply\nutilize our bilateral slicing contrastive learning to enhance recommending\nperformance without incurring extra expenses.\n","authors":["Jiaheng Yu","Jing Li","Yue He","Kai Zhu","Shuyi Zhang","Wen Hu"],"pdf_url":"https://arxiv.org/pdf/2403.15075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10435v3","updated":"2024-03-22T05:57:48Z","published":"2023-09-19T08:54:47Z","title":"Language Modeling for Content-enriched Recommendation","summary":"  Recommender systems are indispensable in the realm of online applications,\nand sequential recommendation has enjoyed considerable prevalence due to its\ncapacity to encapsulate the dynamic shifts in user interests. However, previous\nsequential modeling methods still have limitations in capturing contextual\ninformation. The primary reason is the lack of understanding of domain-specific\nknowledge and item-related textual content by language models. Fortunately, the\nemergence of powerful language models has unlocked the potential to incorporate\nextensive world knowledge into recommendation algorithms, enabling them to go\nbeyond simple item attributes and truly understand the world surrounding user\npreferences. To achieve this, we propose LANCER, which leverages the semantic\nunderstanding capabilities of pre-trained language models to generate\npersonalized recommendations. Our approach bridges the gap between language\nmodels and recommender systems, resulting in more human-like recommendations.\nWe demonstrate the effectiveness of our approach through a series of\nexperiments conducted on multiple benchmark datasets, showing promising results\nand providing valuable insights into the influence of our model on sequential\nrecommendation tasks. Furthermore, our experimental codes are publicly\navailable.\n","authors":["Junzhe Jiang","Shang Qu","Mingyue Cheng","Qi Liu","Zhiding Liu","Hao Zhang","Rujiao Zhang","Kai Zhang","Rui Li","Jiatong Li","Min Gao"],"pdf_url":"https://arxiv.org/pdf/2309.10435v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08699v3","updated":"2024-03-22T03:31:22Z","published":"2024-01-14T12:38:49Z","title":"On Image Search in Histopathology","summary":"  Pathology images of histopathology can be acquired from camera-mounted\nmicroscopes or whole slide scanners. Utilizing similarity calculations to match\npatients based on these images holds significant potential in research and\nclinical contexts. Recent advancements in search technologies allow for\nimplicit quantification of tissue morphology across diverse primary sites,\nfacilitating comparisons and enabling inferences about diagnosis, and\npotentially prognosis, and predictions for new patients when compared against a\ncurated database of diagnosed and treated cases. In this paper, we\ncomprehensively review the latest developments in image search technologies for\nhistopathology, offering a concise overview tailored for computational\npathology researchers seeking effective, fast and efficient image search\nmethods in their work.\n","authors":["H. R. Tizhoosh","Liron Pantanowitz"],"pdf_url":"https://arxiv.org/pdf/2401.08699v3.pdf","comment":"A chapter in the Book \"Artificial INtelligence in Digital Pathology\"\n  by Cohen and Chauhan, 2024"},{"id":"http://arxiv.org/abs/2403.09738v3","updated":"2024-03-22T01:08:42Z","published":"2024-03-13T18:16:21Z","title":"Evaluating Large Language Models as Generative User Simulators for\n  Conversational Recommendation","summary":"  Synthetic users are cost-effective proxies for real users in the evaluation\nof conversational recommender systems. Large language models show promise in\nsimulating human-like behavior, raising the question of their ability to\nrepresent a diverse population of users. We introduce a new protocol to measure\nthe degree to which language models can accurately emulate human behavior in\nconversational recommendation. This protocol is comprised of five tasks, each\ndesigned to evaluate a key property that a synthetic user should exhibit:\nchoosing which items to talk about, expressing binary preferences, expressing\nopen-ended preferences, requesting recommendations, and giving feedback.\nThrough evaluation of baseline simulators, we demonstrate these tasks\neffectively reveal deviations of language models from human behavior, and offer\ninsights on how to reduce the deviations with model selection and prompting\nstrategies.\n","authors":["Se-eun Yoon","Zhankui He","Jessica Maria Echterhoff","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2403.09738v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.15548v1","updated":"2024-03-22T18:03:27Z","published":"2024-03-22T18:03:27Z","title":"Spectral Initialization for High-Dimensional Phase Retrieval with Biased\n  Spatial Directions","summary":"  We explore a spectral initialization method that plays a central role in\ncontemporary research on signal estimation in nonconvex scenarios. In a\nnoiseless phase retrieval framework, we precisely analyze the method's\nperformance in the high-dimensional limit when sensing vectors follow a\nmultivariate Gaussian distribution for two rotationally invariant models of the\ncovariance matrix C. In the first model C is a projector on a lower dimensional\nspace while in the second it is a Wishart matrix. Our analytical results extend\nthe well-established case when C is the identity matrix. Our examination shows\nthat the introduction of biased spatial directions leads to a substantial\nimprovement in the spectral method's effectiveness, particularly when the\nnumber of measurements is less than the signal's dimension. This extension also\nconsistently reveals a phase transition phenomenon dependent on the ratio\nbetween sample size and signal dimension. Surprisingly, both of these models\nshare the same threshold value.\n","authors":["Pierre Bousseyroux","Marc Potters"],"pdf_url":"https://arxiv.org/pdf/2403.15548v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.15520v1","updated":"2024-03-22T12:22:44Z","published":"2024-03-22T12:22:44Z","title":"GTC: GNN-Transformer Co-contrastive Learning for Self-supervised\n  Heterogeneous Graph Representation","summary":"  Graph Neural Networks (GNNs) have emerged as the most powerful weapon for\nvarious graph tasks due to the message-passing mechanism's great local\ninformation aggregation ability. However, over-smoothing has always hindered\nGNNs from going deeper and capturing multi-hop neighbors. Unlike GNNs,\nTransformers can model global information and multi-hop interactions via\nmulti-head self-attention and a proper Transformer structure can show more\nimmunity to the over-smoothing problem. So, can we propose a novel framework to\ncombine GNN and Transformer, integrating both GNN's local information\naggregation and Transformer's global information modeling ability to eliminate\nthe over-smoothing problem? To realize this, this paper proposes a\ncollaborative learning scheme for GNN-Transformer and constructs GTC\narchitecture. GTC leverages the GNN and Transformer branch to encode node\ninformation from different views respectively, and establishes contrastive\nlearning tasks based on the encoded cross-view information to realize\nself-supervised heterogeneous graph representation. For the Transformer branch,\nwe propose Metapath-aware Hop2Token and CG-Hetphormer, which can cooperate with\nGNN to attentively encode neighborhood information from different levels. As\nfar as we know, this is the first attempt in the field of graph representation\nlearning to utilize both GNN and Transformer to collaboratively capture\ndifferent view information and conduct cross-view contrastive learning. The\nexperiments on real datasets show that GTC exhibits superior performance\ncompared with state-of-the-art methods. Codes can be available at\nhttps://github.com/PHD-lanyu/GTC.\n","authors":["Yundong Sun","Dongjie Zhu","Yansong Wang","Zhaoshuo Tian"],"pdf_url":"https://arxiv.org/pdf/2403.15520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15529v1","updated":"2024-03-22T17:31:43Z","published":"2024-03-22T17:31:43Z","title":"LimGen: Probing the LLMs for Generating Suggestive Limitations of\n  Research Papers","summary":"  Examining limitations is a crucial step in the scholarly research reviewing\nprocess, revealing aspects where a study might lack decisiveness or require\nenhancement. This aids readers in considering broader implications for further\nresearch. In this article, we present a novel and challenging task of\nSuggestive Limitation Generation (SLG) for research papers. We compile a\ndataset called LimGen, encompassing 4068 research papers and their associated\nlimitations from the ACL anthology. We investigate several approaches to\nharness large language models (LLMs) for producing suggestive limitations, by\nthoroughly examining the related challenges, practical insights, and potential\nopportunities. Our LimGen dataset and code can be accessed at\nhttps://github.com/armbf/LimGen.\n","authors":["Abdur Rahman Bin Md Faizullah","Ashok Urlana","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2403.15529v1.pdf","comment":"16 pages, 3 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.15389v1","updated":"2024-03-22T17:59:58Z","published":"2024-03-22T17:59:58Z","title":"DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from\n  Partially Annotated Data","summary":"  Recently, there has been an increased interest in the practical problem of\nlearning multiple dense scene understanding tasks from partially annotated\ndata, where each training sample is only labeled for a subset of the tasks. The\nmissing of task labels in training leads to low-quality and noisy predictions,\nas can be observed from state-of-the-art methods. To tackle this issue, we\nreformulate the partially-labeled multi-task dense prediction as a pixel-level\ndenoising problem, and propose a novel multi-task denoising diffusion framework\ncoined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to\nmodel a potential noisy distribution in the task prediction or feature maps and\ngenerate rectified outputs for different tasks. To exploit multi-task\nconsistency in denoising, we further introduce a Multi-Task Conditioning\nstrategy, which can implicitly utilize the complementary nature of the tasks to\nhelp learn the unlabeled tasks, leading to an improvement in the denoising\nperformance of the different tasks. Extensive quantitative and qualitative\nexperiments demonstrate that the proposed multi-task denoising diffusion model\ncan significantly improve multi-task prediction maps, and outperform the\nstate-of-the-art methods on three challenging multi-task benchmarks, under two\ndifferent partial-labeling evaluation settings. The code is available at\nhttps://prismformore.github.io/diffusionmtl/.\n","authors":["Hanrong Ye","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.15389v1.pdf","comment":"The paper is accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.15385v1","updated":"2024-03-22T17:59:37Z","published":"2024-03-22T17:59:37Z","title":"LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis","summary":"  Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.\n","authors":["Kevin Xie","Jonathan Lorraine","Tianshi Cao","Jun Gao","James Lucas","Antonio Torralba","Sanja Fidler","Xiaohui Zeng"],"pdf_url":"https://arxiv.org/pdf/2403.15385v1.pdf","comment":"See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LATTE3D/"},{"id":"http://arxiv.org/abs/2403.08763v2","updated":"2024-03-22T17:56:38Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00652v2","updated":"2024-03-22T17:56:05Z","published":"2023-03-01T16:54:48Z","title":"Finding the right XAI method -- A Guide for the Evaluation and Ranking\n  of Explainable AI Methods in Climate Science","summary":"  Explainable artificial intelligence (XAI) methods shed light on the\npredictions of machine learning algorithms. Several different approaches exist\nand have already been applied in climate science. However, usually missing\nground truth explanations complicate their evaluation and comparison,\nsubsequently impeding the choice of the XAI method. Therefore, in this work, we\nintroduce XAI evaluation in the climate context and discuss different desired\nexplanation properties, namely robustness, faithfulness, randomization,\ncomplexity, and localization. To this end, we chose previous work as a case\nstudy where the decade of annual-mean temperature maps is predicted. After\ntraining both a multi-layer perceptron (MLP) and a convolutional neural network\n(CNN), multiple XAI methods are applied and their skill scores in reference to\na random uniform explanation are calculated for each property. Independent of\nthe network, we find that XAI methods Integrated Gradients, layer-wise\nrelevance propagation, and input times gradients exhibit considerable\nrobustness, faithfulness, and complexity while sacrificing randomization\nperformance. Sensitivity methods -- gradient, SmoothGrad, NoiseGrad, and\nFusionGrad, match the robustness skill but sacrifice faithfulness and\ncomplexity for randomization skill. We find architecture-dependent performance\ndifferences regarding robustness, complexity and localization skills of\ndifferent XAI methods, highlighting the necessity for research task-specific\nevaluation. Overall, our work offers an overview of different evaluation\nproperties in the climate science context and shows how to compare and\nbenchmark different explanation methods, assessing their suitability based on\nstrengths and weaknesses, for the specific research problem at hand. By that,\nwe aim to support climate researchers in the selection of a suitable XAI\nmethod.\n","authors":["Philine Bommer","Marlene Kretschmer","Anna Hedström","Dilyara Bareeva","Marina M. -C. Höhne"],"pdf_url":"https://arxiv.org/pdf/2303.00652v2.pdf","comment":"19 pages, 10 figure, accepted at AIES journal by AMS"},{"id":"http://arxiv.org/abs/2403.15371v1","updated":"2024-03-22T17:50:43Z","published":"2024-03-22T17:50:43Z","title":"Can large language models explore in-context?","summary":"  We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.\n","authors":["Akshay Krishnamurthy","Keegan Harris","Dylan J. Foster","Cyril Zhang","Aleksandrs Slivkins"],"pdf_url":"https://arxiv.org/pdf/2403.15371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15370v1","updated":"2024-03-22T17:49:11Z","published":"2024-03-22T17:49:11Z","title":"Augmented Reality based Simulated Data (ARSim) with multi-view\n  consistency for AV perception networks","summary":"  Detecting a diverse range of objects under various driving scenarios is\nessential for the effectiveness of autonomous driving systems. However, the\nreal-world data collected often lacks the necessary diversity presenting a\nlong-tail distribution. Although synthetic data has been utilized to overcome\nthis issue by generating virtual scenes, it faces hurdles such as a significant\ndomain gap and the substantial efforts required from 3D artists to create\nrealistic environments. To overcome these challenges, we present ARSim, a fully\nautomated, comprehensive, modular framework designed to enhance real multi-view\nimage data with 3D synthetic objects of interest. The proposed method\nintegrates domain adaptation and randomization strategies to address covariate\nshift between real and simulated data by inferring essential domain attributes\nfrom real data and employing simulation-based randomization for other\nattributes. We construct a simplified virtual scene using real data and\nstrategically place 3D synthetic assets within it. Illumination is achieved by\nestimating light distribution from multiple images capturing the surroundings\nof the vehicle. Camera parameters from real data are employed to render\nsynthetic assets in each frame. The resulting augmented multi-view consistent\ndataset is used to train a multi-camera perception network for autonomous\nvehicles. Experimental results on various AV perception tasks demonstrate the\nsuperior performance of networks trained on the augmented dataset.\n","authors":["Aqeel Anwar","Tae Eun Choe","Zian Wang","Sanja Fidler","Minwoo Park"],"pdf_url":"https://arxiv.org/pdf/2403.15370v1.pdf","comment":"17 pages, 15 figures, 7 tables"},{"id":"http://arxiv.org/abs/2403.14617v2","updated":"2024-03-22T17:45:52Z","published":"2024-03-21T17:59:03Z","title":"Videoshop: Localized Semantic Video Editing with Noise-Extrapolated\n  Diffusion Inversion","summary":"  We introduce Videoshop, a training-free video editing algorithm for localized\nsemantic edits. Videoshop allows users to use any editing software, including\nPhotoshop and generative inpainting, to modify the first frame; it\nautomatically propagates those changes, with semantic, spatial, and temporally\nconsistent motion, to the remaining frames. Unlike existing methods that enable\nedits only through imprecise textual instructions, Videoshop allows users to\nadd or remove objects, semantically change objects, insert stock photos into\nvideos, etc. with fine-grained control over locations and appearance. We\nachieve this through image-based video editing by inverting latents with noise\nextrapolation, from which we generate videos conditioned on the edited image.\nVideoshop produces higher quality edits against 6 baselines on 2 editing\nbenchmarks using 10 evaluation metrics.\n","authors":["Xiang Fan","Anand Bhattad","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2403.14617v2.pdf","comment":"Project page at https://videoshop-editing.github.io/"},{"id":"http://arxiv.org/abs/2403.15365v1","updated":"2024-03-22T17:33:11Z","published":"2024-03-22T17:33:11Z","title":"A Transfer Attack to Image Watermarks","summary":"  Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.\n","authors":["Yuepeng Hu","Zhengyuan Jiang","Moyang Guo","Neil Gong"],"pdf_url":"https://arxiv.org/pdf/2403.15365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15363v1","updated":"2024-03-22T17:31:21Z","published":"2024-03-22T17:31:21Z","title":"Cascading Blackout Severity Prediction with Statistically-Augmented\n  Graph Neural Networks","summary":"  Higher variability in grid conditions, resulting from growing renewable\npenetration and increased incidence of extreme weather events, has increased\nthe difficulty of screening for scenarios that may lead to catastrophic\ncascading failures. Traditional power-flow-based tools for assessing cascading\nblackout risk are too slow to properly explore the space of possible failures\nand load/generation patterns. We add to the growing literature of faster\ngraph-neural-network (GNN)-based techniques, developing two novel techniques\nfor the estimation of blackout magnitude from initial grid conditions. First we\npropose several methods for employing an initial classification step to filter\nout safe \"non blackout\" scenarios prior to magnitude estimation. Second, using\ninsights from the statistical properties of cascading blackouts, we propose a\nmethod for facilitating non-local message passing in our GNN models. We\nvalidate these two approaches on a large simulated dataset, and show the\npotential of both to increase blackout size estimation performance.\n","authors":["Joe Gorka","Tim Hsu","Wenting Li","Yury Maximov","Line Roald"],"pdf_url":"https://arxiv.org/pdf/2403.15363v1.pdf","comment":"Accepted to Power Systems Computation Conference (PSCC) 2024"},{"id":"http://arxiv.org/abs/2312.00812v4","updated":"2024-03-22T17:29:01Z","published":"2023-11-28T03:13:09Z","title":"Empowering Autonomous Driving with Large Language Models: A Safety\n  Perspective","summary":"  Autonomous Driving (AD) encounters significant safety hurdles in long-tail\nunforeseen driving scenarios, largely stemming from the non-interpretability\nand poor generalization of the deep neural networks within the AD system,\nparticularly in out-of-distribution and uncertain data. To this end, this paper\nexplores the integration of Large Language Models (LLMs) into AD systems,\nleveraging their robust common-sense knowledge and reasoning abilities. The\nproposed methodologies employ LLMs as intelligent decision-makers in behavioral\nplanning, augmented with a safety verifier shield for contextual safety\nlearning, for enhancing driving performance and safety. We present two key\nstudies in a simulated environment: an adaptive LLM-conditioned Model\nPredictive Control (MPC) and an LLM-enabled interactive behavior planning\nscheme with a state machine. Demonstrating superior performance and safety\nmetrics compared to state-of-the-art approaches, our approach shows the\npromising potential for using LLMs for autonomous vehicles.\n","authors":["Yixuan Wang","Ruochen Jiao","Sinong Simon Zhan","Chengtian Lang","Chao Huang","Zhaoran Wang","Zhuoran Yang","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.00812v4.pdf","comment":"Accepted to LLMAgent workshop @ICLR2024"},{"id":"http://arxiv.org/abs/2309.16512v4","updated":"2024-03-22T17:26:53Z","published":"2023-09-28T15:19:30Z","title":"From Complexity to Clarity: Analytical Expressions of Deep Neural\n  Network Weights via Clifford's Geometric Algebra and Convexity","summary":"  In this paper, we introduce a novel analysis of neural networks based on\ngeometric (Clifford) algebra and convex optimization. We show that optimal\nweights of deep ReLU neural networks are given by the wedge product of training\nsamples when trained with standard regularized loss. Furthermore, the training\nproblem reduces to convex optimization over wedge product features, which\nencode the geometric structure of the training dataset. This structure is given\nin terms of signed volumes of triangles and parallelotopes generated by data\nvectors. The convex problem finds a small subset of samples via $\\ell_1$\nregularization to discover only relevant wedge product features. Our analysis\nprovides a novel perspective on the inner workings of deep neural networks and\nsheds light on the role of the hidden layers.\n","authors":["Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2309.16512v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15361v1","updated":"2024-03-22T17:23:37Z","published":"2024-03-22T17:23:37Z","title":"Learning Topological Representations for Deep Image Understanding","summary":"  In many scenarios, especially biomedical applications, the correct\ndelineation of complex fine-scaled structures such as neurons, tissues, and\nvessels is critical for downstream analysis. Despite the strong predictive\npower of deep learning methods, they do not provide a satisfactory\nrepresentation of these structures, thus creating significant barriers in\nscalable annotation and downstream analysis. In this dissertation, we tackle\nsuch challenges by proposing novel representations of these topological\nstructures in a deep learning framework. We leverage the mathematical tools\nfrom topological data analysis, i.e., persistent homology and discrete Morse\ntheory, to develop principled methods for better segmentation and uncertainty\nestimation, which will become powerful tools for scalable annotation.\n","authors":["Xiaoling Hu"],"pdf_url":"https://arxiv.org/pdf/2403.15361v1.pdf","comment":"Ph.D. thesis from Stony Brook University. This thesis includes works\n  arXiv:1906.05404, arXiv:2110.08335, arXiv:2112.07812, arXiv:2103.09992,\n  arXiv:2206.01742"},{"id":"http://arxiv.org/abs/2403.15360v1","updated":"2024-03-22T17:22:56Z","published":"2024-03-22T17:22:56Z","title":"SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate\n  Time series","summary":"  Transformers have widely adopted attention networks for sequence mixing and\nMLPs for channel mixing, playing a pivotal role in achieving breakthroughs\nacross domains. However, recent literature highlights issues with attention\nnetworks, including low inductive bias and quadratic complexity concerning\ninput sequence length. State Space Models (SSMs) like S4 and others (Hippo,\nGlobal Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address\nthe above issues to help handle longer sequence lengths. Mamba, while being the\nstate-of-the-art SSM, has a stability issue when scaled to large networks for\ncomputer vision datasets. We propose SiMBA, a new architecture that introduces\nEinstein FFT (EinFFT) for channel modeling by specific eigenvalue computations\nand uses the Mamba block for sequence modeling. Extensive performance studies\nacross image and time-series benchmarks demonstrate that SiMBA outperforms\nexisting SSMs, bridging the performance gap with state-of-the-art transformers.\nNotably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet\nand transfer learning benchmarks such as Stanford Car and Flower as well as\ntask learning benchmarks as well as seven time series benchmark datasets. The\nproject page is available on this website\n~\\url{https://github.com/badripatro/Simba}.\n","authors":["Badri N. Patro","Vijay S. Agneeswaran"],"pdf_url":"https://arxiv.org/pdf/2403.15360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09611v3","updated":"2024-03-22T17:03:16Z","published":"2024-03-14T17:51:32Z","title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","summary":"  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n","authors":["Brandon McKinzie","Zhe Gan","Jean-Philippe Fauconnier","Sam Dodge","Bowen Zhang","Philipp Dufter","Dhruti Shah","Xianzhi Du","Futang Peng","Floris Weers","Anton Belyi","Haotian Zhang","Karanjeet Singh","Doug Kang","Ankur Jain","Hongyu Hè","Max Schwarzer","Tom Gunter","Xiang Kong","Aonan Zhang","Jianyu Wang","Chong Wang","Nan Du","Tao Lei","Sam Wiseman","Guoli Yin","Mark Lee","Zirui Wang","Ruoming Pang","Peter Grasch","Alexander Toshev","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2403.09611v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00885v2","updated":"2024-03-22T16:57:37Z","published":"2023-12-30T15:58:32Z","title":"Attractor reconstruction with reservoir computers: The effect of the\n  reservoir's conditional Lyapunov exponents on faithful attractor\n  reconstruction","summary":"  Reservoir computing is a machine learning framework that has been shown to be\nable to replicate the chaotic attractor, including the fractal dimension and\nthe entire Lyapunov spectrum, of the dynamical system on which it is trained.\nWe quantitatively relate the generalized synchronization dynamics of a driven\nreservoir during the training stage to the performance of the trained reservoir\ncomputer at the attractor reconstruction task. We show that, in order to obtain\nsuccessful attractor reconstruction and Lyapunov spectrum estimation, the\nlargest conditional Lyapunov exponent of the driven reservoir must be\nsignificantly more negative than the most negative Lyapunov exponent of the\ntarget system. We also find that the maximal conditional Lyapunov exponent of\nthe reservoir depends strongly on the spectral radius of the reservoir\nadjacency matrix, and therefore, for attractor reconstruction and Lyapunov\nspectrum estimation, small spectral radius reservoir computers perform better\nin general. Our arguments are supported by numerical examples on well-known\nchaotic systems.\n","authors":["Joseph D. Hart"],"pdf_url":"https://arxiv.org/pdf/2401.00885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05469v3","updated":"2024-03-22T16:49:06Z","published":"2023-10-09T07:26:35Z","title":"Learning to Predict Structural Vibrations","summary":"  In mechanical structures like airplanes, cars and houses, noise is generated\nand transmitted through vibrations. To take measures to reduce this noise,\nvibrations need to be simulated with expensive numerical computations.\nSurrogate deep learning models present a promising alternative to classical\nnumerical simulations as they can be evaluated magnitudes faster, while\ntrading-off accuracy. To quantify such trade-offs systematically and foster the\ndevelopment of methods, we present a benchmark on the task of predicting the\nvibration of harmonically excited plates. The benchmark features a total of\n12000 plate geometries with varying forms of beadings, material and sizes with\nassociated numerical solutions. To address the benchmark task, we propose a new\nnetwork architecture, named Frequency-Query Operator, which is trained to map\nplate geometries to their vibration pattern given a specific excitation\nfrequency. Applying principles from operator learning and implicit models for\nshape encoding, our approach effectively addresses the prediction of highly\nvariable frequency response functions occurring in dynamic systems. To quantify\nthe prediction quality, we introduce a set of evaluation metrics and evaluate\nthe method on our vibrating-plates benchmark. Our method outperforms DeepONets,\nFourier Neural Operators and more traditional neural network architectures.\nCode, dataset and visualizations: https://eckerlab.org/code/delden2023_plate\n","authors":["Jan van Delden","Julius Schultz","Christopher Blech","Sabine C. Langer","Timo Lüddecke"],"pdf_url":"https://arxiv.org/pdf/2310.05469v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00401v2","updated":"2024-03-22T16:32:24Z","published":"2023-09-30T14:54:31Z","title":"Learning High-level Semantic-Relational Concepts for SLAM","summary":"  Recent works on SLAM extend their pose graphs with higher-level semantic\nconcepts like Rooms exploiting relationships between them, to provide, not only\na richer representation of the situation/environment but also to improve the\naccuracy of its estimation. Concretely, our previous work, Situational Graphs\n(S-Graphs+), a pioneer in jointly leveraging semantic relationships in the\nfactor optimization process, relies on semantic entities such as Planes and\nRooms, whose relationship is mathematically defined. Nevertheless, there is no\nunique approach to finding all the hidden patterns in lower-level factor-graphs\nthat correspond to high-level concepts of different natures. It is currently\ntackled with ad-hoc algorithms, which limits its graph expressiveness.\n  To overcome this limitation, in this work, we propose an algorithm based on\nGraph Neural Networks for learning high-level semantic-relational concepts that\ncan be inferred from the low-level factor graph. Given a set of mapped Planes\nour algorithm is capable of inferring Room entities relating to the Planes.\nAdditionally, to demonstrate the versatility of our method, our algorithm can\ninfer an additional semantic-relational concept, i.e. Wall, and its\nrelationship with its Planes. We validate our method in both simulated and real\ndatasets demonstrating improved performance over two baseline approaches.\nFurthermore, we integrate our method into the S-Graphs+ algorithm providing\nimproved pose and map accuracy compared to the baseline while further enhancing\nthe scene representation.\n","authors":["Jose Andres Millan-Romera","Hriday Bavle","Muhammad Shaheer","Martin R. Oswald","Holger Voos","Jose Luis Sanchez-Lopez"],"pdf_url":"https://arxiv.org/pdf/2310.00401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08731v2","updated":"2024-03-22T16:30:48Z","published":"2023-10-12T21:38:07Z","title":"Novelty Detection in Reinforcement Learning with World Models","summary":"  Reinforcement learning (RL) using world models has found significant recent\nsuccesses. However, when a sudden change to world mechanics or properties\noccurs then agent performance and reliability can dramatically decline. We\nrefer to the sudden change in visual properties or state transitions as\nnovelties. Implementing novelty detection within generated world model\nframeworks is a crucial task for protecting the agent when deployed. In this\npaper, we propose straightforward bounding approaches to incorporate novelty\ndetection into world model RL agents, by utilizing the misalignment of the\nworld model's hallucinated states and the true observed states as an anomaly\nscore. We provide effective approaches to detecting novelties in a distribution\nof transitions learned by an agent in a world model. Finally, we show the\nadvantage of our work in a novel environment compared to traditional machine\nlearning novelty detection methods as well as currently accepted RL focused\nnovelty detection algorithms.\n","authors":["Geigh Zollicoffer","Kenneth Eaton","Jonathan Balloch","Julia Kim","Mark O. Riedl","Robert Wright"],"pdf_url":"https://arxiv.org/pdf/2310.08731v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04690v2","updated":"2024-03-22T16:26:40Z","published":"2024-03-07T17:35:58Z","title":"Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self\n  Attention at the Threadblock Level","summary":"  Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\nfirst show that neighborhood attention can be represented as a batched GEMM\nproblem, similar to standard attention, and implement it for 1-D and 2-D\nneighborhood attention. These kernels on average provide 895% and 272%\nimprovement in full precision latency compared to existing naive kernels for\n1-D and 2-D neighborhood attention respectively. We find certain inherent\ninefficiencies in all unfused neighborhood attention kernels that bound their\nperformance and lower-precision scalability. We also developed fused\nneighborhood attention; an adaptation of fused dot-product attention kernels\nthat allow fine-grained control over attention across different spatial axes.\nKnown for reducing the quadratic time complexity of self attention to a linear\ncomplexity, neighborhood attention can now enjoy a reduced and constant memory\nfootprint, and record-breaking half precision latency. We observe that our\nfused kernels successfully circumvent some of the unavoidable inefficiencies in\nunfused implementations. While our unfused GEMM-based kernels only improve half\nprecision performance compared to naive kernels by an average of 496% and 113%\nin 1-D and 2-D problems respectively, our fused kernels improve naive kernels\nby an average of 1607% and 581% in 1-D and 2-D problems respectively.\n","authors":["Ali Hassani","Wen-Mei Hwu","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.04690v2.pdf","comment":"Project page: https://github.com/SHI-Labs/NATTEN"},{"id":"http://arxiv.org/abs/2402.07586v2","updated":"2024-03-22T16:25:26Z","published":"2024-02-12T11:35:25Z","title":"Unveiling Group-Specific Distributed Concept Drift: A Fairness\n  Imperative in Federated Learning","summary":"  In the evolving field of machine learning, ensuring fairness has become a\ncritical concern, prompting the development of algorithms designed to mitigate\ndiscriminatory outcomes in decision-making processes. However, achieving\nfairness in the presence of group-specific concept drift remains an unexplored\nfrontier, and our research represents pioneering efforts in this regard.\nGroup-specific concept drift refers to situations where one group experiences\nconcept drift over time while another does not, leading to a decrease in\nfairness even if accuracy remains fairly stable. Within the framework of\nfederated learning, where clients collaboratively train models, its distributed\nnature further amplifies these challenges since each client can experience\ngroup-specific concept drift independently while still sharing the same\nunderlying concept, creating a complex and dynamic environment for maintaining\nfairness. One of the significant contributions of our research is the\nformalization and introduction of the problem of group-specific concept drift\nand its distributed counterpart, shedding light on its critical importance in\nthe realm of fairness. In addition, leveraging insights from prior research, we\nadapt an existing distributed concept drift adaptation algorithm to tackle\ngroup-specific distributed concept drift which utilizes a multi-model approach,\na local group-specific drift detection mechanism, and continuous clustering of\nmodels over time. The findings from our experiments highlight the importance of\naddressing group-specific concept drift and its distributed counterpart to\nadvance fairness in machine learning.\n","authors":["Teresa Salazar","João Gama","Helder Araújo","Pedro Henriques Abreu"],"pdf_url":"https://arxiv.org/pdf/2402.07586v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15316v1","updated":"2024-03-22T16:10:38Z","published":"2024-03-22T16:10:38Z","title":"Ultrasound Imaging based on the Variance of a Diffusion Restoration\n  Model","summary":"  Despite today's prevalence of ultrasound imaging in medicine, ultrasound\nsignal-to-noise ratio is still affected by several sources of noise and\nartefacts. Moreover, enhancing ultrasound image quality involves balancing\nconcurrent factors like contrast, resolution, and speckle preservation.\nRecently, there has been progress in both model-based and learning-based\napproaches addressing the problem of ultrasound image reconstruction. Bringing\nthe best from both worlds, we propose a hybrid reconstruction method combining\nan ultrasound linear direct model with a learning-based prior coming from a\ngenerative Denoising Diffusion model. More specifically, we rely on the\nunsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model\n(DDRM). Given the nature of multiplicative noise inherent to ultrasound, this\npaper proposes an empirical model to characterize the stochasticity of\ndiffusion reconstruction of ultrasound images, and shows the interest of its\nvariance as an echogenicity map estimator. We conduct experiments on synthetic,\nin-vitro, and in-vivo data, demonstrating the efficacy of our variance imaging\napproach in achieving high-quality image reconstructions from single plane-wave\nacquisitions and in comparison to state-of-the-art methods.\n","authors":["Yuxin Zhang","Clément Huneau","Jérôme Idier","Diana Mateus"],"pdf_url":"https://arxiv.org/pdf/2403.15316v1.pdf","comment":"5 pages; submitted to EUSIPCO 2024. arXiv admin note: text overlap\n  with arXiv:2310.20618"},{"id":"http://arxiv.org/abs/2403.09919v2","updated":"2024-03-22T16:06:42Z","published":"2024-03-14T23:40:56Z","title":"Recurrent Drafter for Fast Speculative Decoding in Large Language Models","summary":"  In this paper, we introduce an improved approach of speculative decoding\naimed at enhancing the efficiency of serving large language models. Our method\ncapitalizes on the strengths of two established techniques: the classic\ntwo-model speculative decoding approach, and the more recent single-model\napproach, Medusa. Drawing inspiration from Medusa, our approach adopts a\nsingle-model strategy for speculative decoding. However, our method\ndistinguishes itself by employing a single, lightweight draft head with a\nrecurrent dependency design, akin in essence to the small, draft model uses in\nclassic speculative decoding, but without the complexities of the full\ntransformer architecture. And because of the recurrent dependency, we can use\nbeam search to swiftly filter out undesired candidates with the draft head. The\noutcome is a method that combines the simplicity of single-model design and\navoids the need to create a data-dependent tree attention structure only for\ninference in Medusa. We empirically demonstrate the effectiveness of the\nproposed method on several popular open source language models, along with a\ncomprehensive analysis of the trade-offs involved in adopting this approach.\n","authors":["Aonan Zhang","Chong Wang","Yi Wang","Xuanyu Zhang","Yunfei Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.09919v2.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.15312v1","updated":"2024-03-22T16:04:26Z","published":"2024-03-22T16:04:26Z","title":"A Wasserstein perspective of Vanilla GANs","summary":"  The empirical success of Generative Adversarial Networks (GANs) caused an\nincreasing interest in theoretical research. The statistical literature is\nmainly focused on Wasserstein GANs and generalizations thereof, which\nespecially allow for good dimension reduction properties. Statistical results\nfor Vanilla GANs, the original optimization problem, are still rather limited\nand require assumptions such as smooth activation functions and equal\ndimensions of the latent space and the ambient space. To bridge this gap, we\ndraw a connection from Vanilla GANs to the Wasserstein distance. By doing so,\nexisting results for Wasserstein GANs can be extended to Vanilla GANs. In\nparticular, we obtain an oracle inequality for Vanilla GANs in Wasserstein\ndistance. The assumptions of this oracle inequality are designed to be\nsatisfied by network architectures commonly used in practice, such as\nfeedforward ReLU networks. By providing a quantitative result for the\napproximation of a Lipschitz function by a feedforward ReLU network with\nbounded H\\\"older norm, we conclude a rate of convergence for Vanilla GANs as\nwell as Wasserstein GANs as estimators of the unknown probability distribution.\n","authors":["Lea Kunkel","Mathias Trabs"],"pdf_url":"https://arxiv.org/pdf/2403.15312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10581v2","updated":"2024-03-22T16:00:24Z","published":"2024-03-15T13:25:09Z","title":"Large Language Model-informed ECG Dual Attention Network for Heart\n  Failure Risk Prediction","summary":"  Heart failure (HF) poses a significant public health challenge, with a rising\nglobal mortality rate. Early detection and prevention of HF could significantly\nreduce its impact. We introduce a novel methodology for predicting HF risk\nusing 12-lead electrocardiograms (ECGs). We present a novel, lightweight\ndual-attention ECG network designed to capture complex ECG features essential\nfor early HF risk prediction, despite the notable imbalance between low and\nhigh-risk groups. This network incorporates a cross-lead attention module and\ntwelve lead-specific temporal attention modules, focusing on cross-lead\ninteractions and each lead's local dynamics. To further alleviate model\noverfitting, we leverage a large language model (LLM) with a public ECG-Report\ndataset for pretraining on an ECG-report alignment task. The network is then\nfine-tuned for HF risk prediction using two specific cohorts from the UK\nBiobank study, focusing on patients with hypertension (UKB-HYP) and those who\nhave had a myocardial infarction (UKB-MI).The results reveal that LLM-informed\npre-training substantially enhances HF risk prediction in these cohorts. The\ndual-attention design not only improves interpretability but also predictive\naccuracy, outperforming existing competitive methods with C-index scores of\n0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method's\npotential in advancing HF risk assessment with clinical complex ECG data.\n","authors":["Chen Chen","Lei Li","Marcel Beetz","Abhirup Banerjee","Ramneek Gupta","Vicente Grau"],"pdf_url":"https://arxiv.org/pdf/2403.10581v2.pdf","comment":"Under journal revision"},{"id":"http://arxiv.org/abs/2403.15309v1","updated":"2024-03-22T15:59:24Z","published":"2024-03-22T15:59:24Z","title":"Controlled Training Data Generation with Diffusion Models","summary":"  In this work, we present a method to control a text-to-image generative model\nto produce training data specifically \"useful\" for supervised learning. Unlike\nprevious works that employ an open-loop approach and pre-define prompts to\ngenerate new data using either a language model or human expertise, we develop\nan automated closed-loop system which involves two feedback mechanisms. The\nfirst mechanism uses feedback from a given supervised model and finds\nadversarial prompts that result in image generations that maximize the model\nloss. While these adversarial prompts result in diverse data informed by the\nmodel, they are not informed of the target distribution, which can be\ninefficient. Therefore, we introduce the second feedback mechanism that guides\nthe generation process towards a certain target distribution. We call the\nmethod combining these two mechanisms Guided Adversarial Prompts. We perform\nour evaluations on different tasks, datasets and architectures, with different\ntypes of distribution shifts (spuriously correlated data, unseen domains) and\ndemonstrate the efficiency of the proposed feedback mechanisms compared to\nopen-loop approaches.\n","authors":["Teresa Yeo","Andrei Atanov","Harold Benoit","Aleksandr Alekseev","Ruchira Ray","Pooya Esmaeil Akhoondi","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2403.15309v1.pdf","comment":"Project page at https://adversarial-prompts.epfl.ch/"},{"id":"http://arxiv.org/abs/2403.15304v1","updated":"2024-03-22T15:54:30Z","published":"2024-03-22T15:54:30Z","title":"KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing","summary":"  Knowledge Tracing (KT) is concerned with predicting students' future\nperformance on learning items in intelligent tutoring systems. Learning items\nare tagged with skill labels called knowledge concepts (KCs). Many KT models\nexpand the sequence of item-student interactions into KC-student interactions\nby replacing learning items with their constituting KCs. This often results in\na longer sequence length. This approach addresses the issue of sparse\nitem-student interactions and minimises model parameters. However, two problems\nhave been identified with such models.\n  The first problem is the model's ability to learn correlations between KCs\nbelonging to the same item, which can result in the leakage of ground truth\nlabels and hinder performance. This problem can lead to a significant decrease\nin performance on datasets with a higher number of KCs per item. The second\nproblem is that the available benchmark implementations ignore accounting for\nchanges in sequence length when expanding KCs, leading to different models\nbeing tested with varying sequence lengths but still compared against the same\nbenchmark.\n  To address these problems, we introduce a general masking framework that\nmitigates the first problem and enhances the performance of such KT models\nwhile preserving the original model architecture without significant\nalterations. Additionally, we introduce KTbench, an open-source benchmark\nlibrary designed to ensure the reproducibility of this work while mitigating\nthe second problem.\n","authors":["Yahya Badran","Christine Preisach"],"pdf_url":"https://arxiv.org/pdf/2403.15304v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.15301v1","updated":"2024-03-22T15:51:39Z","published":"2024-03-22T15:51:39Z","title":"Planning with a Learned Policy Basis to Optimally Solve Complex Tasks","summary":"  Conventional reinforcement learning (RL) methods can successfully solve a\nwide range of sequential decision problems. However, learning policies that can\ngeneralize predictably across multiple tasks in a setting with non-Markovian\nreward specifications is a challenging problem. We propose to use successor\nfeatures to learn a policy basis so that each (sub)policy in it solves a\nwell-defined subproblem. In a task described by a finite state automaton (FSA)\nthat involves the same set of subproblems, the combination of these\n(sub)policies can then be used to generate an optimal solution without\nadditional learning. In contrast to other methods that combine (sub)policies\nvia planning, our method asymptotically attains global optimality, even in\nstochastic environments.\n","authors":["Guillermo Infante","David Kuric","Anders Jonsson","Vicenç Gómez","Herke van Hoof"],"pdf_url":"https://arxiv.org/pdf/2403.15301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04147v2","updated":"2024-03-22T15:37:38Z","published":"2023-11-07T17:18:52Z","title":"Multi-resolution Time-Series Transformer for Long-term Forecasting","summary":"  The performance of transformers for time-series forecasting has improved\nsignificantly. Recent architectures learn complex temporal patterns by\nsegmenting a time-series into patches and using the patches as tokens. The\npatch size controls the ability of transformers to learn the temporal patterns\nat different frequencies: shorter patches are effective for learning localized,\nhigh-frequency patterns, whereas mining long-term seasonalities and trends\nrequires longer patches. Inspired by this observation, we propose a novel\nframework, Multi-resolution Time-Series Transformer (MTST), which consists of a\nmulti-branch architecture for simultaneous modeling of diverse temporal\npatterns at different resolutions. In contrast to many existing time-series\ntransformers, we employ relative positional encoding, which is better suited\nfor extracting periodic components at different scales. Extensive experiments\non several real-world datasets demonstrate the effectiveness of MTST in\ncomparison to state-of-the-art forecasting techniques.\n","authors":["Yitian Zhang","Liheng Ma","Soumyasundar Pal","Yingxue Zhang","Mark Coates"],"pdf_url":"https://arxiv.org/pdf/2311.04147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15285v1","updated":"2024-03-22T15:31:37Z","published":"2024-03-22T15:31:37Z","title":"Blockchain-based Pseudonym Management for Vehicle Twin Migrations in\n  Vehicular Edge Metaverse","summary":"  Driven by the great advances in metaverse and edge computing technologies,\nvehicular edge metaverses are expected to disrupt the current paradigm of\nintelligent transportation systems. As highly computerized avatars of Vehicular\nMetaverse Users (VMUs), the Vehicle Twins (VTs) deployed in edge servers can\nprovide valuable metaverse services to improve driving safety and on-board\nsatisfaction for their VMUs throughout journeys. To maintain uninterrupted\nmetaverse experiences, VTs must be migrated among edge servers following the\nmovements of vehicles. This can raise concerns about privacy breaches during\nthe dynamic communications among vehicular edge metaverses. To address these\nconcerns and safeguard location privacy, pseudonyms as temporary identifiers\ncan be leveraged by both VMUs and VTs to realize anonymous communications in\nthe physical space and virtual spaces. However, existing pseudonym management\nmethods fall short in meeting the extensive pseudonym demands in vehicular edge\nmetaverses, thus dramatically diminishing the performance of privacy\npreservation. To this end, we present a cross-metaverse empowered dual\npseudonym management framework. We utilize cross-chain technology to enhance\nmanagement efficiency and data security for pseudonyms. Furthermore, we propose\na metric to assess the privacy level and employ a Multi-Agent Deep\nReinforcement Learning (MADRL) approach to obtain an optimal pseudonym\ngenerating strategy. Numerical results demonstrate that our proposed schemes\nare high-efficiency and cost-effective, showcasing their promising applications\nin vehicular edge metaverses.\n","authors":["Jiawen Kang","Xiaofeng Luo","Jiangtian Nie","Tianhao Wu","Haibo Zhou","Yonghua Wang","Dusit Niyato","Shiwen Mao","Shengli Xie"],"pdf_url":"https://arxiv.org/pdf/2403.15285v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2308.13712v3","updated":"2024-03-22T15:30:57Z","published":"2023-08-25T23:54:15Z","title":"Residual Denoising Diffusion Models","summary":"  We propose residual denoising diffusion models (RDDM), a novel dual diffusion\nprocess that decouples the traditional single denoising diffusion process into\nresidual diffusion and noise diffusion. This dual diffusion framework expands\nthe denoising-based diffusion models, initially uninterpretable for image\nrestoration, into a unified and interpretable model for both image generation\nand restoration by introducing residuals. Specifically, our residual diffusion\nrepresents directional diffusion from the target image to the degraded input\nimage and explicitly guides the reverse generation process for image\nrestoration, while noise diffusion represents random perturbations in the\ndiffusion process. The residual prioritizes certainty, while the noise\nemphasizes diversity, enabling RDDM to effectively unify tasks with varying\ncertainty or diversity requirements, such as image generation and restoration.\nWe demonstrate that our sampling process is consistent with that of DDPM and\nDDIM through coefficient transformation, and propose a partially\npath-independent generation process to better understand the reverse process.\nNotably, our RDDM enables a generic UNet, trained with only an L1 loss and a\nbatch size of 1, to compete with state-of-the-art image restoration methods. We\nprovide code and pre-trained models to encourage further exploration,\napplication, and development of our innovative framework\n(https://github.com/nachifur/RDDM).\n","authors":["Jiawei Liu","Qiang Wang","Huijie Fan","Yinong Wang","Yandong Tang","Liangqiong Qu"],"pdf_url":"https://arxiv.org/pdf/2308.13712v3.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2309.09510v2","updated":"2024-03-22T15:25:04Z","published":"2023-09-18T06:43:30Z","title":"Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive\n  Instruction-Tuning Benchmark for Speech","summary":"  Text language models have shown remarkable zero-shot capability in\ngeneralizing to unseen tasks when provided with well-formulated instructions.\nHowever, existing studies in speech processing primarily focus on limited or\nspecific tasks. Moreover, the lack of standardized benchmarks hinders a fair\ncomparison across different approaches. Thus, we present Dynamic-SUPERB, a\nbenchmark designed for building universal speech models capable of leveraging\ninstruction tuning to perform multiple tasks in a zero-shot fashion. To achieve\ncomprehensive coverage of diverse speech tasks and harness instruction tuning,\nwe invite the community to collaborate and contribute, facilitating the dynamic\ngrowth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation\ninstances by combining 33 tasks and 22 datasets. This spans a broad spectrum of\ndimensions, providing a comprehensive platform for evaluation. Additionally, we\npropose several approaches to establish benchmark baselines. These include the\nutilization of speech models, text language models, and the multimodal encoder.\nEvaluation results indicate that while these baselines perform reasonably on\nseen tasks, they struggle with unseen ones. We release all materials to the\npublic and welcome researchers to collaborate on the project, advancing\ntechnologies in the field together.\n","authors":["Chien-yu Huang","Ke-Han Lu","Shih-Heng Wang","Chi-Yuan Hsiao","Chun-Yi Kuan","Haibin Wu","Siddhant Arora","Kai-Wei Chang","Jiatong Shi","Yifan Peng","Roshan Sharma","Shinji Watanabe","Bhiksha Ramakrishnan","Shady Shehata","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2309.09510v2.pdf","comment":"To appear in the proceedings of ICASSP 2024"},{"id":"http://arxiv.org/abs/2402.00705v2","updated":"2024-03-22T15:13:17Z","published":"2024-02-01T16:00:21Z","title":"Combining the Strengths of Dutch Survey and Register Data in a Data\n  Challenge to Predict Fertility (PreFer)","summary":"  The social sciences have produced an impressive body of research on\ndeterminants of fertility outcomes, or whether and when people have children.\nHowever, the strength of these determinants and underlying theories are rarely\nevaluated on their predictive ability on new data. This prevents us from\nsystematically comparing studies, hindering the evaluation and accumulation of\nknowledge. In this paper, we present two datasets which can be used to study\nthe predictability of fertility outcomes in the Netherlands. One dataset is\nbased on the LISS panel, a longitudinal survey which includes thousands of\nvariables on a wide range of topics, including individual preferences and\nvalues. The other is based on the Dutch register data which lacks attitudinal\ndata but includes detailed information about the life courses of millions of\nDutch residents. We provide information about the datasets and the samples, and\ndescribe the fertility outcome of interest. We also introduce the fertility\nprediction data challenge PreFer which is based on these datasets and will\nstart in Spring 2024. We outline the ways in which measuring the predictability\nof fertility outcomes using these datasets and combining their strengths in the\ndata challenge can advance our understanding of fertility behaviour and\ncomputational social science. We further provide details for participants on\nhow to take part in the data challenge.\n","authors":["Elizaveta Sivak","Paulina Pankowska","Adrienne Mendrik","Tom Emery","Javier Garcia-Bernardo","Seyit Hocuk","Kasia Karpinska","Angelica Maineri","Joris Mulder","Malvina Nissim","Gert Stulp"],"pdf_url":"https://arxiv.org/pdf/2402.00705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15267v1","updated":"2024-03-22T15:06:31Z","published":"2024-03-22T15:06:31Z","title":"Parametric PDE Control with Deep Reinforcement Learning and\n  Differentiable L0-Sparse Polynomial Policies","summary":"  Optimal control of parametric partial differential equations (PDEs) is\ncrucial in many applications in engineering and science. In recent years, the\nprogress in scientific machine learning has opened up new frontiers for the\ncontrol of parametric PDEs. In particular, deep reinforcement learning (DRL)\nhas the potential to solve high-dimensional and complex control problems in a\nlarge variety of applications. Most DRL methods rely on deep neural network\n(DNN) control policies. However, for many dynamical systems, DNN-based control\npolicies tend to be over-parametrized, which means they need large amounts of\ntraining data, show limited robustness, and lack interpretability. In this\nwork, we leverage dictionary learning and differentiable L$_0$ regularization\nto learn sparse, robust, and interpretable control policies for parametric\nPDEs. Our sparse policy architecture is agnostic to the DRL method and can be\nused in different policy-gradient and actor-critic DRL algorithms without\nchanging their policy-optimization procedure. We test our approach on the\nchallenging tasks of controlling parametric Kuramoto-Sivashinsky and\nconvection-diffusion-reaction PDEs. We show that our method (1) outperforms\nbaseline DNN-based DRL policies, (2) allows for the derivation of interpretable\nequations of the learned optimal control laws, and (3) generalizes to unseen\nparameters of the PDE without retraining the policies.\n","authors":["Nicolò Botteghi","Urban Fasel"],"pdf_url":"https://arxiv.org/pdf/2403.15267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15263v1","updated":"2024-03-22T15:02:24Z","published":"2024-03-22T15:02:24Z","title":"Federated Bayesian Deep Learning: The Application of Statistical\n  Aggregation Methods to Bayesian Models","summary":"  Federated learning (FL) is an approach to training machine learning models\nthat takes advantage of multiple distributed datasets while maintaining data\nprivacy and reducing communication costs associated with sharing local\ndatasets. Aggregation strategies have been developed to pool or fuse the\nweights and biases of distributed deterministic models; however, modern\ndeterministic deep learning (DL) models are often poorly calibrated and lack\nthe ability to communicate a measure of epistemic uncertainty in prediction,\nwhich is desirable for remote sensing platforms and safety-critical\napplications. Conversely, Bayesian DL models are often well calibrated and\ncapable of quantifying and communicating a measure of epistemic uncertainty\nalong with a competitive prediction accuracy. Unfortunately, because the\nweights and biases in Bayesian DL models are defined by a probability\ndistribution, simple application of the aggregation methods associated with FL\nschemes for deterministic models is either impossible or results in sub-optimal\nperformance. In this work, we use independent and identically distributed (IID)\nand non-IID partitions of the CIFAR-10 dataset and a fully variational\nResNet-20 architecture to analyze six different aggregation strategies for\nBayesian DL models. Additionally, we analyze the traditional federated\naveraging approach applied to an approximate Bayesian Monte Carlo dropout model\nas a lightweight alternative to more complex variational inference methods in\nFL. We show that aggregation strategy is a key hyperparameter in the design of\na Bayesian FL system with downstream effects on accuracy, calibration,\nuncertainty quantification, training stability, and client compute\nrequirements.\n","authors":["John Fischer","Marko Orescanin","Justin Loomis","Patrick McClure"],"pdf_url":"https://arxiv.org/pdf/2403.15263v1.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2312.12973v2","updated":"2024-03-22T15:00:47Z","published":"2023-12-20T12:31:28Z","title":"Sparse Mean Field Load Balancing in Large Localized Queueing Systems","summary":"  Scalable load balancing algorithms are of great interest in cloud networks\nand data centers, necessitating the use of tractable techniques to compute\noptimal load balancing policies for good performance. However, most existing\nscalable techniques, especially asymptotically scaling methods based on mean\nfield theory, have not been able to model large queueing networks with strong\nlocality. Meanwhile, general multi-agent reinforcement learning techniques can\nbe hard to scale and usually lack a theoretical foundation. In this work, we\naddress this challenge by leveraging recent advances in sparse mean field\ntheory to learn a near-optimal load balancing policy in sparsely connected\nqueueing networks in a tractable manner, which may be preferable to global\napproaches in terms of wireless communication overhead. Importantly, we obtain\na general load balancing framework for a large class of sparse bounded-degree\nwireless topologies. By formulating a novel mean field control problem in the\ncontext of graphs with bounded degree, we reduce the otherwise difficult\nmulti-agent problem to a single-agent problem. Theoretically, the approach is\njustified by approximation guarantees. Empirically, the proposed methodology\nperforms well on several realistic and scalable wireless network topologies as\ncompared to a number of well-known load balancing heuristics and existing\nscalable multi-agent reinforcement learning methods.\n","authors":["Anam Tahir","Kai Cui","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2312.12973v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15250v1","updated":"2024-03-22T14:47:35Z","published":"2024-03-22T14:47:35Z","title":"Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A\n  Multifaceted Statistical Approach","summary":"  Amidst the rapid evolution of LLMs, the significance of evaluation in\ncomprehending and propelling these models forward is increasingly paramount.\nEvaluations have revealed that factors such as scaling, training types,\narchitectures and other factors profoundly impact the performance of LLMs.\nHowever, the extent and nature of these impacts continue to be subjects of\ndebate because most assessments have been restricted to a limited number of\nmodels and data points. Clarifying the effects of these factors on performance\nscores can be more effectively achieved through a statistical lens. Our study\nembarks on a thorough re-examination of these LLMs, targeting the inadequacies\nin current evaluation methods. With the advent of a uniform evaluation\nframework, our research leverages an expansive dataset of evaluation results,\nintroducing a comprehensive statistical methodology. This includes the\napplication of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering\na robust and transparent approach to deciphering LLM performance data. Contrary\nto prevailing findings, our results challenge assumptions about emergent\nabilities and the influence of given training types and architectures in LLMs.\nThese findings furnish new perspectives on the characteristics, intrinsic\nnature, and developmental trajectories of LLMs. By providing straightforward\nand reliable methods to scrutinize and reassess LLM performance data, this\nstudy contributes a nuanced perspective on LLM efficiency and potentials.\n","authors":["Kun Sun","Rong Wang","Haitao Liu","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2403.15250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15249v1","updated":"2024-03-22T14:47:18Z","published":"2024-03-22T14:47:18Z","title":"Spectral Motion Alignment for Video Motion Transfer using Diffusion\n  Models","summary":"  The evolution of diffusion models has greatly impacted video generation and\nunderstanding. Particularly, text-to-video diffusion models (VDMs) have\nsignificantly facilitated the customization of input video with target\nappearance, motion, etc. Despite these advances, challenges persist in\naccurately distilling motion information from video frames. While existing\nworks leverage the consecutive frame residual as the target motion vector, they\ninherently lack global motion context and are vulnerable to frame-wise\ndistortions. To address this, we present Spectral Motion Alignment (SMA), a\nnovel framework that refines and aligns motion vectors using Fourier and\nwavelet transforms. SMA learns motion patterns by incorporating\nfrequency-domain regularization, facilitating the learning of whole-frame\nglobal motion dynamics, and mitigating spatial artifacts. Extensive experiments\ndemonstrate SMA's efficacy in improving motion transfer while maintaining\ncomputational efficiency and compatibility across various video customization\nframeworks.\n","authors":["Geon Yeong Park","Hyeonho Jeong","Sang Wan Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2403.15249v1.pdf","comment":"Project page:\n  https://geonyeong-park.github.io/spectral-motion-alignment/"},{"id":"http://arxiv.org/abs/2403.05752v2","updated":"2024-03-22T14:44:17Z","published":"2024-03-09T01:17:26Z","title":"Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and\n  Efficient Modeling","summary":"  A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range\nof node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular\nfor training machine learning tasks like node classification and link\nprediction on KGs. However, HGNN methods exhibit excessive complexity\ninfluenced by the KG's size, density, and the number of node and edge types. AI\npractitioners handcraft a subgraph of a KG G relevant to a specific task. We\nrefer to this subgraph as a task-oriented subgraph (TOSG), which contains a\nsubset of task-related node and edge types in G. Training the task using TOSG\ninstead of G alleviates the excessive computation required for a large KG.\nCrafting the TOSG demands a deep understanding of the KG's structure and the\ntask's objectives. Hence, it is challenging and time-consuming. This paper\nproposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented\nHGNN training on a large KG. In KG-TOSA, we define a generic graph pattern that\ncaptures the KG's local and global structure relevant to a specific task. We\nexplore different techniques to extract subgraphs matching our graph pattern:\nnamely (i) two techniques sampling around targeted nodes using biased random\nwalk or influence scores, and (ii) a SPARQL-based extraction method leveraging\nRDF engines' built-in indices. Hence, it achieves negligible preprocessing\noverhead compared to the sampling techniques. We develop a benchmark of real\nKGs of large sizes and various tasks for node classification and link\nprediction. Our experiments show that KG-TOSA helps state-of-the-art HGNN\nmethods reduce training time and memory usage by up to 70% while improving the\nmodel performance, e.g., accuracy and inference time.\n","authors":["Hussein Abdallah","Waleed Afandi","Panos Kalnis","Essam Mansour"],"pdf_url":"https://arxiv.org/pdf/2403.05752v2.pdf","comment":"12 pages,9 Figures, 3 Tables, ICDE:2024"},{"id":"http://arxiv.org/abs/2403.15246v1","updated":"2024-03-22T14:42:29Z","published":"2024-03-22T14:42:29Z","title":"FollowIR: Evaluating and Teaching Information Retrieval Models to Follow\n  Instructions","summary":"  Modern Large Language Models (LLMs) are capable of following long and complex\ninstructions that enable a diverse amount of user tasks. However, despite\nInformation Retrieval (IR) models using LLMs as the backbone of their\narchitectures, nearly all of them still only take queries as input, with no\ninstructions. For the handful of recent models that do take instructions, it's\nunclear how they use them. We introduce our dataset FollowIR, which contains a\nrigorous instruction evaluation benchmark as well as a training set for helping\nIR models learn to better follow real-world instructions. FollowIR builds off\nthe long history of the TREC conferences: as TREC provides human annotators\nwith instructions (also known as narratives) to determine document relevance,\nso should IR models be able to understand and decide relevance based on these\ndetailed instructions. Our evaluation benchmark starts with three deeply judged\nTREC collections and alters the annotator instructions, re-annotating relevant\ndocuments. Through this process, we can measure how well IR models follow\ninstructions, through a new pairwise evaluation framework. Our results indicate\nthat existing retrieval models fail to correctly use instructions, using them\nfor basic keywords and struggling to understand long-form information. However,\nwe show that it is possible for IR models to learn to follow complex\ninstructions: our new FollowIR-7B model has significant improvements (over 13%)\nafter fine-tuning on our training set.\n","authors":["Orion Weller","Benjamin Chang","Sean MacAvaney","Kyle Lo","Arman Cohan","Benjamin Van Durme","Dawn Lawrie","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2403.15246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15245v1","updated":"2024-03-22T14:41:55Z","published":"2024-03-22T14:41:55Z","title":"Reasoning-Enhanced Object-Centric Learning for Videos","summary":"  Object-centric learning aims to break down complex visual scenes into more\nmanageable object representations, enhancing the understanding and reasoning\nabilities of machine learning systems toward the physical world. Recently,\nslot-based video models have demonstrated remarkable proficiency in segmenting\nand tracking objects, but they overlook the importance of the effective\nreasoning module. In the real world, reasoning and predictive abilities play a\ncrucial role in human perception and object tracking; in particular, these\nabilities are closely related to human intuitive physics. Inspired by this, we\ndesigned a novel reasoning module called the Slot-based Time-Space Transformer\nwith Memory buffer (STATM) to enhance the model's perception ability in complex\nscenes. The memory buffer primarily serves as storage for slot information from\nupstream modules, the Slot-based Time-Space Transformer makes predictions\nthrough slot-based spatiotemporal attention computations and fusion. Our\nexperiment results on various datasets show that STATM can significantly\nenhance object-centric learning capabilities of slot-based video models.\n","authors":["Jian Li","Pu Ren","Yang Liu","Hao Sun"],"pdf_url":"https://arxiv.org/pdf/2403.15245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15244v1","updated":"2024-03-22T14:40:29Z","published":"2024-03-22T14:40:29Z","title":"A Stochastic Quasi-Newton Method for Non-convex Optimization with\n  Non-uniform Smoothness","summary":"  Classical convergence analyses for optimization algorithms rely on the\nwidely-adopted uniform smoothness assumption. However, recent experimental\nstudies have demonstrated that many machine learning problems exhibit\nnon-uniform smoothness, meaning the smoothness factor is a function of the\nmodel parameter instead of a universal constant. In particular, it has been\nobserved that the smoothness grows with respect to the gradient norm along the\ntraining trajectory. Motivated by this phenomenon, the recently introduced\n$(L_0, L_1)$-smoothness is a more general notion, compared to traditional\n$L$-smoothness, that captures such positive relationship between smoothness and\ngradient norm. Under this type of non-uniform smoothness, existing literature\nhas designed stochastic first-order algorithms by utilizing gradient clipping\ntechniques to obtain the optimal $\\mathcal{O}(\\epsilon^{-3})$ sample complexity\nfor finding an $\\epsilon$-approximate first-order stationary solution.\nNevertheless, the studies of quasi-Newton methods are still lacking.\nConsidering higher accuracy and more robustness for quasi-Newton methods, in\nthis paper we propose a fast stochastic quasi-Newton method when there exists\nnon-uniformity in smoothness. Leveraging gradient clipping and variance\nreduction, our algorithm can achieve the best-known\n$\\mathcal{O}(\\epsilon^{-3})$ sample complexity and enjoys convergence speedup\nwith simple hyperparameter tuning. Our numerical experiments show that our\nproposed algorithm outperforms the state-of-the-art approaches.\n","authors":["Zhenyu Sun","Ermin Wei"],"pdf_url":"https://arxiv.org/pdf/2403.15244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15243v1","updated":"2024-03-22T14:36:39Z","published":"2024-03-22T14:36:39Z","title":"Robust Utility Optimization via a GAN Approach","summary":"  Robust utility optimization enables an investor to deal with market\nuncertainty in a structured way, with the goal of maximizing the worst-case\noutcome. In this work, we propose a generative adversarial network (GAN)\napproach to (approximately) solve robust utility optimization problems in\ngeneral and realistic settings. In particular, we model both the investor and\nthe market by neural networks (NN) and train them in a mini-max zero-sum game.\nThis approach is applicable for any continuous utility function and in\nrealistic market settings with trading costs, where only observable information\nof the market can be used. A large empirical study shows the versatile\nusability of our method. Whenever an optimal reference strategy is available,\nour method performs on par with it and in the (many) settings without known\noptimal strategy, our method outperforms all other reference strategies.\nMoreover, we can conclude from our study that the trained path-dependent\nstrategies do not outperform Markovian ones. Lastly, we uncover that our\ngenerative approach for learning optimal, (non-) robust investments under\ntrading costs generates universally applicable alternatives to well known\nasymptotic strategies of idealized settings.\n","authors":["Florian Krach","Josef Teichmann","Hanna Wutte"],"pdf_url":"https://arxiv.org/pdf/2403.15243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12333v2","updated":"2024-03-22T14:36:29Z","published":"2023-07-23T14:00:33Z","title":"An axiomatized PDE model of deep neural networks","summary":"  Inspired by the relation between deep neural network (DNN) and partial\ndifferential equations (PDEs), we study the general form of the PDE models of\ndeep neural networks. To achieve this goal, we formulate DNN as an evolution\noperator from a simple base model. Based on several reasonable assumptions, we\nprove that the evolution operator is actually determined by\nconvection-diffusion equation. This convection-diffusion equation model gives\nmathematical explanation for several effective networks. Moreover, we show that\nthe convection-diffusion model improves the robustness and reduces the\nRademacher complexity. Based on the convection-diffusion equation, we design a\nnew training method for ResNets. Experiments validate the performance of the\nproposed method.\n","authors":["Tangjun Wang","Wenqi Tao","Chenglong Bao","Zuoqiang Shi"],"pdf_url":"https://arxiv.org/pdf/2307.12333v2.pdf","comment":"The experiment design in the paper lacks careful thought and may be\n  misleading in demonstrating our contribution"},{"id":"http://arxiv.org/abs/2403.15239v1","updated":"2024-03-22T14:32:27Z","published":"2024-03-22T14:32:27Z","title":"Guided Decoding for Robot Motion Generation and Adaption","summary":"  We address motion generation for high-DoF robot arms in complex settings with\nobstacles, via points, etc. A significant advancement in this domain is\nachieved by integrating Learning from Demonstration (LfD) into the motion\ngeneration process. This integration facilitates rapid adaptation to new tasks\nand optimizes the utilization of accumulated expertise by allowing robots to\nlearn and generalize from demonstrated trajectories.\n  We train a transformer architecture on a large dataset of simulated\ntrajectories. This architecture, based on a conditional variational autoencoder\ntransformer, learns essential motion generation skills and adapts these to meet\nauxiliary tasks and constraints. Our auto-regressive approach enables real-time\nintegration of feedback from the physical system, enhancing the adaptability\nand efficiency of motion generation. We show that our model can generate motion\nfrom initial and target points, but also that it can adapt trajectories in\nnavigating complex tasks, including obstacle avoidance, via points, and meeting\nvelocity and acceleration constraints, across platforms.\n","authors":["Nutan Chen","Elie Aljalbout","Botond Cseke","Patrick van der Smagt"],"pdf_url":"https://arxiv.org/pdf/2403.15239v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2401.12764v3","updated":"2024-03-22T14:29:14Z","published":"2024-01-23T13:44:15Z","title":"Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving\n  $O(1/k)$ Finite-Sample Complexity","summary":"  This paper proposes to develop a new variant of the two-time-scale stochastic\napproximation to find the roots of two coupled nonlinear operators, assuming\nonly noisy samples of these operators can be observed. Our key idea is to\nleverage the classic Ruppert-Polyak averaging technique to dynamically estimate\nthe operators through their samples. The estimated values of these averaging\nsteps will then be used in the two-time-scale stochastic approximation updates\nto find the desired solution. Our main theoretical result is to show that under\nthe strongly monotone condition of the underlying nonlinear operators the\nmean-squared errors of the iterates generated by the proposed method converge\nto zero at an optimal rate $O(1/k)$, where $k$ is the number of iterations. Our\nresult significantly improves the existing result of two-time-scale stochastic\napproximation, where the best known finite-time convergence rate is\n$O(1/k^{2/3})$. We illustrate this result by applying the proposed method to\ndevelop new reinforcement learning algorithms with improved performance.\n","authors":["Thinh T. Doan"],"pdf_url":"https://arxiv.org/pdf/2401.12764v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15230v1","updated":"2024-03-22T14:23:21Z","published":"2024-03-22T14:23:21Z","title":"An Exploratory Investigation into Code License Infringements in Large\n  Language Model Training Datasets","summary":"  Does the training of large language models potentially infringe upon code\nlicenses? Furthermore, are there any datasets available that can be safely used\nfor training these models without violating such licenses? In our study, we\nassess the current trends in the field and the importance of incorporating code\ninto the training of large language models. Additionally, we examine publicly\navailable datasets to see whether these models can be trained on them without\nthe risk of legal issues in the future. To accomplish this, we compiled a list\nof 53 large language models trained on file-level code. We then extracted their\ndatasets and analyzed how much they overlap with a dataset we created,\nconsisting exclusively of strong copyleft code.\n  Our analysis revealed that every dataset we examined contained license\ninconsistencies, despite being selected based on their associated repository\nlicenses. We analyzed a total of 514 million code files, discovering 38 million\nexact duplicates present in our strong copyleft dataset. Additionally, we\nexamined 171 million file-leading comments, identifying 16 million with strong\ncopyleft licenses and another 11 million comments that discouraged copying\nwithout explicitly mentioning a license. Based on the findings of our study,\nwhich highlights the pervasive issue of license inconsistencies in large\nlanguage models trained on code, our recommendation for both researchers and\nthe community is to prioritize the development and adoption of best practices\nfor dataset creation and management.\n","authors":["Jonathan Katzy","Răzvan-Mihai Popescu","Arie van Deursen","Maliheh Izadi"],"pdf_url":"https://arxiv.org/pdf/2403.15230v1.pdf","comment":"Accepted to FORGE 2024"},{"id":"http://arxiv.org/abs/2303.02204v3","updated":"2024-03-22T14:14:45Z","published":"2023-03-03T20:31:04Z","title":"KGLiDS: A Platform for Semantic Abstraction, Linking, and Automation of\n  Data Science","summary":"  In recent years, we have witnessed the growing interest from academia and\nindustry in applying data science technologies to analyze large amounts of\ndata. In this process, a myriad of artifacts (datasets, pipeline scripts, etc.)\nare created. However, there has been no systematic attempt to holistically\ncollect and exploit all the knowledge and experiences that are implicitly\ncontained in those artifacts. Instead, data scientists recover information and\nexpertise from colleagues or learn via trial and error. Hence, this paper\npresents a scalable platform, KGLiDS, that employs machine learning and\nknowledge graph technologies to abstract and capture the semantics of data\nscience artifacts and their connections. Based on this information, KGLiDS\nenables various downstream applications, such as data discovery and pipeline\nautomation. Our comprehensive evaluation covers use cases in data discovery,\ndata cleaning, transformation, and AutoML. It shows that KGLiDS is\nsignificantly faster with a lower memory footprint than the state-of-the-art\nsystems while achieving comparable or better accuracy.\n","authors":["Mossad Helali","Niki Monjazeb","Shubham Vashisth","Philippe Carrier","Ahmed Helal","Antonio Cavalcante","Khaled Ammar","Katja Hose","Essam Mansour"],"pdf_url":"https://arxiv.org/pdf/2303.02204v3.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2206.00759v3","updated":"2024-03-22T14:13:56Z","published":"2022-06-01T20:48:24Z","title":"Interpretability Guarantees with Merlin-Arthur Classifiers","summary":"  We propose an interactive multi-agent classifier that provides provable\ninterpretability guarantees even for complex agents such as neural networks.\nThese guarantees consist of lower bounds on the mutual information between\nselected features and the classification decision. Our results are inspired by\nthe Merlin-Arthur protocol from Interactive Proof Systems and express these\nbounds in terms of measurable metrics such as soundness and completeness.\nCompared to existing interactive setups, we rely neither on optimal agents nor\non the assumption that features are distributed independently. Instead, we use\nthe relative strength of the agents as well as the new concept of Asymmetric\nFeature Correlation which captures the precise kind of correlations that make\ninterpretability guarantees difficult. We evaluate our results on two\nsmall-scale datasets where high mutual information can be verified explicitly.\n","authors":["Stephan Wäldchen","Kartikey Sharma","Berkant Turan","Max Zimmer","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2206.00759v3.pdf","comment":"AISTATS24 Camera-Ready Version, 34 pages total (9 pages main part, 3\n  pages references, 22 pages appendix), 17 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.15218v1","updated":"2024-03-22T14:07:07Z","published":"2024-03-22T14:07:07Z","title":"Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment\n  Anything Model for Crowd-Sourcing Medical Image Annotations","summary":"  Curating annotations for medical image segmentation is a labor-intensive and\ntime-consuming task that requires domain expertise, resulting in \"narrowly\"\nfocused deep learning (DL) models with limited translational utility. Recently,\nfoundation models like the Segment Anything Model (SAM) have revolutionized\nsemantic segmentation with exceptional zero-shot generalizability across\nvarious domains, including medical imaging, and hold a lot of promise for\nstreamlining the annotation process. However, SAM has yet to be evaluated in a\ncrowd-sourced setting to curate annotations for training 3D DL segmentation\nmodels. In this work, we explore the potential of SAM for crowd-sourcing\n\"sparse\" annotations from non-experts to generate \"dense\" segmentation masks\nfor training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our\nresults indicate that while SAM-generated annotations exhibit high mean Dice\nscores compared to ground-truth annotations, nnU-Net models trained on\nSAM-generated annotations perform significantly worse than nnU-Net models\ntrained on ground-truth annotations ($p<0.001$, all).\n","authors":["Pranav Kulkarni","Adway Kanhere","Dharmam Savani","Andrew Chan","Devina Chatterjee","Paul H. Yi","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2403.15218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15210v1","updated":"2024-03-22T13:52:53Z","published":"2024-03-22T13:52:53Z","title":"Early Period of Training Impacts Out-of-Distribution Generalization","summary":"  Prior research has found that differences in the early period of neural\nnetwork training significantly impact the performance of in-distribution (ID)\ntasks. However, neural networks are often sensitive to out-of-distribution\n(OOD) data, making them less reliable in downstream applications. Yet, the\nimpact of the early training period on OOD generalization remains understudied\ndue to its complexity and lack of effective analytical methodologies. In this\nwork, we investigate the relationship between learning dynamics and OOD\ngeneralization during the early period of neural network training. We utilize\nthe trace of Fisher Information and sharpness, with a focus on gradual\nunfreezing (i.e. progressively unfreezing parameters during training) as the\nmethodology for investigation. Through a series of empirical experiments, we\nshow that 1) selecting the number of trainable parameters at different times\nduring training, i.e. realized by gradual unfreezing -- has a minuscule impact\non ID results, but greatly affects the generalization to OOD data; 2) the\nabsolute values of sharpness and trace of Fisher Information at the initial\nperiod of training are not indicative for OOD generalization, but the relative\nvalues could be; 3) the trace of Fisher Information and sharpness may be used\nas indicators for the removal of interventions during early period of training\nfor better OOD generalization.\n","authors":["Chen Cecilia Liu","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2403.15210v1.pdf","comment":"WIP"},{"id":"http://arxiv.org/abs/2403.06020v2","updated":"2024-03-22T13:51:55Z","published":"2024-03-09T21:45:31Z","title":"Multi-conditioned Graph Diffusion for Neural Architecture Search","summary":"  Neural architecture search automates the design of neural network\narchitectures usually by exploring a large and thus complex architecture search\nspace. To advance the architecture search, we present a graph diffusion-based\nNAS approach that uses discrete conditional graph diffusion processes to\ngenerate high-performing neural network architectures. We then propose a\nmulti-conditioned classifier-free guidance approach applied to graph diffusion\nnetworks to jointly impose constraints such as high accuracy and low hardware\nlatency. Unlike the related work, our method is completely differentiable and\nrequires only a single model training. In our evaluations, we show promising\nresults on six standard benchmarks, yielding novel and unique architectures at\na fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we\ndemonstrate the generalisability and efficiency of our method through\nexperiments on ImageNet dataset.\n","authors":["Rohan Asthana","Joschua Conrad","Youssef Dawoud","Maurits Ortmanns","Vasileios Belagiannis"],"pdf_url":"https://arxiv.org/pdf/2403.06020v2.pdf","comment":"Accepted at Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2308.01674v3","updated":"2024-03-22T13:51:19Z","published":"2023-08-03T10:21:53Z","title":"End-to-End Reinforcement Learning of Koopman Models for Economic\n  Nonlinear Model Predictive Control","summary":"  (Economic) nonlinear model predictive control ((e)NMPC) requires dynamic\nmodels that are sufficiently accurate and computationally tractable.\nData-driven surrogate models for mechanistic models can reduce the\ncomputational burden of (e)NMPC; however, such models are typically trained by\nsystem identification for maximum prediction accuracy on simulation samples and\nperform suboptimally in (e)NMPC. We present a method for end-to-end\nreinforcement learning of Koopman surrogate models for optimal performance as\npart of (e)NMPC. We apply our method to two applications derived from an\nestablished nonlinear continuous stirred-tank reactor model. The controller\nperformance is compared to that of (e)NMPCs utilizing models trained using\nsystem identification, and model-free neural network controllers trained using\nreinforcement learning. We show that the end-to-end trained models outperform\nthose trained using system identification in (e)NMPC, and that, in contrast to\nthe neural network controllers, the (e)NMPC controllers can react to changes in\nthe control setting without retraining.\n","authors":["Daniel Mayfrank","Alexander Mitsos","Manuel Dahmen"],"pdf_url":"https://arxiv.org/pdf/2308.01674v3.pdf","comment":"manuscript (18 pages, 7 figures, 5 tables), supplementary materials\n  (3 pages, 2 tables)"},{"id":"http://arxiv.org/abs/2403.15207v1","updated":"2024-03-22T13:49:53Z","published":"2024-03-22T13:49:53Z","title":"Robust optimization for adversarial learning with finite sample\n  complexity guarantees","summary":"  Decision making and learning in the presence of uncertainty has attracted\nsignificant attention in view of the increasing need to achieve robust and\nreliable operations. In the case where uncertainty stems from the presence of\nadversarial attacks this need is becoming more prominent. In this paper we\nfocus on linear and nonlinear classification problems and propose a novel\nadversarial training method for robust classifiers, inspired by Support Vector\nMachine (SVM) margins. We view robustness under a data driven lens, and derive\nfinite sample complexity bounds for both linear and non-linear classifiers in\nbinary and multi-class scenarios. Notably, our bounds match natural\nclassifiers' complexity. Our algorithm minimizes a worst-case surrogate loss\nusing Linear Programming (LP) and Second Order Cone Programming (SOCP) for\nlinear and non-linear models. Numerical experiments on the benchmark MNIST and\nCIFAR10 datasets show our approach's comparable performance to state-of-the-art\nmethods, without needing adversarial examples during training. Our work offers\na comprehensive framework for enhancing binary linear and non-linear classifier\nrobustness, embedding robustness in learning under the presence of adversaries.\n","authors":["André Bertolace","Konstatinos Gatsis","Kostas Margellos"],"pdf_url":"https://arxiv.org/pdf/2403.15207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15195v1","updated":"2024-03-22T13:31:24Z","published":"2024-03-22T13:31:24Z","title":"FSD-Inference: Fully Serverless Distributed Inference with Scalable\n  Cloud Communication","summary":"  Serverless computing offers attractive scalability, elasticity and\ncost-effectiveness. However, constraints on memory, CPU and function runtime\nhave hindered its adoption for data-intensive applications and machine learning\n(ML) workloads. Traditional 'server-ful' platforms enable distributed\ncomputation via fast networks and well-established inter-process communication\n(IPC) mechanisms such as MPI and shared memory. In the absence of such\nsolutions in the serverless domain, parallel computation with significant IPC\nrequirements is challenging. We present FSD-Inference, the first fully\nserverless and highly scalable system for distributed ML inference. We explore\npotential communication channels, in conjunction with Function-as-a-Service\n(FaaS) compute, to design a state-of-the-art solution for distributed ML within\nthe context of serverless data-intensive computing. We introduce novel fully\nserverless communication schemes for ML inference workloads, leveraging both\ncloud-based publish-subscribe/queueing and object storage offerings. We\ndemonstrate how publish-subscribe/queueing services can be adapted for FaaS IPC\nwith comparable performance to object storage, while offering significantly\nreduced cost at high parallelism levels. We conduct in-depth experiments on\nbenchmark DNNs of various sizes. The results show that when compared to\nserver-based alternatives, FSD-Inference is significantly more cost-effective\nand scalable, and can even achieve competitive performance against optimized\nHPC solutions. Experiments also confirm that our serverless solution can handle\nlarge distributed workloads and leverage high degrees of FaaS parallelism.\n","authors":["Joe Oakley","Hakan Ferhatosmanoglu"],"pdf_url":"https://arxiv.org/pdf/2403.15195v1.pdf","comment":"In Proceedings of 2024 IEEE 40th International Conference on Data\n  Engineering (ICDE) (to appear)"},{"id":"http://arxiv.org/abs/2403.15194v1","updated":"2024-03-22T13:27:57Z","published":"2024-03-22T13:27:57Z","title":"Your Image is My Video: Reshaping the Receptive Field via Image-To-Video\n  Differentiable AutoAugmentation and Fusion","summary":"  The landscape of deep learning research is moving towards innovative\nstrategies to harness the true potential of data. Traditionally, emphasis has\nbeen on scaling model architectures, resulting in large and complex neural\nnetworks, which can be difficult to train with limited computational resources.\nHowever, independently of the model size, data quality (i.e. amount and\nvariability) is still a major factor that affects model generalization. In this\nwork, we propose a novel technique to exploit available data through the use of\nautomatic data augmentation for the tasks of image classification and semantic\nsegmentation. We introduce the first Differentiable Augmentation Search method\n(DAS) to generate variations of images that can be processed as videos.\nCompared to previous approaches, DAS is extremely fast and flexible, allowing\nthe search on very large search spaces in less than a GPU day. Our intuition is\nthat the increased receptive field in the temporal dimension provided by DAS\ncould lead to benefits also to the spatial receptive field. More specifically,\nwe leverage DAS to guide the reshaping of the spatial receptive field by\nselecting task-dependant transformations. As a result, compared to standard\naugmentation alternatives, we improve in terms of accuracy on ImageNet,\nCifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when\nplugging-in our DAS over different light-weight video backbones.\n","authors":["Sofia Casarin","Cynthia I. Ugwu","Sergio Escalera","Oswald Lanz"],"pdf_url":"https://arxiv.org/pdf/2403.15194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14413v2","updated":"2024-03-22T13:15:20Z","published":"2024-03-21T13:59:19Z","title":"Model Uncertainty in Evolutionary Optimization and Bayesian\n  Optimization: A Comparative Analysis","summary":"  Black-box optimization problems, which are common in many real-world\napplications, require optimization through input-output interactions without\naccess to internal workings. This often leads to significant computational\nresources being consumed for simulations. Bayesian Optimization (BO) and\nSurrogate-Assisted Evolutionary Algorithm (SAEA) are two widely used\ngradient-free optimization techniques employed to address such challenges. Both\napproaches follow a similar iterative procedure that relies on surrogate models\nto guide the search process. This paper aims to elucidate the similarities and\ndifferences in the utilization of model uncertainty between these two methods,\nas well as the impact of model inaccuracies on algorithmic performance. A novel\nmodel-assisted strategy is introduced, which utilizes unevaluated solutions to\ngenerate offspring, leveraging the population-based search capabilities of\nevolutionary algorithm to enhance the effectiveness of model-assisted\noptimization. Experimental results demonstrate that the proposed approach\noutperforms mainstream Bayesian optimization algorithms in terms of accuracy\nand efficiency.\n","authors":["Hao Hao","Xiaoqun Zhang","Aimin Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.14413v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15182v1","updated":"2024-03-22T13:11:26Z","published":"2024-03-22T13:11:26Z","title":"PDE-CNNs: Axiomatic Derivations and Applications","summary":"  PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of\ngeometrically meaningful evolution PDEs as substitutes for the conventional\ncomponents in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer\nparameters, inherent equivariance, better performance, data efficiency, and\ngeometric interpretability. In this article we focus on Euclidean equivariant\nPDE-G-CNNs where the feature maps are two dimensional throughout. We call this\nvariant of the framework a PDE-CNN. We list several practically desirable\naxioms and derive from these which PDEs should be used in a PDE-CNN. Here our\napproach to geometric learning via PDEs is inspired by the axioms of classical\nlinear and morphological scale-space theory, which we generalize by introducing\nsemifield-valued signals. Furthermore, we experimentally confirm for small\nnetworks that PDE-CNNs offer fewer parameters, better performance, and data\nefficiency in comparison to CNNs. We also investigate what effect the use of\ndifferent semifields has on the performance of the models.\n","authors":["Gijs Bellaard","Sei Sakata","Bart M. N. Smets","Remco Duits"],"pdf_url":"https://arxiv.org/pdf/2403.15182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15180v1","updated":"2024-03-22T13:09:10Z","published":"2024-03-22T13:09:10Z","title":"Self-Improvement for Neural Combinatorial Optimization: Sample without\n  Replacement, but Improvement","summary":"  Current methods for end-to-end constructive neural combinatorial optimization\nusually train a policy using behavior cloning from expert solutions or policy\ngradient methods from reinforcement learning. While behavior cloning is\nstraightforward, it requires expensive expert solutions, and policy gradient\nmethods are often computationally demanding and complex to fine-tune. In this\nwork, we bridge the two and simplify the training process by sampling multiple\nsolutions for random instances using the current model in each epoch and then\nselecting the best solution as an expert trajectory for supervised imitation\nlearning. To achieve progressively improving solutions with minimal sampling,\nwe introduce a method that combines round-wise Stochastic Beam Search with an\nupdate strategy derived from a provable policy improvement. This strategy\nrefines the policy between rounds by utilizing the advantage of the sampled\nsequences with almost no computational overhead. We evaluate our approach on\nthe Traveling Salesman Problem and the Capacitated Vehicle Routing Problem. The\nmodels trained with our method achieve comparable performance and\ngeneralization to those trained with expert data. Additionally, we apply our\nmethod to the Job Shop Scheduling Problem using a transformer-based\narchitecture and outperform existing state-of-the-art methods by a wide margin.\n","authors":["Jonathan Pirnay","Dominik G. Grimm"],"pdf_url":"https://arxiv.org/pdf/2403.15180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15170v1","updated":"2024-03-22T12:46:58Z","published":"2024-03-22T12:46:58Z","title":"Exploring the Task-agnostic Trait of Self-supervised Learning in the\n  Context of Detecting Mental Disorders","summary":"  Self-supervised learning (SSL) has been investigated to generate\ntask-agnostic representations across various domains. However, such\ninvestigation has not been conducted for detecting multiple mental disorders.\nThe rationale behind the existence of a task-agnostic representation lies in\nthe overlapping symptoms among multiple mental disorders. Consequently, the\nbehavioural data collected for mental health assessment may carry a mixed bag\nof attributes related to multiple disorders. Motivated by that, in this study,\nwe explore a task-agnostic representation derived through SSL in the context of\ndetecting major depressive disorder (MDD) and post-traumatic stress disorder\n(PTSD) using audio and video data collected during interactive sessions. This\nstudy employs SSL models trained by predicting multiple fixed targets or masked\nframes. We propose a list of fixed targets to make the generated representation\nmore efficient for detecting MDD and PTSD. Furthermore, we modify the\nhyper-parameters of the SSL encoder predicting fixed targets to generate global\nrepresentations that capture varying temporal contexts. Both these innovations\nare noted to yield improved detection performances for considered mental\ndisorders and exhibit task-agnostic traits. In the context of the SSL model\npredicting masked frames, the generated global representations are also noted\nto exhibit task-agnostic traits.\n","authors":["Rohan Kumar Gupta","Rohit Sinha"],"pdf_url":"https://arxiv.org/pdf/2403.15170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15167v1","updated":"2024-03-22T12:37:14Z","published":"2024-03-22T12:37:14Z","title":"Transition Graph Properties of Target Class Classification","summary":"  Target class classification is a mixed classification and transition model\nwhose integrated goal is to assign objects to a certain, so called target or\nnormal class. The classification process is iterative, and in each step an\nobject in a certain class undergoes an action attached to that class,\ninitiating the transition of the object to one of the classes. The sequence of\ntransitions, which we call class transitions, must be designed to provide the\nfinal assignment of objects to the target class. The transition process can be\ndescribed in the form of a directed graph, and the success of the final\nclassification is mainly due to the properties of this graph. In our previous\nresearch we showed that the desirable structure of the transition graph is an\noriented rooted tree with orientation towards the root vertex, which\ncorresponds to the normal class. It is clear that the transition graph of an\narbitrary algorithm (policy) may not have this property. In this paper we study\nthe structure of realistic transition graphs, which makes it possible to find\nclassification inconsistencies, helping to transfer it into the desired form.\nThe medical interpretation of dynamic treatment regime considered in the\narticle further clarifies the investigated framework.\n","authors":["Levon Aslanyan","Hasmik Sahakyan"],"pdf_url":"https://arxiv.org/pdf/2403.15167v1.pdf","comment":"14pages, 4 figures"},{"id":"http://arxiv.org/abs/2306.13185v2","updated":"2024-03-22T12:20:30Z","published":"2023-06-22T20:03:05Z","title":"An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression","summary":"  We study the cost of overfitting in noisy kernel ridge regression (KRR),\nwhich we define as the ratio between the test error of the interpolating\nridgeless model and the test error of the optimally-tuned model. We take an\n\"agnostic\" view in the following sense: we consider the cost as a function of\nsample size for any target function, even if the sample size is not large\nenough for consistency or the target is outside the RKHS. We analyze the cost\nof overfitting under a Gaussian universality ansatz using recently derived\n(non-rigorous) risk estimates in terms of the task eigenstructure. Our analysis\nprovides a more refined characterization of benign, tempered and catastrophic\noverfitting (cf. Mallinar et al. 2022).\n","authors":["Lijia Zhou","James B. Simon","Gal Vardi","Nathan Srebro"],"pdf_url":"https://arxiv.org/pdf/2306.13185v2.pdf","comment":"This is the ICLR CR version"},{"id":"http://arxiv.org/abs/2403.15150v1","updated":"2024-03-22T12:06:40Z","published":"2024-03-22T12:06:40Z","title":"An In-Depth Analysis of Data Reduction Methods for Sustainable Deep\n  Learning","summary":"  In recent years, Deep Learning has gained popularity for its ability to solve\ncomplex classification tasks, increasingly delivering better results thanks to\nthe development of more accurate models, the availability of huge volumes of\ndata and the improved computational capabilities of modern computers. However,\nthese improvements in performance also bring efficiency problems, related to\nthe storage of datasets and models, and to the waste of energy and time\ninvolved in both the training and inference processes. In this context, data\nreduction can help reduce energy consumption when training a deep learning\nmodel. In this paper, we present up to eight different methods to reduce the\nsize of a tabular training dataset, and we develop a Python package to apply\nthem. We also introduce a representativeness metric based on topology to\nmeasure how similar are the reduced datasets and the full training dataset.\nAdditionally, we develop a methodology to apply these data reduction methods to\nimage datasets for object detection tasks. Finally, we experimentally compare\nhow these data reduction methods affect the representativeness of the reduced\ndataset, the energy consumption and the predictive performance of the model.\n","authors":["Víctor Toscano-Durán","Javier Perera-Lago","Eduardo Paluzo-Hidalgo","Rocío Gonzalez-Diaz","Miguel Ángel Gutierrez-Naranjo","Matteo Rucco"],"pdf_url":"https://arxiv.org/pdf/2403.15150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16427v3","updated":"2024-03-22T12:05:02Z","published":"2023-12-27T06:23:29Z","title":"Learning to Embed Time Series Patches Independently","summary":"  Masked time series modeling has recently gained much attention as a\nself-supervised representation learning strategy for time series. Inspired by\nmasked image modeling in computer vision, recent works first patchify and\npartially mask out time series, and then train Transformers to capture the\ndependencies between patches by predicting masked patches from unmasked\npatches. However, we argue that capturing such patch dependencies might not be\nan optimal strategy for time series representation learning; rather, learning\nto embed patches independently results in better time series representations.\nSpecifically, we propose to use 1) the simple patch reconstruction task, which\nautoencode each patch without looking at other patches, and 2) the simple\npatch-wise MLP that embeds each patch independently. In addition, we introduce\ncomplementary contrastive learning to hierarchically capture adjacent time\nseries information efficiently. Our proposed method improves time series\nforecasting and classification performance compared to state-of-the-art\nTransformer-based models, while it is more efficient in terms of the number of\nparameters and training/inference time. Code is available at this repository:\nhttps://github.com/seunghan96/pits.\n","authors":["Seunghan Lee","Taeyoung Park","Kibok Lee"],"pdf_url":"https://arxiv.org/pdf/2312.16427v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2312.16424v3","updated":"2024-03-22T12:02:42Z","published":"2023-12-27T06:15:00Z","title":"Soft Contrastive Learning for Time Series","summary":"  Contrastive learning has shown to be effective to learn representations from\ntime series in a self-supervised way. However, contrasting similar time series\ninstances or values from adjacent timestamps within a time series leads to\nignore their inherent correlations, which results in deteriorating the quality\nof learned representations. To address this issue, we propose SoftCLT, a simple\nyet effective soft contrastive learning strategy for time series. This is\nachieved by introducing instance-wise and temporal contrastive loss with soft\nassignments ranging from zero to one. Specifically, we define soft assignments\nfor 1) instance-wise contrastive loss by the distance between time series on\nthe data space, and 2) temporal contrastive loss by the difference of\ntimestamps. SoftCLT is a plug-and-play method for time series contrastive\nlearning that improves the quality of learned representations without bells and\nwhistles. In experiments, we demonstrate that SoftCLT consistently improves the\nperformance in various downstream tasks including classification,\nsemi-supervised learning, transfer learning, and anomaly detection, showing\nstate-of-the-art performance. Code is available at this repository:\nhttps://github.com/seunghan96/softclt.\n","authors":["Seunghan Lee","Taeyoung Park","Kibok Lee"],"pdf_url":"https://arxiv.org/pdf/2312.16424v3.pdf","comment":"ICLR 2024 Spotlight"},{"id":"http://arxiv.org/abs/2403.15146v1","updated":"2024-03-22T11:57:51Z","published":"2024-03-22T11:57:51Z","title":"On the Convergence of Adam under Non-uniform Smoothness: Separability\n  from SGDM and Beyond","summary":"  This paper aims to clearly distinguish between Stochastic Gradient Descent\nwith Momentum (SGDM) and Adam in terms of their convergence rates. We\ndemonstrate that Adam achieves a faster convergence compared to SGDM under the\ncondition of non-uniformly bounded smoothness. Our findings reveal that: (1) in\ndeterministic environments, Adam can attain the known lower bound for the\nconvergence rate of deterministic first-order optimizers, whereas the\nconvergence rate of Gradient Descent with Momentum (GDM) has higher order\ndependence on the initial function value; (2) in stochastic setting, Adam's\nconvergence rate upper bound matches the lower bounds of stochastic first-order\noptimizers, considering both the initial function value and the final error,\nwhereas there are instances where SGDM fails to converge with any learning\nrate. These insights distinctly differentiate Adam and SGDM regarding their\nconvergence rates. Additionally, by introducing a novel stopping-time based\ntechnique, we further prove that if we consider the minimum gradient norm\nduring iterations, the corresponding convergence rate can match the lower\nbounds across all problem hyperparameters. The technique can also help proving\nthat Adam with a specific hyperparameter scheduler is parameter-agnostic, which\nhence can be of independent interest.\n","authors":["Bohan Wang","Huishuai Zhang","Qi Meng","Ruoyu Sun","Zhi-Ming Ma","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2403.15146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11220v3","updated":"2024-03-22T11:42:40Z","published":"2024-03-17T13:43:10Z","title":"CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object\n  Detection under Unknown Degradations","summary":"  Object detection methods under known single degradations have been\nextensively investigated. However, existing approaches require prior knowledge\nof the degradation type and train a separate model for each, limiting their\npractical applications in unpredictable environments. To address this\nchallenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer,\nCPA-Enhancer, for object detection under unknown degradations. Specifically,\nCPA-Enhancer progressively adapts its enhancement strategy under the\nstep-by-step guidance of CoT prompts, that encode degradation-related\ninformation. To the best of our knowledge, it's the first work that exploits\nCoT prompting for object detection tasks. Overall, CPA-Enhancer is a\nplug-and-play enhancement model that can be integrated into any generic\ndetectors to achieve substantial gains on degraded images, without knowing the\ndegradation type priorly. Experimental results demonstrate that CPA-Enhancer\nnot only sets the new state of the art for object detection but also boosts the\nperformance of other downstream vision tasks under unknown degradations.\n","authors":["Yuwei Zhang","Yan Wu","Yanming Liu","Xinyue Peng"],"pdf_url":"https://arxiv.org/pdf/2403.11220v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15123v1","updated":"2024-03-22T11:25:38Z","published":"2024-03-22T11:25:38Z","title":"Quantification using Permutation-Invariant Networks based on Histograms","summary":"  Quantification, also known as class prevalence estimation, is the supervised\nlearning task in which a model is trained to predict the prevalence of each\nclass in a given bag of examples. This paper investigates the application of\ndeep neural networks to tasks of quantification in scenarios where it is\npossible to apply a symmetric supervised approach that eliminates the need for\nclassification as an intermediary step, directly addressing the quantification\nproblem. Additionally, it discusses existing permutation-invariant layers\ndesigned for set processing and assesses their suitability for quantification.\nIn light of our analysis, we propose HistNetQ, a novel neural architecture that\nrelies on a permutation-invariant representation based on histograms that is\nspecially suited for quantification problems. Our experiments carried out in\nthe only quantification competition held to date, show that HistNetQ\noutperforms other deep neural architectures devised for set processing, as well\nas the state-of-the-art quantification methods. Furthermore, HistNetQ offers\ntwo significant advantages over traditional quantification methods: i) it does\nnot require the labels of the training examples but only the prevalence values\nof a collection of training bags, making it applicable to new scenarios; and\nii) it is able to optimize any custom quantification-oriented loss function.\n","authors":["Olaya Pérez-Mon","Alejandro Moreo","Juan José del Coz","Pablo González"],"pdf_url":"https://arxiv.org/pdf/2403.15123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15112v1","updated":"2024-03-22T11:08:48Z","published":"2024-03-22T11:08:48Z","title":"Text clustering with LLM embeddings","summary":"  Text clustering is an important approach for organising the growing amount of\ndigital content, helping to structure and find hidden patterns in uncategorised\ndata. In this research, we investigated how different textual embeddings -\nparticularly those used in large language models (LLMs) - and clustering\nalgorithms affect how text datasets are clustered. A series of experiments were\nconducted to assess how embeddings influence clustering results, the role\nplayed by dimensionality reduction through summarisation, and embedding size\nadjustment. Results reveal that LLM embeddings excel at capturing the nuances\nof structured language, while BERT leads the lightweight options in\nperformance. In addition, we find that increasing embedding dimensionality and\nsummarisation techniques do not uniformly improve clustering efficiency,\nsuggesting that these strategies require careful analysis to use in real-life\nmodels. These results highlight a complex balance between the need for nuanced\ntext representation and computational feasibility in text clustering\napplications. This study extends traditional text clustering frameworks by\nincorporating embeddings from LLMs, thereby paving the way for improved\nmethodologies and opening new avenues for future research in various types of\ntextual analysis.\n","authors":["Alina Petukhova","Joao P. Matos-Carvalho","Nuno Fachada"],"pdf_url":"https://arxiv.org/pdf/2403.15112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13869v2","updated":"2024-03-22T10:59:56Z","published":"2024-03-20T14:00:29Z","title":"Accurately Predicting Probabilities of Safety-Critical Rare Events for\n  Intelligent Systems","summary":"  Intelligent systems are increasingly integral to our daily lives, yet rare\nsafety-critical events present significant latent threats to their practical\ndeployment. Addressing this challenge hinges on accurately predicting the\nprobability of safety-critical events occurring within a given time step from\nthe current state, a metric we define as 'criticality'. The complexity of\npredicting criticality arises from the extreme data imbalance caused by rare\nevents in high dimensional variables associated with the rare events, a\nchallenge we refer to as the curse of rarity. Existing methods tend to be\neither overly conservative or prone to overlooking safety-critical events, thus\nstruggling to achieve both high precision and recall rates, which severely\nlimits their applicability. This study endeavors to develop a criticality\nprediction model that excels in both precision and recall rates for evaluating\nthe criticality of safety-critical autonomous systems. We propose a multi-stage\nlearning framework designed to progressively densify the dataset, mitigating\nthe curse of rarity across stages. To validate our approach, we evaluate it in\ntwo cases: lunar lander and bipedal walker scenarios. The results demonstrate\nthat our method surpasses traditional approaches, providing a more accurate and\ndependable assessment of criticality in intelligent systems.\n","authors":["Ruoxuan Bai","Jingxuan Yang","Weiduo Gong","Yi Zhang","Qiujing Lu","Shuo Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.06015v4","updated":"2024-03-22T10:59:43Z","published":"2022-10-12T08:39:35Z","title":"EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural\n  Architecture Search","summary":"  Energy consumption from the selection, training, and deployment of deep\nlearning models has seen a significant uptick recently. This work aims to\nfacilitate the design of energy-efficient deep learning models that require\nless computational resources and prioritize environmental sustainability by\nfocusing on the energy consumption. Neural architecture search (NAS) benefits\nfrom tabular benchmarks, which evaluate NAS strategies cost-effectively through\nprecomputed performance statistics. We advocate for including energy efficiency\nas an additional performance criterion in NAS. To this end, we introduce an\nenhanced tabular benchmark encompassing data on energy consumption for varied\narchitectures. The benchmark, designated as EC-NAS, has been made available in\nan open-source format to advance research in energy-conscious NAS. EC-NAS\nincorporates a surrogate model to predict energy consumption, aiding in\ndiminishing the energy expenditure of the dataset creation. Our findings\nemphasize the potential of EC-NAS by leveraging multi-objective optimization\nalgorithms, revealing a balance between energy usage and accuracy. This\nsuggests the feasibility of identifying energy-lean architectures with little\nor no compromise in performance.\n","authors":["Pedram Bakhtiarifard","Christian Igel","Raghavendra Selvan"],"pdf_url":"https://arxiv.org/pdf/2210.06015v4.pdf","comment":"Accepted to be presented at the International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP-2024). Source code at\n  https://github.com/saintslab/EC-NAS-Bench"},{"id":"http://arxiv.org/abs/2403.15108v1","updated":"2024-03-22T10:51:55Z","published":"2024-03-22T10:51:55Z","title":"Active Learning for Regression based on Wasserstein distance and\n  GroupSort Neural Networks","summary":"  This paper addresses a new active learning strategy for regression problems.\nThe presented Wasserstein active regression model is based on the principles of\ndistribution-matching to measure the representativeness of the labeled dataset.\nThe Wasserstein distance is computed using GroupSort Neural Networks. The use\nof such networks provides theoretical foundations giving a way to quantify\nerrors with explicit bounds for their size and depth. This solution is combined\nwith another uncertainty-based approach that is more outlier-tolerant to\ncomplete the query strategy. Finally, this method is compared with other\nclassical and recent solutions. The study empirically shows the pertinence of\nsuch a representativity-uncertainty approach, which provides good estimation\nall along the query procedure. Moreover, the Wasserstein active regression\noften achieves more precise estimations and tends to improve accuracy faster\nthan other models.\n","authors":["Benjamin Bobbia","Matthias Picard"],"pdf_url":"https://arxiv.org/pdf/2403.15108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15103v1","updated":"2024-03-22T10:42:25Z","published":"2024-03-22T10:42:25Z","title":"Improving cross-domain brain tissue segmentation in fetal MRI with\n  synthetic data","summary":"  Segmentation of fetal brain tissue from magnetic resonance imaging (MRI)\nplays a crucial role in the study of in utero neurodevelopment. However,\nautomated tools face substantial domain shift challenges as they must be robust\nto highly heterogeneous clinical data, often limited in numbers and lacking\nannotations. Indeed, high variability of the fetal brain morphology, MRI\nacquisition parameters, and superresolution reconstruction (SR) algorithms\nadversely affect the model's performance when evaluated out-of-domain. In this\nwork, we introduce FetalSynthSeg, a domain randomization method to segment\nfetal brain MRI, inspired by SynthSeg. Our results show that models trained\nsolely on synthetic data outperform models trained on real data in out-ofdomain\nsettings, validated on a 120-subject cross-domain dataset. Furthermore, we\nextend our evaluation to 40 subjects acquired using lowfield (0.55T) MRI and\nreconstructed with novel SR models, showcasing robustness across different\nmagnetic field strengths and SR algorithms. Leveraging a generative synthetic\napproach, we tackle the domain shift problem in fetal brain MRI and offer\ncompelling prospects for applications in fields with limited and highly\nheterogeneous data.\n","authors":["Vladyslav Zalevskyi","Thomas Sanchez","Margaux Roulet","Jordina Aviles Verddera","Jana Hutter","Hamza Kebiri","Meritxell Bach Cuadra"],"pdf_url":"https://arxiv.org/pdf/2403.15103v1.pdf","comment":"10 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.11522v2","updated":"2024-03-22T10:28:05Z","published":"2024-03-18T07:22:31Z","title":"LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers","summary":"  While polyhedral compilers have shown success in implementing advanced code\ntransformations, they still have challenges in selecting the most profitable\ntransformations that lead to the best speedups. This has motivated the use of\nmachine learning to build cost models to guide the search for polyhedral\noptimizations. State-of-the-art polyhedral compilers have demonstrated a viable\nproof-of-concept of this approach. While such a proof-of-concept has shown\npromise, it still has significant limitations. State-of-the-art polyhedral\ncompilers that use a deep-learning cost model only support a small subset of\naffine transformations, limiting their ability to apply complex code\ntransformations. They also only support simple programs that have a single loop\nnest and a rectangular iteration domain, limiting their applicability to many\nprograms. These limitations significantly impact the generality of such\ncompilers and autoschedulers and put into question the whole approach. In this\npaper, we introduce LOOPer, the first polyhedral autoscheduler that uses a\ndeep-learning based cost model and covers a large set of affine transformations\nand programs. It supports the exploration of a large set of affine\ntransformations, allowing the application of complex sequences of polyhedral\ntransformations. It also supports the optimization of programs with multiple\nloop nests and with rectangular and non-rectangular iteration domains, allowing\nthe optimization of an extensive set of programs. We implement and evaluate\nLOOPer and show that it achieves speedups over the state-of-the-art. On the\nPolybench benchmark, LOOPer achieves a geometric mean speedup of 1.59x over\nTiramisu. LOOPer also achieves competitive speedups with a geometric mean\nspeedup of 1.34x over Pluto, a state-of-the-art polyhedral compiler that does\nnot use a machine-learning based cost model.\n","authors":["Massinissa Merouani","Khaled Afif Boudaoud","Iheb Nassim Aouadj","Nassim Tchoulak","Islem Kara Bernou","Hamza Benyamina","Fatima Benbouzid-Si Tayeb","Karima Benatchba","Hugh Leather","Riyadh Baghdadi"],"pdf_url":"https://arxiv.org/pdf/2403.11522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15095v1","updated":"2024-03-22T10:23:48Z","published":"2024-03-22T10:23:48Z","title":"End-to-End Mineral Exploration with Artificial Intelligence and Ambient\n  Noise Tomography","summary":"  This paper presents an innovative end-to-end workflow for mineral\nexploration, integrating ambient noise tomography (ANT) and artificial\nintelligence (AI) to enhance the discovery and delineation of mineral resources\nessential for the global transition to a low carbon economy. We focus on copper\nas a critical element, required in significant quantities for renewable energy\nsolutions. We show the benefits of utilising ANT, characterised by its speed,\nscalability, depth penetration, resolution, and low environmental impact,\nalongside artificial intelligence (AI) techniques to refine a continent-scale\nprospectivity model at the deposit scale by fine-tuning our model on local\nhigh-resolution data. We show the promise of the method by first presenting a\nnew data-driven AI prospectivity model for copper within Australia, which\nserves as our foundation model for further fine-tuning. We then focus on the\nHillside IOCG deposit on the prospective Yorke Peninsula. We show that with\nrelatively few local training samples (orebody intercepts), we can fine tune\nthe foundation model to provide a good estimate of the Hillside orebody\noutline. Our methodology demonstrates how AI can augment geophysical data\ninterpretation, providing a novel approach to mineral exploration with improved\ndecision-making capabilities for targeting mineralization, thereby addressing\nthe urgent need for increased mineral resource discovery.\n","authors":["Jack Muir","Gerrit Olivier","Anthony Reid"],"pdf_url":"https://arxiv.org/pdf/2403.15095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15091v1","updated":"2024-03-22T10:20:09Z","published":"2024-03-22T10:20:09Z","title":"Improved Long Short-Term Memory-based Wastewater Treatment Simulators\n  for Deep Reinforcement Learning","summary":"  Even though Deep Reinforcement Learning (DRL) showed outstanding results in\nthe fields of Robotics and Games, it is still challenging to implement it in\nthe optimization of industrial processes like wastewater treatment. One of the\nchallenges is the lack of a simulation environment that will represent the\nactual plant as accurately as possible to train DRL policies. Stochasticity and\nnon-linearity of wastewater treatment data lead to unstable and incorrect\npredictions of models over long time horizons. One possible reason for the\nmodels' incorrect simulation behavior can be related to the issue of\ncompounding error, which is the accumulation of errors throughout the\nsimulation. The compounding error occurs because the model utilizes its\npredictions as inputs at each time step. The error between the actual data and\nthe prediction accumulates as the simulation continues. We implemented two\nmethods to improve the trained models for wastewater treatment data, which\nresulted in more accurate simulators: 1- Using the model's prediction data as\ninput in the training step as a tool of correction, and 2- Change in the loss\nfunction to consider the long-term predicted shape (dynamics). The experimental\nresults showed that implementing these methods can improve the behavior of\nsimulators in terms of Dynamic Time Warping throughout a year up to 98%\ncompared to the base model. These improvements demonstrate significant promise\nin creating simulators for biological processes that do not need pre-existing\nknowledge of the process but instead depend exclusively on time series data\nobtained from the system.\n","authors":["Esmaeel Mohammadi","Daniel Ortiz-Arroyo","Mikkel Stokholm-Bjerregaard","Aviaja Anna Hansen","Petar Durdevic"],"pdf_url":"https://arxiv.org/pdf/2403.15091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15083v1","updated":"2024-03-22T10:06:42Z","published":"2024-03-22T10:06:42Z","title":"SIMAP: A simplicial-map layer for neural networks","summary":"  In this paper, we present SIMAP, a novel layer integrated into deep learning\nmodels, aimed at enhancing the interpretability of the output. The SIMAP layer\nis an enhanced version of Simplicial-Map Neural Networks (SMNNs), an\nexplainable neural network based on support sets and simplicial maps (functions\nused in topology to transform shapes while preserving their structural\nconnectivity). The novelty of the methodology proposed in this paper is\ntwo-fold: Firstly, SIMAP layers work in combination with other deep learning\narchitectures as an interpretable layer substituting classic dense final\nlayers. Secondly, unlike SMNNs, the support set is based on a fixed maximal\nsimplex, the barycentric subdivision being efficiently computed with a\nmatrix-based multiplication algorithm.\n","authors":["Rocio Gonzalez-Diaz","Miguel A. Gutiérrez-Naranjo","Eduardo Paluzo-Hidalgo"],"pdf_url":"https://arxiv.org/pdf/2403.15083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15079v1","updated":"2024-03-22T10:05:21Z","published":"2024-03-22T10:05:21Z","title":"Automated Feature Selection for Inverse Reinforcement Learning","summary":"  Inverse reinforcement learning (IRL) is an imitation learning approach to\nlearning reward functions from expert demonstrations. Its use avoids the\ndifficult and tedious procedure of manual reward specification while retaining\nthe generalization power of reinforcement learning. In IRL, the reward is\nusually represented as a linear combination of features. In continuous state\nspaces, the state variables alone are not sufficiently rich to be used as\nfeatures, but which features are good is not known in general. To address this\nissue, we propose a method that employs polynomial basis functions to form a\ncandidate set of features, which are shown to allow the matching of statistical\nmoments of state distributions. Feature selection is then performed for the\ncandidates by leveraging the correlation between trajectory probabilities and\nfeature expectations. We demonstrate the approach's effectiveness by recovering\nreward functions that capture expert policies across non-linear control tasks\nof increasing complexity. Code, data, and videos are available at\nhttps://sites.google.com/view/feature4irl.\n","authors":["Daulet Baimukashev","Gokhan Alcan","Ville Kyrki"],"pdf_url":"https://arxiv.org/pdf/2403.15079v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.15077v1","updated":"2024-03-22T10:02:13Z","published":"2024-03-22T10:02:13Z","title":"GTAGCN: Generalized Topology Adaptive Graph Convolutional Networks","summary":"  Graph Neural Networks (GNN) have emerged as a popular and standard approach\nfor learning from graph-structured data. The literature on GNN highlights the\npotential of this evolving research area and its widespread adoption in\nreal-life applications. However, most of the approaches are either new in\nconcept or derived from specific techniques. Therefore, the potential of more\nthan one approach in hybrid form has not been studied extensively, which can be\nwell utilized for sequenced data or static data together. We derive a hybrid\napproach based on two established techniques as generalized aggregation\nnetworks and topology adaptive graph convolution networks that solve our\npurpose to apply on both types of sequenced and static nature of data,\neffectively. The proposed method applies to both node and graph classification.\nOur empirical analysis reveals that the results are at par with literature\nresults and better for handwritten strokes as sequenced data, where graph\nstructures have not been explored.\n","authors":["Sukhdeep Singh","Anuj Sharma","Vinod Kumar Chauhan"],"pdf_url":"https://arxiv.org/pdf/2403.15077v1.pdf","comment":"2 figures, 3 tables and 26 pages"},{"id":"http://arxiv.org/abs/2403.15073v1","updated":"2024-03-22T09:54:04Z","published":"2024-03-22T09:54:04Z","title":"On the Inclusion of Charge and Spin States in Cartesian Tensor Neural\n  Network Potentials","summary":"  In this letter, we present an extension to TensorNet, a state-of-the-art\nequivariant Cartesian tensor neural network potential, allowing it to handle\ncharged molecules and spin states without architectural changes or increased\ncosts. By incorporating these attributes, we address input degeneracy issues,\nenhancing the model's predictive accuracy across diverse chemical systems. This\nadvancement significantly broadens TensorNet's applicability, maintaining its\nefficiency and accuracy.\n","authors":["Guillem Simeon","Antonio Mirarchi","Raul P. Pelaez","Raimondas Galvelis","Gianni De Fabritiis"],"pdf_url":"https://arxiv.org/pdf/2403.15073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.01368v3","updated":"2024-03-22T09:42:26Z","published":"2022-04-04T10:28:11Z","title":"Training Fully Connected Neural Networks is $\\exists\\mathbb{R}$-Complete","summary":"  We consider the problem of finding weights and biases for a two-layer fully\nconnected neural network to fit a given set of data points as well as possible,\nalso known as EmpiricalRiskMinimization. Our main result is that the associated\ndecision problem is $\\exists\\mathbb{R}$-complete, that is, polynomial-time\nequivalent to determining whether a multivariate polynomial with integer\ncoefficients has any real roots. Furthermore, we prove that algebraic numbers\nof arbitrarily large degree are required as weights to be able to train some\ninstances to optimality, even if all data points are rational. Our result\nalready applies to fully connected instances with two inputs, two outputs, and\none hidden layer of ReLU neurons. Thereby, we strengthen a result by\nAbrahamsen, Kleist and Miltzow [NeurIPS 2021]. A consequence of this is that a\ncombinatorial search algorithm like the one by Arora, Basu, Mianjy and\nMukherjee [ICLR 2018] is impossible for networks with more than one output\ndimension, unless $\\mathsf{NP}=\\exists\\mathbb{R}$.\n","authors":["Daniel Bertschinger","Christoph Hertrich","Paul Jungeblut","Tillmann Miltzow","Simon Weber"],"pdf_url":"https://arxiv.org/pdf/2204.01368v3.pdf","comment":"39 pages, 17 figures. Changes in version 2: Added algebraic\n  universality result, improved interpretation of results Changes in version 3:\n  Improved exposition by formalizing properties of gadgets"},{"id":"http://arxiv.org/abs/2203.05222v2","updated":"2024-03-22T09:38:29Z","published":"2022-03-10T08:02:03Z","title":"Similarity-based Label Inference Attack against Training and Inference\n  of Split Learning","summary":"  Split learning is a promising paradigm for privacy-preserving distributed\nlearning. The learning model can be cut into multiple portions to be\ncollaboratively trained at the participants by exchanging only the intermediate\nresults at the cut layer. Understanding the security performance of split\nlearning is critical for many privacy-sensitive applications. This paper shows\nthat the exchanged intermediate results, including the smashed data (i.e.,\nextracted features from the raw data) and gradients during training and\ninference of split learning, can already reveal the private labels. We\nmathematically analyze the potential label leakages and propose the cosine and\nEuclidean similarity measurements for gradients and smashed data, respectively.\nThen, the two similarity measurements are shown to be unified in Euclidean\nspace. Based on the similarity metric, we design three label inference attacks\nto efficiently recover the private labels during both the training and\ninference phases. Experimental results validate that the proposed approaches\ncan achieve close to 100% accuracy of label attacks. The proposed attack can\nstill achieve accurate predictions against various state-of-the-art defense\nmechanisms, including DP-SGD, label differential privacy, gradient compression,\nand Marvell.\n","authors":["Junlin Liu","Xinchen Lyu","Qimei Cui","Xiaofeng Tao"],"pdf_url":"https://arxiv.org/pdf/2203.05222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14033v2","updated":"2024-03-22T09:35:13Z","published":"2023-11-23T14:38:10Z","title":"Multivariate Scenario Generation of Day-Ahead Electricity Prices using\n  Normalizing Flows","summary":"  Trading on the day-ahead electricity markets requires accurate information\nabout the realization of electricity prices and the uncertainty attached to the\npredictions. Deriving accurate forecasting models presents a difficult task due\nto the day-ahead price's non-stationarity resulting from changing market\nconditions, e.g., due to changes resulting from the energy crisis in 2021. We\npresent a probabilistic forecasting approach for day-ahead electricity prices\nusing the fully data-driven deep generative model called normalizing flow. Our\nmodeling approach generates full-day scenarios of day-ahead electricity prices\nbased on conditional features such as residual load forecasts. Furthermore, we\npropose extended feature sets of prior realizations and a periodic retraining\nscheme that allows the normalizing flow to adapt to the changing conditions of\nmodern electricity markets. Our results highlight that the normalizing flow\ngenerates high-quality scenarios that reproduce the true price distribution and\nyield accurate forecasts. Additionally, our analysis highlights how our\nimprovements towards adaptations in changing regimes allow the normalizing flow\nto adapt to changing market conditions and enable continued sampling of\nhigh-quality day-ahead price scenarios.\n","authors":["Hannes Hilger","Dirk Witthaut","Manuel Dahmen","Leonardo Rydin Gorjao","Julius Trebbien","Eike Cramer"],"pdf_url":"https://arxiv.org/pdf/2311.14033v2.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2302.05614v3","updated":"2024-03-22T09:34:11Z","published":"2023-02-11T06:32:28Z","title":"Cross-domain Random Pre-training with Prototypes for Reinforcement\n  Learning","summary":"  This work has been submitted to the IEEE for possible publication. Copyright\nmay be transferred without notice, after which this version may no longer be\naccessible. Unsupervised cross-domain Reinforcement Learning (RL) pre-training\nshows great potential for challenging continuous visual control but poses a big\nchallenge. In this paper, we propose \\textbf{C}ross-domain \\textbf{R}andom\n\\textbf{P}re-\\textbf{T}raining with \\textbf{pro}totypes (CRPTpro), a novel,\nefficient, and effective self-supervised cross-domain RL pre-training\nframework. CRPTpro decouples data sampling from encoder pre-training, proposing\ndecoupled random collection to easily and quickly generate a qualified\ncross-domain pre-training dataset. Moreover, a novel prototypical\nself-supervised algorithm is proposed to pre-train an effective visual encoder\nthat is generic across different domains. Without finetuning, the cross-domain\nencoder can be implemented for challenging downstream tasks defined in\ndifferent domains, either seen or unseen. Compared with recent advanced\nmethods, CRPTpro achieves better performance on downstream policy learning\nwithout extra training on exploration agents for data collection, greatly\nreducing the burden of pre-training. We conduct extensive experiments across\neight challenging continuous visual-control domains, including balance control,\nrobot locomotion, and manipulation. CRPTpro significantly outperforms the next\nbest Proto-RL(C) on 11/12 cross-domain downstream tasks with only 54\\%\nwall-clock pre-training time, exhibiting state-of-the-art pre-training\nperformance with greatly improved pre-training efficiency. The complete code is\navailable at https://github.com/liuxin0824/CRPTpro.\n","authors":["Xin Liu","Yaran Chen","Haoran Li","Boyu Li","Dongbin Zhao"],"pdf_url":"https://arxiv.org/pdf/2302.05614v3.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2302.05440v2","updated":"2024-03-22T09:31:26Z","published":"2023-02-10T18:56:53Z","title":"Forward Learning with Top-Down Feedback: Empirical and Analytical\n  Characterization","summary":"  \"Forward-only\" algorithms, which train neural networks while avoiding a\nbackward pass, have recently gained attention as a way of solving the\nbiologically unrealistic aspects of backpropagation. Here, we first address\ncompelling challenges related to the \"forward-only\" rules, which include\nreducing the performance gap with backpropagation and providing an analytical\nunderstanding of their dynamics. To this end, we show that the forward-only\nalgorithm with top-down feedback is well-approximated by an\n\"adaptive-feedback-alignment\" algorithm, and we analytically track its\nperformance during learning in a prototype high-dimensional setting. Then, we\ncompare different versions of forward-only algorithms, focusing on the\nForward-Forward and PEPITA frameworks, and we show that they share the same\nlearning principles. Overall, our work unveils the connections between three\nkey neuro-inspired learning rules, providing a link between \"forward-only\"\nalgorithms, i.e., Forward-Forward and PEPITA, and an approximation of\nbackpropagation, i.e., Feedback Alignment.\n","authors":["Ravi Srinivasan","Francesca Mignacco","Martino Sorbaro","Maria Refinetti","Avi Cooper","Gabriel Kreiman","Giorgia Dellaferrera"],"pdf_url":"https://arxiv.org/pdf/2302.05440v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15587v2","updated":"2024-03-22T09:28:02Z","published":"2023-11-27T07:25:47Z","title":"Quantum Langevin Dynamics for Optimization","summary":"  We initiate the study of utilizing Quantum Langevin Dynamics (QLD) to solve\noptimization problems, particularly those non-convex objective functions that\npresent substantial obstacles for traditional gradient descent algorithms.\nSpecifically, we examine the dynamics of a system coupled with an infinite heat\nbath. This interaction induces both random quantum noise and a deterministic\ndamping effect to the system, which nudge the system towards a steady state\nthat hovers near the global minimum of objective functions. We theoretically\nprove the convergence of QLD in convex landscapes, demonstrating that the\naverage energy of the system can approach zero in the low temperature limit\nwith an exponential decay rate correlated with the evolution time. Numerically,\nwe first show the energy dissipation capability of QLD by retracing its origins\nto spontaneous emission. Furthermore, we conduct detailed discussion of the\nimpact of each parameter. Finally, based on the observations when comparing QLD\nwith classical Fokker-Plank-Smoluchowski equation, we propose a time-dependent\nQLD by making temperature and $\\hbar$ time-dependent parameters, which can be\ntheoretically proven to converge better than the time-independent case and also\noutperforms a series of state-of-the-art quantum and classical optimization\nalgorithms in many non-convex landscapes.\n","authors":["Zherui Chen","Yuchen Lu","Hao Wang","Yizhou Liu","Tongyang Li"],"pdf_url":"https://arxiv.org/pdf/2311.15587v2.pdf","comment":"52 pages, 1 table, 25 figures"},{"id":"http://arxiv.org/abs/2310.15929v2","updated":"2024-03-22T09:18:24Z","published":"2023-10-24T15:27:15Z","title":"E-Sparse: Boosting the Large Language Model Inference through\n  Entropy-based N:M Sparsity","summary":"  Traditional pruning methods are known to be challenging to work in Large\nLanguage Models (LLMs) for Generative AI because of their unaffordable training\nprocess and large computational demands. For the first time, we introduce the\ninformation entropy of hidden state features into a pruning metric design,\nnamely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse\nemploys the information richness to leverage the channel importance, and\nfurther incorporates several novel techniques to put it into effect: (1) it\nintroduces information entropy to enhance the significance of parameter weights\nand input feature norms as a novel pruning metric, and performs N:M sparsity\nwithout modifying the remaining weights. (2) it designs global naive shuffle\nand local block shuffle to quickly optimize the information distribution and\nadequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is\nimplemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA Ampere\nGPUs. Extensive experiments on the LLaMA family and OPT models show that\nE-Sparse can significantly speed up the model inference over the dense model\n(up to 1.53X) and obtain significant memory saving (up to 43.52%), with\nacceptable accuracy loss.\n","authors":["Yun Li","Lin Niu","Xipeng Zhang","Kai Liu","Jianchen Zhu","Zhanhui Kang"],"pdf_url":"https://arxiv.org/pdf/2310.15929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02733v3","updated":"2024-03-22T09:17:24Z","published":"2024-02-05T05:25:33Z","title":"ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer","summary":"  Face re-aging is a prominent field in computer vision and graphics, with\nsignificant applications in photorealistic domains such as movies, advertising,\nand live streaming. Recently, the need to apply face re-aging to\nnon-photorealistic images, like comics, illustrations, and animations, has\nemerged as an extension in various entertainment sectors. However, the lack of\na network that can seamlessly edit the apparent age in NPR images has limited\nthese tasks to a naive, sequential approach. This often results in unpleasant\nartifacts and a loss of facial attributes due to domain discrepancies. In this\npaper, we introduce a novel one-stage method for face re-aging combined with\nportrait style transfer, executed in a single generative step. We leverage\nexisting face re-aging and style transfer networks, both trained within the\nsame PR domain. Our method uniquely fuses distinct latent vectors, each\nresponsible for managing aging-related attributes and NPR appearance. By\nadopting an exemplar-based approach, our method offers greater flexibility\ncompared to domain-level fine-tuning approaches, which typically require\nseparate training or fine-tuning for each domain. This effectively addresses\nthe limitation of requiring paired datasets for re-aging and domain-level,\ndata-driven approaches for stylization. Our experiments show that our model can\neffortlessly generate re-aged images while simultaneously transferring the\nstyle of examples, maintaining both natural appearance and controllability.\n","authors":["Bumsoo Kim","Abdul Muqeet","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2402.02733v3.pdf","comment":"14 pages, 15 figures, 1 table"},{"id":"http://arxiv.org/abs/2203.16155v3","updated":"2024-03-22T09:14:22Z","published":"2022-03-30T08:56:14Z","title":"BBE-LSWCM: A Bootstrapped Ensemble of Long and Short Window Clickstream\n  Models","summary":"  We consider the problem of developing a clickstream modeling framework for\nreal-time customer event prediction problems in SaaS products like QBO. We\ndevelop a low-latency, cost-effective, and robust ensemble architecture\n(BBE-LSWCM), which combines both aggregated user behavior data from a longer\nhistorical window (e.g., over the last few weeks) as well as user activities\nover a short window in recent-past (e.g., in the current session). As compared\nto other baseline approaches, we demonstrate the superior performance of the\nproposed method for two important real-time event prediction problems:\nsubscription cancellation and intended task detection for QBO subscribers.\nFinally, we present details of the live deployment and results from online\nexperiments in QBO.\n","authors":["Arnab Chakraborty","Vikas Raturi","Shrutendra Harsola"],"pdf_url":"https://arxiv.org/pdf/2203.16155v3.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2403.15048v1","updated":"2024-03-22T09:13:09Z","published":"2024-03-22T09:13:09Z","title":"Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning","summary":"  Large-scale Text-to-Image (TTI) models have become a common approach for\ngenerating training data in various generative fields. However, visual\nhallucinations, which contain perceptually critical defects, remain a concern,\nespecially in non-photorealistic styles like cartoon characters. We propose a\nnovel visual hallucination detection system for cartoon character images\ngenerated by TTI models. Our approach leverages pose-aware in-context visual\nlearning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB\nimages and pose information. By incorporating pose guidance from a fine-tuned\npose estimator, we enable VLMs to make more accurate decisions. Experimental\nresults demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images. This\nresearch advances TTI models by mitigating visual hallucinations, expanding\ntheir potential in non-photorealistic domains.\n","authors":["Bumsoo Kim","Wonseop Shin","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2403.15048v1.pdf","comment":"11 pages, 12 figures, 1 table, Project page:\n  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/"},{"id":"http://arxiv.org/abs/2402.15220v2","updated":"2024-03-22T09:03:10Z","published":"2024-02-23T09:29:19Z","title":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition","summary":"  Self-attention is an essential component of large language models(LLMs) but a\nsignificant source of inference latency for long sequences. In multi-tenant\nLLMs serving scenarios, the compute and memory operation cost of self-attention\ncan be optimized by using the probability that multiple LLM requests have\nshared system prompts in prefixes. In this paper, we introduce ChunkAttention,\na prefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the start-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.\n","authors":["Lu Ye","Ze Tao","Yong Huang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2402.15220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15045v1","updated":"2024-03-22T09:02:12Z","published":"2024-03-22T09:02:12Z","title":"DP-Dueling: Learning from Preference Feedback without Compromising User\n  Privacy","summary":"  We consider the well-studied dueling bandit problem, where a learner aims to\nidentify near-optimal actions using pairwise comparisons, under the constraint\nof differential privacy. We consider a general class of utility-based\npreference matrices for large (potentially unbounded) decision spaces and give\nthe first differentially private dueling bandit algorithm for active learning\nwith user preferences. Our proposed algorithms are computationally efficient\nwith near-optimal performance, both in terms of the private and non-private\nregret bound. More precisely, we show that when the decision space is of finite\nsize $K$, our proposed algorithm yields order optimal $O\\Big(\\sum_{i =\n2}^K\\log\\frac{KT}{\\Delta_i} + \\frac{K}{\\epsilon}\\Big)$ regret bound for pure\n$\\epsilon$-DP, where $\\Delta_i$ denotes the suboptimality gap of the $i$-th\narm. We also present a matching lower bound analysis which proves the\noptimality of our algorithms. Finally, we extend our results to any general\ndecision space in $d$-dimensions with potentially infinite arms and design an\n$\\epsilon$-DP algorithm with regret $\\tilde{O} \\left( \\frac{d^6}{\\kappa\n\\epsilon } + \\frac{ d\\sqrt{T }}{\\kappa} \\right)$, providing privacy for free\nwhen $T \\gg d$.\n","authors":["Aadirupa Saha","Hilal Asi"],"pdf_url":"https://arxiv.org/pdf/2403.15045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15038v1","updated":"2024-03-22T08:42:41Z","published":"2024-03-22T08:42:41Z","title":"Estimation of multiple mean vectors in high dimension","summary":"  We endeavour to estimate numerous multi-dimensional means of various\nprobability distributions on a common space based on independent samples. Our\napproach involves forming estimators through convex combinations of empirical\nmeans derived from these samples. We introduce two strategies to find\nappropriate data-dependent convex combination weights: a first one employing a\ntesting procedure to identify neighbouring means with low variance, which\nresults in a closed-form plug-in formula for the weights, and a second one\ndetermining weights via minimization of an upper confidence bound on the\nquadratic risk.Through theoretical analysis, we evaluate the improvement in\nquadratic risk offered by our methods compared to the empirical means. Our\nanalysis focuses on a dimensional asymptotics perspective, showing that our\nmethods asymptotically approach an oracle (minimax) improvement as the\neffective dimension of the data increases.We demonstrate the efficacy of our\nmethods in estimating multiple kernel mean embeddings through experiments on\nboth simulated and real-world datasets.\n","authors":["Gilles Blanchard","Jean-Baptiste Fermanian","Hannah Marienwald"],"pdf_url":"https://arxiv.org/pdf/2403.15038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11838v6","updated":"2024-03-22T08:42:14Z","published":"2023-08-23T00:10:29Z","title":"A Benchmark Study on Calibration","summary":"  Deep neural networks are increasingly utilized in various machine learning\ntasks. However, as these models grow in complexity, they often face calibration\nissues, despite enhanced prediction accuracy. Many studies have endeavored to\nimprove calibration performance through the use of specific loss functions,\ndata preprocessing and training frameworks. Yet, investigations into\ncalibration properties have been somewhat overlooked. Our study leverages the\nNeural Architecture Search (NAS) search space, offering an exhaustive model\narchitecture space for thorough calibration properties exploration. We\nspecifically create a model calibration dataset. This dataset evaluates 90\nbin-based and 12 additional calibration measurements across 117,702 unique\nneural networks within the widely employed NATS-Bench search space. Our\nanalysis aims to answer several longstanding questions in the field, using our\nproposed dataset: (i) Can model calibration be generalized across different\ndatasets? (ii) Can robustness be used as a calibration measurement? (iii) How\nreliable are calibration metrics? (iv) Does a post-hoc calibration method\naffect all models uniformly? (v) How does calibration interact with accuracy?\n(vi) What is the impact of bin size on calibration measurement? (vii) Which\narchitectural designs are beneficial for calibration? Additionally, our study\nbridges an existing gap by exploring calibration within NAS. By providing this\ndataset, we enable further research into NAS calibration. As far as we are\naware, our research represents the first large-scale investigation into\ncalibration properties and the premier study of calibration issues within NAS.\nThe project page can be found at https://www.taolinwei.com/calibration-study\n","authors":["Linwei Tao","Younan Zhu","Haolan Guo","Minjing Dong","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2308.11838v6.pdf","comment":"ICLR 2024 poster"},{"id":"http://arxiv.org/abs/2402.11558v2","updated":"2024-03-22T08:41:08Z","published":"2024-02-18T11:59:04Z","title":"A Temporally Disentangled Contrastive Diffusion Model for Spatiotemporal\n  Imputation","summary":"  Spatiotemporal data analysis is pivotal across various domains, such as\ntransportation, meteorology, and healthcare. The data collected in real-world\nscenarios are often incomplete due to device malfunctions and network errors.\nSpatiotemporal imputation aims to predict missing values by exploiting the\nspatial and temporal dependencies in the observed data. Traditional imputation\napproaches based on statistical and machine learning techniques require the\ndata to conform to their distributional assumptions, while graph and recurrent\nneural networks are prone to error accumulation problems due to their recurrent\nstructures. Generative models, especially diffusion models, can potentially\ncircumvent the reliance on inaccurate, previously imputed values for future\npredictions; However, diffusion models still face challenges in generating\nstable results. We propose to address these challenges by designing conditional\ninformation to guide the generative process and expedite the training process.\nWe introduce a conditional diffusion framework called C$^2$TSD, which\nincorporates disentangled temporal (trend and seasonality) representations as\nconditional information and employs contrastive learning to improve\ngeneralizability. Our extensive experiments on three real-world datasets\ndemonstrate the superior performance of our approach compared to a number of\nstate-of-the-art baselines.\n","authors":["Yakun Chen","Kaize Shi","Zhangkai Wu","Juan Chen","Xianzhi Wang","Julian McAuley","Guandong Xu","Shui Yu"],"pdf_url":"https://arxiv.org/pdf/2402.11558v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15031v1","updated":"2024-03-22T08:26:31Z","published":"2024-03-22T08:26:31Z","title":"Image Classification with Rotation-Invariant Variational Quantum\n  Circuits","summary":"  Variational quantum algorithms are gaining attention as an early application\nof Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of\nvariational methods lies in the phenomenon of Barren Plateaus, present in the\noptimization of variational parameters. Adding geometric inductive bias to the\nquantum models has been proposed as a potential solution to mitigate this\nproblem, leading to a new field called Geometric Quantum Machine Learning. In\nthis work, an equivariant architecture for variational quantum classifiers is\nintroduced to create a label-invariant model for image classification with\n$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against\ntwo different architectures, and it is experimentally observed that the\ngeometric approach boosts the model's performance. Finally, a classical\nequivariant convolution operation is proposed to extend the quantum model for\nthe processing of larger images, employing the resources available in NISQ\ndevices.\n","authors":["Paul San Sebastian","Mikel Cañizo","Román Orús"],"pdf_url":"https://arxiv.org/pdf/2403.15031v1.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2311.18598v2","updated":"2024-03-22T08:26:20Z","published":"2023-11-30T14:45:51Z","title":"Generalisable Agents for Neural Network Optimisation","summary":"  Optimising deep neural networks is a challenging task due to complex training\ndynamics, high computational requirements, and long training times. To address\nthis difficulty, we propose the framework of Generalisable Agents for Neural\nNetwork Optimisation (GANNO) -- a multi-agent reinforcement learning (MARL)\napproach that learns to improve neural network optimisation by dynamically and\nresponsively scheduling hyperparameters during training. GANNO utilises an\nagent per layer that observes localised network dynamics and accordingly takes\nactions to adjust these dynamics at a layerwise level to collectively improve\nglobal performance. In this paper, we use GANNO to control the layerwise\nlearning rate and show that the framework can yield useful and responsive\nschedules that are competitive with handcrafted heuristics. Furthermore, GANNO\nis shown to perform robustly across a wide variety of unseen initial\nconditions, and can successfully generalise to harder problems than it was\ntrained on. Our work presents an overview of the opportunities that this\nparadigm offers for training neural networks, along with key challenges that\nremain to be overcome.\n","authors":["Kale-ab Tessera","Callum Rhys Tilbury","Sasha Abramowitz","Ruan de Kock","Omayma Mahjoub","Benjamin Rosman","Sara Hooker","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2311.18598v2.pdf","comment":"Accepted at the Workshop on Advanced Neural Network Training (WANT)\n  and Optimization for Machine Learning (OPT) at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2403.15027v1","updated":"2024-03-22T08:17:00Z","published":"2024-03-22T08:17:00Z","title":"Grey-informed neural network for time-series forecasting","summary":"  Neural network models have shown outstanding performance and successful\nresolutions to complex problems in various fields. However, the majority of\nthese models are viewed as black-box, requiring a significant amount of data\nfor development. Consequently, in situations with limited data, constructing\nappropriate models becomes challenging due to the lack of transparency and\nscarcity of data. To tackle these challenges, this study suggests the\nimplementation of a grey-informed neural network (GINN). The GINN ensures that\nthe output of the neural network follows the differential equation model of the\ngrey system, improving interpretability. Moreover, incorporating prior\nknowledge from grey system theory enables traditional neural networks to\neffectively handle small data samples. Our proposed model has been observed to\nuncover underlying patterns in the real world and produce reliable forecasts\nbased on empirical data.\n","authors":["Wanli Xie","Ruibin Zhao","Zhenguo Xu","Tingting Liang"],"pdf_url":"https://arxiv.org/pdf/2403.15027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15025v1","updated":"2024-03-22T08:13:33Z","published":"2024-03-22T08:13:33Z","title":"Robust Conformal Prediction under Distribution Shift via\n  Physics-Informed Structural Causal Model","summary":"  Uncertainty is critical to reliable decision-making with machine learning.\nConformal prediction (CP) handles uncertainty by predicting a set on a test\ninput, hoping the set to cover the true label with at least $(1-\\alpha)$\nconfidence. This coverage can be guaranteed on test data even if the marginal\ndistributions $P_X$ differ between calibration and test datasets. However, as\nit is common in practice, when the conditional distribution $P_{Y|X}$ is\ndifferent on calibration and test data, the coverage is not guaranteed and it\nis essential to measure and minimize the coverage loss under distributional\nshift at \\textit{all} possible confidence levels. To address these issues, we\nupper bound the coverage difference at all levels using the cumulative density\nfunctions of calibration and test conformal scores and Wasserstein distance.\nInspired by the invariance of physics across data distributions, we propose a\nphysics-informed structural causal model (PI-SCM) to reduce the upper bound. We\nvalidated that PI-SCM can improve coverage robustness along confidence level\nand test domain on a traffic speed prediction task and an epidemic spread task\nwith multiple real-world datasets.\n","authors":["Rui Xu","Yue Sun","Chao Chen","Parv Venkitasubramaniam","Sihong Xie"],"pdf_url":"https://arxiv.org/pdf/2403.15025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15022v1","updated":"2024-03-22T08:11:14Z","published":"2024-03-22T08:11:14Z","title":"Insights into the Lottery Ticket Hypothesis and the Iterative Magnitude\n  Pruning","summary":"  Lottery ticket hypothesis for deep neural networks emphasizes the importance\nof initialization used to re-train the sparser networks obtained using the\niterative magnitude pruning process. An explanation for why the specific\ninitialization proposed by the lottery ticket hypothesis tends to work better\nin terms of generalization (and training) performance has been lacking.\nMoreover, the underlying principles in iterative magnitude pruning, like the\npruning of smaller magnitude weights and the role of the iterative process,\nlack full understanding and explanation. In this work, we attempt to provide\ninsights into these phenomena by empirically studying the volume/geometry and\nloss landscape characteristics of the solutions obtained at various stages of\nthe iterative magnitude pruning process.\n","authors":["Tausifa Jan Saleem","Ramanjit Ahuja","Surendra Prasad","Brejesh Lall"],"pdf_url":"https://arxiv.org/pdf/2403.15022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15017v1","updated":"2024-03-22T08:03:10Z","published":"2024-03-22T08:03:10Z","title":"Vehicle Detection Performance in Nordic Region","summary":"  This paper addresses the critical challenge of vehicle detection in the harsh\nwinter conditions in the Nordic regions, characterized by heavy snowfall,\nreduced visibility, and low lighting. Due to their susceptibility to\nenvironmental distortions and occlusions, traditional vehicle detection methods\nhave struggled in these adverse conditions. The advanced proposed deep learning\narchitectures brought promise, yet the unique difficulties of detecting\nvehicles in Nordic winters remain inadequately addressed. This study uses the\nNordic Vehicle Dataset (NVD), which has UAV images from northern Sweden, to\nevaluate the performance of state-of-the-art vehicle detection algorithms under\nchallenging weather conditions. Our methodology includes a comprehensive\nevaluation of single-stage, two-stage, and transformer-based detectors against\nthe NVD. We propose a series of enhancements tailored to each detection\nframework, including data augmentation, hyperparameter tuning, transfer\nlearning, and novel strategies designed explicitly for the DETR model. Our\nfindings not only highlight the limitations of current detection systems in the\nNordic environment but also offer promising directions for enhancing these\nalgorithms for improved robustness and accuracy in vehicle detection amidst the\ncomplexities of winter landscapes. The code and the dataset are available at\nhttps://nvd.ltu-ai.dev\n","authors":["Hamam Mokayed","Rajkumar Saini","Oluwatosin Adewumi","Lama Alkhaled","Bjorn Backe","Palaiahnakote Shivakumara","Olle Hagner","Yan Chai Hum"],"pdf_url":"https://arxiv.org/pdf/2403.15017v1.pdf","comment":"submitted to ICPR2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.15336v1","updated":"2024-03-22T16:41:45Z","published":"2024-03-22T16:41:45Z","title":"Dialogue Understandability: Why are we streaming movies with subtitles?","summary":"  Watching movies and TV shows with subtitles enabled is not simply down to\naudibility or speech intelligibility. A variety of evolving factors related to\ntechnological advances, cinema production and social behaviour challenge our\nperception and understanding. This study seeks to formalise and give context to\nthese influential factors under a wider and novel term referred to as Dialogue\nUnderstandability. We propose a working definition for Dialogue\nUnderstandability being a listener's capacity to follow the story without undue\ncognitive effort or concentration being required that impacts their Quality of\nExperience (QoE). The paper identifies, describes and categorises the factors\nthat influence Dialogue Understandability mapping them over the QoE framework,\na media streaming lifecycle, and the stakeholders involved. We then explore\navailable measurement tools in the literature and link them to the factors they\ncould potentially be used for. The maturity and suitability of these tools is\nevaluated over a set of pilot experiments. Finally, we reflect on the gaps that\nstill need to be filled, what we can measure and what not, future subjective\nexperiments, and new research trends that could help us to fully characterise\nDialogue Understandability.\n","authors":["Helard Becerra","Alessandro Ragano","Diptasree Debnath","Asad Ullah","Crisron Rudolf Lucas","Martin Walsh","Andrew Hines"],"pdf_url":"https://arxiv.org/pdf/2403.15336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15256v1","updated":"2024-03-22T14:57:12Z","published":"2024-03-22T14:57:12Z","title":"Experimental Studies of Metaverse Streaming","summary":"  Metaverse aims to construct a large, unified, immersive, and shared digital\nrealm by combining various technologies, namely XR (extended reality),\nblockchain, and digital twin, among others. This article explores the Metaverse\nfrom the perspective of multimedia communication by conducting and analyzing\nreal-world experiments on four different Metaverse platforms: VR (virtual\nreality) Vircadia, VR Mozilla Hubs, VRChat, and MR (mixed reality) Virtual\nCity. We first investigate the traffic patterns and network performance in the\nthree VR platforms. After raising the challenges of the Metaverse streaming and\ninvestigating the potential methods to enhance Metaverse performance, we\npropose a remote rendering architecture and verify its advantages through a\nprototype involving the campus network and MR multimodal interaction by\ncomparison with local rendering.\n","authors":["Haopeng Wang","Roberto Martinez-Velazquez","Haiwei Dong","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2403.15256v1.pdf","comment":"Accepted by IEEE Consumer Electronics Magazine"},{"id":"http://arxiv.org/abs/2308.04025v3","updated":"2024-03-22T14:49:31Z","published":"2023-08-08T03:43:24Z","title":"MSAC: Multiple Speech Attribute Control Method for Reliable Speech\n  Emotion Recognition","summary":"  Despite notable progress, speech emotion recognition (SER) remains\nchallenging due to the intricate and ambiguous nature of speech emotion,\nparticularly in wild world. While current studies primarily focus on\nrecognition and generalization abilities, our research pioneers an\ninvestigation into the reliability of SER methods in the presence of semantic\ndata shifts and explores how to exert fine-grained control over various\nattributes inherent in speech signals to enhance speech emotion modeling. In\nthis paper, we first introduce MSAC-SERNet, a novel unified SER framework\ncapable of simultaneously handling both single-corpus and cross-corpus SER.\nSpecifically, concentrating exclusively on the speech emotion attribute, a\nnovel CNN-based SER model is presented to extract discriminative emotional\nrepresentations, guided by additive margin softmax loss. Considering\ninformation overlap between various speech attributes, we propose a novel\nlearning paradigm based on correlations of different speech attributes, termed\nMultiple Speech Attribute Control (MSAC), which empowers the proposed SER model\nto simultaneously capture fine-grained emotion-related features while\nmitigating the negative impact of emotion-agnostic representations.\nFurthermore, we make a first attempt to examine the reliability of the\nMSAC-SERNet framework using out-of-distribution detection methods. Experiments\non both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNet\nnot only consistently outperforms the baseline in all aspects, but achieves\nsuperior performance compared to state-of-the-art SER approaches.\n","authors":["Yu Pan","Yuguang Yang","Yuheng Huang","Jixun Yao","Jingjing Yin","Yanni Hu","Heng Lu","Lei Ma","Jianjun Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.04025v3.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2403.15226v1","updated":"2024-03-22T14:20:34Z","published":"2024-03-22T14:20:34Z","title":"Not All Attention is Needed: Parameter and Computation Efficient\n  Transfer Learning for Multi-modal Large Language Models","summary":"  In this paper, we propose a novel parameter and computation efficient tuning\nmethod for Multi-modal Large Language Models (MLLMs), termed Efficient\nAttention Skipping (EAS). Concretely, we first reveal that multi-head\nattentions (MHAs), the main computational overhead of MLLMs, are often\nredundant to downstream tasks. Based on this observation, EAS evaluates the\nattention redundancy and skips the less important MHAs to speed up inference.\nBesides, we also propose a novel propagation-of-information adapter (PIA) to\nserve the attention skipping of EAS and keep parameter efficiency, which can be\nfurther re-parameterized into feed-forward networks (FFNs) for zero-extra\nlatency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN\nand a classic VL pre-trained model called METER, and conduct extensive\nexperiments on a set of benchmarks. The experiments show that EAS not only\nretains high performance and parameter efficiency, but also greatly speeds up\ninference speed. For instance, LaVIN-EAS can obtain 89.98\\% accuracy on\nScineceQA while speeding up inference by 2.2 times to LaVIN\n","authors":["Qiong Wu","Weihao Ye","Yiyi Zhou","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2403.15226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.14899v2","updated":"2024-03-22T13:24:35Z","published":"2023-06-26T17:59:55Z","title":"FunQA: Towards Surprising Video Comprehension","summary":"  Surprising videos, such as funny clips, creative performances, or visual\nillusions, attract significant attention. Enjoyment of these videos is not\nsimply a response to visual stimuli; rather, it hinges on the human capacity to\nunderstand (and appreciate) commonsense violations depicted in these videos. We\nintroduce FunQA, a challenging video question-answering (QA) dataset\nspecifically designed to evaluate and enhance the depth of video reasoning\nbased on counter-intuitive and fun videos. Unlike most video QA benchmarks\nwhich focus on less surprising contexts, e.g., cooking or instructional videos,\nFunQA covers three previously unexplored types of surprising videos: 1)\nHumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous\nQA tasks designed to assess the model's capability in counter-intuitive\ntimestamp localization, detailed video description, and reasoning around\ncounter-intuitiveness. We also pose higher-level tasks, such as attributing a\nfitting and vivid title to the video and scoring the video creativity. In\ntotal, the FunQA benchmark consists of 312K free-text QA pairs derived from\n4.3K video clips, spanning a total of 24 video hours. Moreover, we propose\nFunMentor, an agent designed for Vision-Language Models (VLMs) that uses\nmulti-turn dialogues to enhance models' understanding of counter-intuitiveness.\nExtensive experiments with existing VLMs demonstrate the effectiveness of\nFunMentor and reveal significant performance gaps for the FunQA videos across\nspatial-temporal reasoning, visual-centered reasoning, and free-text\ngeneration.\n","authors":["Binzhu Xie","Sicheng Zhang","Zitang Zhou","Bo Li","Yuanhan Zhang","Jack Hessel","Jingkang Yang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2306.14899v2.pdf","comment":"Project Page: https://funqa-benchmark.github.io/ Codebase:\n  https://github.com/Jingkang50/FunQA"},{"id":"http://arxiv.org/abs/2402.02733v3","updated":"2024-03-22T09:17:24Z","published":"2024-02-05T05:25:33Z","title":"ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer","summary":"  Face re-aging is a prominent field in computer vision and graphics, with\nsignificant applications in photorealistic domains such as movies, advertising,\nand live streaming. Recently, the need to apply face re-aging to\nnon-photorealistic images, like comics, illustrations, and animations, has\nemerged as an extension in various entertainment sectors. However, the lack of\na network that can seamlessly edit the apparent age in NPR images has limited\nthese tasks to a naive, sequential approach. This often results in unpleasant\nartifacts and a loss of facial attributes due to domain discrepancies. In this\npaper, we introduce a novel one-stage method for face re-aging combined with\nportrait style transfer, executed in a single generative step. We leverage\nexisting face re-aging and style transfer networks, both trained within the\nsame PR domain. Our method uniquely fuses distinct latent vectors, each\nresponsible for managing aging-related attributes and NPR appearance. By\nadopting an exemplar-based approach, our method offers greater flexibility\ncompared to domain-level fine-tuning approaches, which typically require\nseparate training or fine-tuning for each domain. This effectively addresses\nthe limitation of requiring paired datasets for re-aging and domain-level,\ndata-driven approaches for stylization. Our experiments show that our model can\neffortlessly generate re-aged images while simultaneously transferring the\nstyle of examples, maintaining both natural appearance and controllability.\n","authors":["Bumsoo Kim","Abdul Muqeet","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2402.02733v3.pdf","comment":"14 pages, 15 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.15048v1","updated":"2024-03-22T09:13:09Z","published":"2024-03-22T09:13:09Z","title":"Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning","summary":"  Large-scale Text-to-Image (TTI) models have become a common approach for\ngenerating training data in various generative fields. However, visual\nhallucinations, which contain perceptually critical defects, remain a concern,\nespecially in non-photorealistic styles like cartoon characters. We propose a\nnovel visual hallucination detection system for cartoon character images\ngenerated by TTI models. Our approach leverages pose-aware in-context visual\nlearning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB\nimages and pose information. By incorporating pose guidance from a fine-tuned\npose estimator, we enable VLMs to make more accurate decisions. Experimental\nresults demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images. This\nresearch advances TTI models by mitigating visual hallucinations, expanding\ntheir potential in non-photorealistic domains.\n","authors":["Bumsoo Kim","Wonseop Shin","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2403.15048v1.pdf","comment":"11 pages, 12 figures, 1 table, Project page:\n  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/"},{"id":"http://arxiv.org/abs/2403.11700v2","updated":"2024-03-22T08:13:11Z","published":"2024-03-18T11:56:35Z","title":"Virbo: Multimodal Multilingual Avatar Video Generation in Digital\n  Marketing","summary":"  With the widespread popularity of internet celebrity marketing all over the\nworld, short video production has gradually become a popular way of presenting\nproducts information. However, the traditional video production industry\nusually includes series of procedures as script writing, video filming in a\nprofessional studio, video clipping, special effects rendering, customized\npost-processing, and so forth. Not to mention that multilingual videos is not\naccessible for those who could not speak multilingual languages. These\ncomplicated procedures usually needs a professional team to complete, and this\nmade short video production costly in both time and money. This paper presents\nan intelligent system that supports the automatic generation of talking avatar\nvideos, namely Virbo. With simply a user-specified script, Virbo could use a\ndeep generative model to generate a target talking videos. Meanwhile, the\nsystem also supports multimodal inputs to customize the video with specified\nface, specified voice and special effects. This system also integrated a\nmultilingual customization module that supports generate multilingual talking\navatar videos in a batch with hundreds of delicate templates and creative\nspecial effects. Through a series of user studies and demo tests, we found that\nVirbo can generate talking avatar videos that maintained a high quality of\nvideos as those from a professional team while reducing the entire production\ncosts significantly. This intelligent system will effectively promote the video\nproduction industry and facilitate the internet marketing neglecting of\nlanguage barriers and cost challenges.\n","authors":["Juan Zhang","Jiahao Chen","Cheng Wang","Zhiwang Yu","Tangquan Qi","Can Liu","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14972v1","updated":"2024-03-22T06:03:07Z","published":"2024-03-22T06:03:07Z","title":"A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal\n  Reasoning","summary":"  This paper presents a pilot study aimed at introducing multi-agent debate\ninto multimodal reasoning. The study addresses two key challenges: the\ntrivialization of opinions resulting from excessive summarization and the\ndiversion of focus caused by distractor concepts introduced from images. These\nchallenges stem from the inductive (bottom-up) nature of existing debating\nschemes. To address the issue, we propose a deductive (top-down) debating\napproach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are\nconfined to a blueprint graph to prevent opinion trivialization through\nworld-level summarization. Moreover, by storing evidence in branches within the\ngraph, BDoG mitigates distractions caused by frequent but irrelevant concepts.\nExtensive experiments validate BDoG, achieving state-of-the-art results in\nScience QA and MMBench with significant improvements over previous methods.\n","authors":["Changmeng Zheng","Dayong Liang","Wengyu Zhang","Xiao-Yong Wei","Tat-Seng Chua","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.14972v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2403.14468v2","updated":"2024-03-22T02:16:40Z","published":"2024-03-21T15:15:00Z","title":"AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks","summary":"  Video-to-video editing involves editing a source video along with additional\ncontrol (such as text prompts, subjects, or styles) to generate a new video\nthat aligns with the source video and the provided control. Traditional methods\nhave been constrained to certain editing types, limiting their ability to meet\nthe wide range of user demands. In this paper, we introduce AnyV2V, a novel\ntraining-free framework designed to simplify video editing into two primary\nsteps: (1) employing an off-the-shelf image editing model (e.g.\nInstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an\nexisting image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion\nand feature injection. In the first stage, AnyV2V can plug in any existing\nimage editing tools to support an extensive array of video editing tasks.\nBeyond the traditional prompt-based editing methods, AnyV2V also can support\nnovel video editing tasks, including reference-based style transfer,\nsubject-driven editing, and identity manipulation, which were unattainable by\nprevious methods. In the second stage, AnyV2V can plug in any existing\nimage-to-video models to perform DDIM inversion and intermediate feature\ninjection to maintain the appearance and motion consistency with the source\nvideo. On the prompt-based editing, we show that AnyV2V can outperform the\nprevious best approach by 35\\% on prompt alignment, and 25\\% on human\npreference. On the three novel tasks, we show that AnyV2V also achieves a high\nsuccess rate. We believe AnyV2V will continue to thrive due to its ability to\nseamlessly integrate the fast-evolving image editing methods. Such\ncompatibility can help AnyV2V to increase its versatility to cater to diverse\nuser demands.\n","authors":["Max Ku","Cong Wei","Weiming Ren","Harry Yang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2403.14468v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.15336v1","updated":"2024-03-22T16:41:45Z","published":"2024-03-22T16:41:45Z","title":"Dialogue Understandability: Why are we streaming movies with subtitles?","summary":"  Watching movies and TV shows with subtitles enabled is not simply down to\naudibility or speech intelligibility. A variety of evolving factors related to\ntechnological advances, cinema production and social behaviour challenge our\nperception and understanding. This study seeks to formalise and give context to\nthese influential factors under a wider and novel term referred to as Dialogue\nUnderstandability. We propose a working definition for Dialogue\nUnderstandability being a listener's capacity to follow the story without undue\ncognitive effort or concentration being required that impacts their Quality of\nExperience (QoE). The paper identifies, describes and categorises the factors\nthat influence Dialogue Understandability mapping them over the QoE framework,\na media streaming lifecycle, and the stakeholders involved. We then explore\navailable measurement tools in the literature and link them to the factors they\ncould potentially be used for. The maturity and suitability of these tools is\nevaluated over a set of pilot experiments. Finally, we reflect on the gaps that\nstill need to be filled, what we can measure and what not, future subjective\nexperiments, and new research trends that could help us to fully characterise\nDialogue Understandability.\n","authors":["Helard Becerra Martinez","Alessandro Ragano","Diptasree Debnath","Asad Ullah","Crisron Rudolf Lucas","Martin Walsh","Andrew Hines"],"pdf_url":"https://arxiv.org/pdf/2403.15336v1.pdf","comment":null}]},"2024-03-25T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.16804v1","updated":"2024-03-25T14:23:03Z","published":"2024-03-25T14:23:03Z","title":"TEI2GO: A Multilingual Approach for Fast Temporal Expression\n  Identification","summary":"  Temporal expression identification is crucial for understanding texts written\nin natural language. Although highly effective systems such as HeidelTime\nexist, their limited runtime performance hampers adoption in large-scale\napplications and production environments. In this paper, we introduce the\nTEI2GO models, matching HeidelTime's effectiveness but with significantly\nimproved runtime, supporting six languages, and achieving state-of-the-art\nresults in four of them. To train the TEI2GO models, we used a combination of\nmanually annotated reference corpus and developed ``Professor HeidelTime'', a\ncomprehensive weakly labeled corpus of news texts annotated with HeidelTime.\nThis corpus comprises a total of $138,069$ documents (over six languages) with\n$1,050,921$ temporal expressions, the largest open-source annotated dataset for\ntemporal expression identification to date. By describing how the models were\nproduced, we aim to encourage the research community to further explore,\nrefine, and extend the set of models to additional languages and domains. Code,\nannotations, and models are openly available for community exploration and use.\nThe models are conveniently on HuggingFace for seamless integration and\napplication.\n","authors":["Hugo Sousa","Ricardo Campos","Alípio Jorge"],"pdf_url":"https://arxiv.org/pdf/2403.16804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16792v1","updated":"2024-03-25T14:07:27Z","published":"2024-03-25T14:07:27Z","title":"Iterative Refinement of Project-Level Code Context for Precise Code\n  Generation with Compiler Feedback","summary":"  Large language models (LLMs) have shown remarkable progress in automated code\ngeneration. Yet, incorporating LLM-based code generation into real-life\nsoftware projects poses challenges, as the generated code may contain errors in\nAPI usage, class, data structure, or missing project-specific information. As\nmuch of this project-specific context cannot fit into the prompts of LLMs, we\nmust find ways to allow the model to explore the project-level code context. To\nthis end, this paper puts forward a novel approach, termed ProCoder, which\niteratively refines the project-level code context for precise code generation,\nguided by the compiler feedback. In particular, ProCoder first leverages\ncompiler techniques to identify a mismatch between the generated code and the\nproject's context. It then iteratively aligns and fixes the identified errors\nusing information extracted from the code repository. We integrate ProCoder\nwith two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and\napply it to Python code generation. Experimental results show that ProCoder\nsignificantly improves the vanilla LLMs by over 80% in generating code\ndependent on project context, and consistently outperforms the existing\nretrieval-based code generation baselines.\n","authors":["Zhangqian Bi","Yao Wan","Zheng Wang","Hongyu Zhang","Batu Guan","Fangxin Lu","Zili Zhang","Yulei Sui","Xuanhua Shi","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2403.16792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16777v1","updated":"2024-03-25T13:53:04Z","published":"2024-03-25T13:53:04Z","title":"Can Machine Translation Bridge Multilingual Pretraining and\n  Cross-lingual Transfer Learning?","summary":"  Multilingual pretraining and fine-tuning have remarkably succeeded in various\nnatural language processing tasks. Transferring representations from one\nlanguage to another is especially crucial for cross-lingual learning. One can\nexpect machine translation objectives to be well suited to fostering such\ncapabilities, as they involve the explicit alignment of semantically equivalent\nsentences from different languages. This paper investigates the potential\nbenefits of employing machine translation as a continued training objective to\nenhance language representation learning, bridging multilingual pretraining and\ncross-lingual applications. We study this question through two lenses: a\nquantitative evaluation of the performance of existing models and an analysis\nof their latent representations. Our results show that, contrary to\nexpectations, machine translation as the continued training fails to enhance\ncross-lingual representation learning in multiple cross-lingual natural\nlanguage understanding tasks. We conclude that explicit sentence-level\nalignment in the cross-lingual scenario is detrimental to cross-lingual\ntransfer pretraining, which has important implications for future cross-lingual\ntransfer studies. We furthermore provide evidence through similarity measures\nand investigation of parameters that this lack of positive influence is due to\noutput separability -- which we argue is of use for machine translation but\ndetrimental elsewhere.\n","authors":["Shaoxiong Ji","Timothee Mickus","Vincent Segonne","Jörg Tiedemann"],"pdf_url":"https://arxiv.org/pdf/2403.16777v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16771v1","updated":"2024-03-25T13:50:11Z","published":"2024-03-25T13:50:11Z","title":"Synthetic Data Generation and Joint Learning for Robust Code-Mixed\n  Translation","summary":"  The widespread online communication in a modern multilingual world has\nprovided opportunities to blend more than one language (aka code-mixed\nlanguage) in a single utterance. This has resulted a formidable challenge for\nthe computational models due to the scarcity of annotated data and presence of\nnoise. A potential solution to mitigate the data scarcity problem in\nlow-resource setup is to leverage existing data in resource-rich language\nthrough translation. In this paper, we tackle the problem of code-mixed\n(Hinglish and Bengalish) to English machine translation. First, we\nsynthetically develop HINMIX, a parallel corpus of Hinglish to English, with\n~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation\nbased joint-training model that learns to handle noise in the real-world\ncode-mixed text by parameter sharing across clean and noisy words. Further, we\nshow the adaptability of RCMT in a zero-shot setup for Bengalish to English\ntranslation. Our evaluation and comprehensive analyses qualitatively and\nquantitatively demonstrate the superiority of RCMT over state-of-the-art\ncode-mixed and robust translation methods.\n","authors":[" Kartik","Sanjana Soni","Anoop Kunchukuttan","Tanmoy Chakraborty","Md Shad Akhtar"],"pdf_url":"https://arxiv.org/pdf/2403.16771v1.pdf","comment":"9 pages, 2 figures, to be published in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.06764v2","updated":"2024-03-25T13:29:30Z","published":"2024-03-11T14:35:32Z","title":"An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference\n  Acceleration for Large Vision-Language Models","summary":"  In this study, we identify the inefficient attention phenomena in Large\nVision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,\nQwenVL-Chat and Video-LLaVA. We find out that the attention computation over\nvisual tokens is of extreme inefficiency in the deep layers of popular LVLMs,\nsuggesting a need for a sparser approach compared to textual data handling. To\nthis end, we introduce FastV, a versatile plug-and-play method designed to\noptimize computational efficiency by learning adaptive attention patterns in\nearly layers and pruning visual tokens in subsequent ones. Our evaluations\ndemonstrate FastV's ability to dramatically reduce computational costs (e.g., a\n45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a\nwide range of image and video understanding tasks. The computational efficiency\nand performance trade-off of FastV are highly customizable and\npareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve\na lower budget than that of a 7B-parameter model, while still maintaining\nsuperior performance. We believe FastV has practical values for deployment of\nLVLMs in edge devices and commercial models. Code is released at\nhttps://github.com/pkunlp-icler/FastV.\n","authors":["Liang Chen","Haozhe Zhao","Tianyu Liu","Shuai Bai","Junyang Lin","Chang Zhou","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2403.06764v2.pdf","comment":"21 papes, 8 figures, code is released at\n  https://github.com/pkunlp-icler/FastV"},{"id":"http://arxiv.org/abs/2401.17919v3","updated":"2024-03-25T12:52:42Z","published":"2024-01-31T15:33:37Z","title":"LOCOST: State-Space Models for Long Document Abstractive Summarization","summary":"  State-space models are a low-complexity alternative to transformers for\nencoding long sequences and capturing long-term dependencies. We propose\nLOCOST: an encoder-decoder architecture based on state-space models for\nconditional text generation with long context inputs. With a computational\ncomplexity of $O(L \\log L)$, this architecture can handle significantly longer\nsequences than state-of-the-art models that are based on sparse attention\npatterns. We evaluate our model on a series of long document abstractive\nsummarization tasks. The model reaches a performance level that is 93-96%\ncomparable to the top-performing sparse transformers of the same size while\nsaving up to 50% memory during training and up to 87% during inference.\nAdditionally, LOCOST effectively handles input texts exceeding 600K tokens at\ninference time, setting new state-of-the-art results on full-book summarization\nand opening new perspectives for long input processing.\n","authors":["Florian Le Bronnec","Song Duong","Mathieu Ravaut","Alexandre Allauzen","Nancy F. Chen","Vincent Guigue","Alberto Lumbreras","Laure Soulier","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2401.17919v3.pdf","comment":"9 pages, 5 figures, 7 tables, EACL 2024 conference"},{"id":"http://arxiv.org/abs/2403.10963v2","updated":"2024-03-25T12:37:16Z","published":"2024-03-16T16:17:47Z","title":"Pointer-Generator Networks for Low-Resource Machine Translation: Don't\n  Copy That!","summary":"  While Transformer-based neural machine translation (NMT) is very effective in\nhigh-resource settings, many languages lack the necessary large parallel\ncorpora to benefit from it. In the context of low-resource (LR) MT between two\nclosely-related languages, a natural intuition is to seek benefits from\nstructural \"shortcuts\", such as copying subwords from the source to the target,\ngiven that such language pairs often share a considerable number of identical\nwords, cognates, and borrowings. We test Pointer-Generator Networks for this\npurpose for six language pairs over a variety of resource ranges, and find weak\nimprovements for most settings. However, analysis shows that the model does not\nshow greater improvements for closely-related vs. more distant language pairs,\nor for lower resource ranges, and that the models do not exhibit the expected\nusage of the mechanism for shared subwords. Our discussion of the reasons for\nthis behaviour highlights several general challenges for LR NMT, such as modern\ntokenization strategies, noisy real-world conditions, and linguistic\ncomplexities. We call for better scrutiny of linguistically motivated\nimprovements to NMT given the blackbox nature of Transformer models, as well as\nfor a focus on the above problems in the field.\n","authors":["Niyati Bafna","Philipp Koehn","David Yarowsky"],"pdf_url":"https://arxiv.org/pdf/2403.10963v2.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.16702v1","updated":"2024-03-25T12:34:33Z","published":"2024-03-25T12:34:33Z","title":"ProCQA: A Large-scale Community-based Programming Question Answering\n  Dataset for Code Search","summary":"  Retrieval-based code question answering seeks to match user queries in\nnatural language to relevant code snippets. Previous approaches typically rely\non pretraining models using crafted bi-modal and uni-modal datasets to align\ntext and code representations. In this paper, we introduce ProCQA, a\nlarge-scale programming question answering dataset extracted from the\nStackOverflow community, offering naturally structured mixed-modal QA pairs. To\nvalidate its effectiveness, we propose a modality-agnostic contrastive\npre-training approach to improve the alignment of text and code representations\nof current code language models. Compared to previous models that primarily\nemploy bimodal and unimodal pairs extracted from CodeSearchNet for\npre-training, our model exhibits significant performance improvements across a\nwide range of code retrieval benchmarks.\n","authors":["Zehan Li","Jianfei Zhang","Chuantao Yin","Yuanxin Ouyang","Wenge Rong"],"pdf_url":"https://arxiv.org/pdf/2403.16702v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16685v1","updated":"2024-03-25T12:21:38Z","published":"2024-03-25T12:21:38Z","title":"ToXCL: A Unified Framework for Toxic Speech Detection and Explanation","summary":"  The proliferation of online toxic speech is a pertinent problem posing\nthreats to demographic groups. While explicit toxic speech contains offensive\nlexical signals, implicit one consists of coded or indirect language.\nTherefore, it is crucial for models not only to detect implicit toxic speech\nbut also to explain its toxicity. This draws a unique need for unified\nframeworks that can effectively detect and explain implicit toxic speech. Prior\nworks mainly formulated the task of toxic speech detection and explanation as a\ntext generation problem. Nonetheless, models trained using this strategy can be\nprone to suffer from the consequent error propagation problem. Moreover, our\nexperiments reveal that the detection results of such models are much lower\nthan those that focus only on the detection task. To bridge these gaps, we\nintroduce ToXCL, a unified framework for the detection and explanation of\nimplicit toxic speech. Our model consists of three modules: a (i) Target Group\nGenerator to generate the targeted demographic group(s) of a given post; an\n(ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit\ntoxic speech and is boosted by a (iii) Teacher Classifier via knowledge\ndistillation, and the decoder generates the necessary explanation. ToXCL\nachieves new state-of-the-art effectiveness, and outperforms baselines\nsignificantly.\n","authors":["Nhat M. Hoang","Xuan Long Do","Duc Anh Do","Duc Anh Vu","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2403.16685v1.pdf","comment":"Accepted at NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.16668v1","updated":"2024-03-25T12:07:21Z","published":"2024-03-25T12:07:21Z","title":"Who is bragging more online? A large scale analysis of bragging in\n  social media","summary":"  Bragging is the act of uttering statements that are likely to be positively\nviewed by others and it is extensively employed in human communication with the\naim to build a positive self-image of oneself. Social media is a natural\nplatform for users to employ bragging in order to gain admiration, respect,\nattention and followers from their audiences. Yet, little is known about the\nscale of bragging online and its characteristics. This paper employs\ncomputational sociolinguistics methods to conduct the first large scale study\nof bragging behavior on Twitter (U.S.) by focusing on its overall prevalence,\ntemporal dynamics and impact of demographic factors. Our study shows that the\nprevalence of bragging decreases over time within the same population of users.\nIn addition, younger, more educated and popular users in the U.S. are more\nlikely to brag. Finally, we conduct an extensive linguistics analysis to unveil\nspecific bragging themes associated with different user traits.\n","authors":["Mali Jin","Daniel Preoţiuc-Pietro","A. Seza Doğruöz","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2403.16668v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.02930v2","updated":"2024-03-25T12:07:13Z","published":"2024-03-05T12:48:29Z","title":"A Second Look on BASS -- Boosting Abstractive Summarization with Unified\n  Semantic Graphs -- A Replication Study","summary":"  We present a detailed replication study of the BASS framework, an abstractive\nsummarization system based on the notion of Unified Semantic Graphs. Our\ninvestigation includes challenges in replicating key components and an ablation\nstudy to systematically isolate error sources rooted in replicating novel\ncomponents. Our findings reveal discrepancies in performance compared to the\noriginal work. We highlight the significance of paying careful attention even\nto reasonably omitted details for replicating advanced frameworks like BASS,\nand emphasize key practices for writing replicable papers.\n","authors":["Osman Alperen Koraş","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2403.02930v2.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Advances in Information Retrieval, 46th European Conference on\n  Information Retrieval, ECIR 2024. 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16662v1","updated":"2024-03-25T11:56:29Z","published":"2024-03-25T11:56:29Z","title":"RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking\n  on Russia-Ukraine Conflict","summary":"  Fact-checking is the task of verifying the factuality of a given claim by\nexamining the available evidence. High-quality evidence plays a vital role in\nenhancing fact-checking systems and facilitating the generation of explanations\nthat are understandable to humans. However, the provision of both sufficient\nand relevant evidence for explainable fact-checking systems poses a challenge.\nTo tackle this challenge, we propose a method based on a Large Language Model\nto automatically retrieve and summarize evidence from the Web. Furthermore, we\nconstruct RU22Fact, a novel multilingual explainable fact-checking dataset on\nthe Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world\nclaims, optimized evidence, and referenced explanation. To establish a baseline\nfor our dataset, we also develop an end-to-end explainable fact-checking system\nto verify claims and generate explanations. Experimental results demonstrate\nthe prospect of optimized evidence in increasing fact-checking performance and\nalso indicate the possibility of further progress in the end-to-end claim\nverification and explanation generation tasks.\n","authors":["Yirong Zeng","Xiao Ding","Yi Zhao","Xiangyu Li","Jie Zhang","Chao Yao","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2403.16662v1.pdf","comment":"12 pages, 3 figures, accepted by lrec-coling2024"},{"id":"http://arxiv.org/abs/2402.10685v2","updated":"2024-03-25T11:50:32Z","published":"2024-02-16T13:39:34Z","title":"LongHeads: Multi-Head Attention is Secretly a Long Context Processor","summary":"  Large language models (LLMs) have achieved impressive performance in numerous\ndomains but often struggle to process lengthy inputs effectively and\nefficiently due to limited length generalization and attention's quadratic\ncomputational demands. Many sought to mitigate this by restricting the\nattention window within the pre-trained length. However, these methods\nintroduce new issues such as ignoring the middle context and requiring\nadditional training. To address these problems, we propose LongHeads, a\ntraining-free framework that enhances LLM's long context ability by unlocking\nmulti-head attention's untapped potential. Instead of allowing each head to\nattend to the full sentence, which struggles with generalizing to longer\nsequences due to out-of-distribution (OOD) issues, we allow each head to\nprocess in-distribution length by selecting and attending to important context\nchunks. To this end, we propose a chunk selection strategy that relies on the\ninherent correlation between the query and the key representations, efficiently\ndistributing context chunks to different heads. In this way, each head ensures\nit can effectively process attended tokens within the trained length, while\ndifferent heads in different layers can collectively process longer contexts.\nLongHeads works efficiently in linear time, fits seamlessly with many LLMs that\nuse relative positional encoding. LongHeads achieves 100% accuracy at the 128k\nlength on passkey retrieval task, verifying LongHeads's efficacy in extending\nthe usable context window for existing models. We release our code at\nhttps://github.com/LuLuLuyi/LongHeads .\n","authors":["Yi Lu","Xin Zhou","Wei He","Jun Zhao","Tao Ji","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2402.10685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16655v1","updated":"2024-03-25T11:45:21Z","published":"2024-03-25T11:45:21Z","title":"Grammatical vs Spelling Error Correction: An Investigation into the\n  Responsiveness of Transformer-based Language Models using BART and MarianMT","summary":"  Text continues to remain a relevant form of representation for information.\nText documents are created either in digital native platforms or through the\nconversion of other media files such as images and speech. While the digital\nnative text is invariably obtained through physical or virtual keyboards,\ntechnologies such as OCR and speech recognition are utilized to transform the\nimages and speech signals into text content. All these variety of mechanisms of\ntext generation also introduce errors into the captured text.\n  This project aims at analyzing different kinds of error that occurs in text\ndocuments. The work employs two of the advanced deep neural network-based\nlanguage models, namely, BART and MarianMT, to rectify the anomalies present in\nthe text. Transfer learning of these models with available dataset is performed\nto finetune their capacity for error correction. A comparative study is\nconducted to investigate the effectiveness of these models in handling each of\nthe defined error categories. It is observed that while both models can bring\ndown the erroneous sentences by 20+%, BART can handle spelling errors far\nbetter (24.6%) than grammatical errors (8.8%).\n","authors":["Rohit Raju","Peeta Basa Pati","SA Gandheesh","Gayatri Sanjana Sannala","Suriya KS"],"pdf_url":"https://arxiv.org/pdf/2403.16655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16630v1","updated":"2024-03-25T11:20:23Z","published":"2024-03-25T11:20:23Z","title":"A comparative analysis of embedding models for patent similarity","summary":"  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n","authors":["Grazia Sveva Ascione","Valerio Sterzi"],"pdf_url":"https://arxiv.org/pdf/2403.16630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10666v3","updated":"2024-03-25T10:59:04Z","published":"2023-05-18T02:57:54Z","title":"A unified front-end framework for English text-to-speech synthesis","summary":"  The front-end is a critical component of English text-to-speech (TTS)\nsystems, responsible for extracting linguistic features that are essential for\na text-to-speech model to synthesize speech, such as prosodies and phonemes.\nThe English TTS front-end typically consists of a text normalization (TN)\nmodule, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme\n(G2P) module. However, current research on the English TTS front-end focuses\nsolely on individual modules, neglecting the interdependence between them and\nresulting in sub-optimal performance for each module. Therefore, this paper\nproposes a unified front-end framework that captures the dependencies among the\nEnglish TTS front-end modules. Extensive experiments have demonstrated that the\nproposed method achieves state-of-the-art (SOTA) performance in all modules.\n","authors":["Zelin Ying","Chen Li","Yu Dong","Qiuqiang Kong","Qiao Tian","Yuanyuan Huo","Yuxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2305.10666v3.pdf","comment":"Accepted in ICASSP 2024"},{"id":"http://arxiv.org/abs/2309.12263v2","updated":"2024-03-25T10:52:14Z","published":"2023-09-21T17:13:21Z","title":"On the Relationship between Skill Neurons and Robustness in Prompt\n  Tuning","summary":"  Prompt Tuning is a popular parameter-efficient finetuning method for\npre-trained large language models (PLMs). Based on experiments with RoBERTa, it\nhas been suggested that Prompt Tuning activates specific neurons in the\ntransformer's feed-forward networks, that are highly predictive and selective\nfor the given task. In this paper, we study the robustness of Prompt Tuning in\nrelation to these \"skill neurons\", using RoBERTa and T5. We show that prompts\ntuned for a specific task are transferable to tasks of the same type but are\nnot very robust to adversarial data. While prompts tuned for RoBERTa yield\nbelow-chance performance on adversarial data, prompts tuned for T5 are slightly\nmore robust and retain above-chance performance in two out of three cases. At\nthe same time, we replicate the finding that skill neurons exist in RoBERTa and\nfurther show that skill neurons also exist in T5. Interestingly, the skill\nneurons of T5 determined on non-adversarial data are also among the most\npredictive neurons on the adversarial data, which is not the case for RoBERTa.\nWe conclude that higher adversarial robustness may be related to a model's\nability to consistently activate the relevant skill neurons on adversarial\ndata.\n","authors":["Leon Ackermann","Xenia Ohmer"],"pdf_url":"https://arxiv.org/pdf/2309.12263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16614v1","updated":"2024-03-25T10:44:38Z","published":"2024-03-25T10:44:38Z","title":"Semantically Enriched Cross-Lingual Sentence Embeddings for\n  Crisis-related Social Media Texts","summary":"  Tasks such as semantic search and clustering on crisis-related social media\ntexts enhance our comprehension of crisis discourse, aiding decision-making and\ntargeted interventions. Pre-trained language models have advanced performance\nin crisis informatics, but their contextual embeddings lack semantic\nmeaningfulness. Although the CrisisTransformers family includes a sentence\nencoder to address the semanticity issue, it remains monolingual, processing\nonly English texts. Furthermore, employing separate models for different\nlanguages leads to embeddings in distinct vector spaces, introducing challenges\nwhen comparing semantic similarities between multi-lingual texts. Therefore, we\npropose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed\ncrisis-related social media texts for over 50 languages, such that texts with\nsimilar meanings are in close proximity within the same vector space,\nirrespective of language diversity. Results in sentence encoding and sentence\nmatching tasks are promising, suggesting these models could serve as robust\nbaselines when embedding multi-lingual crisis-related social media texts. The\nmodels are publicly available at: https://huggingface.co/crisistransformers.\n","authors":["Rabindra Lamsal","Maria Rodriguez Read","Shanika Karunasekera"],"pdf_url":"https://arxiv.org/pdf/2403.16614v1.pdf","comment":"Accepted to ISCRAM 2024"},{"id":"http://arxiv.org/abs/2403.16609v1","updated":"2024-03-25T10:39:18Z","published":"2024-03-25T10:39:18Z","title":"Conversational Grounding: Annotation and Analysis of Grounding Acts and\n  Grounding Units","summary":"  Successful conversations often rest on common understanding, where all\nparties are on the same page about the information being shared. This process,\nknown as conversational grounding, is crucial for building trustworthy dialog\nsystems that can accurately keep track of and recall the shared information.\nThe proficiencies of an agent in grounding the conveyed information\nsignificantly contribute to building a reliable dialog system. Despite recent\nadvancements in dialog systems, there exists a noticeable deficit in their\ngrounding capabilities. Traum provided a framework for conversational grounding\nintroducing Grounding Acts and Grounding Units, but substantial progress,\nespecially in the realm of Large Language Models, remains lacking. To bridge\nthis gap, we present the annotation of two dialog corpora employing Grounding\nActs, Grounding Units, and a measure of their degree of grounding. We discuss\nour key findings during the annotation and also provide a baseline model to\ntest the performance of current Language Models in categorizing the grounding\nacts of the dialogs. Our work aims to provide a useful resource for further\nresearch in making conversations with machines better understood and more\nreliable in natural day-to-day collaborative dialogs.\n","authors":["Biswesh Mohapatra","Seemab Hassan","Laurent Romary","Justine Cassell"],"pdf_url":"https://arxiv.org/pdf/2403.16609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16592v1","updated":"2024-03-25T10:09:03Z","published":"2024-03-25T10:09:03Z","title":"TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain\n  Machine Generated Text Detection Techniques","summary":"  The Large Language Models (LLMs) exhibit remarkable ability to generate\nfluent content across a wide spectrum of user queries. However, this capability\nhas raised concerns regarding misinformation and personal information leakage.\nIn this paper, we present our methods for the SemEval2024 Task8, aiming to\ndetect machine-generated text across various domains in both mono-lingual and\nmulti-lingual contexts. Our study comprehensively analyzes various methods to\ndetect machine-generated text, including statistical, neural, and pre-trained\nmodel approaches. We also detail our experimental setup and perform a in-depth\nerror analysis to evaluate the effectiveness of these methods. Our methods\nobtain an accuracy of 86.9\\% on the test set of subtask-A mono and 83.7\\% for\nsubtask-B. Furthermore, we also highlight the challenges and essential factors\nfor consideration in future studies.\n","authors":["Ashok Urlana","Aditya Saibewar","Bala Mallikarjunarao Garlapati","Charaka Vinayak Kumar","Ajeet Kumar Singh","Srinivasa Rao Chalamala"],"pdf_url":"https://arxiv.org/pdf/2403.16592v1.pdf","comment":"8 pages, 1 Figure"},{"id":"http://arxiv.org/abs/2403.16584v1","updated":"2024-03-25T09:51:54Z","published":"2024-03-25T09:51:54Z","title":"Can Large Language Models (or Humans) Distill Text?","summary":"  We investigate the potential of large language models (LLMs) to distill text:\nto remove the textual traces of an undesired forbidden variable. We employ a\nrange of LLMs with varying architectures and training approaches to distill\ntext by identifying and removing information about the target variable while\npreserving other relevant signals. Our findings shed light on the strengths and\nlimitations of LLMs in addressing the distillation and provide insights into\nthe strategies for leveraging these models in computational social science\ninvestigations involving text data. In particular, we show that in the strong\ntest of removing sentiment, the statistical association between the processed\ntext and sentiment is still clearly detectable to machine learning classifiers\npost-LLM-distillation. Furthermore, we find that human annotators also struggle\nto distill sentiment while preserving other semantic content. This suggests\nthere may be limited separability between concept variables in some text\ncontexts, highlighting limitations of methods relying on text-level\ntransformations and also raising questions about the robustness of distillation\nmethods that achieve statistical independence in representation space if this\nis difficult for human coders operating on raw text to attain.\n","authors":["Nicolas Audinet de Pieuchon","Adel Daoud","Connor Thomas Jerzak","Moa Johansson","Richard Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.16584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15248v2","updated":"2024-03-25T09:36:54Z","published":"2024-02-23T10:27:42Z","title":"Chitchat as Interference: Adding User Backstories to Task-Oriented\n  Dialogues","summary":"  During task-oriented dialogues (TODs), human users naturally introduce\nchitchat that is beyond the immediate scope of the task, interfering with the\nflow of the conversation. To address this issue without the need for expensive\nmanual data creation, we use few-shot prompting with Llama-2-70B to enhance the\nMultiWOZ dataset with user backstories, a typical example of chitchat\ninterference in TODs. We assess the impact of this addition by testing two\nmodels: one trained solely on TODs and another trained on TODs with a\npreliminary chitchat interaction. Our analysis demonstrates that our enhanced\ndataset poses a challenge for these systems. Moreover, we demonstrate that our\ndataset can be effectively used for training purposes, enabling a system to\nconsistently acknowledge the user's backstory while also successfully moving\nthe task forward in the same turn, as confirmed by human evaluation. These\nfindings highlight the benefits of generating novel chitchat-TOD scenarios to\ntest TOD systems more thoroughly and improve their resilience to natural user\ninterferences\n","authors":["Armand Stricker","Patrick Paroubek"],"pdf_url":"https://arxiv.org/pdf/2402.15248v2.pdf","comment":"Accepted @ LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16571v1","updated":"2024-03-25T09:36:51Z","published":"2024-03-25T09:36:51Z","title":"NSINA: A News Corpus for Sinhala","summary":"  The introduction of large language models (LLMs) has advanced natural\nlanguage processing (NLP), but their effectiveness is largely dependent on\npre-training resources. This is especially evident in low-resource languages,\nsuch as Sinhala, which face two primary challenges: the lack of substantial\ntraining data and limited benchmarking datasets. In response, this study\nintroduces NSINA, a comprehensive news corpus of over 500,000 articles from\npopular Sinhala news websites, along with three NLP tasks: news media\nidentification, news category prediction, and news headline generation. The\nrelease of NSINA aims to provide a solution to challenges in adapting LLMs to\nSinhala, offering valuable resources and benchmarks for improving NLP in the\nSinhala language. NSINA is the largest news corpus for Sinhala, available up to\ndate.\n","authors":["Hansi Hettiarachchi","Damith Premasiri","Lasitha Uyangodage","Tharindu Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2403.16571v1.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.16554v1","updated":"2024-03-25T09:04:14Z","published":"2024-03-25T09:04:14Z","title":"PE: A Poincare Explanation Method for Fast Text Hierarchy Generation","summary":"  The black-box nature of deep learning models in NLP hinders their widespread\napplication. The research focus has shifted to Hierarchical Attribution (HA)\nfor its ability to model feature interactions. Recent works model\nnon-contiguous combinations with a time-costly greedy search in Eculidean\nspaces, neglecting underlying linguistic information in feature\nrepresentations. In this work, we introduce a novel method, namely Poincar\\'e\nExplanation (PE), for modeling feature interactions using hyperbolic spaces in\nan $O(n^2logn)$ time complexity. Inspired by Poincar\\'e model, we propose a\nframework to project the embeddings into hyperbolic spaces, which exhibit\nbetter inductive biases for syntax and semantic hierarchical structures.\nEventually, we prove that the hierarchical clustering process in the projected\nspace could be viewed as building a minimum spanning tree and propose a time\nefficient algorithm. Experimental results demonstrate the effectiveness of our\napproach.\n","authors":["Qian Chen","Xiaofeng He","Hongzhao Li","Hongyu Yi"],"pdf_url":"https://arxiv.org/pdf/2403.16554v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2310.19531v6","updated":"2024-03-25T08:46:58Z","published":"2023-10-30T13:33:21Z","title":"MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties\n  in Generative Language Models","summary":"  Generative language models are usually pretrained on large text corpus via\npredicting the next token (i.e., sub-word/word/phrase) given the previous ones.\nRecent works have demonstrated the impressive performance of large generative\nlanguage models on downstream tasks. However, existing generative language\nmodels generally neglect an inherent challenge in text corpus during training,\ni.e., the imbalance between frequent tokens and infrequent ones. It can lead a\nlanguage model to be dominated by common and easy-to-learn tokens, thereby\noverlooking the infrequent and difficult-to-learn ones. To alleviate that, we\npropose a MiLe Loss function for mitigating the bias of learning difficulties\nwith tokens. During training, it can dynamically assess the learning difficulty\nof a to-be-learned token, according to the information entropy of the\ncorresponding predicted probability distribution over the vocabulary. Then it\nscales the training loss adaptively, trying to lead the model to focus more on\nthe difficult-to-learn tokens. On the Pile dataset, we train generative\nlanguage models at different scales of 468M, 1.2B, and 6.7B parameters.\nExperiments reveal that models incorporating the proposed MiLe Loss can gain\nconsistent performance improvement on downstream benchmarks.\n","authors":["Zhenpeng Su","Xing Wu","Xue Bai","Zijia Lin","Hui Chen","Guiguang Ding","Wei Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2310.19531v6.pdf","comment":"This paper has been accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.01479v3","updated":"2024-03-25T08:46:15Z","published":"2024-03-03T11:13:44Z","title":"Align-to-Distill: Trainable Attention Alignment for Knowledge\n  Distillation in Neural Machine Translation","summary":"  The advent of scalable deep models and large datasets has improved the\nperformance of Neural Machine Translation. Knowledge Distillation (KD) enhances\nefficiency by transferring knowledge from a teacher model to a more compact\nstudent model. However, KD approaches to Transformer architecture often rely on\nheuristics, particularly when deciding which teacher layers to distill from. In\nthis paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to\naddress the feature mapping problem by adaptively aligning student attention\nheads with their teacher counterparts during training. The Attention Alignment\nModule in A2D performs a dense head-by-head comparison between student and\nteacher attention heads across layers, turning the combinatorial mapping\nheuristics into a learning problem. Our experiments show the efficacy of A2D,\ndemonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb\nand WMT-2014 En->De, respectively, compared to Transformer baselines.\n","authors":["Heegon Jin","Seonil Son","Jemin Park","Youngseok Kim","Hyungjong Noh","Yeonsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2403.01479v3.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2307.06708v2","updated":"2024-03-25T08:44:53Z","published":"2023-07-13T12:06:48Z","title":"To share or not to share: What risks would laypeople accept to give\n  sensitive data to differentially-private NLP systems?","summary":"  Although the NLP community has adopted central differential privacy as a\ngo-to framework for privacy-preserving model training or data sharing, the\nchoice and interpretation of the key parameter, privacy budget $\\varepsilon$\nthat governs the strength of privacy protection, remains largely arbitrary. We\nargue that determining the $\\varepsilon$ value should not be solely in the\nhands of researchers or system developers, but must also take into account the\nactual people who share their potentially sensitive data. In other words: Would\nyou share your instant messages for $\\varepsilon$ of 10? We address this\nresearch gap by designing, implementing, and conducting a behavioral experiment\n(311 lay participants) to study the behavior of people in uncertain\ndecision-making situations with respect to privacy-threatening situations.\nFraming the risk perception in terms of two realistic NLP scenarios and using a\nvignette behavioral study help us determine what $\\varepsilon$ thresholds would\nlead lay people to be willing to share sensitive textual data - to our\nknowledge, the first study of its kind.\n","authors":["Christopher Weiss","Frauke Kreuter","Ivan Habernal"],"pdf_url":"https://arxiv.org/pdf/2307.06708v2.pdf","comment":"Accepted at LREC-COLING 2024; final camera-ready version"},{"id":"http://arxiv.org/abs/2403.16543v1","updated":"2024-03-25T08:36:06Z","published":"2024-03-25T08:36:06Z","title":"Efficient Information Extraction in Few-Shot Relation Classification\n  through Contrastive Representation Learning","summary":"  Differentiating relationships between entity pairs with limited labeled\ninstances poses a significant challenge in few-shot relation classification.\nRepresentations of textual data extract rich information spanning the domain,\nentities, and relations. In this paper, we introduce a novel approach to\nenhance information extraction combining multiple sentence representations and\ncontrastive learning. While representations in relation classification are\ncommonly extracted using entity marker tokens, we argue that substantial\ninformation within the internal model representations remains untapped. To\naddress this, we propose aligning multiple sentence representations, such as\nthe [CLS] token, the [MASK] token used in prompting, and entity marker tokens.\nOur method employs contrastive learning to extract complementary discriminative\ninformation from these individual representations. This is particularly\nrelevant in low-resource settings where information is scarce. Leveraging\nmultiple sentence representations is especially effective in distilling\ndiscriminative information for relation classification when additional\ninformation, like relation descriptions, are not available. We validate the\nadaptability of our approach, maintaining robust performance in scenarios that\ninclude relation descriptions, and showcasing its flexibility to adapt to\ndifferent resource constraints.\n","authors":["Philipp Borchert","Jochen De Weerdt","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2403.16543v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2309.08503v2","updated":"2024-03-25T08:33:37Z","published":"2023-09-15T16:05:48Z","title":"HealthFC: Verifying Health Claims with Evidence-Based Medical\n  Fact-Checking","summary":"  In the digital age, seeking health advice on the Internet has become a common\npractice. At the same time, determining the trustworthiness of online medical\ncontent is increasingly challenging. Fact-checking has emerged as an approach\nto assess the veracity of factual claims using evidence from credible knowledge\nsources. To help advance automated Natural Language Processing (NLP) solutions\nfor this task, in this paper we introduce a novel dataset HealthFC. It consists\nof 750 health-related claims in German and English, labeled for veracity by\nmedical experts and backed with evidence from systematic reviews and clinical\ntrials. We provide an analysis of the dataset, highlighting its characteristics\nand challenges. The dataset can be used for NLP tasks related to automated\nfact-checking, such as evidence retrieval, claim verification, or explanation\ngeneration. For testing purposes, we provide baseline systems based on\ndifferent approaches, examine their performance, and discuss the findings. We\nshow that the dataset is a challenging test bed with a high potential for\nfuture use.\n","authors":["Juraj Vladika","Phillip Schneider","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2309.08503v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.11504v2","updated":"2024-03-25T08:16:06Z","published":"2024-01-21T14:28:41Z","title":"With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation","summary":"  Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.\n","authors":["Y. Wang","D. Ma","D. Cai"],"pdf_url":"https://arxiv.org/pdf/2401.11504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16527v1","updated":"2024-03-25T08:11:02Z","published":"2024-03-25T08:11:02Z","title":"Hallucination Detection in Foundation Models for Decision-Making: A\n  Flexible Definition and Review of the State of the Art","summary":"  Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to\nagricultural field robots, and from health care assistants to the entertainment\nindustry. The majority of these systems are developed with modular\nsub-components for decision-making, planning, and control that may be\nhand-engineered or learning-based. While these existing approaches have been\nshown to perform well under the situations they were specifically designed for,\nthey can perform especially poorly in rare, out-of-distribution scenarios that\nwill undoubtedly arise at test-time. The rise of foundation models trained on\nmultiple tasks with impressively large datasets from a variety of fields has\nled researchers to believe that these models may provide common sense reasoning\nthat existing planners are missing. Researchers posit that this common sense\nreasoning will bridge the gap between algorithm development and deployment to\nout-of-distribution tasks, like how humans adapt to unexpected scenarios. Large\nlanguage models have already penetrated the robotics and autonomous systems\ndomains as researchers are scrambling to showcase their potential use cases in\ndeployment. While this application direction is very promising empirically,\nfoundation models are known to hallucinate and generate decisions that may\nsound reasonable, but are in fact poor. We argue there is a need to step back\nand simultaneously design systems that can quantify the certainty of a model's\ndecision, and detect when it may be hallucinating. In this work, we discuss the\ncurrent use cases of foundation models for decision-making tasks, provide a\ngeneral definition for hallucinations with examples, discuss existing\napproaches to hallucination detection and mitigation with a focus on decision\nproblems, and explore areas for further research in this exciting field.\n","authors":["Neeloy Chakraborty","Melkior Ornik","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2403.16527v1.pdf","comment":"31 pages, 2 tables"},{"id":"http://arxiv.org/abs/2403.16516v1","updated":"2024-03-25T08:00:43Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v1.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2403.16512v1","updated":"2024-03-25T07:55:29Z","published":"2024-03-25T07:55:29Z","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","summary":"  In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages.\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2403.16512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16504v1","updated":"2024-03-25T07:38:40Z","published":"2024-03-25T07:38:40Z","title":"LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent\n  Classification","summary":"  Following the significant achievements of large language models (LLMs),\nresearchers have employed in-context learning for text classification tasks.\nHowever, these studies focused on monolingual, single-turn classification\ntasks. In this paper, we introduce LARA (Linguistic-Adaptive\nRetrieval-Augmented Language Models), designed to enhance accuracy in\nmulti-turn classification tasks across six languages, accommodating numerous\nintents in chatbot interactions. Multi-turn intent classification is notably\nchallenging due to the complexity and evolving nature of conversational\ncontexts. LARA tackles these issues by combining a fine-tuned smaller model\nwith a retrieval-augmented mechanism, integrated within the architecture of\nLLMs. This integration allows LARA to dynamically utilize past dialogues and\nrelevant intents, thereby improving the understanding of the context.\nFurthermore, our adaptive retrieval techniques bolster the cross-lingual\ncapabilities of LLMs without extensive retraining and fine-tune. Comprehensive\nexperiments demonstrate that LARA achieves state-of-the-art performance on\nmulti-turn intent classification tasks, enhancing the average accuracy by 3.67%\ncompared to existing methods.\n","authors":["Liu Junhua","Tan Yong Keat","Fu Bin"],"pdf_url":"https://arxiv.org/pdf/2403.16504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16483v1","updated":"2024-03-25T07:08:13Z","published":"2024-03-25T07:08:13Z","title":"Automatic Construction of a Large-Scale Corpus for Geoparsing Using\n  Wikipedia Hyperlinks","summary":"  Geoparsing is the task of estimating the latitude and longitude (coordinates)\nof location expressions in texts. Geoparsing must deal with the ambiguity of\nthe expressions that indicate multiple locations with the same notation. For\nevaluating geoparsing systems, several corpora have been proposed in previous\nwork. However, these corpora are small-scale and suffer from the coverage of\nlocation expressions on general domains. In this paper, we propose Wikipedia\nHyperlink-based Location Linking (WHLL), a novel method to construct a\nlarge-scale corpus for geoparsing from Wikipedia articles. WHLL leverages\nhyperlinks in Wikipedia to annotate multiple location expressions with\ncoordinates. With this method, we constructed the WHLL corpus, a new\nlarge-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles,\neach containing about 7.8 unique location expressions. 45.6% of location\nexpressions are ambiguous and refer to more than one location with the same\nnotation. In each article, location expressions of the article title and those\nhyperlinks to other articles are assigned with coordinates. By utilizing\nhyperlinks, we can accurately assign location expressions with coordinates even\nwith ambiguous location expressions in the texts. Experimental results show\nthat there remains room for improvement by disambiguating location expressions.\n","authors":["Keyaki Ohno","Hirotaka Kameko","Keisuke Shirai","Taichi Nishimura","Shinsuke Mori"],"pdf_url":"https://arxiv.org/pdf/2403.16483v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2306.04357v4","updated":"2024-03-25T06:54:10Z","published":"2023-06-07T11:40:07Z","title":"Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue\n  Systems","summary":"  Dialogue response selection aims to select an appropriate response from\nseveral candidates based on a given user and system utterance history. Most\nexisting works primarily focus on post-training and fine-tuning tailored for\ncross-encoders. However, there are no post-training methods tailored for dense\nencoders in dialogue response selection. We argue that when the current\nlanguage model, based on dense dialogue systems (such as BERT), is employed as\na dense encoder, it separately encodes dialogue context and response, leading\nto a struggle to achieve the alignment of both representations. Thus, we\npropose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward\nyet effective post-training technique tailored for dense encoders in dialogue\nresponse selection. Dial-MAE uses an asymmetric encoder-decoder architecture to\ncompress the dialogue semantics into dense vectors, which achieves better\nalignment between the features of the dialogue context and response. Our\nexperiments have demonstrated that Dial-MAE is highly effective, achieving\nstate-of-the-art performance on two commonly evaluated benchmarks.\n","authors":["Zhenpeng Su","Xing Wu","Wei Zhou","Guangyuan Ma","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2306.04357v4.pdf","comment":"This paper has been accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2309.13182v2","updated":"2024-03-25T06:49:16Z","published":"2023-09-22T21:15:28Z","title":"Effective Distillation of Table-based Reasoning Ability from LLMs","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing tasks. However, their enormous\nparameter size and extremely high requirements for compute power pose\nchallenges for their practical deployment. Recent research has revealed that\nspecific capabilities of LLMs, such as numerical reasoning, can be transferred\nto smaller models through distillation. Some studies explore the potential of\nleveraging LLMs to perform table-based reasoning. However, there has been no\nprior work focusing on table reasoning skills in smaller models specifically\ntailored for scientific table-to-text generation tasks. In this paper, we\npropose a novel table-based reasoning distillation approach, with the aim of\ndistilling LLMs into tailored smaller models. Our experimental results have\nshown that a 220 million parameter model (Flan-T5-base) fine-tuned using\ndistilled data, not only achieves a significant improvement compared to\ntraditionally fine-tuned baselines, but also surpasses specific LLMs on a\nscientific table-to-text generation dataset. Our code is available at\nhttps://github.com/Bernard-Yang/DistillTableCoT.\n","authors":["Bohao Yang","Chen Tang","Kun Zhao","Chenghao Xiao","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2309.13182v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16463v1","updated":"2024-03-25T06:45:09Z","published":"2024-03-25T06:45:09Z","title":"Few-shot Named Entity Recognition via Superposition Concept\n  Discrimination","summary":"  Few-shot NER aims to identify entities of target types with only limited\nnumber of illustrative instances. Unfortunately, few-shot NER is severely\nchallenged by the intrinsic precise generalization problem, i.e., it is hard to\naccurately determine the desired target type due to the ambiguity stemming from\ninformation deficiency. In this paper, we propose Superposition Concept\nDiscriminator (SuperCD), which resolves the above challenge via an active\nlearning paradigm. Specifically, a concept extractor is first introduced to\nidentify superposition concepts from illustrative instances, with each concept\ncorresponding to a possible generalization boundary. Then a superposition\ninstance retriever is applied to retrieve corresponding instances of these\nsuperposition concepts from large-scale text corpus. Finally, annotators are\nasked to annotate the retrieved instances and these annotated instances\ntogether with original illustrative instances are used to learn FS-NER models.\nTo this end, we learn a universal concept extractor and superposition instance\nretriever using a large-scale openly available knowledge bases. Experiments\nshow that SuperCD can effectively identify superposition concepts from\nillustrative instances, retrieve superposition instances from large-scale\ncorpus, and significantly improve the few-shot NER performance with minimal\nadditional efforts.\n","authors":["Jiawei Chen","Hongyu Lin","Xianpei Han","Yaojie Lu","Shanshan Jiang","Bin Dong","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2403.16463v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16447v1","updated":"2024-03-25T06:18:18Z","published":"2024-03-25T06:18:18Z","title":"A Study on How Attention Scores in the BERT Model are Aware of Lexical\n  Categories in Syntactic and Semantic Tasks on the GLUE Benchmark","summary":"  This study examines whether the attention scores between tokens in the BERT\nmodel significantly vary based on lexical categories during the fine-tuning\nprocess for downstream tasks. Drawing inspiration from the notion that in human\nlanguage processing, syntactic and semantic information is parsed differently,\nwe categorize tokens in sentences according to their lexical categories and\nfocus on changes in attention scores among these categories. Our hypothesis\nposits that in downstream tasks that prioritize semantic information, attention\nscores centered on content words are enhanced, while in cases emphasizing\nsyntactic information, attention scores centered on function words are\nintensified. Through experimentation conducted on six tasks from the GLUE\nbenchmark dataset, we substantiate our hypothesis regarding the fine-tuning\nprocess. Furthermore, our additional investigations reveal the presence of BERT\nlayers that consistently assign more bias to specific lexical categories,\nirrespective of the task, highlighting the existence of task-agnostic lexical\ncategory preferences.\n","authors":["Dongjun Jang","Sungjoo Byun","Hyopil Shin"],"pdf_url":"https://arxiv.org/pdf/2403.16447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16446v1","updated":"2024-03-25T06:17:54Z","published":"2024-03-25T06:17:54Z","title":"Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric,\n  Data, and Algorithm","summary":"  Large language models (LLMs) are gaining increasing interests to improve\nclinical efficiency for medical diagnosis, owing to their unprecedented\nperformance in modelling natural language. Ensuring the safe and reliable\nclinical applications, the evaluation of LLMs indeed becomes critical for\nbetter mitigating the potential risks, e.g., hallucinations. However, current\nevaluation methods heavily rely on labor-intensive human participation to\nachieve human-preferred judgements. To overcome this challenge, we propose an\nautomatic evaluation paradigm tailored to assess the LLMs' capabilities in\ndelivering clinical services, e.g., disease diagnosis and treatment. The\nevaluation paradigm contains three basic elements: metric, data, and algorithm.\nSpecifically, inspired by professional clinical practice pathways, we formulate\na LLM-specific clinical pathway (LCP) to define the clinical capabilities that\na doctor agent should possess. Then, Standardized Patients (SPs) from the\nmedical education are introduced as the guideline for collecting medical data\nfor evaluation, which can well ensure the completeness of the evaluation\nprocedure. Leveraging these steps, we develop a multi-agent framework to\nsimulate the interactive environment between SPs and a doctor agent, which is\nequipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the\nbehaviors of a doctor agent are in accordance with LCP. The above paradigm can\nbe extended to any similar clinical scenarios to automatically evaluate the\nLLMs' medical capabilities. Applying such paradigm, we construct an evaluation\nbenchmark in the field of urology, including a LCP, a SPs dataset, and an\nautomated RAE. Extensive experiments are conducted to demonstrate the\neffectiveness of the proposed approach, providing more insights for LLMs' safe\nand reliable deployments in clinical practice.\n","authors":["Lei Liu","Xiaoyan Yang","Fangzhou Li","Chenfei Chi","Yue Shen","Shiwei Lyu Ming Zhang","Xiaowei Ma","Xiangguo Lyu","Liya Ma","Zhiqiang Zhang","Wei Xue","Yiran Huang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2403.16446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16444v1","updated":"2024-03-25T06:15:21Z","published":"2024-03-25T06:15:21Z","title":"KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for\n  Fine-Tuning Korean Large Language Models","summary":"  Instruction Tuning on Large Language Models is an essential process for model\nto function well and achieve high performance in specific tasks. Accordingly,\nin mainstream languages such as English, instruction-based datasets are being\nconstructed and made publicly available. In the case of Korean, publicly\navailable models and datasets all rely on using the output of ChatGPT or\ntranslating datasets built in English. In this paper, We introduce\n\\textit{KIT-19} as an instruction dataset for the development of LLM in Korean.\n\\textit{KIT-19} is a dataset created in an instruction format, comprising 19\nexisting open-source datasets for Korean NLP tasks. In this paper, we train a\nKorean Pretrained LLM using \\textit{KIT-19} to demonstrate its effectiveness.\nThe experimental results show that the model trained on \\textit{KIT-19}\nsignificantly outperforms existing Korean LLMs. Based on the its quality and\nempirical results, this paper proposes that \\textit{KIT-19} has the potential\nto make a substantial contribution to the future improvement of Korean LLMs'\nperformance.\n","authors":["Dongjun Jang","Sungjoo Byun","Hyemi Jo","Hyopil Shin"],"pdf_url":"https://arxiv.org/pdf/2403.16444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16443v1","updated":"2024-03-25T06:09:55Z","published":"2024-03-25T06:09:55Z","title":"CodeS: Natural Language to Code Repository via Multi-Layer Sketch","summary":"  The impressive performance of large language models (LLMs) on code-related\ntasks has shown the potential of fully automated software development. In light\nof this, we introduce a new software engineering task, namely Natural Language\nto code Repository (NL2Repo). This task aims to generate an entire code\nrepository from its natural language requirements. To address this task, we\npropose a simple yet effective framework CodeS, which decomposes NL2Repo into\nmultiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three\nmodules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first\ngenerates a repository's directory structure for given requirements;\nFileSketcher then generates a file sketch for each file in the generated\nstructure; SketchFiller finally fills in the details for each function in the\ngenerated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry\nout evaluations through both automated benchmarking and manual feedback\nanalysis. For benchmark-based evaluation, we craft a repository-oriented\nbenchmark, SketchEval, and design an evaluation metric, SketchBLEU. For\nfeedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30\nparticipants in conducting empirical studies. Extensive experiments prove the\neffectiveness and practicality of CodeS on the NL2Repo task.\n","authors":["Daoguang Zan","Ailun Yu","Wei Liu","Dong Chen","Bo Shen","Wei Li","Yafen Yao","Yongshun Gong","Xiaolin Chen","Bei Guan","Zhiguang Yang","Yongji Wang","Qianxiang Wang","Lizhen Cui"],"pdf_url":"https://arxiv.org/pdf/2403.16443v1.pdf","comment":"https://github.com/NL2Code/CodeS"},{"id":"http://arxiv.org/abs/2403.16442v1","updated":"2024-03-25T06:05:50Z","published":"2024-03-25T06:05:50Z","title":"If CLIP Could Talk: Understanding Vision-Language Model Representations\n  Through Their Preferred Concept Descriptions","summary":"  Recent works often assume that Vision-Language Model (VLM) representations\nare based on visual attributes like shape. However, it is unclear to what\nextent VLMs prioritize this information to represent concepts. We propose\nExtract and Explore (EX2), a novel approach to characterize important textual\nfeatures for VLMs. EX2 uses reinforcement learning to align a large language\nmodel with VLM preferences and generates descriptions that incorporate the\nimportant features for the VLM. Then, we inspect the descriptions to identify\nthe features that contribute to VLM representations. We find that spurious\ndescriptions have a major role in VLM representations despite providing no\nhelpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,\namong informative descriptions, VLMs rely significantly on non-visual\nattributes like habitat to represent visual concepts. Also, our analysis\nreveals that different VLMs prioritize different attributes in their\nrepresentations. Overall, we show that VLMs do not simply match images to scene\ndescriptions and that non-visual or even spurious descriptions significantly\ninfluence their representations.\n","authors":["Reza Esfandiarpoor","Cristina Menghini","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2403.16442v1.pdf","comment":"Code: https://github.com/BatsResearch/ex2"},{"id":"http://arxiv.org/abs/2310.14566v5","updated":"2024-03-25T06:05:24Z","published":"2023-10-23T04:49:09Z","title":"HallusionBench: An Advanced Diagnostic Suite for Entangled Language\n  Hallucination and Visual Illusion in Large Vision-Language Models","summary":"  We introduce HallusionBench, a comprehensive benchmark designed for the\nevaluation of image-context reasoning. This benchmark presents significant\nchallenges to advanced large visual-language models (LVLMs), such as\nGPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing\nnuanced understanding and interpretation of visual data. The benchmark\ncomprises 346 images paired with 1129 questions, all meticulously crafted by\nhuman experts. We introduce a novel structure for these visual questions\ndesigned to establish control groups. This structure enables us to conduct a\nquantitative analysis of the models' response tendencies, logical consistency,\nand various failure modes. In our evaluation on HallusionBench, we benchmarked\n15 different models, highlighting a 31.42% question-pair accuracy achieved by\nthe state-of-the-art GPT-4V. Notably, all other evaluated models achieve\naccuracy below 16%. Moreover, our analysis not only highlights the observed\nfailure modes, including language hallucination and visual illusion, but also\ndeepens an understanding of these pitfalls. Our comprehensive case studies\nwithin HallusionBench shed light on the challenges of hallucination and\nillusion in LVLMs. Based on these insights, we suggest potential pathways for\ntheir future improvement. The benchmark and codebase can be accessed at\nhttps://github.com/tianyi-lab/HallusionBench.\n","authors":["Tianrui Guan","Fuxiao Liu","Xiyang Wu","Ruiqi Xian","Zongxia Li","Xiaoyu Liu","Xijun Wang","Lichang Chen","Furong Huang","Yaser Yacoob","Dinesh Manocha","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.14566v5.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2311.08298v2","updated":"2024-03-25T06:01:49Z","published":"2023-11-14T16:43:29Z","title":"A Survey of Confidence Estimation and Calibration in Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks in various domains. Despite their impressive performance,\nthey can be unreliable due to factual errors in their generations. Assessing\ntheir confidence and calibrating them across different tasks can help mitigate\nrisks and enable LLMs to produce better generations. There has been a lot of\nrecent research aiming to address this, but there has been no comprehensive\noverview to organize it and outline the main lessons learned. The present\nsurvey aims to bridge this gap. In particular, we outline the challenges and we\nsummarize recent technical advancements for LLM confidence estimation and\ncalibration. We further discuss their applications and suggest promising\ndirections for future work.\n","authors":["Jiahui Geng","Fengyu Cai","Yuxia Wang","Heinz Koeppl","Preslav Nakov","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2311.08298v2.pdf","comment":"16 pages, 1 page, 1 table"},{"id":"http://arxiv.org/abs/2403.16437v1","updated":"2024-03-25T05:37:16Z","published":"2024-03-25T05:37:16Z","title":"Evaluating Large Language Models with Runtime Behavior of Program\n  Execution","summary":"  Large language models for code (i.e., code LLMs) have shown strong code\nunderstanding and generation capabilities. To evaluate the capabilities of code\nLLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval\nand ClassEval). Code reasoning is one of the most essential abilities of code\nLLMs, but existing benchmarks for code reasoning are not sufficient. Typically,\nthey focus on predicting the input and output of a program, ignoring the\nevaluation of the intermediate behavior during program execution, as well as\nthe logical consistency (e.g., the model should not give the correct output if\nthe prediction of execution path is wrong) when performing the reasoning. To\naddress these problems, in this paper, we propose a framework, namely REval,\nfor evaluating code reasoning abilities and consistency of code LLMs with\nprogram execution. We utilize existing code benchmarks and adapt them to new\nbenchmarks within our framework. A large-scale empirical study is conducted and\nmost LLMs show unsatisfactory performance on both Runtime Behavior Reasoning\n(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation\n(i.e., an average IC score of 10.3). Evaluation results of current code LLMs\nreflect the urgent need for the community to strengthen the code reasoning\ncapability of code LLMs.\n","authors":["Junkai Chen","Zhiyuan Pan","Xing Hu","Zhenhao Li","Ge Li","Xin Xia"],"pdf_url":"https://arxiv.org/pdf/2403.16437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06199v4","updated":"2024-03-25T05:36:56Z","published":"2024-03-10T12:43:27Z","title":"Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small\n  Language Models","summary":"  Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.\n","authors":["Minjie Zhu","Yichen Zhu","Xin Liu","Ning Liu","Zhiyuan Xu","Chaomin Shen","Yaxin Peng","Zhicai Ou","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2403.06199v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14643v2","updated":"2024-03-25T05:35:12Z","published":"2024-02-21T16:44:35Z","title":"Exploring ChatGPT and its Impact on Society","summary":"  Artificial intelligence has been around for a while, but suddenly it has\nreceived more attention than ever before. Thanks to innovations from companies\nlike Google, Microsoft, Meta, and other major brands in technology. OpenAI,\nthough, has triggered the button with its ground-breaking invention ChatGPT.\nChatGPT is a Large Language Model (LLM) based on Transformer architecture that\nhas the ability to generate human-like responses in a conversational context.\nIt uses deep learning algorithms to generate natural language responses to\ninput text. Its large number of parameters, contextual generation, and\nopen-domain training make it a versatile and effective tool for a wide range of\napplications, from chatbots to customer service to language translation. It has\nthe potential to revolutionize various industries and transform the way we\ninteract with technology. However, the use of ChatGPT has also raised several\nconcerns, including ethical, social, and employment challenges, which must be\ncarefully considered to ensure the responsible use of this technology. The\narticle provides an overview of ChatGPT, delving into its architecture and\ntraining process. It highlights the potential impacts of ChatGPT on the\nsociety. In this paper, we suggest some approaches involving technology,\nregulation, education, and ethics in an effort to maximize ChatGPT's benefits\nwhile minimizing its negative impacts. This study is expected to contribute to\na greater understanding of ChatGPT and aid in predicting the potential changes\nit may bring about.\n","authors":["Md. Asraful Haque","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2403.14643v2.pdf","comment":"13 Pages"},{"id":"http://arxiv.org/abs/2403.16435v1","updated":"2024-03-25T05:31:22Z","published":"2024-03-25T05:31:22Z","title":"InstUPR : Instruction-based Unsupervised Passage Reranking with Large\n  Language Models","summary":"  This paper introduces InstUPR, an unsupervised passage reranking method based\non large language models (LLMs). Different from existing approaches that rely\non extensive training with query-document pairs or retrieval-specific\ninstructions, our method leverages the instruction-following capabilities of\ninstruction-tuned LLMs for passage reranking without any additional\nfine-tuning. To achieve this, we introduce a soft score aggregation technique\nand employ pairwise reranking for unsupervised passage reranking. Experiments\non the BEIR benchmark demonstrate that InstUPR outperforms unsupervised\nbaselines as well as an instruction-tuned reranker, highlighting its\neffectiveness and superiority. Source code to reproduce all experiments is\nopen-sourced at https://github.com/MiuLab/InstUPR\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16435v1.pdf","comment":"Preprint. This manuscript was originally written and submitted in\n  June 2023"},{"id":"http://arxiv.org/abs/2403.16432v1","updated":"2024-03-25T05:27:35Z","published":"2024-03-25T05:27:35Z","title":"$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on\n  Prompt-based Language Models","summary":"  Prompt-based learning is a new language model training paradigm that adapts\nthe Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes\nthe performance benchmarks across various natural language processing (NLP)\ntasks. Instead of using a fixed prompt template to fine-tune the model, some\nresearch demonstrates the effectiveness of searching for the prompt via\noptimization. Such prompt optimization process of prompt-based learning on PLMs\nalso gives insight into generating adversarial prompts to mislead the model,\nraising concerns about the adversarial vulnerability of this paradigm. Recent\nstudies have shown that universal adversarial triggers (UATs) can be generated\nto alter not only the predictions of the target PLMs but also the prediction of\ncorresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based\nlearning paradigm. However, UATs found in previous works are often unreadable\ntokens or characters and can be easily distinguished from natural texts with\nadaptive defenses. In this work, we consider the naturalness of the UATs and\ndevelop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs\nby a gradient-based beam search algorithm that not only effectively attacks the\ntarget PLMs and PFMs but also maintains the naturalness among the trigger\ntokens. Extensive results demonstrate the effectiveness of\n$\\textit{LinkPrompt}$, as well as the transferability of UATs generated by\n\\textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and\nAPI-accessed LLM GPT-3.5-turbo.\n","authors":["Yue Xu","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16432v1.pdf","comment":"Accepted to the main conference of NAACL2024"},{"id":"http://arxiv.org/abs/2310.01777v2","updated":"2024-03-25T04:04:05Z","published":"2023-10-03T03:56:26Z","title":"SEA: Sparse Linear Attention with Estimated Attention Mask","summary":"  The transformer architecture has driven breakthroughs in recent years on\ntasks which require modeling pairwise relationships between sequential\nelements, as is the case in natural language understanding. However, long\nseqeuences pose a problem due to the quadratic complexity of the attention\noperation. Previous research has aimed to lower the complexity by sparsifying\nor linearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix and often\nrequire complete retraining from scratch. Furthermore, previous sparse and\nlinear approaches lose interpretability if they cannot produce full attention\nmatrices. To address these challenges, we propose SEA: Sparse linear attention\nwith an Estimated Attention mask. SEA estimates the attention matrix with\nlinear complexity via kernel-based linear attention, then subsequently creates\na sparse attention matrix with a top-k selection to perform a sparse attention\noperation. For language modeling tasks (Wikitext2), previous linear and sparse\nattention methods show roughly two-fold worse perplexity scores over the\nquadratic OPT-1.3B baseline, while SEA achieves better perplexity than\nOPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable\nattention matrix. We believe that our work will have a large practical impact,\nas it opens the possibility of running large transformers on resource-limited\ndevices with less memory.\n","authors":["Heejun Lee","Jina Kim","Jeffrey Willette","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2310.01777v2.pdf","comment":"9 main pages"},{"id":"http://arxiv.org/abs/2308.14115v2","updated":"2024-03-25T03:54:48Z","published":"2023-08-27T14:14:28Z","title":"Situated Natural Language Explanations","summary":"  Natural language is among the most accessible tools for explaining decisions\nto humans, and large pretrained language models (PLMs) have demonstrated\nimpressive abilities to generate coherent natural language explanations (NLE).\nThe existing NLE research perspectives do not take the audience into account.\nAn NLE can have high textual quality, but it might not accommodate audiences'\nneeds and preference. To address this limitation, we propose an alternative\nperspective, \\textit{situated} NLE. On the evaluation side, we set up automated\nevaluation scores. These scores describe the properties of NLEs in lexical,\nsemantic, and pragmatic categories. On the generation side, we identify three\nprompt engineering techniques and assess their applicability on the situations.\nSituated NLE provides a perspective and facilitates further research on the\ngeneration and evaluation of explanations.\n","authors":["Zining Zhu","Haoming Jiang","Jingfeng Yang","Sreyashi Nag","Chao Zhang","Jie Huang","Yifan Gao","Frank Rudzicz","Bing Yin"],"pdf_url":"https://arxiv.org/pdf/2308.14115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16396v1","updated":"2024-03-25T03:19:20Z","published":"2024-03-25T03:19:20Z","title":"Is There a One-Model-Fits-All Approach to Information Extraction?\n  Revisiting Task Definition Biases","summary":"  Definition bias is a negative phenomenon that can mislead models. Definition\nbias in information extraction appears not only across datasets from different\ndomains but also within datasets sharing the same domain. We identify two types\nof definition bias in IE: bias among information extraction datasets and bias\nbetween information extraction datasets and instruction tuning datasets. To\nsystematically investigate definition bias, we conduct three probing\nexperiments to quantitatively analyze it and discover the limitations of\nunified information extraction and large language models in solving definition\nbias. To mitigate definition bias in information extraction, we propose a\nmulti-stage framework consisting of definition bias measurement, bias-aware\nfine-tuning, and task-specific bias mitigation. Experimental results\ndemonstrate the effectiveness of our framework in addressing definition bias.\nResources of this paper can be found at\nhttps://github.com/EZ-hwh/definition-bias\n","authors":["Wenhao Huang","Qianyu He","Zhixu Li","Jiaqing Liang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.16396v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16394v1","updated":"2024-03-25T03:18:39Z","published":"2024-03-25T03:18:39Z","title":"Skews in the Phenomenon Space Hinder Generalization in Text-to-Image\n  Generation","summary":"  The literature on text-to-image generation is plagued by issues of faithfully\ncomposing entities with relations. But there lacks a formal understanding of\nhow entity-relation compositions can be effectively learned. Moreover, the\nunderlying phenomenon space that meaningfully reflects the problem structure is\nnot well-defined, leading to an arms race for larger quantities of data in the\nhope that generalization emerges out of large-scale pretraining. We hypothesize\nthat the underlying phenomenological coverage has not been proportionally\nscaled up, leading to a skew of the presented phenomenon which harms\ngeneralization. We introduce statistical metrics that quantify both the\nlinguistic and visual skew of a dataset for relational learning, and show that\ngeneralization failures of text-to-image generation are a direct result of\nincomplete or unbalanced phenomenological coverage. We first perform\nexperiments in a synthetic domain and demonstrate that systematically\ncontrolled metrics are strongly predictive of generalization performance. Then\nwe move to natural images and show that simple distribution perturbations in\nlight of our theories boost generalization without enlarging the absolute data\nsize. This work informs an important direction towards quality-enhancing the\ndata diversity or balance orthogonal to scaling up the absolute size. Our\ndiscussions point out important open questions on 1) Evaluation of generated\nentity-relation compositions, and 2) Better models for reasoning with abstract\nrelations.\n","authors":["Yingshan Chang","Yasi Zhang","Zhiyuan Fang","Yingnian Wu","Yonatan Bisk","Feng Gao"],"pdf_url":"https://arxiv.org/pdf/2403.16394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15365v2","updated":"2024-03-25T03:06:08Z","published":"2024-03-22T17:33:11Z","title":"A Transfer Attack to Image Watermarks","summary":"  Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.\n","authors":["Yuepeng Hu","Zhengyuan Jiang","Moyang Guo","Neil Gong"],"pdf_url":"https://arxiv.org/pdf/2403.15365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16385v1","updated":"2024-03-25T03:02:27Z","published":"2024-03-25T03:02:27Z","title":"Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators\n  for Reasoning-Based Chart VQA","summary":"  Understanding data visualizations like charts and plots requires reasoning\nabout both visual elements and numerics. Although strong in extractive\nquestions, current chart visual question answering (chart VQA) models suffer on\ncomplex reasoning questions. In this work, we address the lack of reasoning\nability by data augmentation. We leverage Large Language Models (LLMs), which\nhave shown to have strong reasoning ability, as an automatic data annotator\nthat generates question-answer annotations for chart images. The key innovation\nin our method lies in the Synthesize Step-by-Step strategy: our LLM-based data\ngenerator learns to decompose the complex question into step-by-step\nsub-questions (rationales), which are then used to derive the final answer\nusing external tools, i.e. Python. This step-wise generation procedure is\ntrained on synthetic data generated using a template-based QA generation\npipeline. Experimental results highlight the significance of the proposed\nstep-by-step generation. By training with the LLM-augmented data (LAMENDA), we\nsignificantly enhance the chart VQA models, achieving the state-of-the-art\naccuracy on the ChartQA and PlotQA datasets. In particular, our approach\nimproves the accuracy of the previous state-of-the-art approach from 38% to 54%\non the human-written questions in the ChartQA dataset, which needs strong\nreasoning. We hope our work underscores the potential of synthetic data and\nencourages further exploration of data augmentation using LLMs for\nreasoning-heavy tasks.\n","authors":["Li Zhuowan","Jasani Bhavan","Tang Peng","Ghadar Shabnam"],"pdf_url":"https://arxiv.org/pdf/2403.16385v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2308.11432v4","updated":"2024-03-25T02:56:58Z","published":"2023-08-22T13:30:37Z","title":"A Survey on Large Language Model based Autonomous Agents","summary":"  Autonomous agents have long been a prominent research focus in both academic\nand industry communities. Previous research in this field often focuses on\ntraining agents with limited knowledge within isolated environments, which\ndiverges significantly from human learning processes, and thus makes the agents\nhard to achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating LLM-based autonomous agents. In this paper, we\npresent a comprehensive survey of these studies, delivering a systematic review\nof the field of LLM-based autonomous agents from a holistic perspective. More\nspecifically, we first discuss the construction of LLM-based autonomous agents,\nfor which we propose a unified framework that encompasses a majority of the\nprevious work. Then, we present a comprehensive overview of the diverse\napplications of LLM-based autonomous agents in the fields of social science,\nnatural science, and engineering. Finally, we delve into the evaluation\nstrategies commonly used for LLM-based autonomous agents. Based on the previous\nstudies, we also present several challenges and future directions in this\nfield. To keep track of this field and continuously update our survey, we\nmaintain a repository of relevant references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.\n","authors":["Lei Wang","Chen Ma","Xueyang Feng","Zeyu Zhang","Hao Yang","Jingsen Zhang","Zhiyuan Chen","Jiakai Tang","Xu Chen","Yankai Lin","Wayne Xin Zhao","Zhewei Wei","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2308.11432v4.pdf","comment":"35 pages, 5 figures, 3 tables, has been accepted by frontiers of\n  computer science (FCS), doi={10.1007/s11704-024-40231-1}"},{"id":"http://arxiv.org/abs/2402.10670v2","updated":"2024-03-25T02:52:43Z","published":"2024-02-16T13:21:33Z","title":"OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via\n  Vision-Language Foundation Models","summary":"  Object navigation (ObjectNav) requires an agent to navigate through unseen\nenvironments to find queried objects. Many previous methods attempted to solve\nthis task by relying on supervised or reinforcement learning, where they are\ntrained on limited household datasets with close-set objects. However, two key\nchallenges are unsolved: understanding free-form natural language instructions\nthat demand open-set objects, and generalizing to new environments in a\nzero-shot manner. Aiming to solve the two challenges, in this paper, we propose\nOpenFMNav, an Open-set Foundation Model based framework for zero-shot object\nNavigation. We first unleash the reasoning abilities of large language models\n(LLMs) to extract proposed objects from natural language instructions that meet\nthe user's demand. We then leverage the generalizability of large vision\nlanguage models (VLMs) to actively discover and detect candidate objects from\nthe scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting\ncommon sense reasoning on VSSM, our method can perform effective\nlanguage-guided exploration and exploitation of the scene and finally reach the\ngoal. By leveraging the reasoning and generalizing abilities of foundation\nmodels, our method can understand free-form human instructions and perform\neffective open-set zero-shot navigation in diverse environments. Extensive\nexperiments on the HM3D ObjectNav benchmark show that our method surpasses all\nthe strong baselines on all metrics, proving our method's effectiveness.\nFurthermore, we perform real robot demonstrations to validate our method's\nopen-set-ness and generalizability to real-world environments.\n","authors":["Yuxuan Kuang","Hai Lin","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2402.10670v2.pdf","comment":"NAACL 2024 Findings"},{"id":"http://arxiv.org/abs/2403.16345v1","updated":"2024-03-25T00:43:44Z","published":"2024-03-25T00:43:44Z","title":"Enhanced Facet Generation with LLM Editing","summary":"  In information retrieval, facet identification of a user query is an\nimportant task. If a search service can recognize the facets of a user's query,\nit has the potential to offer users a much broader range of search results.\nPrevious studies can enhance facet prediction by leveraging retrieved documents\nand related queries obtained through a search engine. However, there are\nchallenges in extending it to other applications when a search engine operates\nas part of the model. First, search engines are constantly updated. Therefore,\nadditional information may change during training and test, which may reduce\nperformance. The second challenge is that public search engines cannot search\nfor internal documents. Therefore, a separate search system needs to be built\nto incorporate documents from private domains within the company. We propose\ntwo strategies that focus on a framework that can predict facets by taking only\nqueries as input without a search engine. The first strategy is multi-task\nlearning to predict SERP. By leveraging SERP as a target instead of a source,\nthe proposed model deeply understands queries without relying on external\nmodules. The second strategy is to enhance the facets by combining Large\nLanguage Model (LLM) and the small model. Overall performance improves when\nsmall model and LLM are combined rather than facet generation individually.\n","authors":["Joosung Lee","Jinhong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.16345v1.pdf","comment":"Accepted at LREC-COLING 2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.08651v2","updated":"2024-03-25T14:41:07Z","published":"2024-03-13T16:06:07Z","title":"HAIFIT: Human-Centered AI for Fashion Image Translation","summary":"  In the realm of fashion design, sketches serve as the canvas for expressing\nan artist's distinctive drawing style and creative vision, capturing intricate\ndetails like stroke variations and texture nuances. The advent of\nsketch-to-image cross-modal translation technology has notably aided designers.\nHowever, existing methods often compromise these sketch details during image\ngeneration, resulting in images that deviate from the designer's intended\nconcept. This limitation hampers the ability to offer designers a precise\npreview of the final output. To overcome this challenge, we introduce HAIFIT, a\nnovel approach that transforms sketches into high-fidelity, lifelike clothing\nimages by integrating multi-scale features and capturing extensive feature map\ndependencies from diverse perspectives. Through extensive qualitative and\nquantitative evaluations conducted on our self-collected dataset, our method\ndemonstrates superior performance compared to existing methods in generating\nphotorealistic clothing images. Our method excels in preserving the distinctive\nstyle and intricate details essential for fashion design applications.\n","authors":["Jianan Jiang","Xinglin Li","Weiren Yu","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08651v2.pdf","comment":"8 pages,8 figures"},{"id":"http://arxiv.org/abs/2402.19463v2","updated":"2024-03-25T14:27:03Z","published":"2024-02-29T18:54:53Z","title":"SeMoLi: What Moves Together Belongs Together","summary":"  We tackle semi-supervised object detection based on motion cues. Recent\nresults suggest that heuristic-based clustering methods in conjunction with\nobject trackers can be used to pseudo-label instances of moving objects and use\nthese as supervisory signals to train 3D object detectors in Lidar data without\nmanual supervision. We re-think this approach and suggest that both, object\ndetection, as well as motion-inspired pseudo-labeling, can be tackled in a\ndata-driven manner. We leverage recent advances in scene flow estimation to\nobtain point trajectories from which we extract long-term, class-agnostic\nmotion patterns. Revisiting correlation clustering in the context of message\npassing networks, we learn to group those motion patterns to cluster points to\nobject instances. By estimating the full extent of the objects, we obtain\nper-scan 3D bounding boxes that we use to supervise a Lidar object detection\nnetwork. Our method not only outperforms prior heuristic-based approaches (57.5\nAP, +14 improvement over prior work), more importantly, we show we can\npseudo-label and train object detectors across datasets.\n","authors":["Jenny Seidenschwarz","Aljoša Ošep","Francesco Ferroni","Simon Lucey","Laura Leal-Taixé"],"pdf_url":"https://arxiv.org/pdf/2402.19463v2.pdf","comment":"Accepted to CVPR 2024!"},{"id":"http://arxiv.org/abs/2403.16803v1","updated":"2024-03-25T14:21:49Z","published":"2024-03-25T14:21:49Z","title":"Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View\n  Planning","summary":"  Object reconstruction is relevant for many autonomous robotic tasks that\nrequire interaction with the environment. A key challenge in such scenarios is\nplanning view configurations to collect informative measurements for\nreconstructing an initially unknown object. One-shot view planning enables\nefficient data collection by predicting view configurations and planning the\nglobally shortest path connecting all views at once. However, geometric priors\nabout the object are required to conduct one-shot view planning. In this work,\nwe propose a novel one-shot view planning approach that utilizes the powerful\n3D generation capabilities of diffusion models as priors. By incorporating such\ngeometric priors into our pipeline, we achieve effective one-shot view planning\nstarting with only a single RGB image of the object to be reconstructed. Our\nplanning experiments in simulation and real-world setups indicate that our\napproach balances well between object reconstruction quality and movement cost.\n","authors":["Sicong Pan","Liren Jin","Xuying Huang","Cyrill Stachniss","Marija Popović","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2403.16803v1.pdf","comment":"Sicong Pan and Liren Jin have equal contribution. Submitted to IROS\n  2024"},{"id":"http://arxiv.org/abs/2403.16794v1","updated":"2024-03-25T14:13:09Z","published":"2024-03-25T14:13:09Z","title":"CurbNet: Curb Detection Framework Based on LiDAR Point Cloud\n  Segmentation","summary":"  Curb detection is an important function in intelligent driving and can be\nused to determine drivable areas of the road. However, curbs are difficult to\ndetect due to the complex road environment. This paper introduces CurbNet, a\nnovel framework for curb detection, leveraging point cloud segmentation.\nAddressing the dearth of comprehensive curb datasets and the absence of 3D\nannotations, we have developed the 3D-Curb dataset, encompassing 7,100 frames,\nwhich represents the largest and most categorically diverse collection of curb\npoint clouds currently available. Recognizing that curbs are primarily\ncharacterized by height variations, our approach harnesses spatially-rich 3D\npoint clouds for training. To tackle the challenges presented by the uneven\ndistribution of curb features on the xy-plane and their reliance on z-axis\nhigh-frequency features, we introduce the multi-scale and channel attention\n(MSCA) module, a bespoke solution designed to optimize detection performance.\nMoreover, we propose an adaptive weighted loss function group, specifically\nformulated to counteract the imbalance in the distribution of curb point clouds\nrelative to other categories. Our extensive experimentation on 2 major datasets\nhas yielded results that surpass existing benchmarks set by leading curb\ndetection and point cloud segmentation models. By integrating multi-clustering\nand curve fitting techniques in our post-processing stage, we have\nsubstantially reduced noise in curb detection, thereby enhancing precision to\n0.8744. Notably, CurbNet has achieved an exceptional average metrics of over\n0.95 at a tolerance of just 0.15m, thereby establishing a new benchmark.\nFurthermore, corroborative real-world experiments and dataset analyzes mutually\nvalidate each other, solidifying CurbNet's superior detection proficiency and\nits robust generalizability.\n","authors":["Guoyang Zhao","Fulong Ma","Yuxuan Liu","Weiqing Qi","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16788v1","updated":"2024-03-25T14:02:33Z","published":"2024-03-25T14:02:33Z","title":"HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic\n  Segmentation","summary":"  Event-based semantic segmentation has gained popularity due to its capability\nto deal with scenarios under high-speed motion and extreme lighting conditions,\nwhich cannot be addressed by conventional RGB cameras. Since it is hard to\nannotate event data, previous approaches rely on event-to-image reconstruction\nto obtain pseudo labels for training. However, this will inevitably introduce\nnoise, and learning from noisy pseudo labels, especially when generated from a\nsingle source, may reinforce the errors. This drawback is also called\nconfirmation bias in pseudo-labeling. In this paper, we propose a novel hybrid\npseudo-labeling framework for unsupervised event-based semantic segmentation,\nHPL-ESS, to alleviate the influence of noisy pseudo labels. In particular, we\nfirst employ a plain unsupervised domain adaptation framework as our baseline,\nwhich can generate a set of pseudo labels through self-training. Then, we\nincorporate offline event-to-image reconstruction into the framework, and\nobtain another set of pseudo labels by predicting segmentation maps on the\nreconstructed images. A noisy label learning strategy is designed to mix the\ntwo sets of pseudo labels and enhance the quality. Moreover, we propose a soft\nprototypical alignment module to further improve the consistency of target\ndomain features. Extensive experiments show that our proposed method\noutperforms existing state-of-the-art methods by a large margin on the\nDSEC-Semantic dataset (+5.88% accuracy, +10.32% mIoU), which even surpasses\nseveral supervised methods.\n","authors":["Linglin Jing","Yiming Ding","Yunpeng Gao","Zhigang Wang","Xu Yan","Dong Wang","Gerald Schaefer","Hui Fang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2403.16788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16782v1","updated":"2024-03-25T13:57:45Z","published":"2024-03-25T13:57:45Z","title":"The Anatomy of Adversarial Attacks: Concept-based XAI Dissection","summary":"  Adversarial attacks (AAs) pose a significant threat to the reliability and\nrobustness of deep neural networks. While the impact of these attacks on model\npredictions has been extensively studied, their effect on the learned\nrepresentations and concepts within these models remains largely unexplored. In\nthis work, we perform an in-depth analysis of the influence of AAs on the\nconcepts learned by convolutional neural networks (CNNs) using eXplainable\nartificial intelligence (XAI) techniques. Through an extensive set of\nexperiments across various network architectures and targeted AA techniques, we\nunveil several key findings. First, AAs induce substantial alterations in the\nconcept composition within the feature space, introducing new concepts or\nmodifying existing ones. Second, the adversarial perturbation itself can be\nlinearly decomposed into a set of latent vector components, with a subset of\nthese being responsible for the attack's success. Notably, we discover that\nthese components are target-specific, i.e., are similar for a given target\nclass throughout different AA techniques and starting classes. Our findings\nprovide valuable insights into the nature of AAs and their impact on learned\nrepresentations, paving the way for the development of more robust and\ninterpretable deep learning models, as well as effective defenses against\nadversarial threats.\n","authors":["Georgii Mikriukov","Gesina Schwalbe","Franz Motzkus","Korinna Bade"],"pdf_url":"https://arxiv.org/pdf/2403.16782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16776v1","updated":"2024-03-25T13:52:48Z","published":"2024-03-25T13:52:48Z","title":"Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases","summary":"  Anatomical atlases are widely used for population analysis. Conditional\natlases target a particular sub-population defined via certain conditions (e.g.\ndemographics or pathologies) and allow for the investigation of fine-grained\nanatomical differences - such as morphological changes correlated with age.\nExisting approaches use either registration-based methods that are unable to\nhandle large anatomical variations or generative models, which can suffer from\ntraining instabilities and hallucinations. To overcome these limitations, we\nuse latent diffusion models to generate deformation fields, which transform a\ngeneral population atlas into one representing a specific sub-population. By\ngenerating a deformation field and registering the conditional atlas to a\nneighbourhood of images, we ensure structural plausibility and avoid\nhallucinations, which can occur during direct image synthesis. We compare our\nmethod to several state-of-the-art atlas generation methods in experiments\nusing 5000 brain as well as whole-body MR images from UK Biobank. Our method\ngenerates highly realistic atlases with smooth transformations and high\nanatomical fidelity, outperforming the baselines.\n","authors":["Sophie Starck","Vasiliki Sideri-Lampretsa","Bernhard Kainz","Martin Menten","Tamara Mueller","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2403.16776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14897v2","updated":"2024-03-25T13:46:03Z","published":"2024-03-22T01:02:09Z","title":"Geometric Generative Models based on Morphological Equivariant PDEs and\n  GANs","summary":"  Content and image generation consist in creating or generating data from\nnoisy information by extracting specific features such as texture, edges, and\nother thin image structures. We are interested here in generative models, and\ntwo main problems are addressed. Firstly, the improvements of specific feature\nextraction while accounting at multiscale levels intrinsic geometric features;\nand secondly, the equivariance of the network to reduce its complexity and\nprovide a geometric interpretability. To proceed, we propose a geometric\ngenerative model based on an equivariant partial differential equation (PDE)\nfor group convolution neural networks (G-CNNs), so called PDE-G-CNNs, built on\nmorphology operators and generative adversarial networks (GANs). Equivariant\nmorphological PDE layers are composed of multiscale dilations and erosions\nformulated in Riemannian manifolds, while group symmetries are defined on a Lie\ngroup. We take advantage of the Lie group structure to properly integrate the\nequivariance in layers, and are able to use the Riemannian metric to solve the\nmultiscale morphological operations. Each point of the Lie group is associated\nwith a unique point in the manifold, which helps us derive a metric on the\nRiemannian manifold from a tensor field invariant under the Lie group so that\nthe induced metric has the same symmetries. The proposed geometric\nmorphological GAN (GM-GAN) is obtained by using the proposed morphological\nequivariant convolutions in PDE-G-CNNs to bring nonlinearity in classical CNNs.\nGM-GAN is evaluated on MNIST data and compared with GANs. Preliminary results\nshow that GM-GAN model outperforms classical GAN.\n","authors":["El Hadji S. Diop","Thierno Fall","Alioune Mbengue","Mohamed Daoudi"],"pdf_url":"https://arxiv.org/pdf/2403.14897v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16700v2","updated":"2024-03-25T13:33:51Z","published":"2024-01-30T03:00:25Z","title":"Towards Precise 3D Human Pose Estimation with Multi-Perspective\n  Spatial-Temporal Relational Transformers","summary":"  3D human pose estimation captures the human joint points in three-dimensional\nspace while keeping the depth information and physical structure. That is\nessential for applications that require precise pose information, such as\nhuman-computer interaction, scene understanding, and rehabilitation training.\nDue to the challenges in data collection, mainstream datasets of 3D human pose\nestimation are primarily composed of multi-view video data collected in\nlaboratory environments, which contains rich spatial-temporal correlation\ninformation besides the image frame content. Given the remarkable\nself-attention mechanism of transformers, capable of capturing the\nspatial-temporal correlation from multi-view video datasets, we propose a\nmulti-stage framework for 3D sequence-to-sequence (seq2seq) human pose\ndetection. Firstly, the spatial module represents the human pose feature by\nintra-image content, while the frame-image relation module extracts temporal\nrelationships and 3D spatial positional relationship features between the\nmulti-perspective images. Secondly, the self-attention mechanism is adopted to\neliminate the interference from non-human body parts and reduce computing\nresources. Our method is evaluated on Human3.6M, a popular 3D human pose\ndetection dataset. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance on this dataset. The source code will be available\nat https://github.com/WUJINHUAN/3D-human-pose.\n","authors":["Jianbin Jiao","Xina Cheng","Weijie Chen","Xiaoting Yin","Hao Shi","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2401.16700v2.pdf","comment":"Accepted to IJCNN 2024. The source code will be available at\n  https://github.com/WUJINHUAN/3D-human-pose"},{"id":"http://arxiv.org/abs/2402.04599v2","updated":"2024-03-25T13:30:37Z","published":"2024-02-07T05:47:31Z","title":"Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via\n  Temporal-Viewpoint Alignment","summary":"  Video sequences exhibit significant nuisance variations (undesired effects)\nof speed of actions, temporal locations, and subjects' poses, leading to\ntemporal-viewpoint misalignment when comparing two sets of frames or evaluating\nthe similarity of two sequences. Thus, we propose Joint tEmporal and cAmera\nviewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D\nskeleton sequences whose camera and subjects' poses can be easily manipulated\nin 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where\nmatching well temporal blocks (temporal chunks that make up a sequence) of\nsupport-query sequence pairs (by factoring out nuisance variations) is\nessential due to limited samples of novel classes. Given a query sequence, we\ncreate its several views by simulating several camera locations. For a support\nsequence, we match it with view-simulated query sequences, as in the popular\nDynamic Time Warping (DTW). Specifically, each support temporal block can be\nmatched to the query temporal block with the same or adjacent (next) temporal\nindex, and adjacent camera views to achieve joint local temporal-viewpoint\nwarping. JEANIE selects the smallest distance among matching paths with\ndifferent temporal-viewpoint warping patterns, an advantage over DTW which only\nperforms temporal alignment. We also propose an unsupervised FSAR akin to\nclustering of sequences with JEANIE as a distance measure. JEANIE achieves\nstate-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D\nMultiview Activity II on supervised and unsupervised FSAR, and their\nmeta-learning inspired fusion.\n","authors":["Lei Wang","Jun Liu","Liang Zheng","Tom Gedeon","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2402.04599v2.pdf","comment":"Accepted by the International Journal of Computer Vision (IJCV). An\n  extension of our ACCV'22 paper [arXiv:arXiv:2210.16820] which was\n  distinguished by the Sang Uk Lee Best Student Paper Award"},{"id":"http://arxiv.org/abs/2403.06764v2","updated":"2024-03-25T13:29:30Z","published":"2024-03-11T14:35:32Z","title":"An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference\n  Acceleration for Large Vision-Language Models","summary":"  In this study, we identify the inefficient attention phenomena in Large\nVision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,\nQwenVL-Chat and Video-LLaVA. We find out that the attention computation over\nvisual tokens is of extreme inefficiency in the deep layers of popular LVLMs,\nsuggesting a need for a sparser approach compared to textual data handling. To\nthis end, we introduce FastV, a versatile plug-and-play method designed to\noptimize computational efficiency by learning adaptive attention patterns in\nearly layers and pruning visual tokens in subsequent ones. Our evaluations\ndemonstrate FastV's ability to dramatically reduce computational costs (e.g., a\n45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a\nwide range of image and video understanding tasks. The computational efficiency\nand performance trade-off of FastV are highly customizable and\npareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve\na lower budget than that of a 7B-parameter model, while still maintaining\nsuperior performance. We believe FastV has practical values for deployment of\nLVLMs in edge devices and commercial models. Code is released at\nhttps://github.com/pkunlp-icler/FastV.\n","authors":["Liang Chen","Haozhe Zhao","Tianyu Liu","Shuai Bai","Junyang Lin","Chang Zhou","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2403.06764v2.pdf","comment":"21 papes, 8 figures, code is released at\n  https://github.com/pkunlp-icler/FastV"},{"id":"http://arxiv.org/abs/2402.15648v2","updated":"2024-03-25T13:27:26Z","published":"2024-02-23T23:15:54Z","title":"MambaIR: A Simple Baseline for Image Restoration with State-Space Model","summary":"  Recent years have seen significant advancements in image restoration, largely\nattributed to the development of modern deep neural networks, such as CNNs and\nTransformers. However, existing restoration backbones often face the dilemma\nbetween global receptive fields and efficient computation, hindering their\napplication in practice. Recently, the Selective Structured State Space Model,\nespecially the improved version Mamba, has shown great potential for long-range\ndependency modeling with linear complexity, which offers a way to resolve the\nabove dilemma. However, the standard Mamba still faces certain challenges in\nlow-level vision such as local pixel forgetting and channel redundancy. In this\nwork, we introduce a simple but effective baseline, named MambaIR, which\nintroduces both local enhancement and channel attention to improve the vanilla\nMamba. In this way, our MambaIR takes advantage of the local pixel similarity\nand reduces the channel redundancy. Extensive experiments demonstrate the\nsuperiority of our method, for example, MambaIR outperforms SwinIR by up to\n0.45dB on image SR, using similar computational cost but with a global\nreceptive field. Code is available at \\url{https://github.com/csguoh/MambaIR}.\n","authors":["Hang Guo","Jinmin Li","Tao Dai","Zhihao Ouyang","Xudong Ren","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2402.15648v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2312.11897v2","updated":"2024-03-25T13:15:22Z","published":"2023-12-19T06:42:47Z","title":"Text-Conditioned Resampler For Long Form Video Understanding","summary":"  In this paper we present a text-conditioned video resampler (TCR) module that\nuses a pre-trained and frozen visual encoder and large language model (LLM) to\nprocess long video sequences for a task. TCR localises relevant visual features\nfrom the video given a text condition and provides them to a LLM to generate a\ntext response. Due to its lightweight design and use of cross-attention, TCR\ncan process more than 100 frames at a time with plain attention and without\noptimised implementations. We make the following contributions: (i) we design a\ntransformer-based sampling architecture that can process long videos\nconditioned on a task, together with a training method that enables it to\nbridge pre-trained visual and language models; (ii) we identify tasks that\ncould benefit from longer video perception; and (iii) we empirically validate\nits efficacy on a wide variety of evaluation tasks including NextQA, EgoSchema,\nand the EGO4D-LTA challenge.\n","authors":["Bruno Korbar","Yongqin Xian","Alessio Tonioni","Andrew Zisserman","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2312.11897v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16736v1","updated":"2024-03-25T13:09:40Z","published":"2024-03-25T13:09:40Z","title":"Creating a Digital Twin of Spinal Surgery: A Proof of Concept","summary":"  Surgery digitalization is the process of creating a virtual replica of\nreal-world surgery, also referred to as a surgical digital twin (SDT). It has\nsignificant applications in various fields such as education and training,\nsurgical planning, and automation of surgical tasks. Given their detailed\nrepresentations of surgical procedures, SDTs are an ideal foundation for\nmachine learning methods, enabling automatic generation of training data. In\nrobotic surgery, SDTs can provide realistic virtual environments in which\nrobots may learn through trial and error. In this paper, we present a proof of\nconcept (PoC) for surgery digitalization that is applied to an ex-vivo spinal\nsurgery performed in realistic conditions. The proposed digitalization focuses\non the acquisition and modelling of the geometry and appearance of the entire\nsurgical scene. We employ five RGB-D cameras for dynamic 3D reconstruction of\nthe surgeon, a high-end camera for 3D reconstruction of the anatomy, an\ninfrared stereo camera for surgical instrument tracking, and a laser scanner\nfor 3D reconstruction of the operating room and data fusion. We justify the\nproposed methodology, discuss the challenges faced and further extensions of\nour prototype. While our PoC partially relies on manual data curation, its high\nquality and great potential motivate the development of automated methods for\nthe creation of SDTs. The quality of our SDT can be assessed in a rendered\nvideo available at https://youtu.be/LqVaWGgaTMY .\n","authors":["Jonas Hein","Frederic Giraud","Lilian Calvet","Alexander Schwarz","Nicola Alessandro Cavalcanti","Sergey Prokudin","Mazda Farshad","Siyu Tang","Marc Pollefeys","Fabio Carrillo","Philipp Fürnstahl"],"pdf_url":"https://arxiv.org/pdf/2403.16736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00374v3","updated":"2024-03-25T13:01:27Z","published":"2023-12-31T02:25:41Z","title":"EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via\n  Expressive Masked Audio Gesture Modeling","summary":"  We propose EMAGE, a framework to generate full-body human gestures from audio\nand masked gestures, encompassing facial, local body, hands, and global\nmovements. To achieve this, we first introduce BEAT2 (BEAT-SMPLX-FLAME), a new\nmesh-level holistic co-speech dataset. BEAT2 combines MoShed SMPLX body with\nFLAME head parameters and further refines the modeling of head, neck, and\nfinger movements, offering a community-standardized, high-quality 3D motion\ncaptured dataset. EMAGE leverages masked body gesture priors during training to\nboost inference performance. It involves a Masked Audio Gesture Transformer,\nfacilitating joint training on audio-to-gesture generation and masked gesture\nreconstruction to effectively encode audio and body gesture hints. Encoded body\nhints from masked gestures are then separately employed to generate facial and\nbody movements. Moreover, EMAGE adaptively merges speech features from the\naudio's rhythm and content and utilizes four compositional VQ-VAEs to enhance\nthe results' fidelity and diversity. Experiments demonstrate that EMAGE\ngenerates holistic gestures with state-of-the-art performance and is flexible\nin accepting predefined spatial-temporal gesture inputs, generating complete,\naudio-synchronized results. Our code and dataset are available at\nhttps://pantomatrix.github.io/EMAGE/\n","authors":["Haiyang Liu","Zihao Zhu","Giorgio Becherini","Yichen Peng","Mingyang Su","You Zhou","Xuefei Zhe","Naoya Iwamoto","Bo Zheng","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2401.00374v3.pdf","comment":"CVPR Camera Ready; Project Page: https://pantomatrix.github.io/EMAGE/"},{"id":"http://arxiv.org/abs/2402.07310v2","updated":"2024-03-25T12:58:45Z","published":"2024-02-11T21:16:42Z","title":"BioNeRF: Biologically Plausible Neural Radiance Fields for View\n  Synthesis","summary":"  This paper presents BioNeRF, a biologically plausible architecture that\nmodels scenes in a 3D representation and synthesizes new views through radiance\nfields. Since NeRF relies on the network weights to store the scene's\n3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism\nthat fuses inputs from multiple sources into a memory-like structure, improving\nthe storing capacity and extracting more intrinsic and correlated information.\nBioNeRF also mimics a behavior observed in pyramidal cells concerning\ncontextual information, in which the memory is provided as the context and\ncombined with the inputs of two subsequent neural models, one responsible for\nproducing the volumetric densities and the other the colors used to render the\nscene. Experimental results show that BioNeRF outperforms state-of-the-art\nresults concerning a quality measure that encodes human perception in two\ndatasets: real-world images and synthetic data.\n","authors":["Leandro A. Passos","Douglas Rodrigues","Danilo Jodas","Kelton A. P. Costa","Ahsan Adeel","João Paulo Papa"],"pdf_url":"https://arxiv.org/pdf/2402.07310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02969v2","updated":"2024-03-25T12:45:03Z","published":"2024-03-05T13:45:46Z","title":"Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception","summary":"  Multimodal Large Language Model (MLLMs) leverages Large Language Models as a\ncognitive framework for diverse visual-language tasks. Recent efforts have been\nmade to equip MLLMs with visual perceiving and grounding capabilities. However,\nthere still remains a gap in providing fine-grained pixel-level perceptions and\nextending interactions beyond text-specific inputs. In this work, we propose\n{\\bf{AnyRef}}, a general MLLM model that can generate pixel-wise object\nperceptions and natural language descriptions from multi-modality references,\nsuch as texts, boxes, images, or audio. This innovation empowers users with\ngreater flexibility to engage with the model beyond textual and regional\nprompts, without modality-specific designs. Through our proposed refocusing\nmechanism, the generated grounding output is guided to better focus on the\nreferenced object, implicitly incorporating additional pixel-level supervision.\nThis simple modification utilizes attention scores generated during the\ninference of LLM, eliminating the need for extra computations while exhibiting\nperformance enhancements in both grounding masks and referring expressions.\nWith only publicly available training data, our model achieves state-of-the-art\nresults across multiple benchmarks, including diverse modality referring\nsegmentation and region-level referring expression generation.\n","authors":["Junwen He","Yifan Wang","Lijun Wang","Huchuan Lu","Jun-Yan He","Jin-Peng Lan","Bin Luo","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2403.02969v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16697v1","updated":"2024-03-25T12:31:01Z","published":"2024-03-25T12:31:01Z","title":"DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization","summary":"  Source-Free Domain Generalization (SFDG) aims to develop a model that works\nfor unseen target domains without relying on any source domain. Recent work,\nPromptStyler, employs text prompts to simulate different distribution shifts in\nthe joint vision-language space, allowing the model to generalize effectively\nto unseen domains without using any images. However, 1) PromptStyler's style\ngeneration strategy has limitations, as all style patterns are fixed after the\nfirst training phase. This leads to the training set in the second training\nphase being restricted to a limited set of styles. Additionally, 2) the frozen\ntext encoder in PromptStyler result in the encoder's output varying with the\nstyle of the input text prompts, making it difficult for the model to learn\ndomain-invariant features. In this paper, we introduce Dynamic PromptStyler\n(DPStyler), comprising Style Generation and Style Removal modules to address\nthese issues. The Style Generation module refreshes all styles at every\ntraining epoch, while the Style Removal module eliminates variations in the\nencoder's output features caused by input styles. Moreover, since the Style\nGeneration module, responsible for generating style word vectors using random\nsampling or style mixing, makes the model sensitive to input text prompts, we\nintroduce a model ensemble method to mitigate this sensitivity. Extensive\nexperiments demonstrate that our framework outperforms state-of-the-art methods\non benchmark datasets.\n","authors":["Yunlong Tang","Yuxuan Wan","Lei Qi","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2403.16697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16695v1","updated":"2024-03-25T12:26:32Z","published":"2024-03-25T12:26:32Z","title":"Assessing the Performance of Deep Learning for Automated Gleason Grading\n  in Prostate Cancer","summary":"  Prostate cancer is a dominant health concern calling for advanced diagnostic\ntools. Utilizing digital pathology and artificial intelligence, this study\nexplores the potential of 11 deep neural network architectures for automated\nGleason grading in prostate carcinoma focusing on comparing traditional and\nrecent architectures. A standardized image classification pipeline, based on\nthe AUCMEDI framework, facilitated robust evaluation using an in-house dataset\nconsisting of 34,264 annotated tissue tiles. The results indicated varying\nsensitivity across architectures, with ConvNeXt demonstrating the strongest\nperformance. Notably, newer architectures achieved superior performance, even\nthough with challenges in differentiating closely related Gleason grades. The\nConvNeXt model was capable of learning a balance between complexity and\ngeneralizability. Overall, this study lays the groundwork for enhanced Gleason\ngrading systems, potentially improving diagnostic efficiency for prostate\ncancer.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Daniel Hieber","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Frank Kramer","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16689v1","updated":"2024-03-25T12:23:39Z","published":"2024-03-25T12:23:39Z","title":"Synapse: Learning Preferential Concepts from Visual Demonstrations","summary":"  This paper addresses the problem of preference learning, which aims to learn\nuser-specific preferences (e.g., \"good parking spot\", \"convenient drop-off\nlocation\") from visual input. Despite its similarity to learning factual\nconcepts (e.g., \"red cube\"), preference learning is a fundamentally harder\nproblem due to its subjective nature and the paucity of person-specific\ntraining data. We address this problem using a new framework called Synapse,\nwhich is a neuro-symbolic approach designed to efficiently learn preferential\nconcepts from limited demonstrations. Synapse represents preferences as\nneuro-symbolic programs in a domain-specific language (DSL) that operates over\nimages, and leverages a novel combination of visual parsing, large language\nmodels, and program synthesis to learn programs representing individual\npreferences. We evaluate Synapse through extensive experimentation including a\nuser case study focusing on mobility-related concepts in mobile robotics and\nautonomous driving. Our evaluation demonstrates that Synapse significantly\noutperforms existing baselines as well as its own ablations. The code and other\ndetails can be found on the project website https://amrl.cs.utexas.edu/synapse .\n","authors":["Sadanand Modak","Noah Patton","Isil Dillig","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2403.16689v1.pdf","comment":"23 pages, 7 figures; Preprint"},{"id":"http://arxiv.org/abs/2403.16678v1","updated":"2024-03-25T12:15:42Z","published":"2024-03-25T12:15:42Z","title":"DeepGleason: a System for Automated Gleason Grading of Prostate Cancer\n  using Deep Neural Networks","summary":"  Advances in digital pathology and artificial intelligence (AI) offer\npromising opportunities for clinical decision support and enhancing diagnostic\nworkflows. Previous studies already demonstrated AI's potential for automated\nGleason grading, but lack state-of-the-art methodology and model reusability.\nTo address this issue, we propose DeepGleason: an open-source deep neural\nnetwork based image classification system for automated Gleason grading using\nwhole-slide histopathology images from prostate tissue sections. Implemented\nwith the standardized AUCMEDI framework, our tool employs a tile-wise\nclassification approach utilizing fine-tuned image preprocessing techniques in\ncombination with a ConvNeXt architecture which was compared to various\nstate-of-the-art architectures. The neural network model was trained and\nvalidated on an in-house dataset of 34,264 annotated tiles from 369 prostate\ncarcinoma slides. We demonstrated that DeepGleason is capable of highly\naccurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,\nAUC of 0.991, and Accuracy of 0.974. The internal architecture comparison\nrevealed that the ConvNeXt model was superior performance-wise on our dataset\nto established and other modern architectures like transformers. Furthermore,\nwe were able to outperform the current state-of-the-art in tile-wise\nfine-classification with a sensitivity and specificity of 0.94 and 0.98 for\nbenign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs\nGleason 4 & 5 classification, respectively. Our tool contributes to the wider\nadoption of AI-based Gleason grading within the research community and paves\nthe way for broader clinical application of deep learning models in digital\npathology. DeepGleason is open-source and publicly available for research\napplication in the following Git repository:\nhttps://github.com/frankkramer-lab/DeepGleason.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16677v1","updated":"2024-03-25T12:14:48Z","published":"2024-03-25T12:14:48Z","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression","summary":"  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on perceptual quality measures at lower bitrates. We\nextensively evaluate transfer cost reduction by including the peculiarity of\nintermittently available network connections in low earth orbit. Lastly, we\ntest the feasibility of our system for standardized nanosatellite form factors.\nWe demonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n","authors":["Alireza Furutanpey","Qiyang Zhang","Philipp Raith","Tobias Pfandzelter","Shangguang Wang","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2403.16677v1.pdf","comment":"18 pages, double column, 19 figures, 7 tables, Initial Submission to\n  IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2403.16669v1","updated":"2024-03-25T12:07:24Z","published":"2024-03-25T12:07:24Z","title":"Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression\n  Network","summary":"  Visual detection of Micro Air Vehicles (MAVs) has attracted increasing\nattention in recent years due to its important application in various tasks.\nThe existing methods for MAV detection assume that the training set and testing\nset have the same distribution. As a result, when deployed in new domains, the\ndetectors would have a significant performance degradation due to domain\ndiscrepancy. In this paper, we study the problem of cross-domain MAV detection.\nThe contributions of this paper are threefold. 1) We propose a\nMulti-MAV-Multi-Domain (M3D) dataset consisting of both simulation and\nrealistic images. Compared to other existing datasets, the proposed one is more\ncomprehensive in the sense that it covers rich scenes, diverse MAV types, and\nvarious viewing angles. A new benchmark for cross-domain MAV detection is\nproposed based on the proposed dataset. 2) We propose a Noise Suppression\nNetwork (NSN) based on the framework of pseudo-labeling and a large-to-small\ntraining procedure. To reduce the challenging pseudo-label noises, two novel\nmodules are designed in this network. The first is a prior-based curriculum\nlearning module for allocating adaptive thresholds for pseudo labels with\ndifferent difficulties. The second is a masked copy-paste augmentation module\nfor pasting truly-labeled MAVs on unlabeled target images and thus decreasing\npseudo-label noises. 3) Extensive experimental results verify the superior\nperformance of the proposed method compared to the state-of-the-art ones. In\nparticular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on\nthe tasks of simulation-to-real adaptation, cross-scene adaptation, and\ncross-camera adaptation, respectively.\n","authors":["Yin Zhang","Jinhong Deng","Peidong Liu","Wen Li","Shiyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16669v1.pdf","comment":"17 pages, 11 figures. Accepted by IEEE Transactions on Automation\n  Science and Engineering"},{"id":"http://arxiv.org/abs/2308.10299v3","updated":"2024-03-25T12:04:41Z","published":"2023-08-20T15:38:40Z","title":"Boosting Adversarial Transferability by Block Shuffle and Rotation","summary":"  Adversarial examples mislead deep neural networks with imperceptible\nperturbations and have brought significant threats to deep learning. An\nimportant aspect is their transferability, which refers to their ability to\ndeceive other models, thus enabling attacks in the black-box setting. Though\nvarious methods have been proposed to boost transferability, the performance\nstill falls short compared with white-box attacks. In this work, we observe\nthat existing input transformation based attacks, one of the mainstream\ntransfer-based attacks, result in different attention heatmaps on various\nmodels, which might limit the transferability. We also find that breaking the\nintrinsic relation of the image can disrupt the attention heatmap of the\noriginal image. Based on this finding, we propose a novel input transformation\nbased attack called block shuffle and rotation (BSR). Specifically, BSR splits\nthe input image into several blocks, then randomly shuffles and rotates these\nblocks to construct a set of new images for gradient calculation. Empirical\nevaluations on the ImageNet dataset demonstrate that BSR could achieve\nsignificantly better transferability than the existing input transformation\nbased methods under single-model and ensemble-model settings. Combining BSR\nwith the current input transformation method can further improve the\ntransferability, which significantly outperforms the state-of-the-art methods.\nCode is available at https://github.com/Trustworthy-AI-Group/BSR\n","authors":["Kunyu Wang","Xuanran He","Wenxuan Wang","Xiaosen Wang"],"pdf_url":"https://arxiv.org/pdf/2308.10299v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2311.16515v2","updated":"2024-03-25T12:01:59Z","published":"2023-11-25T14:24:49Z","title":"Word4Per: Zero-shot Composed Person Retrieval","summary":"  Searching for specific person has great social benefits and security value,\nand it often involves a combination of visual and textual information.\nConventional person retrieval methods, whether image-based or text-based,\nusually fall short in effectively harnessing both types of information, leading\nto the loss of accuracy. In this paper, a whole new task called Composed Person\nRetrieval (CPR) is proposed to jointly utilize both image and text information\nfor target person retrieval. However, the supervised CPR requires very costly\nmanual annotation dataset, while there are currently no available resources. To\nmitigate this issue, we firstly introduce the Zero-shot Composed Person\nRetrieval (ZS-CPR), which leverages existing domain-related data to resolve the\nCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we\npropose a two-stage learning framework, Word4Per, where a lightweight Textual\nInversion Network (TINet) and a text-based person retrieval model based on\nfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned\nwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed\nPerson Retrieval (ITCPR) dataset is built as the benchmark to assess the\nperformance of the proposed Word4Per framework. Extensive experiments under\nboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR\ntask, surpassing the comparative methods by over 10\\%. The code and ITCPR\ndataset will be publicly available at\nhttps://github.com/Delong-liu-bupt/Word4Per.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05305v2","updated":"2024-03-25T11:48:27Z","published":"2024-02-07T22:50:47Z","title":"Knowledge Distillation for Road Detection based on cross-model\n  Semi-Supervised Learning","summary":"  The advancement of knowledge distillation has played a crucial role in\nenabling the transfer of knowledge from larger teacher models to smaller and\nmore efficient student models, and is particularly beneficial for online and\nresource-constrained applications. The effectiveness of the student model\nheavily relies on the quality of the distilled knowledge received from the\nteacher. Given the accessibility of unlabelled remote sensing data,\nsemi-supervised learning has become a prevalent strategy for enhancing model\nperformance. However, relying solely on semi-supervised learning with smaller\nmodels may be insufficient due to their limited capacity for feature\nextraction. This limitation restricts their ability to exploit training data.\nTo address this issue, we propose an integrated approach that combines\nknowledge distillation and semi-supervised learning methods. This hybrid\napproach leverages the robust capabilities of large models to effectively\nutilise large unlabelled data whilst subsequently providing the small student\nmodel with rich and informative features for enhancement. The proposed\nsemi-supervised learning-based knowledge distillation (SSLKD) approach\ndemonstrates a notable improvement in the performance of the student model, in\nthe application of road segmentation, surpassing the effectiveness of\ntraditional semi-supervised learning methods.\n","authors":["Wanli Ma","Oktay Karakus","Paul L. Rosin"],"pdf_url":"https://arxiv.org/pdf/2402.05305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02935v2","updated":"2024-03-25T11:45:58Z","published":"2023-08-05T18:32:49Z","title":"Unveiling the Blind Spots: A Critical Examination of Fairness in\n  Autonomous Driving Systems","summary":"  Autonomous driving systems have extended the spectrum of Web of Things for\nintelligent vehicles and have become an important component of the Web\necosystem. Similar to traditional Web-based applications, fairness is an\nessential aspect for ensuring the high quality of autonomous driving systems,\nparticularly in the context of pedestrian detectors within them. However, there\nis an absence in the literature of a comprehensive assessment of the fairness\nof current Deep Learning (DL)-based pedestrian detectors. To fill the gap, we\nevaluate eight widely-explored DL-based pedestrian detectors across demographic\ngroups on large-scale real-world datasets. To enable a thorough fairness\nevaluation, we provide extensive annotations for the datasets, resulting in\n8,311 images with 16,070 gender labels, 20,115 age labels, and 3,513 skin tone\nlabels. Our findings reveal significant fairness issues related to age. The\nundetected proportions for adults are 20.14% lower compared to children.\nFurthermore, we explore how various driving scenarios affect the fairness of\npedestrian detectors. We find that the bias may exacerbate for children and\nfemales towards low brightness and low contrast.\n","authors":["Xinyue Li","Zhenpeng Chen","Jie M. Zhang","Federica Sarro","Ying Zhang","Xuanzhe Liu"],"pdf_url":"https://arxiv.org/pdf/2308.02935v2.pdf","comment":"Update the models evaluated and the experimental results"},{"id":"http://arxiv.org/abs/2310.06744v2","updated":"2024-03-25T11:35:55Z","published":"2023-10-10T16:14:20Z","title":"HiFi-123: Towards High-fidelity One Image to 3D Content Generation","summary":"  Recent advances in diffusion models have enabled 3D generation from a single\nimage. However, current methods often produce suboptimal results for novel\nviews, with blurred textures and deviations from the reference image, limiting\ntheir practical applications. In this paper, we introduce HiFi-123, a method\ndesigned for high-fidelity and multi-view consistent 3D generation. Our\ncontributions are twofold: First, we propose a Reference-Guided Novel View\nEnhancement (RGNV) technique that significantly improves the fidelity of\ndiffusion-based zero-shot novel view synthesis methods. Second, capitalizing on\nthe RGNV, we present a novel Reference-Guided State Distillation (RGSD) loss.\nWhen incorporated into the optimization-based image-to-3D pipeline, our method\nsignificantly improves 3D generation quality, achieving state-of-the-art\nperformance. Comprehensive evaluations demonstrate the effectiveness of our\napproach over existing methods, both qualitatively and quantitatively. Video\nresults are available on the project page.\n","authors":["Wangbo Yu","Li Yuan","Yan-Pei Cao","Xiangjun Gao","Xiaoyu Li","Wenbo Hu","Long Quan","Ying Shan","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2310.06744v2.pdf","comment":"Project Page: https://drexubery.github.io/HiFi-123/"},{"id":"http://arxiv.org/abs/2403.16646v1","updated":"2024-03-25T11:32:05Z","published":"2024-03-25T11:32:05Z","title":"Clustering Propagation for Universal Medical Image Segmentation","summary":"  Prominent solutions for medical image segmentation are typically tailored for\nautomatic or interactive setups, posing challenges in facilitating progress\nachieved in one task to another.$_{\\!}$ This$_{\\!}$ also$_{\\!}$\nnecessitates$_{\\!}$ separate$_{\\!}$ models for each task, duplicating both\ntraining time and parameters.$_{\\!}$ To$_{\\!}$ address$_{\\!}$ above$_{\\!}$\nissues,$_{\\!}$ we$_{\\!}$ introduce$_{\\!}$ S2VNet,$_{\\!}$ a$_{\\!}$\nuniversal$_{\\!}$ framework$_{\\!}$ that$_{\\!}$ leverages$_{\\!}$\nSlice-to-Volume$_{\\!}$ propagation$_{\\!}$ to$_{\\!}$ unify automatic/interactive\nsegmentation within a single model and one training session. Inspired by\nclustering-based segmentation techniques, S2VNet makes full use of the\nslice-wise structure of volumetric data by initializing cluster centers from\nthe cluster$_{\\!}$ results$_{\\!}$ of$_{\\!}$ previous$_{\\!}$ slice.$_{\\!}$ This\nenables knowledge acquired from prior slices to assist in the segmentation of\nthe current slice, further efficiently bridging the communication between\nremote slices using mere 2D networks. Moreover, such a framework readily\naccommodates interactive segmentation with no architectural change, simply by\ninitializing centroids from user inputs. S2VNet distinguishes itself by swift\ninference speeds and reduced memory consumption compared to prevailing 3D\nsolutions. It can also handle multi-class interactions with each of them\nserving to initialize different centroids. Experiments on three benchmarks\ndemonstrate S2VNet surpasses task-specified solutions on both\nautomatic/interactive setups.\n","authors":["Yuhang Ding","Liulei Li","Wenguan Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.16646v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.16643v1","updated":"2024-03-25T11:29:19Z","published":"2024-03-25T11:29:19Z","title":"Self-Adaptive Reality-Guided Diffusion for Artifact-Free\n  Super-Resolution","summary":"  Artifact-free super-resolution (SR) aims to translate low-resolution images\ninto their high-resolution counterparts with a strict integrity of the original\ncontent, eliminating any distortions or synthetic details. While traditional\ndiffusion-based SR techniques have demonstrated remarkable abilities to enhance\nimage detail, they are prone to artifact introduction during iterative\nprocedures. Such artifacts, ranging from trivial noise to unauthentic textures,\ndeviate from the true structure of the source image, thus challenging the\nintegrity of the super-resolution process. In this work, we propose\nSelf-Adaptive Reality-Guided Diffusion (SARGD), a training-free method that\ndelves into the latent space to effectively identify and mitigate the\npropagation of artifacts. Our SARGD begins by using an artifact detector to\nidentify implausible pixels, creating a binary mask that highlights artifacts.\nFollowing this, the Reality Guidance Refinement (RGR) process refines artifacts\nby integrating this mask with realistic latent representations, improving\nalignment with the original image. Nonetheless, initial realistic-latent\nrepresentations from lower-quality images result in over-smoothing in the final\noutput. To address this, we introduce a Self-Adaptive Guidance (SAG) mechanism.\nIt dynamically computes a reality score, enhancing the sharpness of the\nrealistic latent. These alternating mechanisms collectively achieve\nartifact-free super-resolution. Extensive experiments demonstrate the\nsuperiority of our method, delivering detailed artifact-free high-resolution\nimages while reducing sampling steps by 2X. We release our code at\nhttps://github.com/ProAirVerse/Self-Adaptive-Guidance-Diffusion.git.\n","authors":["Qingping Zheng","Ling Zheng","Yuanfan Guo","Ying Li","Songcen Xu","Jiankang Deng","Hang Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16640v1","updated":"2024-03-25T11:28:52Z","published":"2024-03-25T11:28:52Z","title":"Multi-Scale Texture Loss for CT denoising with GANs","summary":"  Generative Adversarial Networks (GANs) have proved as a powerful framework\nfor denoising applications in medical imaging. However, GAN-based denoising\nalgorithms still suffer from limitations in capturing complex relationships\nwithin the images. In this regard, the loss function plays a crucial role in\nguiding the image generation process, encompassing how much a synthetic image\ndiffers from a real image. To grasp highly complex and non-linear textural\nrelationships in the training process, this work presents a loss function that\nleverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence\nMatrix (GLCM). Although the recent advances in deep learning have demonstrated\nsuperior performance in classification and detection tasks, we hypothesize that\nits information content can be valuable when integrated into GANs' training. To\nthis end, we propose a differentiable implementation of the GLCM suited for\ngradient-based optimization. Our approach also introduces a self-attention\nlayer that dynamically aggregates the multi-scale texture information extracted\nfrom the images. We validate our approach by carrying out extensive experiments\nin the context of low-dose CT denoising, a challenging application that aims to\nenhance the quality of noisy CT scans. We utilize three publicly available\ndatasets, including one simulated and two real datasets. The results are\npromising as compared to other well-established loss functions, being also\nconsistent across three different GAN architectures. The code is available at:\nhttps://github.com/FrancescoDiFeola/DenoTextureLoss\n","authors":["Francesco Di Feola","Lorenzo Tronchin","Valerio Guarrasi","Paolo Soda"],"pdf_url":"https://arxiv.org/pdf/2403.16640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16638v1","updated":"2024-03-25T11:26:18Z","published":"2024-03-25T11:26:18Z","title":"AI-Generated Video Detection via Spatio-Temporal Anomaly Learning","summary":"  The advancement of generation models has led to the emergence of highly\nrealistic artificial intelligence (AI)-generated videos. Malicious users can\neasily create non-existent videos to spread false information. This letter\nproposes an effective AI-generated video detection (AIGVDet) scheme by\ncapturing the forensic traces with a two-branch spatio-temporal convolutional\nneural network (CNN). Specifically, two ResNet sub-detectors are learned\nseparately for identifying the anomalies in spatical and optical flow domains,\nrespectively. Results of such sub-detectors are fused to further enhance the\ndiscrimination ability. A large-scale generated video dataset (GVD) is\nconstructed as a benchmark for model training and evaluation. Extensive\nexperimental results verify the high generalization and robustness of our\nAIGVDet scheme. Code and dataset will be available at\nhttps://github.com/multimediaFor/AIGVDet.\n","authors":["Jianfa Bai","Man Lin","Gang Cao"],"pdf_url":"https://arxiv.org/pdf/2403.16638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16476v4","updated":"2024-03-25T11:24:45Z","published":"2023-12-27T08:50:01Z","title":"SVGDreamer: Text Guided SVG Generation with Diffusion Model","summary":"  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown\npromise in domains such as iconography and sketch. However, existing\ntext-to-SVG generation methods lack editability and struggle with visual\nquality and result diversity. To address these limitations, we propose a novel\ntext-guided vector graphics synthesis method called SVGDreamer. SVGDreamer\nincorporates a semantic-driven image vectorization (SIVE) process that enables\nthe decomposition of synthesis into foreground objects and background, thereby\nenhancing editability. Specifically, the SIVE process introduce attention-based\nprimitive control and an attention-mask loss function for effective control and\nmanipulation of individual elements. Additionally, we propose a Vectorized\nParticle-based Score Distillation (VPSD) approach to tackle the challenges of\nshape over-smoothing, color over-saturation, limited diversity in results, and\nslow convergence in existing text-to-SVG generation methods. VPSD models SVGs\nas distributions of control points and colors to counteract over-smoothing and\nover-saturation. Furthermore, VPSD leverages a reward model to reweight vector\nparticles, which improves aesthetic appeal and accelerates convergence.\nExtensive experiments have been conducted to validate the effectiveness of\nSVGDreamer, demonstrating its superiority over baseline methods in terms of\neditability, visual quality, and diversity. The code and demo of SVGDreamer can\nbe found at https://ximinng.github.io/SVGDreamer-project/\n","authors":["Ximing Xing","Haitao Zhou","Chuang Wang","Jing Zhang","Dong Xu","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2312.16476v4.pdf","comment":"Accepted by CVPR 2024. project link:\n  https://ximinng.github.io/SVGDreamer-project/"},{"id":"http://arxiv.org/abs/2403.16635v1","updated":"2024-03-25T11:24:02Z","published":"2024-03-25T11:24:02Z","title":"V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster","summary":"  The objective of the collaborative vehicle-to-everything perception task is\nto enhance the individual vehicle's perception capability through message\ncommunication among neighboring traffic agents. Previous methods focus on\nachieving optimal performance within bandwidth limitations and typically adopt\nBEV maps as the basic collaborative message units. However, we demonstrate that\ncollaboration with dense representations is plagued by object feature\ndestruction during message packing, inefficient message aggregation for\nlong-range collaboration, and implicit structure representation communication.\nTo tackle these issues, we introduce a brand new message unit, namely point\ncluster, designed to represent the scene sparsely with a combination of\nlow-level structure information and high-level semantic information. The point\ncluster inherently preserves object information while packing messages, with\nweak relevance to the collaboration range, and supports explicit structure\nmodeling. Building upon this representation, we propose a novel framework\nV2X-PC for collaborative perception. This framework includes a Point Cluster\nPacking (PCP) module to keep object feature and manage bandwidth through the\nmanipulation of cluster point numbers. As for effective message aggregation, we\npropose a Point Cluster Aggregation (PCA) module to match and merge point\nclusters associated with the same object. To further handle time latency and\npose errors encountered in real-world scenarios, we propose parameter-free\nsolutions that can adapt to different noisy levels without finetuning.\nExperiments on two widely recognized collaborative perception benchmarks\nshowcase the superior performance of our method compared to the previous\nstate-of-the-art approaches relying on BEV maps.\n","authors":["Si Liu","Zihan Ding","Jiahui Fu","Hongyu Li","Siheng Chen","Shifeng Zhang","Xu Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.16635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16627v1","updated":"2024-03-25T11:16:23Z","published":"2024-03-25T11:16:23Z","title":"SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions","summary":"  Recent advancements in diffusion models have positioned them at the forefront\nof image generation. Despite their superior performance, diffusion models are\nnot without drawbacks; they are characterized by complex architectures and\nsubstantial computational demands, resulting in significant latency due to\ntheir iterative sampling process. To mitigate these limitations, we introduce a\ndual approach involving model miniaturization and a reduction in sampling\nsteps, aimed at significantly decreasing model latency. Our methodology\nleverages knowledge distillation to streamline the U-Net and image decoder\narchitectures, and introduces an innovative one-step DM training technique that\nutilizes feature matching and score distillation. We present two models,\nSDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS\n(30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU,\nrespectively. Moreover, our training approach offers promising applications in\nimage-conditioned control, facilitating efficient image-to-image translation.\n","authors":["Yuda Song","Zehao Sun","Xuanwu Yin"],"pdf_url":"https://arxiv.org/pdf/2403.16627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17744v2","updated":"2024-03-25T11:04:17Z","published":"2023-11-29T15:49:31Z","title":"Variational Bayes image restoration with compressive autoencoders","summary":"  Regularization of inverse problems is of paramount importance in\ncomputational imaging. The ability of neural networks to learn efficient image\nrepresentations has been recently exploited to design powerful data-driven\nregularizers. While state-of-the-art plug-and-play methods rely on an implicit\nregularization provided by neural denoisers, alternative Bayesian approaches\nconsider Maximum A Posteriori (MAP) estimation in the latent space of a\ngenerative model, thus with an explicit regularization. However,\nstate-of-the-art deep generative models require a huge amount of training data\ncompared to denoisers. Besides, their complexity hampers the optimization\ninvolved in latent MAP derivation. In this work, we first propose to use\ncompressive autoencoders instead. These networks, which can be seen as\nvariational autoencoders with a flexible latent prior, are smaller and easier\nto train than state-of-the-art generative models. As a second contribution, we\nintroduce the Variational Bayes Latent Estimation (VBLE) algorithm, which\nperforms latent estimation within the framework of variational inference.\nThanks to a simple yet efficient parameterization of the variational posterior,\nVBLE allows for fast and easy (approximate) posterior sampling. Experimental\nresults on image datasets BSD and FFHQ demonstrate that VBLE reaches similar\nperformance than state-of-the-art plug-and-play methods, while being able to\nquantify uncertainties faster than other existing posterior sampling\ntechniques.\n","authors":["Maud Biquard","Marie Chabert","Thomas Oberlin"],"pdf_url":"https://arxiv.org/pdf/2311.17744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12198v2","updated":"2024-03-25T11:04:04Z","published":"2023-12-19T14:34:36Z","title":"Mask Grounding for Referring Image Segmentation","summary":"  Referring Image Segmentation (RIS) is a challenging task that requires an\nalgorithm to segment objects referred by free-form language expressions.\nDespite significant progress in recent years, most state-of-the-art (SOTA)\nmethods still suffer from considerable language-image modality gap at the pixel\nand word level. These methods generally 1) rely on sentence-level language\nfeatures for language-image alignment and 2) lack explicit training supervision\nfor fine-grained visual grounding. Consequently, they exhibit weak object-level\ncorrespondence between visual and language features. Without well-grounded\nfeatures, prior methods struggle to understand complex expressions that require\nstrong reasoning over relationships among multiple objects, especially when\ndealing with rarely used or ambiguous clauses. To tackle this challenge, we\nintroduce a novel Mask Grounding auxiliary task that significantly improves\nvisual grounding within language features, by explicitly teaching the model to\nlearn fine-grained correspondence between masked textual tokens and their\nmatching visual objects. Mask Grounding can be directly used on prior RIS\nmethods and consistently bring improvements. Furthermore, to holistically\naddress the modality gap, we also design a cross-modal alignment loss and an\naccompanying alignment module. These additions work synergistically with Mask\nGrounding. With all these techniques, our comprehensive approach culminates in\nMagNet (Mask-grounded Network), an architecture that significantly outperforms\nprior arts on three key benchmarks (RefCOCO, RefCOCO+ and G-Ref), demonstrating\nour method's effectiveness in addressing current limitations of RIS algorithms.\nOur code and pre-trained weights will be released.\n","authors":["Yong Xien Chng","Henry Zheng","Yizeng Han","Xuchong Qiu","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2312.12198v2.pdf","comment":"Accepted by CVPR2024; Project page:\n  https://yxchng.github.io/projects/mask-grounding"},{"id":"http://arxiv.org/abs/2403.16612v1","updated":"2024-03-25T10:42:48Z","published":"2024-03-25T10:42:48Z","title":"Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting","summary":"  Seasonal forecasting is a crucial task when it comes to detecting the extreme\nheat and colds that occur due to climate change. Confidence in the predictions\nshould be reliable since a small increase in the temperatures in a year has a\nbig impact on the world. Calibration of the neural networks provides a way to\nensure our confidence in the predictions. However, calibrating regression\nmodels is an under-researched topic, especially in forecasters. We calibrate a\nUNet++ based architecture, which was shown to outperform physics-based models\nin temperature anomalies. We show that with a slight trade-off between\nprediction error and calibration error, it is possible to get more reliable and\nsharper forecasts. We believe that calibration should be an important part of\nsafety-critical machine learning applications such as weather forecasters.\n","authors":["Busra Asan","Abdullah Akgul","Alper Unal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2403.16612v1.pdf","comment":"Accepted as a workshop paper at \"ICLR 2024 Tackling Climate Change\n  with Machine Learning\""},{"id":"http://arxiv.org/abs/2403.16607v1","updated":"2024-03-25T10:38:17Z","published":"2024-03-25T10:38:17Z","title":"Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction\n  and Defect-Focus","summary":"  Addressing the challenge of data scarcity in industrial domains, transfer\nlearning emerges as a pivotal paradigm. This work introduces Style Filter, a\ntailored methodology for industrial contexts. By selectively filtering source\ndomain data before knowledge transfer, Style Filter reduces the quantity of\ndata while maintaining or even enhancing the performance of transfer learning\nstrategy. Offering label-free operation, minimal reliance on prior knowledge,\nindependence from specific models, and re-utilization, Style Filter is\nevaluated on authentic industrial datasets, highlighting its effectiveness when\nemployed before conventional transfer strategies in the deep learning domain.\nThe results underscore the effectiveness of Style Filter in real-world\nindustrial applications.\n","authors":["Chen Li","Ruijie Ma","Xiang Qian","Xiaohao Wang","Xinghui Li"],"pdf_url":"https://arxiv.org/pdf/2403.16607v1.pdf","comment":"17 pages, 11 figures,4 tables"},{"id":"http://arxiv.org/abs/2403.16605v1","updated":"2024-03-25T10:30:22Z","published":"2024-03-25T10:30:22Z","title":"SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for\n  Aerial Semantic Segmentation","summary":"  In recent years, semantic segmentation has become a pivotal tool in\nprocessing and interpreting satellite imagery. Yet, a prevalent limitation of\nsupervised learning techniques remains the need for extensive manual\nannotations by experts. In this work, we explore the potential of generative\nimage diffusion to address the scarcity of annotated data in earth observation\ntasks. The main idea is to learn the joint data manifold of images and labels,\nleveraging recent advancements in denoising diffusion probabilistic models. To\nthe best of our knowledge, we are the first to generate both images and\ncorresponding masks for satellite segmentation. We find that the obtained pairs\nnot only display high quality in fine-scale features but also ensure a wide\nsampling diversity. Both aspects are crucial for earth observation data, where\nsemantic classes can vary severely in scale and occurrence frequency. We employ\nthe novel data instances for downstream segmentation, as a form of data\naugmentation. In our experiments, we provide comparisons to prior works based\non discriminative diffusion models or GANs. We demonstrate that integrating\ngenerated samples yields significant quantitative improvements for satellite\nsemantic segmentation -- both compared to baselines and when training only on\nthe original data.\n","authors":["Aysim Toker","Marvin Eisenberger","Daniel Cremers","Laura Leal-Taixé"],"pdf_url":"https://arxiv.org/pdf/2403.16605v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.16594v1","updated":"2024-03-25T10:13:52Z","published":"2024-03-25T10:13:52Z","title":"EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for\n  Medical Image Segmentation","summary":"  Deploying deep learning (DL) models in medical applications relies on\npredictive performance and other critical factors, such as conveying\ntrustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide\npotential solutions for evaluating prediction reliability and improving the\nmodel confidence calibration. Despite increasing interest in UE, challenges\npersist, such as the need for explicit methods to capture aleatoric uncertainty\nand align uncertainty estimates with real-life disagreements among domain\nexperts. This paper proposes an Expert Disagreement-Guided Uncertainty\nEstimation (EDUE) for medical image segmentation. By leveraging variability in\nground-truth annotations from multiple raters, we guide the model during\ntraining and incorporate random sampling-based strategies to enhance\ncalibration confidence. Our method achieves 55% and 23% improvement in\ncorrelation on average with expert disagreements at the image and pixel levels,\nrespectively, better calibration, and competitive segmentation performance\ncompared to the state-of-the-art deep ensembles, requiring only a single\nforward pass.\n","authors":["Kudaibergen Abutalip","Numan Saeed","Ikboljon Sobirov","Vincent Andrearczyk","Adrien Depeursinge","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.16594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14828v2","updated":"2024-03-25T10:12:46Z","published":"2024-03-21T20:43:10Z","title":"Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing","summary":"  Fashion illustration is a crucial medium for designers to convey their\ncreative vision and transform design concepts into tangible representations\nthat showcase the interplay between clothing and the human body. In the context\nof fashion design, computer vision techniques have the potential to enhance and\nstreamline the design process. Departing from prior research primarily focused\non virtual try-on, this paper tackles the task of multimodal-conditioned\nfashion image editing. Our approach aims to generate human-centric fashion\nimages guided by multimodal prompts, including text, human body poses, garment\nsketches, and fabric textures. To address this problem, we propose extending\nlatent diffusion models to incorporate these multiple modalities and modifying\nthe structure of the denoising network, taking multimodal prompts as input. To\ncondition the proposed architecture on fabric textures, we employ textual\ninversion techniques and let diverse cross-attention layers of the denoising\nnetwork attend to textual and texture information, thus incorporating different\ngranularity conditioning details. Given the lack of datasets for the task, we\nextend two existing fashion datasets, Dress Code and VITON-HD, with multimodal\nannotations. Experimental evaluations demonstrate the effectiveness of our\nproposed approach in terms of realism and coherence concerning the provided\nmultimodal inputs.\n","authors":["Alberto Baldrati","Davide Morelli","Marcella Cornia","Marco Bertini","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2403.14828v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16582v1","updated":"2024-03-25T09:49:42Z","published":"2024-03-25T09:49:42Z","title":"In the Search for Optimal Multi-view Learning Models for Crop\n  Classification with Global Remote Sensing Data","summary":"  Crop classification is of critical importance due to its role in studying\ncrop pattern changes, resource management, and carbon sequestration. When\nemploying data-driven techniques for its prediction, utilizing various temporal\ndata sources is necessary. Deep learning models have proven to be effective for\nthis task by mapping time series data to high-level representation for\nprediction. However, they face substantial challenges when dealing with\nmultiple input patterns. The literature offers limited guidance for Multi-View\nLearning (MVL) scenarios, as it has primarily focused on exploring fusion\nstrategies with specific encoders and validating them in local regions. In\ncontrast, we investigate the impact of simultaneous selection of the fusion\nstrategy and the encoder architecture evaluated on a global-scale cropland and\ncrop-type classifications. We use a range of five fusion strategies (Input,\nFeature, Decision, Ensemble, Hybrid) and five temporal encoder architectures\n(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The\nvalidation is on the CropHarvest dataset that provides optical, radar, and\nweather time series, and topographic information as input data. We found that\nin scenarios with a limited number of labeled samples, a unique configuration\nis insufficient for all the cases. Instead, a specialized combination,\nincluding encoder and fusion strategy, should be meticulously sought. To\nstreamline this search process, we suggest initially identifying the optimal\nencoder architecture tailored for a particular fusion strategy, and then\ndetermining the most suitable fusion strategy for the classification task. We\nprovide a technical framework for researchers exploring crop classification or\nrelated tasks through a MVL approach.\n","authors":["Francisco Mena","Diego Arenas","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.16582v1.pdf","comment":"submitted to journal"},{"id":"http://arxiv.org/abs/2403.16578v1","updated":"2024-03-25T09:43:56Z","published":"2024-03-25T09:43:56Z","title":"SegICL: A Universal In-context Learning Framework for Enhanced\n  Segmentation in Medical Imaging","summary":"  Medical image segmentation models adapting to new tasks in a training-free\nmanner through in-context learning is an exciting advancement. Universal\nsegmentation models aim to generalize across the diverse modality of medical\nimages, yet their effectiveness often diminishes when applied to\nout-of-distribution (OOD) data modalities and tasks, requiring intricate\nfine-tuning of model for optimal performance. For addressing this challenge, we\nintroduce SegICL, a novel approach leveraging In-Context Learning (ICL) for\nimage segmentation. Unlike existing methods, SegICL has the capability to\nemploy text-guided segmentation and conduct in-context learning with a small\nset of image-mask pairs, eliminating the need for training the model from\nscratch or fine-tuning for OOD tasks (including OOD modality and dataset).\nExtensive experimental validation of SegICL demonstrates a positive correlation\nbetween the number of prompt samples and segmentation performance on OOD\nmodalities and tasks. This indicates that SegICL effectively address new\nsegmentation tasks based on contextual information. Additionally, SegICL also\nexhibits comparable segmentation performance to mainstream models on OOD and\nin-distribution tasks. Our code will be released soon.\n","authors":["Lingdong Shen","Fangxin Shang","Yehui Yang","Xiaoshuang Huang","Shining Xiang"],"pdf_url":"https://arxiv.org/pdf/2403.16578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10615v2","updated":"2024-03-25T09:42:13Z","published":"2024-03-15T18:26:33Z","title":"LightIt: Illumination Modeling and Control for Diffusion Models","summary":"  We introduce LightIt, a method for explicit illumination control for image\ngeneration. Recent generative methods lack lighting control, which is crucial\nto numerous artistic aspects of image generation such as setting the overall\nmood or cinematic appearance. To overcome these limitations, we propose to\ncondition the generation on shading and normal maps. We model the lighting with\nsingle bounce shading, which includes cast shadows. We first train a shading\nestimation module to generate a dataset of real-world images and shading pairs.\nThen, we train a control network using the estimated shading and normals as\ninput. Our method demonstrates high-quality image generation and lighting\ncontrol in numerous scenes. Additionally, we use our generated dataset to train\nan identity-preserving relighting model, conditioned on an image and a target\nshading. Our method is the first that enables the generation of images with\ncontrollable, consistent lighting and performs on par with specialized\nrelighting state-of-the-art methods.\n","authors":["Peter Kocsis","Julien Philip","Kalyan Sunkavalli","Matthias Nießner","Yannick Hold-Geoffroy"],"pdf_url":"https://arxiv.org/pdf/2403.10615v2.pdf","comment":"Project page: https://peter-kocsis.github.io/LightIt/ Video:\n  https://youtu.be/cCfSBD5aPLI"},{"id":"http://arxiv.org/abs/2403.15353v2","updated":"2024-03-25T09:36:42Z","published":"2024-03-22T17:08:03Z","title":"Fully automated workflow for the design of patient-specific orthopaedic\n  implants: application to total knee arthroplasty","summary":"  Arthroplasty is commonly performed to treat joint osteoarthritis, reducing\npain and improving mobility. While arthroplasty has known several technical\nimprovements, a significant share of patients are still unsatisfied with their\nsurgery. Personalised arthroplasty improves surgical outcomes however current\nsolutions require delays, making it difficult to integrate in clinical routine.\nWe propose a fully automated workflow to design patient-specific implants,\npresented for total knee arthroplasty, the most widely performed arthroplasty\nin the world nowadays.\n  The proposed pipeline first uses artificial neural networks to segment the\nproximal and distal extremities of the femur and tibia. Then the full bones are\nreconstructed using augmented statistical shape models, combining shape and\nlandmarks information. Finally, 77 morphological parameters are computed to\ndesign patient-specific implants. The developed workflow has been trained using\n91 CT scans of lower limb and evaluated on 41 CT scans manually segmented, in\nterms of accuracy and execution time.\n  The workflow accuracy was $0.4\\pm0.2mm$ for the segmentation, $1.2\\pm0.4mm$\nfor the full bones reconstruction, and $2.8\\pm2.2mm$ for the anatomical\nlandmarks determination. The custom implants fitted the patients' anatomy with\n$0.6\\pm0.2mm$ accuracy. The whole process from segmentation to implants' design\nlasted about 5 minutes.\n  The proposed workflow allows for a fast and reliable personalisation of knee\nimplants, directly from the patient CT image without requiring any manual\nintervention. It establishes a patient-specific pre-operative planning for TKA\nin a very short time making it easily available for all patients. Combined with\nefficient implant manufacturing techniques, this solution could help answer the\ngrowing number of arthroplasties while reducing complications and improving the\npatients' satisfaction.\n","authors":["Aziliz Guezou-Philippe","Arnaud Clavé","Ehouarn Maguet","Ludivine Maintier","Charles Garraud","Jean-Rassaire Fouefack","Valérie Burdin","Eric Stindel","Guillaume Dardenne"],"pdf_url":"https://arxiv.org/pdf/2403.15353v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16569v1","updated":"2024-03-25T09:36:10Z","published":"2024-03-25T09:36:10Z","title":"Revealing Vulnerabilities of Neural Networks in Parameter Learning and\n  Defense Against Explanation-Aware Backdoors","summary":"  Explainable Artificial Intelligence (XAI) strategies play a crucial part in\nincreasing the understanding and trustworthiness of neural networks.\nNonetheless, these techniques could potentially generate misleading\nexplanations. Blinding attacks can drastically alter a machine learning\nalgorithm's prediction and explanation, providing misleading information by\nadding visually unnoticeable artifacts into the input, while maintaining the\nmodel's accuracy. It poses a serious challenge in ensuring the reliability of\nXAI methods. To ensure the reliability of XAI methods poses a real challenge,\nwe leverage statistical analysis to highlight the changes in CNN weights within\na CNN following blinding attacks. We introduce a method specifically designed\nto limit the effectiveness of such attacks during the evaluation phase,\navoiding the need for extra training. The method we suggest defences against\nmost modern explanation-aware adversarial attacks, achieving an approximate\ndecrease of ~99\\% in the Attack Success Rate (ASR) and a ~91\\% reduction in the\nMean Square Error (MSE) between the original explanation and the defended\n(post-attack) explanation across three unique types of attacks.\n","authors":["Md Abdul Kadir","GowthamKrishna Addluri","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2403.16569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16558v1","updated":"2024-03-25T09:17:15Z","published":"2024-03-25T09:17:15Z","title":"Elysium: Exploring Object-level Perception in Videos via MLLM","summary":"  Multi-modal Large Language Models (MLLMs) have demonstrated their ability to\nperceive objects in still images, but their application in video-related tasks,\nsuch as object tracking, remains understudied. This lack of exploration is\nprimarily due to two key challenges. Firstly, extensive pretraining on\nlarge-scale video datasets is required to equip MLLMs with the capability to\nperceive objects across multiple frames and understand inter-frame\nrelationships. Secondly, processing a large number of frames within the context\nwindow of Large Language Models (LLMs) can impose a significant computational\nburden. To address the first challenge, we introduce ElysiumTrack-1M, a\nlarge-scale video dataset paired with novel tasks: Referring Single Object\nTracking (RSOT) and Video Referring Expression Generation (Video-REG).\nElysiumTrack-1M contains 1.27 million annotated video frames with corresponding\nobject boxes and descriptions. Leveraging this dataset, we conduct training of\nMLLMs and propose a token-compression model T-Selector to tackle the second\nchallenge. Our proposed approach, Elysium: Exploring Object-level Perception in\nVideos via MLLM, is an end-to-end trainable MLLM that makes the first attempt\nto conduct object-level tasks in videos without requiring any additional\nplug-in or expert models.\n","authors":["Han Wang","Yanjie Wang","Yongjie Ye","Yuxiang Nie","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2403.16558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16552v1","updated":"2024-03-25T08:57:27Z","published":"2024-03-25T08:57:27Z","title":"QKFormer: Hierarchical Spiking Transformer using Q-K Attention","summary":"  Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with\nTransformer architectures, have attracted significant attention due to their\npotential for energy efficiency and high performance. However, existing models\nin this domain still suffer from suboptimal performance. We introduce several\ninnovations to improve the performance: i) We propose a novel spike-form Q-K\nattention mechanism, tailored for SNNs, which efficiently models the importance\nof token or channel dimensions through binary vectors with linear complexity.\nii) We incorporate the hierarchical structure, which significantly benefits the\nperformance of both the brain and artificial neural networks, into spiking\ntransformers to obtain multi-scale spiking representation. iii) We design a\nversatile and powerful patch embedding module with a deformed shortcut\nspecifically for spiking transformers. Together, we develop QKFormer, a\nhierarchical spiking transformer based on Q-K attention with direct training.\nQKFormer shows significantly superior performance over existing\nstate-of-the-art SNN models on various mainstream datasets. Notably, with\ncomparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a\ngroundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially\noutperforming Spikformer by 10.84%. To our best knowledge, this is the first\ntime that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The\ncode and models are publicly available at\nhttps://github.com/zhouchenlin2096/QKFormer\n","authors":["Chenlin Zhou","Han Zhang","Zhaokun Zhou","Liutao Yu","Liwei Huang","Xiaopeng Fan","Li Yuan","Zhengyu Ma","Huihui Zhou","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2403.16552v1.pdf","comment":"10 pages, code: https://github.com/zhouchenlin2096/QKFormer"},{"id":"http://arxiv.org/abs/2403.11854v2","updated":"2024-03-25T08:54:33Z","published":"2024-03-18T15:03:56Z","title":"denoiSplit: a method for joint image splitting and unsupervised\n  denoising","summary":"  In this work we present denoiSplit, a method to tackle a new analysis task,\ni.e. the challenge of joint semantic image splitting and unsupervised\ndenoising. This dual approach has important applications in fluorescence\nmicroscopy, where semantic image splitting has important applications but noise\ndoes generally hinder the downstream analysis of image content. Image splitting\ninvolves dissecting an image into its distinguishable semantic structures. We\nshow that the current state-of-the-art method for this task struggles in the\npresence of image noise, inadvertently also distributing the noise across the\npredicted outputs. The method we present here can deal with image noise by\nintegrating an unsupervised denoising sub-task. This integration results in\nimproved semantic image unmixing, even in the presence of notable and realistic\nlevels of imaging noise. A key innovation in denoiSplit is the use of\nspecifically formulated noise models and the suitable adjustment of\nKL-divergence loss for the high-dimensional hierarchical latent space we are\ntraining. We showcase the performance of denoiSplit across 4 tasks on\nreal-world microscopy images. Additionally, we perform qualitative and\nquantitative evaluations and compare results to existing benchmarks,\ndemonstrating the effectiveness of using denoiSplit: a single Variational\nSplitting Encoder-Decoder (VSE) Network using two suitable noise models to\njointly perform semantic splitting and denoising.\n","authors":["Ashesh Ashesh","Florian Jug"],"pdf_url":"https://arxiv.org/pdf/2403.11854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02970v5","updated":"2024-03-25T08:50:42Z","published":"2023-04-06T09:54:06Z","title":"Unraveling Instance Associations: A Closer Look for Audio-Visual\n  Segmentation","summary":"  Audio-visual segmentation (AVS) is a challenging task that involves\naccurately segmenting sounding objects based on audio-visual cues. The\neffectiveness of audio-visual learning critically depends on achieving accurate\ncross-modal alignment between sound and visual objects. Successful audio-visual\nlearning requires two essential components: 1) a challenging dataset with\nhigh-quality pixel-level multi-class annotated images associated with audio\nfiles, and 2) a model that can establish strong links between audio information\nand its corresponding visual object. However, these requirements are only\npartially addressed by current methods, with training sets containing biased\naudio-visual data, and models that generalise poorly beyond this biased\ntraining set. In this work, we propose a new cost-effective strategy to build\nchallenging and relatively unbiased high-quality audio-visual segmentation\nbenchmarks. We also propose a new informative sample mining method for\naudio-visual supervised contrastive learning to leverage discriminative\ncontrastive samples to enforce cross-modal understanding. We show empirical\nresults that demonstrate the effectiveness of our benchmark. Furthermore,\nexperiments conducted on existing AVS datasets and on our new benchmark show\nthat our method achieves state-of-the-art (SOTA) segmentation accuracy.\n","authors":["Yuanhong Chen","Yuyuan Liu","Hu Wang","Fengbei Liu","Chong Wang","Helen Frazer","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2304.02970v5.pdf","comment":"Code is available at https://github.com/cyh-0/CAVP"},{"id":"http://arxiv.org/abs/2403.06904v2","updated":"2024-03-25T08:45:37Z","published":"2024-03-11T16:56:37Z","title":"FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in\n  Human-Centric Tasks","summary":"  We propose FocusCLIP, integrating subject-level guidance--a specialized\nmechanism for target-specific supervision--into the CLIP framework for improved\nzero-shot transfer on human-centric tasks. Our novel contributions enhance CLIP\non both the vision and text sides. On the vision side, we incorporate ROI\nheatmaps emulating human visual attention mechanisms to emphasize\nsubject-relevant image regions. On the text side, we introduce human pose\ndescriptions to provide rich contextual information. For human-centric tasks,\nFocusCLIP is trained with images from the MPII Human Pose dataset. The proposed\napproach surpassed CLIP by an average of 8.61% across five previously unseen\ndatasets covering three human-centric tasks. FocusCLIP achieved an average\naccuracy of 33.65% compared to 25.04% by CLIP. We observed a 3.98% improvement\nin activity recognition, a 14.78% improvement in age classification, and a\n7.06% improvement in emotion recognition. Moreover, using our proposed\nsingle-shot LLM prompting strategy, we release a high-quality MPII Pose\nDescriptions dataset to encourage further research in multimodal learning for\nhuman-centric tasks. Furthermore, we also demonstrate the effectiveness of our\nsubject-level supervision on non-human-centric tasks. FocusCLIP shows a 2.47%\nimprovement over CLIP in zero-shot bird classification using the CUB dataset.\nOur findings emphasize the potential of integrating subject-level guidance with\ngeneral pretraining methods for enhanced downstream performance.\n","authors":["Muhammad Saif Ullah Khan","Muhammad Ferjad Naeem","Federico Tombari","Luc Van Gool","Didier Stricker","Muhammad Zeshan Afzal"],"pdf_url":"https://arxiv.org/pdf/2403.06904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.00247v4","updated":"2024-03-25T08:34:15Z","published":"2023-08-01T03:00:36Z","title":"Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive\n  Review","summary":"  The advent of deep learning has brought a revolutionary transformation to\nimage denoising techniques. However, the persistent challenge of acquiring\nnoise-clean pairs for supervised methods in real-world scenarios remains\nformidable, necessitating the exploration of more practical self-supervised\nimage denoising. This paper focuses on self-supervised image denoising methods\nthat offer effective solutions to address this challenge. Our comprehensive\nreview thoroughly analyzes the latest advancements in self-supervised image\ndenoising approaches, categorizing them into three distinct classes: General\nmethods, Blind Spot Network (BSN)-based methods, and Transformer-based methods.\nFor each class, we provide a concise theoretical analysis along with their\npractical applications. To assess the effectiveness of these methods, we\npresent both quantitative and qualitative experimental results on various\ndatasets, utilizing classical algorithms as benchmarks. Additionally, we\ncritically discuss the current limitations of these methods and propose\npromising directions for future research. By offering a detailed overview of\nrecent developments in self-supervised image denoising, this review serves as\nan invaluable resource for researchers and practitioners in the field,\nfacilitating a deeper understanding of this emerging domain and inspiring\nfurther advancements.\n","authors":["Dan Zhang","Fangfang Zhou","Felix Albu","Yuanzhou Wei","Xiao Yang","Yuan Gu","Qiang Li"],"pdf_url":"https://arxiv.org/pdf/2308.00247v4.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2403.16539v1","updated":"2024-03-25T08:31:14Z","published":"2024-03-25T08:31:14Z","title":"DOrA: 3D Visual Grounding with Order-Aware Referring","summary":"  3D visual grounding aims to identify the target object within a 3D point\ncloud scene referred to by a natural language description. While previous works\nattempt to exploit the verbo-visual relation with proposed cross-modal\ntransformers, unstructured natural utterances and scattered objects might lead\nto undesirable performances. In this paper, we introduce DOrA, a novel 3D\nvisual grounding framework with Order-Aware referring. DOrA is designed to\nleverage Large Language Models (LLMs) to parse language description, suggesting\na referential order of anchor objects. Such ordered anchor objects allow DOrA\nto update visual features and locate the target object during the grounding\nprocess. Experimental results on the NR3D and ScanRefer datasets demonstrate\nour superiority in both low-resource and full-data scenarios. In particular,\nDOrA surpasses current state-of-the-art frameworks by 9.3% and 7.8% grounding\naccuracy under 1% data and 10% data settings, respectively.\n","authors":["Tung-Yu Wu","Sheng-Yu Huang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08262v4","updated":"2024-03-25T08:29:52Z","published":"2024-03-13T05:25:49Z","title":"BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands\n  from a Single Image","summary":"  Creating personalized hand avatars is important to offer a realistic\nexperience to users on AR / VR platforms. While most prior studies focused on\nreconstructing 3D hand shapes, some recent work has tackled the reconstruction\nof hand textures on top of shapes. However, these methods are often limited to\ncapturing pixels on the visible side of a hand, requiring diverse views of the\nhand in a video or multiple images as input. In this paper, we propose a novel\nmethod, BiTT(Bi-directional Texture reconstruction of Two hands), which is the\nfirst end-to-end trainable method for relightable, pose-free texture\nreconstruction of two interacting hands taking only a single RGB image, by\nthree novel components: 1) bi-directional (left $\\leftrightarrow$ right)\ntexture reconstruction using the texture symmetry of left / right hands, 2)\nutilizing a texture parametric model for hand texture recovery, and 3) the\noverall coarse-to-fine stage pipeline for reconstructing personalized texture\nof two interacting hands. BiTT first estimates the scene light condition and\nalbedo image from an input image, then reconstructs the texture of both hands\nthrough the texture parametric model and bi-directional texture reconstructor.\nIn experiments using InterHand2.6M and RGB2Hands datasets, our method\nsignificantly outperforms state-of-the-art hand texture reconstruction methods\nquantitatively and qualitatively. The code is available at\nhttps://github.com/yunminjin2/BiTT\n","authors":["Minje Kim","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2403.08262v4.pdf","comment":"Accepted by CVPR 2024, Project Page:\n  https://yunminjin2.github.io/projects/bitt/"},{"id":"http://arxiv.org/abs/2403.16536v1","updated":"2024-03-25T08:26:42Z","published":"2024-03-25T08:26:42Z","title":"VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate\n  Spatiotemporal Forecasting","summary":"  Combining CNNs or ViTs, with RNNs for spatiotemporal forecasting, has yielded\nunparalleled results in predicting temporal and spatial dynamics. However,\nmodeling extensive global information remains a formidable challenge; CNNs are\nlimited by their narrow receptive fields, and ViTs struggle with the intensive\ncomputational demands of their attention mechanisms. The emergence of recent\nMamba-based architectures has been met with enthusiasm for their exceptional\nlong-sequence modeling capabilities, surpassing established vision models in\nefficiency and accuracy, which motivates us to develop an innovative\narchitecture tailored for spatiotemporal forecasting. In this paper, we propose\nthe VMRNN cell, a new recurrent unit that integrates the strengths of Vision\nMamba blocks with LSTM. We construct a network centered on VMRNN cells to\ntackle spatiotemporal prediction tasks effectively. Our extensive evaluations\nshow that our proposed approach secures competitive results on a variety of\ntasks while maintaining a smaller model size. Our code is available at\nhttps://github.com/yyyujintang/VMRNN-PyTorch.\n","authors":["Yujin Tang","Peijie Dong","Zhenheng Tang","Xiaowen Chu","Junwei Liang"],"pdf_url":"https://arxiv.org/pdf/2403.16536v1.pdf","comment":"11 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:2308.09891 by other authors"},{"id":"http://arxiv.org/abs/2403.16530v1","updated":"2024-03-25T08:16:06Z","published":"2024-03-25T08:16:06Z","title":"An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in\n  Diffusion Models","summary":"  Diffusion models have been widely used for conditional data cross-modal\ngeneration tasks such as text-to-image and text-to-video. However,\nstate-of-the-art models still fail to align the generated visual concepts with\nhigh-level semantics in a language such as object count, spatial relationship,\netc. We approach this problem from a multimodal data fusion perspective and\ninvestigate how different fusion strategies can affect vision-language\nalignment. We discover that compared to the widely used early fusion of\nconditioning text in a pretrained image feature space, a specially designed\nintermediate fusion can: (i) boost text-to-image alignment with improved\ngeneration quality and (ii) improve training and inference efficiency by\nreducing low-rank text-to-image attention calculations. We perform experiments\nusing a text-to-image generation task on the MS-COCO dataset. We compare our\nintermediate fusion mechanism with the classic early fusion mechanism on two\ncommon conditioning methods on a U-shaped ViT backbone. Our intermediate fusion\nmodel achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and\n50% increased training speed compared to a strong U-ViT baseline with an early\nfusion.\n","authors":["Zizhao Hu","Shaochong Jia","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2403.16530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16528v1","updated":"2024-03-25T08:14:22Z","published":"2024-03-25T08:14:22Z","title":"Open-Set Recognition in the Age of Vision-Language Models","summary":"  Are vision-language models (VLMs) open-set models because they are trained on\ninternet-scale datasets? We answer this question with a clear no - VLMs\nintroduce closed-set assumptions via their finite query set, making them\nvulnerable to open-set conditions. We systematically evaluate VLMs for open-set\nrecognition and find they frequently misclassify objects not contained in their\nquery set, leading to alarmingly low precision when tuned for high recall and\nvice versa. We show that naively increasing the size of the query set to\ncontain more and more classes does not mitigate this problem, but instead\ncauses diminishing task performance and open-set performance. We establish a\nrevised definition of the open-set problem for the age of VLMs, define a new\nbenchmark and evaluation protocol to facilitate standardised evaluation and\nresearch in this important area, and evaluate promising baseline approaches\nbased on predictive uncertainty and dedicated negative embeddings on a range of\nVLM classifiers and object detectors.\n","authors":["Dimity Miller","Niko Sünderhauf","Alex Kenna","Keita Mason"],"pdf_url":"https://arxiv.org/pdf/2403.16528v1.pdf","comment":"31 pages, under review"},{"id":"http://arxiv.org/abs/2403.16526v1","updated":"2024-03-25T08:09:22Z","published":"2024-03-25T08:09:22Z","title":"ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise\n  Optimization in Medical Image Registration","summary":"  Deformable image registration plays a crucial role in medical imaging, aiding\nin disease diagnosis and image-guided interventions. Traditional iterative\nmethods are slow, while deep learning (DL) accelerates solutions but faces\nusability and precision challenges. This study introduces a pyramid network\nwith the enhanced motion decomposition Transformer (ModeTv2) operator,\nshowcasing superior pairwise optimization (PO) akin to traditional methods. We\nre-implement ModeT operator with CUDA extensions to enhance its computational\nefficiency. We further propose RegHead module which refines deformation fields,\nimproves the realism of deformation and reduces parameters. By adopting the PO,\nthe proposed network balances accuracy, efficiency, and generalizability.\nExtensive experiments on two public brain MRI datasets and one abdominal CT\ndataset demonstrate the network's suitability for PO, providing a DL model with\nenhanced usability and interpretability. The code is publicly available.\n","authors":["Haiqiao Wang","Zhuoyuan Wang","Dong Ni","Yi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08863v3","updated":"2024-03-25T08:05:16Z","published":"2023-11-15T10:49:15Z","title":"Toulouse Hyperspectral Data Set: a benchmark data set to assess\n  semi-supervised spectral representation learning and pixel-wise\n  classification techniques","summary":"  Airborne hyperspectral images can be used to map the land cover in large\nurban areas, thanks to their very high spatial and spectral resolutions on a\nwide spectral domain. While the spectral dimension of hyperspectral images is\nhighly informative of the chemical composition of the land surface, the use of\nstate-of-the-art machine learning algorithms to map the land cover has been\ndramatically limited by the availability of training data. To cope with the\nscarcity of annotations, semi-supervised and self-supervised techniques have\nlately raised a lot of interest in the community. Yet, the publicly available\nhyperspectral data sets commonly used to benchmark machine learning models are\nnot totally suited to evaluate their generalization performances due to one or\nseveral of the following properties: a limited geographical coverage (which\ndoes not reflect the spectral diversity in metropolitan areas), a small number\nof land cover classes and a lack of appropriate standard train / test splits\nfor semi-supervised and self-supervised learning. Therefore, we release in this\npaper the Toulouse Hyperspectral Data Set that stands out from other data sets\nin the above-mentioned respects in order to meet key issues in spectral\nrepresentation learning and classification over large-scale hyperspectral\nimages with very few labeled pixels. Besides, we discuss and experiment\nself-supervised techniques for spectral representation learning, including the\nMasked Autoencoder, and establish a baseline for pixel-wise classification\nachieving 85% overall accuracy and 77% F1 score. The Toulouse Hyperspectral\nData Set and our code are publicly available at\nhttps://www.toulouse-hyperspectral-data-set.com and\nhttps://www.github.com/Romain3Ch216/tlse-experiments, respectively.\n","authors":["Romain Thoreau","Laurent Risser","Véronique Achard","Béatrice Berthelot","Xavier Briottet"],"pdf_url":"https://arxiv.org/pdf/2311.08863v3.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.16520v1","updated":"2024-03-25T08:02:41Z","published":"2024-03-25T08:02:41Z","title":"CMViM: Contrastive Masked Vim Autoencoder for 3D Multi-modal\n  Representation Learning for AD classification","summary":"  Alzheimer's disease (AD) is an incurable neurodegenerative condition leading\nto cognitive and functional deterioration. Given the lack of a cure, prompt and\nprecise AD diagnosis is vital, a complex process dependent on multiple factors\nand multi-modal data. While successful efforts have been made to integrate\nmulti-modal representation learning into medical datasets, scant attention has\nbeen given to 3D medical images. In this paper, we propose Contrastive Masked\nVim Autoencoder (CMViM), the first efficient representation learning method\ntailored for 3D multi-modal data. Our proposed framework is built on a masked\nVim autoencoder to learn a unified multi-modal representation and\nlong-dependencies contained in 3D medical images. We also introduce an\nintra-modal contrastive learning module to enhance the capability of the\nmulti-modal Vim encoder for modeling the discriminative features in the same\nmodality, and an inter-modal contrastive learning module to alleviate\nmisaligned representation among modalities. Our framework consists of two main\nsteps: 1) incorporate the Vision Mamba (Vim) into the mask autoencoder to\nreconstruct 3D masked multi-modal data efficiently. 2) align the multi-modal\nrepresentations with contrastive learning mechanisms from both intra-modal and\ninter-modal aspects. Our framework is pre-trained and validated ADNI2 dataset\nand validated on the downstream task for AD classification. The proposed CMViM\nyields 2.7\\% AUC performance improvement compared with other state-of-the-art\nmethods.\n","authors":["Guangqian Yang","Kangrui Du","Zhihan Yang","Ye Du","Yongping Zheng","Shujun Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16520v1.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.16516v1","updated":"2024-03-25T08:00:43Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v1.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2403.16513v1","updated":"2024-03-25T07:58:58Z","published":"2024-03-25T07:58:58Z","title":"Let Real Images be as a Judger, Spotting Fake Images Synthesized with\n  Generative Models","summary":"  In the last few years, generative models have shown their powerful\ncapabilities in synthesizing realistic images in both quality and diversity\n(i.e., facial images, and natural subjects). Unfortunately, the artifact\npatterns in fake images synthesized by different generative models are\ninconsistent, leading to the failure of previous research that relied on\nspotting subtle differences between real and fake. In our preliminary\nexperiments, we find that the artifacts in fake images always change with the\ndevelopment of the generative model, while natural images exhibit stable\nstatistical properties. In this paper, we employ natural traces shared only by\nreal images as an additional predictive target in the detector. Specifically,\nthe natural traces are learned from the wild real images and we introduce\nextended supervised contrastive learning to bring them closer to real images\nand further away from fake ones. This motivates the detector to make decisions\nbased on the proximity of images to the natural traces. To conduct a\ncomprehensive experiment, we built a high-quality and diverse dataset that\nincludes generative models comprising 6 GAN and 6 diffusion models, to evaluate\nthe effectiveness in generalizing unknown forgery techniques and robustness in\nsurviving different transformations. Experimental results show that our\nproposed method gives 96.1% mAP significantly outperforms the baselines.\nExtensive experiments conducted on the widely recognized platform Midjourney\nreveal that our proposed method achieves an accuracy exceeding 78.4%,\nunderscoring its practicality for real-world application deployment. The source\ncode and partial self-built dataset are available in supplementary material.\n","authors":["Ziyou Liang","Run Wang","Weifeng Liu","Yuyang Zhang","Wenyuan Yang","Lina Wang","Xingkai Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16510v1","updated":"2024-03-25T07:54:18Z","published":"2024-03-25T07:54:18Z","title":"Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework","summary":"  Despite the remarkable process of talking-head-based avatar-creating\nsolutions, directly generating anchor-style videos with full-body motions\nremains challenging. In this study, we propose Make-Your-Anchor, a novel system\nnecessitating only a one-minute video clip of an individual for training,\nsubsequently enabling the automatic generation of anchor-style videos with\nprecise torso and hand movements. Specifically, we finetune a proposed\nstructure-guided diffusion model on input video to render 3D mesh conditions\ninto human appearances. We adopt a two-stage training strategy for the\ndiffusion model, effectively binding movements with specific appearances. To\nproduce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise\ndiffusion model to a 3D style without additional training cost, and a simple\nyet effective batch-overlapped temporal denoising module is proposed to bypass\nthe constraints on video length during inference. Finally, a novel\nidentity-specific face enhancement module is introduced to improve the visual\nquality of facial regions in the output videos. Comparative experiments\ndemonstrate the effectiveness and superiority of the system in terms of visual\nquality, temporal coherence, and identity preservation, outperforming SOTA\ndiffusion/non-diffusion methods. Project page:\n\\url{https://github.com/ICTMCG/Make-Your-Anchor}.\n","authors":["Ziyao Huang","Fan Tang","Yong Zhang","Xiaodong Cun","Juan Cao","Jintao Li","Tong-Yee Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16510v1.pdf","comment":"accepted at CVPR2024"},{"id":"http://arxiv.org/abs/2305.01309v2","updated":"2024-03-25T07:53:54Z","published":"2023-05-02T10:35:20Z","title":"Geometric Prior Based Deep Human Point Cloud Geometry Compression","summary":"  The emergence of digital avatars has raised an exponential increase in the\ndemand for human point clouds with realistic and intricate details. The\ncompression of such data becomes challenging with overwhelming data amounts\ncomprising millions of points. Herein, we leverage the human geometric prior in\ngeometry redundancy removal of point clouds, greatly promoting the compression\nperformance. More specifically, the prior provides topological constraints as\ngeometry initialization, allowing adaptive adjustments with a compact parameter\nset that could be represented with only a few bits. Therefore, we can envisage\nhigh-resolution human point clouds as a combination of geometric priors and\nstructural deviations. The priors could first be derived with an aligned point\ncloud, and subsequently the difference of features is compressed into a compact\nlatent code. The proposed framework can operate in a play-and-plug fashion with\nexisting learning based point cloud compression methods. Extensive experimental\nresults show that our approach significantly improves the compression\nperformance without deteriorating the quality, demonstrating its promise in a\nvariety of applications.\n","authors":["Xinju Wu","Pingping Zhang","Meng Wang","Peilin Chen","Shiqi Wang","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2305.01309v2.pdf","comment":"Accepted by TCSVT 2024"},{"id":"http://arxiv.org/abs/2311.17315v3","updated":"2024-03-25T07:51:14Z","published":"2023-11-29T02:10:31Z","title":"Explaining CLIP's performance disparities on data from blind/low vision\n  users","summary":"  Large multi-modal models (LMMs) hold the potential to usher in a new era of\nautomated visual assistance for people who are blind or low vision (BLV). Yet,\nthese models have not been systematically evaluated on data captured by BLV\nusers. We address this by empirically assessing CLIP, a widely-used LMM likely\nto underpin many assistive technologies. Testing 25 CLIP variants in a\nzero-shot classification task, we find that their accuracy is 15 percentage\npoints lower on average for images captured by BLV users than web-crawled\nimages. This disparity stems from CLIP's sensitivities to 1) image content\n(e.g. not recognizing disability objects as well as other objects); 2) image\nquality (e.g. not being robust to lighting variation); and 3) text content\n(e.g. not recognizing objects described by tactile adjectives as well as visual\nones). We delve deeper with a textual analysis of three common pre-training\ndatasets: LAION-400M, LAION-2B and DataComp-1B, showing that disability content\nis rarely mentioned. We then provide three examples that illustrate how the\nperformance disparities extend to three downstream models underpinned by CLIP:\nOWL-ViT, CLIPSeg and DALL-E2. We find that few-shot learning with as few as 5\nimages can mitigate CLIP's quality-of-service disparities for BLV users in some\nscenarios, which we discuss alongside a set of other possible mitigations.\n","authors":["Daniela Massiceti","Camilla Longden","Agnieszka Słowik","Samuel Wills","Martin Grayson","Cecily Morrison"],"pdf_url":"https://arxiv.org/pdf/2311.17315v3.pdf","comment":"Accepted at 2024 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR)"},{"id":"http://arxiv.org/abs/2403.16502v1","updated":"2024-03-25T07:35:28Z","published":"2024-03-25T07:35:28Z","title":"Medical Image Registration and Its Application in Retinal Images: A\n  Review","summary":"  Medical image registration is vital for disease diagnosis and treatment with\nits ability to merge diverse information of images, which may be captured under\ndifferent times, angles, or modalities. Although several surveys have reviewed\nthe development of medical image registration, these surveys have not\nsystematically summarized methodologies of existing medical image registration\nmethods. To this end, we provide a comprehensive review of these methods from\ntraditional and deep learning-based directions, aiming to help audiences\nunderstand the development of medical image registration quickly. In\nparticular, we review recent advances in retinal image registration at the end\nof each section, which has not attracted much attention. Additionally, we also\ndiscuss the current challenges of retinal image registration and provide\ninsights and prospects for future research.\n","authors":["Qiushi Nie","Xiaoqing Zhang","Yan Hu","Mingdao Gong","Jiang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16499v1","updated":"2024-03-25T07:34:06Z","published":"2024-03-25T07:34:06Z","title":"Self-Supervised Learning for Medical Image Data with Anatomy-Oriented\n  Imaging Planes","summary":"  Self-supervised learning has emerged as a powerful tool for pretraining deep\nnetworks on unlabeled data, prior to transfer learning of target tasks with\nlimited annotation. The relevance between the pretraining pretext and target\ntasks is crucial to the success of transfer learning. Various pretext tasks\nhave been proposed to utilize properties of medical image data (e.g., three\ndimensionality), which are more relevant to medical image analysis than generic\nones for natural images. However, previous work rarely paid attention to data\nwith anatomy-oriented imaging planes, e.g., standard cardiac magnetic resonance\nimaging views. As these imaging planes are defined according to the anatomy of\nthe imaged organ, pretext tasks effectively exploiting this information can\npretrain the networks to gain knowledge on the organ of interest. In this work,\nwe propose two complementary pretext tasks for this group of medical image data\nbased on the spatial relationship of the imaging planes. The first is to learn\nthe relative orientation between the imaging planes and implemented as\nregressing their intersecting lines. The second exploits parallel imaging\nplanes to regress their relative slice locations within a stack. Both pretext\ntasks are conceptually straightforward and easy to implement, and can be\ncombined in multitask learning for better representation learning. Thorough\nexperiments on two anatomical structures (heart and knee) and representative\ntarget tasks (semantic segmentation and classification) demonstrate that the\nproposed pretext tasks are effective in pretraining deep networks for\nremarkably boosted performance on the target tasks, and superior to other\nrecent approaches.\n","authors":["Tianwei Zhang","Dong Wei","Mengmeng Zhua","Shi Gu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.16499v1.pdf","comment":"Medical Image Analysis"},{"id":"http://arxiv.org/abs/2403.16497v1","updated":"2024-03-25T07:29:18Z","published":"2024-03-25T07:29:18Z","title":"PathoTune: Adapting Visual Foundation Model to Pathological Specialists","summary":"  As natural image understanding moves towards the pretrain-finetune era,\nresearch in pathology imaging is concurrently evolving. Despite the predominant\nfocus on pretraining pathological foundation models, how to adapt foundation\nmodels to downstream tasks is little explored. For downstream adaptation, we\npropose the existence of two domain gaps, i.e., the Foundation-Task Gap and the\nTask-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework\ndesigned to efficiently adapt pathological or even visual foundation models to\npathology-specific tasks via multi-modal prompt tuning. The proposed framework\nleverages Task-specific Visual Prompts and Task-specific Textual Prompts to\nidentify task-relevant features, along with Instance-specific Visual Prompts\nfor encoding single pathological image features. Results across multiple\ndatasets at both patch-level and WSI-level demonstrate its superior performance\nover single-modality prompt tuning approaches. Significantly, PathoTune\nfacilitates the direct adaptation of natural visual foundation models to\npathological tasks, drastically outperforming pathological foundation models\nwith simple linear probing. The code will be available upon acceptance.\n","authors":["Jiaxuan Lu","Fang Yan","Xiaofan Zhang","Yue Gao","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16497v1.pdf","comment":"Submitted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.16494v1","updated":"2024-03-25T07:22:22Z","published":"2024-03-25T07:22:22Z","title":"CT-Bound: Fast Boundary Estimation From Noisy Images Via Hybrid\n  Convolution and Transformer Neural Networks","summary":"  We present CT-Bound, a fast boundary estimation method for noisy images using\na hybrid Convolution and Transformer neural network. The proposed architecture\ndecomposes boundary estimation into two tasks: local detection and global\nregularization of image boundaries. It first estimates a parametric\nrepresentation of boundary structures only using the input image within a small\nreceptive field and then refines the boundary structure in the parameter domain\nwithout accessing the input image. Because of this, a part of the network can\nbe easily trained using naive, synthetic images and still generalized to real\nimages, and the entire architecture is computationally efficient as the\nboundary refinement is non-iterative and not in the image domain. Compared with\nthe previous highest accuracy methods, our experiment shows that CT-Bound is\n100 times faster, producing comparably accurate, high-quality boundary and\ncolor maps. We also demonstrate that CT-Bound can produce boundary and color\nmaps on real captured images without extra fine-tuning and real-time boundary\nmap and color map videos at ten frames per second.\n","authors":["Wei Xu","Junjie Luo","Qi Guo"],"pdf_url":"https://arxiv.org/pdf/2403.16494v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.16481v1","updated":"2024-03-25T07:07:50Z","published":"2024-03-25T07:07:50Z","title":"REFRAME: Reflective Surface Real-Time Rendering for Mobile Devices","summary":"  This work tackles the challenging task of achieving real-time novel view\nsynthesis on various scenes, including highly reflective objects and unbounded\noutdoor scenes. Existing real-time rendering methods, especially those based on\nmeshes, often have subpar performance in modeling surfaces with rich\nview-dependent appearances. Our key idea lies in leveraging meshes for\nrendering acceleration while incorporating a novel approach to parameterize\nview-dependent information. We decompose the color into diffuse and specular,\nand model the specular color in the reflected direction based on a neural\nenvironment map. Our experiments demonstrate that our method achieves\ncomparable reconstruction quality for highly reflective surfaces compared to\nstate-of-the-art offline methods, while also efficiently enabling real-time\nrendering on edge devices such as smartphones.\n","authors":["Chaojie Ji","Yufeng Li","Yiyi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.16481v1.pdf","comment":"Project Page:https://xdimlab.github.io/REFRAME/"},{"id":"http://arxiv.org/abs/2403.06606v2","updated":"2024-03-25T06:57:57Z","published":"2024-03-11T10:50:53Z","title":"Distributionally Generative Augmentation for Fair Facial Attribute\n  Classification","summary":"  Facial Attribute Classification (FAC) holds substantial promise in widespread\napplications. However, FAC models trained by traditional methodologies can be\nunfair by exhibiting accuracy inconsistencies across varied data\nsubpopulations. This unfairness is largely attributed to bias in data, where\nsome spurious attributes (e.g., Male) statistically correlate with the target\nattribute (e.g., Smiling). Most of existing fairness-aware methods rely on the\nlabels of spurious attributes, which may be unavailable in practice. This work\nproposes a novel, generation-based two-stage framework to train a fair FAC\nmodel on biased data without additional annotation. Initially, we identify the\npotential spurious attributes based on generative models. Notably, it enhances\ninterpretability by explicitly showing the spurious attributes in image space.\nFollowing this, for each image, we first edit the spurious attributes with a\nrandom degree sampled from a uniform distribution, while keeping target\nattribute unchanged. Then we train a fair FAC model by fostering model\ninvariance to these augmentation. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our method in promoting fairness in\nFAC without compromising accuracy. Codes are in\nhttps://github.com/heqianpei/DiGA.\n","authors":["Fengda Zhang","Qianpei He","Kun Kuang","Jiashuo Liu","Long Chen","Chao Wu","Jun Xiao","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06606v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.10066v2","updated":"2024-03-25T06:27:57Z","published":"2024-03-15T07:16:07Z","title":"Contrastive Pre-Training with Multi-View Fusion for No-Reference Point\n  Cloud Quality Assessment","summary":"  No-reference point cloud quality assessment (NR-PCQA) aims to automatically\nevaluate the perceptual quality of distorted point clouds without available\nreference, which have achieved tremendous improvements due to the utilization\nof deep neural networks. However, learning-based NR-PCQA methods suffer from\nthe scarcity of labeled data and usually perform suboptimally in terms of\ngeneralization. To solve the problem, we propose a novel contrastive\npre-training framework tailored for PCQA (CoPA), which enables the pre-trained\nmodel to learn quality-aware representations from unlabeled data. To obtain\nanchors in the representation space, we project point clouds with different\ndistortions into images and randomly mix their local patches to form mixed\nimages with multiple distortions. Utilizing the generated anchors, we constrain\nthe pre-training process via a quality-aware contrastive loss following the\nphilosophy that perceptual quality is closely related to both content and\ndistortion. Furthermore, in the model fine-tuning stage, we propose a\nsemantic-guided multi-view fusion module to effectively integrate the features\nof projected images from multiple perspectives. Extensive experiments show that\nour method outperforms the state-of-the-art PCQA methods on popular benchmarks.\nFurther investigations demonstrate that CoPA can also benefit existing\nlearning-based PCQA models.\n","authors":["Ziyu Shan","Yujie Zhang","Qi Yang","Haichen Yang","Yiling Xu","Jenq-Neng Hwang","Xiaozhong Xu","Shan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16450v1","updated":"2024-03-25T06:22:27Z","published":"2024-03-25T06:22:27Z","title":"Camera-aware Label Refinement for Unsupervised Person Re-identification","summary":"  Unsupervised person re-identification aims to retrieve images of a specified\nperson without identity labels. Many recent unsupervised Re-ID approaches adopt\nclustering-based methods to measure cross-camera feature similarity to roughly\ndivide images into clusters. They ignore the feature distribution discrepancy\ninduced by camera domain gap, resulting in the unavoidable performance\ndegradation. Camera information is usually available, and the feature\ndistribution in the single camera usually focuses more on the appearance of the\nindividual and has less intra-identity variance. Inspired by the observation,\nwe introduce a \\textbf{C}amera-\\textbf{A}ware \\textbf{L}abel\n\\textbf{R}efinement~(CALR) framework that reduces camera discrepancy by\nclustering intra-camera similarity. Specifically, we employ intra-camera\ntraining to obtain reliable local pseudo labels within each camera, and then\nrefine global labels generated by inter-camera clustering and train the\ndiscriminative model using more reliable global pseudo labels in a self-paced\nmanner. Meanwhile, we develop a camera-alignment module to align feature\ndistributions under different cameras, which could help deal with the camera\nvariance further. Extensive experiments validate the superiority of our\nproposed method over state-of-the-art approaches. The code is accessible at\nhttps://github.com/leeBooMla/CALR.\n","authors":["Pengna Li","Kangyi Wu","Wenli Huang","Sanping Zhou","Jinjun Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16450v1.pdf","comment":"submitted to IEEE TMM"},{"id":"http://arxiv.org/abs/2312.02480v2","updated":"2024-03-25T06:22:09Z","published":"2023-12-05T04:13:31Z","title":"Differentiable Point-based Inverse Rendering","summary":"  We present differentiable point-based inverse rendering, DPIR, an\nanalysis-by-synthesis method that processes images captured under diverse\nilluminations to estimate shape and spatially-varying BRDF. To this end, we\nadopt point-based rendering, eliminating the need for multiple samplings per\nray, typical of volumetric rendering, thus significantly enhancing the speed of\ninverse rendering. To realize this idea, we devise a hybrid point-volumetric\nrepresentation for geometry and a regularized basis-BRDF representation for\nreflectance. The hybrid geometric representation enables fast rendering through\npoint-based splatting while retaining the geometric details and stability\ninherent to SDF-based representations. The regularized basis-BRDF mitigates the\nill-posedness of inverse rendering stemming from limited light-view angular\nsamples. We also propose an efficient shadow detection method using point-based\nshadow map rendering. Our extensive evaluations demonstrate that DPIR\noutperforms prior works in terms of reconstruction accuracy, computational\nefficiency, and memory footprint. Furthermore, our explicit point-based\nrepresentation and rendering enables intuitive geometry and reflectance\nediting.\n","authors":["Hoon-Gyu Chung","Seokjun Choi","Seung-Hwan Baek"],"pdf_url":"https://arxiv.org/pdf/2312.02480v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16442v1","updated":"2024-03-25T06:05:50Z","published":"2024-03-25T06:05:50Z","title":"If CLIP Could Talk: Understanding Vision-Language Model Representations\n  Through Their Preferred Concept Descriptions","summary":"  Recent works often assume that Vision-Language Model (VLM) representations\nare based on visual attributes like shape. However, it is unclear to what\nextent VLMs prioritize this information to represent concepts. We propose\nExtract and Explore (EX2), a novel approach to characterize important textual\nfeatures for VLMs. EX2 uses reinforcement learning to align a large language\nmodel with VLM preferences and generates descriptions that incorporate the\nimportant features for the VLM. Then, we inspect the descriptions to identify\nthe features that contribute to VLM representations. We find that spurious\ndescriptions have a major role in VLM representations despite providing no\nhelpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,\namong informative descriptions, VLMs rely significantly on non-visual\nattributes like habitat to represent visual concepts. Also, our analysis\nreveals that different VLMs prioritize different attributes in their\nrepresentations. Overall, we show that VLMs do not simply match images to scene\ndescriptions and that non-visual or even spurious descriptions significantly\ninfluence their representations.\n","authors":["Reza Esfandiarpoor","Cristina Menghini","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2403.16442v1.pdf","comment":"Code: https://github.com/BatsResearch/ex2"},{"id":"http://arxiv.org/abs/2310.14566v5","updated":"2024-03-25T06:05:24Z","published":"2023-10-23T04:49:09Z","title":"HallusionBench: An Advanced Diagnostic Suite for Entangled Language\n  Hallucination and Visual Illusion in Large Vision-Language Models","summary":"  We introduce HallusionBench, a comprehensive benchmark designed for the\nevaluation of image-context reasoning. This benchmark presents significant\nchallenges to advanced large visual-language models (LVLMs), such as\nGPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing\nnuanced understanding and interpretation of visual data. The benchmark\ncomprises 346 images paired with 1129 questions, all meticulously crafted by\nhuman experts. We introduce a novel structure for these visual questions\ndesigned to establish control groups. This structure enables us to conduct a\nquantitative analysis of the models' response tendencies, logical consistency,\nand various failure modes. In our evaluation on HallusionBench, we benchmarked\n15 different models, highlighting a 31.42% question-pair accuracy achieved by\nthe state-of-the-art GPT-4V. Notably, all other evaluated models achieve\naccuracy below 16%. Moreover, our analysis not only highlights the observed\nfailure modes, including language hallucination and visual illusion, but also\ndeepens an understanding of these pitfalls. Our comprehensive case studies\nwithin HallusionBench shed light on the challenges of hallucination and\nillusion in LVLMs. Based on these insights, we suggest potential pathways for\ntheir future improvement. The benchmark and codebase can be accessed at\nhttps://github.com/tianyi-lab/HallusionBench.\n","authors":["Tianrui Guan","Fuxiao Liu","Xiyang Wu","Ruiqi Xian","Zongxia Li","Xiaoyu Liu","Xijun Wang","Lichang Chen","Furong Huang","Yaser Yacoob","Dinesh Manocha","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.14566v5.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16440v1","updated":"2024-03-25T06:02:05Z","published":"2024-03-25T06:02:05Z","title":"RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection","summary":"  Three-dimensional object detection is one of the key tasks in autonomous\ndriving. To reduce costs in practice, low-cost multi-view cameras for 3D object\ndetection are proposed to replace the expansive LiDAR sensors. However, relying\nsolely on cameras is difficult to achieve highly accurate and robust 3D object\ndetection. An effective solution to this issue is combining multi-view cameras\nwith the economical millimeter-wave radar sensor to achieve more reliable\nmulti-modal 3D object detection. In this paper, we introduce RCBEVDet, a\nradar-camera fusion 3D object detection method in the bird's eye view (BEV).\nSpecifically, we first design RadarBEVNet for radar BEV feature extraction.\nRadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section\n(RCS) aware BEV encoder. In the dual-stream radar backbone, a point-based\nencoder and a transformer-based encoder are proposed to extract radar features,\nwith an injection and extraction module to facilitate communication between the\ntwo encoders. The RCS-aware BEV encoder takes RCS as the object size prior to\nscattering the point feature in BEV. Besides, we present the Cross-Attention\nMulti-layer Fusion module to automatically align the multi-modal BEV feature\nfrom radar and camera with the deformable attention mechanism, and then fuse\nthe feature with channel and spatial fusion layers. Experimental results show\nthat RCBEVDet achieves new state-of-the-art radar-camera fusion results on\nnuScenes and view-of-delft (VoD) 3D object detection benchmarks. Furthermore,\nRCBEVDet achieves better 3D detection results than all real-time camera-only\nand radar-camera 3D object detectors with a faster inference speed at 21~28\nFPS. The source code will be released at https://github.com/VDIGPKU/RCBEVDet.\n","authors":["Zhiwei Lin","Zhe Liu","Zhongyu Xia","Xinhao Wang","Yongtao Wang","Shengxiang Qi","Yang Dong","Nan Dong","Le Zhang","Ce Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.16440v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.16439v1","updated":"2024-03-25T05:58:33Z","published":"2024-03-25T05:58:33Z","title":"Producing and Leveraging Online Map Uncertainty in Trajectory Prediction","summary":"  High-definition (HD) maps have played an integral role in the development of\nmodern autonomous vehicle (AV) stacks, albeit with high associated labeling and\nmaintenance costs. As a result, many recent works have proposed methods for\nestimating HD maps online from sensor data, enabling AVs to operate outside of\npreviously-mapped regions. However, current online map estimation approaches\nare developed in isolation of their downstream tasks, complicating their\nintegration in AV stacks. In particular, they do not produce uncertainty or\nconfidence estimates. In this work, we extend multiple state-of-the-art online\nmap estimation methods to additionally estimate uncertainty and show how this\nenables more tightly integrating online mapping with trajectory forecasting. In\ndoing so, we find that incorporating uncertainty yields up to 50% faster\ntraining convergence and up to 15% better prediction performance on the\nreal-world nuScenes driving dataset.\n","authors":["Xunjiang Gu","Guanyu Song","Igor Gilitschenski","Marco Pavone","Boris Ivanovic"],"pdf_url":"https://arxiv.org/pdf/2403.16439v1.pdf","comment":"14 pages, 14 figures, 6 tables. CVPR 2024"},{"id":"http://arxiv.org/abs/2403.07371v2","updated":"2024-03-25T05:48:28Z","published":"2024-03-12T07:15:29Z","title":"Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of\n  Altered Diffusion Models","summary":"  This study discusses the critical issues of Virtual Try-On in contemporary\ne-commerce and the prospective metaverse, emphasizing the challenges of\npreserving intricate texture details and distinctive features of the target\nperson and the clothes in various scenarios, such as clothing texture and\nidentity characteristics like tattoos or accessories. In addition to the\nfidelity of the synthesized images, the efficiency of the synthesis process\npresents a significant hurdle. Various existing approaches are explored,\nhighlighting the limitations and unresolved aspects, e.g., identity information\nomission, uncontrollable artifacts, and low synthesis speed. It then proposes a\nnovel diffusion-based solution that addresses garment texture preservation and\nuser identity retention during virtual try-on. The proposed network comprises\ntwo primary modules - a warping module aligning clothing with individual\nfeatures and a try-on module refining the attire and generating missing parts\nintegrated with a mask-aware post-processing technique ensuring the integrity\nof the individual's identity. It demonstrates impressive results, surpassing\nthe state-of-the-art in speed by nearly 20 times during inference, with\nsuperior fidelity in qualitative assessments. Quantitative evaluations confirm\ncomparable performance with the recent SOTA method on the VITON-HD and\nDresscode datasets.\n","authors":["Phuong Dam","Jihoon Jeong","Anh Tran","Daeyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2403.07371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16438v1","updated":"2024-03-25T05:46:06Z","published":"2024-03-25T05:46:06Z","title":"Real-time Neuron Segmentation for Voltage Imaging","summary":"  In voltage imaging, where the membrane potentials of individual neurons are\nrecorded at from hundreds to thousand frames per second using fluorescence\nmicroscopy, data processing presents a challenge. Even a fraction of a minute\nof recording with a limited image size yields gigabytes of video data\nconsisting of tens of thousands of frames, which can be time-consuming to\nprocess. Moreover, millisecond-level short exposures lead to noisy video\nframes, obscuring neuron footprints especially in deep-brain samples where\nnoisy signals are buried in background fluorescence. To address this challenge,\nwe propose a fast neuron segmentation method able to detect multiple,\npotentially overlapping, spiking neurons from noisy video frames, and implement\na data processing pipeline incorporating the proposed segmentation method along\nwith GPU-accelerated motion correction. By testing on existing datasets as well\nas on new datasets we introduce, we show that our pipeline extracts neuron\nfootprints that agree well with human annotation even from cluttered datasets,\nand demonstrate real-time processing of voltage imaging data on a single\ndesktop computer for the first time.\n","authors":["Yosuke Bando","Ramdas Pillai","Atsushi Kajita","Farhan Abdul Hakeem","Yves Quemener","Hua-an Tseng","Kiryl D. Piatkevich","Changyang Linghu","Xue Han","Edward S. Boyden"],"pdf_url":"https://arxiv.org/pdf/2403.16438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06199v4","updated":"2024-03-25T05:36:56Z","published":"2024-03-10T12:43:27Z","title":"Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small\n  Language Models","summary":"  Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.\n","authors":["Minjie Zhu","Yichen Zhu","Xin Liu","Ning Liu","Zhiyuan Xu","Chaomin Shen","Yaxin Peng","Zhicai Ou","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2403.06199v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18287v2","updated":"2024-03-25T05:34:58Z","published":"2023-11-30T06:45:52Z","title":"Dispersed Structured Light for Hyperspectral 3D Imaging","summary":"  Hyperspectral 3D imaging aims to acquire both depth and spectral information\nof a scene. However, existing methods are either prohibitively expensive and\nbulky or compromise on spectral and depth accuracy. In this work, we present\nDispersed Structured Light (DSL), a cost-effective and compact method for\naccurate hyperspectral 3D imaging. DSL modifies a traditional projector-camera\nsystem by placing a sub-millimeter thick diffraction grating film front of the\nprojector. The grating disperses structured light based on light wavelength. To\nutilize the dispersed structured light, we devise a model for dispersive\nprojection image formation and a per-pixel hyperspectral 3D reconstruction\nmethod. We validate DSL by instantiating a compact experimental prototype. DSL\nachieves spectral accuracy of 18.8nm full-width half-maximum (FWHM) and depth\nerror of 1mm. We demonstrate that DSL outperforms prior work on practical\nhyperspectral 3D imaging. DSL promises accurate and practical hyperspectral 3D\nimaging for diverse application domains, including computer vision and\ngraphics, cultural heritage, geology, and biology.\n","authors":["Suhyun Shin","Seokjun Choi","Felix Heide","Seung-Hwan Baek"],"pdf_url":"https://arxiv.org/pdf/2311.18287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16431v1","updated":"2024-03-25T05:22:34Z","published":"2024-03-25T05:22:34Z","title":"DOCTR: Disentangled Object-Centric Transformer for Point Scene\n  Understanding","summary":"  Point scene understanding is a challenging task to process real-world scene\npoint cloud, which aims at segmenting each object, estimating its pose, and\nreconstructing its mesh simultaneously. Recent state-of-the-art method first\nsegments each object and then processes them independently with multiple stages\nfor the different sub-tasks. This leads to a complex pipeline to optimize and\nmakes it hard to leverage the relationship constraints between multiple\nobjects. In this work, we propose a novel Disentangled Object-Centric\nTRansformer (DOCTR) that explores object-centric representation to facilitate\nlearning with multiple objects for the multiple sub-tasks in a unified manner.\nEach object is represented as a query, and a Transformer decoder is adapted to\niteratively optimize all the queries involving their relationship. In\nparticular, we introduce a semantic-geometry disentangled query (SGDQ) design\nthat enables the query features to attend separately to semantic information\nand geometric information relevant to the corresponding sub-tasks. A hybrid\nbipartite matching module is employed to well use the supervisions from all the\nsub-tasks during training. Qualitative and quantitative experimental results\ndemonstrate that our method achieves state-of-the-art performance on the\nchallenging ScanNet dataset. Code is available at\nhttps://github.com/SAITPublic/DOCTR.\n","authors":["Xiaoxuan Yu","Hao Wang","Weiming Li","Qiang Wang","Soonyong Cho","Younghun Sung"],"pdf_url":"https://arxiv.org/pdf/2403.16431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13964v3","updated":"2024-03-25T05:18:04Z","published":"2023-12-21T15:51:12Z","title":"PIA: Your Personalized Image Animator via Plug-and-Play Modules in\n  Text-to-Image Models","summary":"  Recent advancements in personalized text-to-image (T2I) models have\nrevolutionized content creation, empowering non-experts to generate stunning\nimages with unique styles. While promising, adding realistic motions into these\npersonalized images by text poses significant challenges in preserving distinct\nstyles, high-fidelity details, and achieving motion controllability by text. In\nthis paper, we present PIA, a Personalized Image Animator that excels in\naligning with condition images, achieving motion controllability by text, and\nthe compatibility with various personalized T2I models without specific tuning.\nTo achieve these goals, PIA builds upon a base T2I model with well-trained\ntemporal alignment layers, allowing for the seamless transformation of any\npersonalized T2I model into an image animation model. A key component of PIA is\nthe introduction of the condition module, which utilizes the condition frame\nand inter-frame affinity as input to transfer appearance information guided by\nthe affinity hint for individual frame synthesis in the latent space. This\ndesign mitigates the challenges of appearance-related image alignment within\nand allows for a stronger focus on aligning with motion-related guidance.\n","authors":["Yiming Zhang","Zhening Xing","Yanhong Zeng","Youqing Fang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2312.13964v3.pdf","comment":"Project page: https://pi-animator.github.io/"},{"id":"http://arxiv.org/abs/2403.16428v1","updated":"2024-03-25T05:12:21Z","published":"2024-03-25T05:12:21Z","title":"Benchmarks and Challenges in Pose Estimation for Egocentric Hand\n  Interactions with Objects","summary":"  We interact with the world with our hands and see it through our own\n(egocentric) perspective. A holistic 3D understanding of such interactions from\negocentric views is important for tasks in robotics, AR/VR, action recognition\nand motion generation. Accurately reconstructing such interactions in 3D is\nchallenging due to heavy occlusion, viewpoint bias, camera distortion, and\nmotion blur from the head movement. To this end, we designed the HANDS23\nchallenge based on the AssemblyHands and ARCTIC datasets with carefully\ndesigned training and testing splits. Based on the results of the top submitted\nmethods and more recent baselines on the leaderboards, we perform a thorough\nanalysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates\nthe effectiveness of addressing distortion specific to egocentric cameras,\nadopting high-capacity transformers to learn complex hand-object interactions,\nand fusing predictions from different views. Our study further reveals\nchallenging scenarios intractable with state-of-the-art methods, such as fast\nhand motion, object reconstruction from narrow egocentric views, and close\ncontact between two hands and objects. Our efforts will enrich the community's\nknowledge foundation and facilitate future hand studies on egocentric\nhand-object interactions.\n","authors":["Zicong Fan","Takehiko Ohkawa","Linlin Yang","Nie Lin","Zhishan Zhou","Shihao Zhou","Jiajun Liang","Zhong Gao","Xuanyang Zhang","Xue Zhang","Fei Li","Liu Zheng","Feng Lu","Karim Abou Zeid","Bastian Leibe","Jeongwan On","Seungryul Baek","Aditya Prakash","Saurabh Gupta","Kun He","Yoichi Sato","Otmar Hilliges","Hyung Jin Chang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2403.16428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16425v1","updated":"2024-03-25T05:10:34Z","published":"2024-03-25T05:10:34Z","title":"Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in\n  Event Cameras","summary":"  Event cameras are increasingly popular in robotics due to their beneficial\nfeatures, such as low latency, energy efficiency, and high dynamic range.\nNevertheless, their downstream task performance is greatly influenced by the\noptimization of bias parameters. These parameters, for instance, regulate the\nnecessary change in light intensity to trigger an event, which in turn depends\non factors such as the environment lighting and camera motion. This paper\nintroduces feedback control algorithms that automatically tune the bias\nparameters through two interacting methods: 1) An immediate, on-the-fly fast\nadaptation of the refractory period, which sets the minimum interval between\nconsecutive events, and 2) if the event rate exceeds the specified bounds even\nafter changing the refractory period repeatedly, the controller adapts the\npixel bandwidth and event thresholds, which stabilizes after a short period of\nnoise events across all pixels (slow adaptation). Our evaluation focuses on the\nvisual place recognition task, where incoming query images are compared to a\ngiven reference database. We conducted comprehensive evaluations of our\nalgorithms' adaptive feedback control in real-time. To do so, we collected the\nQCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366\nrepeated traversals of a Scout Mini robot navigating through a 100 meter long\nindoor lab setting (totaling over 35km distance traveled) in varying brightness\nconditions with ground truth location information. Our proposed feedback\ncontrollers result in superior performance when compared to the standard bias\nsettings and prior feedback control methods. Our findings also detail the\nimpact of bias adjustments on task performance and feature ablation studies on\nthe fast and slow adaptation mechanisms.\n","authors":["Gokul B. Nair","Michael Milford","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2403.16425v1.pdf","comment":"8 pages, 9 figures, paper under review"},{"id":"http://arxiv.org/abs/2312.03009v2","updated":"2024-03-25T05:04:04Z","published":"2023-12-04T19:01:19Z","title":"I-PHYRE: Interactive Physical Reasoning","summary":"  Current evaluation protocols predominantly assess physical reasoning in\nstationary scenes, creating a gap in evaluating agents' abilities to interact\nwith dynamic events. While contemporary methods allow agents to modify initial\nscene configurations and observe consequences, they lack the capability to\ninteract with events in real time. To address this, we introduce I-PHYRE, a\nframework that challenges agents to simultaneously exhibit intuitive physical\nreasoning, multi-step planning, and in-situ intervention. Here, intuitive\nphysical reasoning refers to a quick, approximate understanding of physics to\naddress complex problems; multi-step denotes the need for extensive sequence\nplanning in I-PHYRE, considering each intervention can significantly alter\nsubsequent choices; and in-situ implies the necessity for timely object\nmanipulation within a scene, where minor timing deviations can result in task\nfailure. We formulate four game splits to scrutinize agents' learning and\ngeneralization of essential principles of interactive physical reasoning,\nfostering learning through interaction with representative scenarios. Our\nexploration involves three planning strategies and examines several supervised\nand reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The\noutcomes highlight a notable gap between existing learning algorithms and human\nperformance, emphasizing the imperative for more research in enhancing agents\nwith interactive physical reasoning capabilities. The environment and baselines\nwill be made publicly available.\n","authors":["Shiqian Li","Kewen Wu","Chi Zhang","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.03009v2.pdf","comment":"21 pages, ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16422v1","updated":"2024-03-25T04:54:49Z","published":"2024-03-25T04:54:49Z","title":"Refining Text-to-Image Generation: Towards Accurate Training-Free\n  Glyph-Enhanced Image Generation","summary":"  Over the past few years, Text-to-Image (T2I) generation approaches based on\ndiffusion models have gained significant attention. However, vanilla diffusion\nmodels often suffer from spelling inaccuracies in the text displayed within the\ngenerated images. The capability to generate visual text is crucial, offering\nboth academic interest and a wide range of practical applications. To produce\naccurate visual text images, state-of-the-art techniques adopt a\nglyph-controlled image generation approach, consisting of a text layout\ngenerator followed by an image generator that is conditioned on the generated\ntext layout. Nevertheless, our study reveals that these models still face three\nprimary challenges, prompting us to develop a testbed to facilitate future\nresearch. We introduce a benchmark, LenCom-Eval, specifically designed for\ntesting models' capability in generating images with Lengthy and Complex visual\ntext. Subsequently, we introduce a training-free framework to enhance the\ntwo-stage generation approaches. We examine the effectiveness of our approach\non both LenCom-Eval and MARIO-Eval benchmarks and demonstrate notable\nimprovements across a range of evaluation metrics, including CLIPScore, OCR\nprecision, recall, F1 score, accuracy, and edit distance scores. For instance,\nour proposed framework improves the backbone model, TextDiffuser, by more than\n23\\% and 13.5\\% in terms of OCR word F1 on LenCom-Eval and MARIO-Eval,\nrespectively. Our work makes a unique contribution to the field by focusing on\ngenerating images with long and rare text sequences, a niche previously\nunexplored by existing literature\n","authors":["Sanyam Lakhanpal","Shivang Chopra","Vinija Jain","Aman Chadha","Man Luo"],"pdf_url":"https://arxiv.org/pdf/2403.16422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03173v3","updated":"2024-03-25T04:42:22Z","published":"2024-03-05T18:08:29Z","title":"Solving the bongard-logo problem by modeling a probabilistic model","summary":"  Abstract reasoning problems challenge the perceptual and cognitive abilities\nof AI algorithms, demanding deeper pattern discernment and inductive reasoning\nbeyond explicit image features. This study introduces PMoC, a tailored\nprobability model for the Bongard-Logo problem, achieving high reasoning\naccuracy by constructing independent probability models. Additionally, we\npresent Pose-Transformer, an enhanced Transformer-Encoder designed for complex\nabstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM.\nPose-Transformer incorporates positional information learning, inspired by\ncapsule networks' pose matrices, enhancing its focus on local positional\nrelationships in image data processing. When integrated with PMoC, it further\nimproves reasoning accuracy. Our approach effectively addresses reasoning\ndifficulties associated with abstract entities' positional changes,\noutperforming previous models on the OIG, D3$\\times$3 subsets of RAVEN, and PGM\ndatabases. This research contributes to advancing AI's capabilities in abstract\nreasoning and cognitive pattern recognition.\n","authors":["Ruizhuo Song","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.03173v3.pdf","comment":"14 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.03190v4","updated":"2024-03-25T04:40:39Z","published":"2024-03-05T18:29:17Z","title":"Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract\n  Reasoning process","summary":"  Abstract reasoning problems pose significant challenges to artificial\nintelligence algorithms, demanding cognitive capabilities beyond those required\nfor perception tasks. This study introduces the Triple-CFN approach to tackle\nthe Bongard-Logo problem, achieving notable reasoning accuracy by implicitly\nreorganizing the concept space of conflicting instances. Additionally, the\nTriple-CFN paradigm proves effective for the RPM problem with necessary\nmodifications, yielding competitive results. To further enhance performance on\nthe RPM issue, we develop the Meta Triple-CFN network, which explicitly\nstructures the problem space while maintaining interpretability on progressive\npatterns. The success of Meta Triple-CFN is attributed to its paradigm of\nmodeling the conceptual space, equivalent to normalizing reasoning information.\nBased on this ideology, we introduce the Re-space layer, enhancing the\nperformance of both Meta Triple-CFN and Triple-CFN. This paper aims to\ncontribute to advancements in machine intelligence by exploring innovative\nnetwork designs for addressing abstract reasoning problems, paving the way for\nfurther breakthroughs in this domain.\n","authors":["Ruizhuo Song","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.03190v4.pdf","comment":"14 pages, 14 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.03452v4","updated":"2024-03-25T04:38:42Z","published":"2024-03-06T04:36:43Z","title":"D4C glove-train: solving the RPM and Bongard-logo problem by\n  distributing and Circumscribing concepts","summary":"  This paper achieves noteworthy progress in the realm of abstract reasoning,\nparticularly in addressing Raven's Progressive Matrices (RPM) and Bongard-Logo\nchallenges. Initially, we introduce Lico-Net, a novel baseline model that\nresolves RPM problems with remarkable accuracy. Leveraging this foundation, we\nadvance with the D3C approach, which advocates representing the underlying\nconcepts in abstract reasoning problems through distributions. This perspective\nenhances the performance of both Lico-Net and a baseline model excelling in\nBongard-Logo tasks. To bolster the computational efficiency of D3C, we present\nthe D3C-cos variant, offering a streamlined yet precise solution. Furthermore,\nwe propose the D2C method, redefining conceptual boundaries within these\ndomains and bridging the divide between high-level abstractions and their\nlower-dimensional counterparts. Finally, we extend our methodology to D4C,\nemploying adversarial techniques to refine conceptual boundaries further and\ndemonstrate substantial improvements in both RPM and Bongard-Logo challenges.\nOverall, our contributions present a fresh outlook and practical advancements\nin the field of abstract reasoning.\n","authors":["Ruizhuo Song","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2403.03452v4.pdf","comment":"18 pages, 19 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.16412v1","updated":"2024-03-25T04:14:07Z","published":"2024-03-25T04:14:07Z","title":"Unsupervised Template-assisted Point Cloud Shape Correspondence Network","summary":"  Unsupervised point cloud shape correspondence aims to establish point-wise\ncorrespondences between source and target point clouds. Existing methods obtain\ncorrespondences directly by computing point-wise feature similarity between\npoint clouds. However, non-rigid objects possess strong deformability and\nunusual shapes, making it a longstanding challenge to directly establish\ncorrespondences between point clouds with unconventional shapes. To address\nthis challenge, we propose an unsupervised Template-Assisted point cloud shape\ncorrespondence Network, termed TANet, including a template generation module\nand a template assistance module. The proposed TANet enjoys several merits.\nFirstly, the template generation module establishes a set of learnable\ntemplates with explicit structures. Secondly, we introduce a template\nassistance module that extensively leverages the generated templates to\nestablish more accurate shape correspondences from multiple perspectives.\nExtensive experiments on four human and animal datasets demonstrate that TANet\nachieves favorable performance against state-of-the-art methods.\n","authors":["Jiacheng Deng","Jiahao Lu","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16412v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.16410v1","updated":"2024-03-25T04:05:23Z","published":"2024-03-25T04:05:23Z","title":"Spike-NeRF: Neural Radiance Field Based On Spike Camera","summary":"  As a neuromorphic sensor with high temporal resolution, spike cameras offer\nnotable advantages over traditional cameras in high-speed vision applications\nsuch as high-speed optical estimation, depth estimation, and object tracking.\nInspired by the success of the spike camera, we proposed Spike-NeRF, the first\nNeural Radiance Field derived from spike data, to achieve 3D reconstruction and\nnovel viewpoint synthesis of high-speed scenes. Instead of the multi-view\nimages at the same time of NeRF, the inputs of Spike-NeRF are continuous spike\nstreams captured by a moving spike camera in a very short time. To reconstruct\na correct and stable 3D scene from high-frequency but unstable spike data, we\ndevised spike masks along with a distinctive loss function. We evaluate our\nmethod qualitatively and numerically on several challenging synthetic scenes\ngenerated by blender with the spike camera simulator. Our results demonstrate\nthat Spike-NeRF produces more visually appealing results than the existing\nmethods and the baseline we proposed in high-speed scenes. Our code and data\nwill be released soon.\n","authors":["Yijia Guo","Yuanxi Bai","Liwen Hu","Mianzhi Liu","Ziyi Guo","Lei Ma","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2403.16410v1.pdf","comment":"This paper is accepted by ICME2024"},{"id":"http://arxiv.org/abs/2403.16407v1","updated":"2024-03-25T03:47:53Z","published":"2024-03-25T03:47:53Z","title":"A Survey on Long Video Generation: Challenges, Methods, and Prospects","summary":"  Video generation is a rapidly advancing research area, garnering significant\nattention due to its broad range of applications. One critical aspect of this\nfield is the generation of long-duration videos, which presents unique\nchallenges and opportunities. This paper presents the first survey of recent\nadvancements in long video generation and summarises them into two key\nparadigms: divide and conquer temporal autoregressive.\n  We delve into the common models employed in each paradigm, including aspects\nof network design and conditioning techniques. Furthermore, we offer a\ncomprehensive overview and classification of the datasets and evaluation\nmetrics which are crucial for advancing long video generation research.\nConcluding with a summary of existing studies, we also discuss the emerging\nchallenges and future directions in this dynamic field. We hope that this\nsurvey will serve as an essential reference for researchers and practitioners\nin the realm of long video generation.\n","authors":["Chengxuan Li","Di Huang","Zeyu Lu","Yang Xiao","Qingqi Pei","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2403.16407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16405v1","updated":"2024-03-25T03:44:36Z","published":"2024-03-25T03:44:36Z","title":"Ensemble Adversarial Defense via Integration of Multiple Dispersed Low\n  Curvature Models","summary":"  The integration of an ensemble of deep learning models has been extensively\nexplored to enhance defense against adversarial attacks. The diversity among\nsub-models increases the attack cost required to deceive the majority of the\nensemble, thereby improving the adversarial robustness. While existing\napproaches mainly center on increasing diversity in feature representations or\ndispersion of first-order gradients with respect to input, the limited\ncorrelation between these diversity metrics and adversarial robustness\nconstrains the performance of ensemble adversarial defense. In this work, we\naim to enhance ensemble diversity by reducing attack transferability. We\nidentify second-order gradients, which depict the loss curvature, as a key\nfactor in adversarial robustness. Computing the Hessian matrix involved in\nsecond-order gradients is computationally expensive. To address this, we\napproximate the Hessian-vector product using differential approximation. Given\nthat low curvature provides better robustness, our ensemble model was designed\nto consider the influence of curvature among different sub-models. We introduce\na novel regularizer to train multiple more-diverse low-curvature network\nmodels. Extensive experiments across various datasets demonstrate that our\nensemble model exhibits superior robustness against a range of attacks,\nunderscoring the effectiveness of our approach.\n","authors":["Kaikang Zhao","Xi Chen","Wei Huang","Liuxin Ding","Xianglong Kong","Fan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16405v1.pdf","comment":"Accepted to The 2024 International Joint Conference on Neural\n  Networks (IJCNN)"},{"id":"http://arxiv.org/abs/2304.06928v2","updated":"2024-03-25T03:40:19Z","published":"2023-04-14T05:25:52Z","title":"CiPR: An Efficient Framework with Cross-instance Positive Relations for\n  Generalized Category Discovery","summary":"  We tackle the issue of generalized category discovery (GCD). GCD considers\nthe open-world problem of automatically clustering a partially labelled\ndataset, in which the unlabelled data may contain instances from both novel\ncategories and labelled classes. In this paper, we address the GCD problem with\nan unknown category number for the unlabelled data. We propose a framework,\nnamed CiPR, to bootstrap the representation by exploiting Cross-instance\nPositive Relations in the partially labelled data for contrastive learning,\nwhich have been neglected in existing methods. To obtain reliable\ncross-instance relations to facilitate representation learning, we introduce a\nsemi-supervised hierarchical clustering algorithm, named selective neighbor\nclustering (SNC), which can produce a clustering hierarchy directly from the\nconnected components of a graph constructed from selective neighbors. We\nfurther present a method to estimate the unknown class number using SNC with a\njoint reference score that considers clustering indexes of both labelled and\nunlabelled data, and extend SNC to allow label assignment for the unlabelled\ninstances with a given class number. We thoroughly evaluate our framework on\npublic generic image recognition datasets and challenging fine-grained\ndatasets, and establish a new state-of-the-art. Code:\nhttps://github.com/haoosz/CiPR\n","authors":["Shaozhe Hao","Kai Han","Kwan-Yee K. Wong"],"pdf_url":"https://arxiv.org/pdf/2304.06928v2.pdf","comment":"Accepted to TMLR. Code: https://github.com/haoosz/CiPR"},{"id":"http://arxiv.org/abs/2311.13614v2","updated":"2024-03-25T03:39:45Z","published":"2023-11-22T04:52:58Z","title":"HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction\n  Data","summary":"  Multi-modal Large Language Models (MLLMs) tuned on machine-generated\ninstruction-following data have demonstrated remarkable performance in various\nmulti-modal understanding and generation tasks. However, the hallucinations\ninherent in machine-generated data, which could lead to hallucinatory outputs\nin MLLMs, remain under-explored. This work aims to investigate various\nhallucinations (i.e., object, relation, attribute hallucinations) and mitigate\nthose hallucinatory toxicities in large-scale machine-generated visual\ninstruction datasets. Drawing on the human ability to identify factual errors,\nwe present a novel hallucination detection and elimination framework,\nHalluciDoctor, based on the cross-checking paradigm. We use our framework to\nidentify and eliminate hallucinations in the training data automatically.\nInterestingly, HalluciDoctor also indicates that spurious correlations arising\nfrom long-tail object co-occurrences contribute to hallucinations. Based on\nthat, we execute counterfactual visual instruction expansion to balance data\ndistribution, thereby enhancing MLLMs' resistance to hallucinations.\nComprehensive experiments on hallucination evaluation benchmarks show that our\nmethod successfully mitigates 44.6% hallucinations relatively and maintains\ncompetitive performance compared to LLaVA. The data and code for this paper are\npublicly available. \\url{https://github.com/Yuqifan1117/HalluciDoctor}.\n","authors":["Qifan Yu","Juncheng Li","Longhui Wei","Liang Pang","Wentao Ye","Bosheng Qin","Siliang Tang","Qi Tian","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2311.13614v2.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16400v1","updated":"2024-03-25T03:30:37Z","published":"2024-03-25T03:30:37Z","title":"ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D\n  Pose Estimation","summary":"  In medical and industrial domains, providing guidance for assembly processes\nis critical to ensure efficiency and safety. Errors in assembly can lead to\nsignificant consequences such as extended surgery times, and prolonged\nmanufacturing or maintenance times in industry. Assembly scenarios can benefit\nfrom in-situ AR visualization to provide guidance, reduce assembly times and\nminimize errors. To enable in-situ visualization 6D pose estimation can be\nleveraged. Existing 6D pose estimation techniques primarily focus on individual\nobjects and static captures. However, assembly scenarios have various dynamics\nincluding occlusion during assembly and dynamics in the assembly objects\nappearance. Existing work, combining object detection/6D pose estimation and\nassembly state detection focuses either on pure deep learning-based approaches,\nor limit the assembly state detection to building blocks. To address the\nchallenges of 6D pose estimation in combination with assembly state detection,\nour approach ASDF builds upon the strengths of YOLOv8, a real-time capable\nobject detection framework. We extend this framework, refine the object pose\nand fuse pose knowledge with network-detected pose information. Utilizing our\nlate fusion in our Pose2State module results in refined 6D pose estimation and\nassembly state detection. By combining both pose and state information, our\nPose2State module predicts the final assembly state with precision. Our\nevaluation on our ASDF dataset shows that our Pose2State module leads to an\nimproved assembly state detection and that the improvement of the assembly\nstate further leads to a more robust 6D pose estimation. Moreover, on the GBOT\ndataset, we outperform the pure deep learning-based network, and even\noutperform the hybrid and pure tracking-based approaches.\n","authors":["Hannah Schieber","Shiyu Li","Niklas Corell","Philipp Beckerle","Julian Kreimeier","Daniel Roth"],"pdf_url":"https://arxiv.org/pdf/2403.16400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17460v3","updated":"2024-03-25T03:21:39Z","published":"2023-11-29T09:02:07Z","title":"W-HMR: Human Mesh Recovery in World Space with Weak-supervised Camera\n  Calibration and Orientation Correction","summary":"  For a long time, in reconstructing 3D human bodies from monocular images,\nmost methods opted to simplify the task by minimizing the influence of the\ncamera. Using a coarse focal length setting results in the reconstructed bodies\nnot aligning well with distorted images. Ignoring camera rotation leads to an\nunrealistic reconstructed body pose in world space. Consequently, the\napplication scenarios of existing methods are confined to controlled\nenvironments. When confronted with complex and diverse in-the-wild images, they\nstruggle to achieve accurate and reasonable reconstruction in world space. To\naddress the above issues, we propose W-HMR, which decouples global body\nrecovery into camera calibration, local body recovery, and global body\norientation correction. We design the first weak-supervised camera calibration\nmethod for body distortion, eliminating dependence on focal length labels and\nachieving finer mesh-image alignment. We propose a novel orientation correction\nmodule to allow the reconstructed human body to remain normal in world space.\nDecoupling body orientation and body pose enables our model to consider the\naccuracy in camera coordinate and the reasonableness in world coordinate\nsimultaneously, expanding the range of applications. As a result, W-HMR\nachieves high-quality reconstruction in dual coordinate systems, particularly\nin challenging scenes. Codes and demos have been released on the project page\nhttps://yw0208.github.io/w-hmr/.\n","authors":["Wei Yao","Hongwen Zhang","Yunlian Sun","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2311.17460v3.pdf","comment":"Project Page: https://yw0208.github.io/w-hmr/"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.16778v1","updated":"2024-03-25T13:54:44Z","published":"2024-03-25T13:54:44Z","title":"GloSIS: The Global Soil Information System Web Ontology","summary":"  Established in 2012 by members of the Food and Agriculture Organisation\n(FAO), the Global Soil Partnership (GSP) is a global network of stakeholders\npromoting sound land and soil management practices towards a sustainable world\nfood system. However, soil survey largely remains a local or regional activity,\nbound to heterogeneous methods and conventions. Recognising the relevance of\nglobal and trans-national policies towards sustainable land management\npractices, the GSP elected data harmonisation and exchange as one of its key\nlines of action. Building upon international standards and previous work\ntowards a global soil data ontology, an improved domain model was eventually\ndeveloped within the GSP [54], the basis for a Global Soil Information System\n(GloSIS). This work also identified the Semantic Web as a possible avenue to\noperationalise the domain model. This article presents the GloSIS web ontology,\nan implementation of the GloSIS domain model with the Web Ontology Language\n(OWL). Thoroughly employing a host of Semantic Web standards (SOSA, SKOS,\nGeoSPARQL, QUDT), GloSIS lays out not only a soil data ontology but also an\nextensive set of ready-to-use code-lists for soil description and\nphysio-chemical analysis. Various examples are provided on the provision and\nuse of GloSIS-compliant linked data, showcasing the contribution of this\nontology to the discovery, exploration, integration and access of soil data.\n","authors":["Raul Palma","Bogusz Janiak","Luís Moreira de Sousa","Kathi Schleidt","Tomáš Řezník","Fenny van Egmond","Johan Leenaars","Dimitrios Moshou","Abdul Mouazen","Peter Wilson","David Medyckyj-Scott","Alistair Ritchie","Yusuf Yigini","Ronald Vargas"],"pdf_url":"https://arxiv.org/pdf/2403.16778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01744v2","updated":"2024-03-25T13:16:43Z","published":"2024-03-04T05:41:29Z","title":"NoteLLM: A Retrievable Large Language Model for Note Recommendation","summary":"  People enjoy sharing \"notes\" including their experiences within online\ncommunities. Therefore, recommending notes aligned with user interests has\nbecome a crucial task. Existing online methods only input notes into BERT-based\nmodels to generate note embeddings for assessing similarity. However, they may\nunderutilize some important cues, e.g., hashtags or categories, which represent\nthe key concepts of notes. Indeed, learning to generate hashtags/categories can\npotentially enhance note embeddings, both of which compress key note\ninformation into limited content. Besides, Large Language Models (LLMs) have\nsignificantly outperformed BERT in understanding natural languages. It is\npromising to introduce LLMs into note recommendation. In this paper, we propose\na novel unified framework called NoteLLM, which leverages LLMs to address the\nitem-to-item (I2I) note recommendation. Specifically, we utilize Note\nCompression Prompt to compress a note into a single special token, and further\nlearn the potentially related notes' embeddings via a contrastive learning\napproach. Moreover, we use NoteLLM to summarize the note and generate the\nhashtag/category automatically through instruction tuning. Extensive\nvalidations on real scenarios demonstrate the effectiveness of our proposed\nmethod compared with the online baseline and show major improvements in the\nrecommendation system of Xiaohongshu.\n","authors":["Chao Zhang","Shiwei Wu","Haoxin Zhang","Tong Xu","Yan Gao","Yao Hu","Di Wu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.01744v2.pdf","comment":"Published as a WWW'24 full paper"},{"id":"http://arxiv.org/abs/2403.16702v1","updated":"2024-03-25T12:34:33Z","published":"2024-03-25T12:34:33Z","title":"ProCQA: A Large-scale Community-based Programming Question Answering\n  Dataset for Code Search","summary":"  Retrieval-based code question answering seeks to match user queries in\nnatural language to relevant code snippets. Previous approaches typically rely\non pretraining models using crafted bi-modal and uni-modal datasets to align\ntext and code representations. In this paper, we introduce ProCQA, a\nlarge-scale programming question answering dataset extracted from the\nStackOverflow community, offering naturally structured mixed-modal QA pairs. To\nvalidate its effectiveness, we propose a modality-agnostic contrastive\npre-training approach to improve the alignment of text and code representations\nof current code language models. Compared to previous models that primarily\nemploy bimodal and unimodal pairs extracted from CodeSearchNet for\npre-training, our model exhibits significant performance improvements across a\nwide range of code retrieval benchmarks.\n","authors":["Zehan Li","Jianfei Zhang","Chuantao Yin","Yuanxin Ouyang","Wenge Rong"],"pdf_url":"https://arxiv.org/pdf/2403.16702v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2311.16515v2","updated":"2024-03-25T12:01:59Z","published":"2023-11-25T14:24:49Z","title":"Word4Per: Zero-shot Composed Person Retrieval","summary":"  Searching for specific person has great social benefits and security value,\nand it often involves a combination of visual and textual information.\nConventional person retrieval methods, whether image-based or text-based,\nusually fall short in effectively harnessing both types of information, leading\nto the loss of accuracy. In this paper, a whole new task called Composed Person\nRetrieval (CPR) is proposed to jointly utilize both image and text information\nfor target person retrieval. However, the supervised CPR requires very costly\nmanual annotation dataset, while there are currently no available resources. To\nmitigate this issue, we firstly introduce the Zero-shot Composed Person\nRetrieval (ZS-CPR), which leverages existing domain-related data to resolve the\nCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we\npropose a two-stage learning framework, Word4Per, where a lightweight Textual\nInversion Network (TINet) and a text-based person retrieval model based on\nfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned\nwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed\nPerson Retrieval (ITCPR) dataset is built as the benchmark to assess the\nperformance of the proposed Word4Per framework. Extensive experiments under\nboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR\ntask, surpassing the comparative methods by over 10\\%. The code and ITCPR\ndataset will be publicly available at\nhttps://github.com/Delong-liu-bupt/Word4Per.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16630v1","updated":"2024-03-25T11:20:23Z","published":"2024-03-25T11:20:23Z","title":"A comparative analysis of embedding models for patent similarity","summary":"  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n","authors":["Grazia Sveva Ascione","Valerio Sterzi"],"pdf_url":"https://arxiv.org/pdf/2403.16630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06182v4","updated":"2024-03-25T09:43:30Z","published":"2023-04-12T22:46:52Z","title":"GNNUERS: Fairness Explanation in GNNs for Recommendation via\n  Counterfactual Reasoning","summary":"  Nowadays, research into personalization has been focusing on explainability\nand fairness. Several approaches proposed in recent works are able to explain\nindividual recommendations in a post-hoc manner or by explanation paths.\nHowever, explainability techniques applied to unfairness in recommendation have\nbeen limited to finding user/item features mostly related to biased\nrecommendations. In this paper, we devised a novel algorithm that leverages\ncounterfactuality methods to discover user unfairness explanations in the form\nof user-item interactions. In our counterfactual framework, interactions are\nrepresented as edges in a bipartite graph, with users and items as nodes. Our\nbipartite graph explainer perturbs the topological structure to find an altered\nversion that minimizes the disparity in utility between the protected and\nunprotected demographic groups. Experiments on four real-world graphs coming\nfrom various domains showed that our method can systematically explain user\nunfairness on three state-of-the-art GNN-based recommendation models. Moreover,\nan empirical evaluation of the perturbed network uncovered relevant patterns\nthat justify the nature of the unfairness discovered by the generated\nexplanations. The source code and the preprocessed data sets are available at\nhttps://github.com/jackmedda/RS-BGExplainer.\n","authors":["Giacomo Medda","Francesco Fabbri","Mirko Marras","Ludovico Boratto","Gianni Fenu"],"pdf_url":"https://arxiv.org/pdf/2304.06182v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.08929v2","updated":"2024-03-25T08:00:38Z","published":"2023-06-15T08:02:07Z","title":"On the resilience of Collaborative Learning-based Recommender Systems\n  Against Community Detection Attack","summary":"  Collaborative-learning-based recommender systems emerged following the\nsuccess of collaborative learning techniques such as Federated Learning (FL)\nand Gossip Learning (GL). In these systems, users participate in the training\nof a recommender system while maintaining their history of consumed items on\ntheir devices. While these solutions seemed appealing for preserving the\nprivacy of the participants at first glance, recent studies have revealed that\ncollaborative learning can be vulnerable to various privacy attacks. In this\npaper, we study the resilience of collaborative learning-based recommender\nsystems against a novel privacy attack called Community Detection Attack (CDA).\nThis attack enables an adversary to identify community members based on a\nchosen set of items (eg., identifying users interested in specific\npoints-of-interest). Through experiments on three real recommendation datasets\nusing two state-of-the-art recommendation models, we evaluate the sensitivity\nof an FL-based recommender system as well as two flavors of Gossip\nLearning-based recommender systems to CDA. The results show that across all\nmodels and datasets, the FL setting is more vulnerable to CDA compared to\nGossip settings. Furthermore, we assess two off-the-shelf mitigation\nstrategies, namely differential privacy (DP) and a \\emph{Share less} policy,\nwhich consists of sharing a subset of less sensitive model parameters. The\nfindings indicate a more favorable privacy-utility trade-off for the\n\\emph{Share less} strategy, particularly in FedRecs.\n","authors":["Yacine Belal","Sonia Ben Mokhtar","Mohamed Maouche","Anthony Simonet-Boulogne"],"pdf_url":"https://arxiv.org/pdf/2306.08929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16504v1","updated":"2024-03-25T07:38:40Z","published":"2024-03-25T07:38:40Z","title":"LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent\n  Classification","summary":"  Following the significant achievements of large language models (LLMs),\nresearchers have employed in-context learning for text classification tasks.\nHowever, these studies focused on monolingual, single-turn classification\ntasks. In this paper, we introduce LARA (Linguistic-Adaptive\nRetrieval-Augmented Language Models), designed to enhance accuracy in\nmulti-turn classification tasks across six languages, accommodating numerous\nintents in chatbot interactions. Multi-turn intent classification is notably\nchallenging due to the complexity and evolving nature of conversational\ncontexts. LARA tackles these issues by combining a fine-tuned smaller model\nwith a retrieval-augmented mechanism, integrated within the architecture of\nLLMs. This integration allows LARA to dynamically utilize past dialogues and\nrelevant intents, thereby improving the understanding of the context.\nFurthermore, our adaptive retrieval techniques bolster the cross-lingual\ncapabilities of LLMs without extensive retraining and fine-tune. Comprehensive\nexperiments demonstrate that LARA achieves state-of-the-art performance on\nmulti-turn intent classification tasks, enhancing the average accuracy by 3.67%\ncompared to existing methods.\n","authors":["Liu Junhua","Tan Yong Keat","Fu Bin"],"pdf_url":"https://arxiv.org/pdf/2403.16504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16435v1","updated":"2024-03-25T05:31:22Z","published":"2024-03-25T05:31:22Z","title":"InstUPR : Instruction-based Unsupervised Passage Reranking with Large\n  Language Models","summary":"  This paper introduces InstUPR, an unsupervised passage reranking method based\non large language models (LLMs). Different from existing approaches that rely\non extensive training with query-document pairs or retrieval-specific\ninstructions, our method leverages the instruction-following capabilities of\ninstruction-tuned LLMs for passage reranking without any additional\nfine-tuning. To achieve this, we introduce a soft score aggregation technique\nand employ pairwise reranking for unsupervised passage reranking. Experiments\non the BEIR benchmark demonstrate that InstUPR outperforms unsupervised\nbaselines as well as an instruction-tuned reranker, highlighting its\neffectiveness and superiority. Source code to reproduce all experiments is\nopen-sourced at https://github.com/MiuLab/InstUPR\n","authors":["Chao-Wei Huang","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16435v1.pdf","comment":"Preprint. This manuscript was originally written and submitted in\n  June 2023"},{"id":"http://arxiv.org/abs/2403.16424v1","updated":"2024-03-25T05:04:52Z","published":"2024-03-25T05:04:52Z","title":"An Experiment with the Use of ChatGPT for LCSH Subject Assignment on\n  Electronic Theses and Dissertations","summary":"  This study delves into the potential use of Large Language Models (LLMs) for\ngenerating Library of Congress Subject Headings (LCSH). The authors employed\nChatGPT to generate subject headings for electronic theses and dissertations\n(ETDs) based on their titles and summaries. The results revealed that although\nsome generated subject headings were valid, there were issues regarding\nspecificity and exhaustiveness. The study showcases that LLMs can serve as a\nstrategic response to the backlog of items awaiting cataloging in academic\nlibraries, while also offering a cost-effective approach for promptly\ngenerating LCSH. Nonetheless, human catalogers remain essential for verifying\nand enhancing the validity, exhaustiveness, and specificity of LCSH generated\nby LLMs.\n","authors":["Eric H. C. Chow","TJ Kao","Xiaoli Li"],"pdf_url":"https://arxiv.org/pdf/2403.16424v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2403.16378v1","updated":"2024-03-25T02:52:42Z","published":"2024-03-25T02:52:42Z","title":"Play to Your Strengths: Collaborative Intelligence of Conventional\n  Recommender Models and Large Language Models","summary":"  The rise of large language models (LLMs) has opened new opportunities in\nRecommender Systems (RSs) by enhancing user behavior modeling and content\nunderstanding. However, current approaches that integrate LLMs into RSs solely\nutilize either LLM or conventional recommender model (CRM) to generate final\nrecommendations, without considering which data segments LLM or CRM excel in.\nTo fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books\ndatasets, and compare the performance of a representative CRM (DCNv2) and an\nLLM (LLaMA2-7B) on various groups of data samples. Our findings reveal that\nLLMs excel in data segments where CRMs exhibit lower confidence and precision,\nwhile samples where CRM excels are relatively challenging for LLM, requiring\nsubstantial training data and a long training time for comparable performance.\nThis suggests potential synergies in the combination between LLM and CRM.\nMotivated by these insights, we propose Collaborative Recommendation with\nconventional Recommender and Large Language Model (dubbed \\textit{CoReLLa}). In\nthis framework, we first jointly train LLM and CRM and address the issue of\ndecision boundary shifts through alignment loss. Then, the resource-efficient\nCRM, with a shorter inference time, handles simple and moderate samples, while\nLLM processes the small subset of challenging samples for CRM. Our experimental\nresults demonstrate that CoReLLa outperforms state-of-the-art CRM and LLM\nmethods significantly, underscoring its effectiveness in recommendation tasks.\n","authors":["Yunjia Xi","Weiwen Liu","Jianghao Lin","Chuhan Wu","Bo Chen","Ruiming Tang","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2403.16378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16371v1","updated":"2024-03-25T02:31:57Z","published":"2024-03-25T02:31:57Z","title":"Uncovering Selective State Space Model's Capabilities in Lifelong\n  Sequential Recommendation","summary":"  Sequential Recommenders have been widely applied in various online services,\naiming to model users' dynamic interests from their sequential interactions.\nWith users increasingly engaging with online platforms, vast amounts of\nlifelong user behavioral sequences have been generated. However, existing\nsequential recommender models often struggle to handle such lifelong sequences.\nThe primary challenges stem from computational complexity and the ability to\ncapture long-range dependencies within the sequence. Recently, a state space\nmodel featuring a selective mechanism (i.e., Mamba) has emerged. In this work,\nwe investigate the performance of Mamba for lifelong sequential recommendation\n(i.e., length>=2k). More specifically, we leverage the Mamba block to model\nlifelong user sequences selectively. We conduct extensive experiments to\nevaluate the performance of representative sequential recommendation models in\nthe setting of lifelong sequences. Experiments on two real-world datasets\ndemonstrate the superiority of Mamba. We found that RecMamba achieves\nperformance comparable to the representative model while significantly reducing\ntraining duration by approximately 70% and memory costs by 80%. Codes and data\nare available at \\url{https://github.com/nancheng58/RecMamba}.\n","authors":["Jiyuan Yang","Yuanzi Li","Jingyu Zhao","Hanbing Wang","Muyang Ma","Jun Ma","Zhaochun Ren","Mengqi Zhang","Xin Xin","Zhumin Chen","Pengjie Ren"],"pdf_url":"https://arxiv.org/pdf/2403.16371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16345v1","updated":"2024-03-25T00:43:44Z","published":"2024-03-25T00:43:44Z","title":"Enhanced Facet Generation with LLM Editing","summary":"  In information retrieval, facet identification of a user query is an\nimportant task. If a search service can recognize the facets of a user's query,\nit has the potential to offer users a much broader range of search results.\nPrevious studies can enhance facet prediction by leveraging retrieved documents\nand related queries obtained through a search engine. However, there are\nchallenges in extending it to other applications when a search engine operates\nas part of the model. First, search engines are constantly updated. Therefore,\nadditional information may change during training and test, which may reduce\nperformance. The second challenge is that public search engines cannot search\nfor internal documents. Therefore, a separate search system needs to be built\nto incorporate documents from private domains within the company. We propose\ntwo strategies that focus on a framework that can predict facets by taking only\nqueries as input without a search engine. The first strategy is multi-task\nlearning to predict SERP. By leveraging SERP as a target instead of a source,\nthe proposed model deeply understands queries without relying on external\nmodules. The second strategy is to enhance the facets by combining Large\nLanguage Model (LLM) and the small model. Overall performance improves when\nsmall model and LLM are combined rather than facet generation individually.\n","authors":["Joosung Lee","Jinhong Kim"],"pdf_url":"https://arxiv.org/pdf/2403.16345v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.09738v4","updated":"2024-03-25T23:53:01Z","published":"2024-03-13T18:16:21Z","title":"Evaluating Large Language Models as Generative User Simulators for\n  Conversational Recommendation","summary":"  Synthetic users are cost-effective proxies for real users in the evaluation\nof conversational recommender systems. Large language models show promise in\nsimulating human-like behavior, raising the question of their ability to\nrepresent a diverse population of users. We introduce a new protocol to measure\nthe degree to which language models can accurately emulate human behavior in\nconversational recommendation. This protocol is comprised of five tasks, each\ndesigned to evaluate a key property that a synthetic user should exhibit:\nchoosing which items to talk about, expressing binary preferences, expressing\nopen-ended preferences, requesting recommendations, and giving feedback.\nThrough evaluation of baseline simulators, we demonstrate these tasks\neffectively reveal deviations of language models from human behavior, and offer\ninsights on how to reduce the deviations with model selection and prompting\nstrategies.\n","authors":["Se-eun Yoon","Zhankui He","Jessica Maria Echterhoff","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2403.09738v4.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17264v1","updated":"2024-03-25T23:15:13Z","published":"2024-03-25T23:15:13Z","title":"EXPLORA: A teacher-apprentice methodology for eliciting natural\n  child-computer interactions","summary":"  Investigating child-computer interactions within their contexts is vital for\ndesigning technology that caters to children's needs. However, determining what\naspects of context are relevant for designing child-centric technology remains\na challenge. We introduce EXPLORA, a multimodal, multistage online methodology\ncomprising three pivotal stages: (1) building a teacher-apprentice\nrelationship,(2) learning from child-teachers, and (3) assessing and\nreinforcing researcher-apprentice learning. Central to EXPLORA is the\ncollection of attitudinal data through pre-observation interviews, offering\nresearchers a deeper understanding of children's characteristics and contexts.\nThis informs subsequent online observations, allowing researchers to focus on\nfrequent interactions. Furthermore, researchers can validate preliminary\nassumptions with children. A means-ends analysis framework aids in the\nsystematic analysis of data, shedding light on context, agency and\nhomework-information searching processes children employ in their activities.\nTo illustrate EXPLORA's capabilities, we present nine single case studies\ninvestigating Brazilian child-caregiver dyads' (children ages 9-11) use of\ntechnology in homework information-searching.\n","authors":["Vanessa Figueiredo","Catherine Ann Cameron"],"pdf_url":"https://arxiv.org/pdf/2403.17264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17210v1","updated":"2024-03-25T21:37:31Z","published":"2024-03-25T21:37:31Z","title":"CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug\n  Interactions","summary":"  Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process\nof drug development. DDIs occur when one drug's properties are affected by the\ninclusion of other drugs. Detecting favorable DDIs has the potential to pave\nthe way for creating and advancing innovative medications applicable in\npractical settings. However, existing DDI prediction models continue to face\nchallenges related to generalization in extreme cases, robust feature\nextraction, and real-life application possibilities. We aim to address these\nchallenges by leveraging the effectiveness of context-aware deep graph learning\nby introducing a novel framework named CADGL. Based on a customized variational\ngraph autoencoder (VGAE), we capture critical structural and physio-chemical\ninformation using two context preprocessors for feature extraction from two\ndifferent perspectives: local neighborhood and molecular context, in a\nheterogeneous graphical structure. Our customized VGAE consists of a graph\nencoder, a latent information encoder, and an MLP decoder. CADGL surpasses\nother state-of-the-art DDI prediction models, excelling in predicting\nclinically valuable novel DDIs, supported by rigorous case studies.\n","authors":["Azmine Toushik Wasi","Taki Hasan Rafi","Raima Islam","Serbetar Karlo","Dong-Kyu Chae"],"pdf_url":"https://arxiv.org/pdf/2403.17210v1.pdf","comment":"8 Pages, 4 Figures; In review in IEEE/ACM Transactions on\n  Computational Biology and Bioinformatics"},{"id":"http://arxiv.org/abs/2403.17209v1","updated":"2024-03-25T21:37:30Z","published":"2024-03-25T21:37:30Z","title":"Generation of Asset Administration Shell with Large Language Model\n  Agents: Interoperability in Digital Twins with Semantic Node","summary":"  This research introduces a novel approach for assisting the creation of Asset\nAdministration Shell (AAS) instances for digital twin modeling within the\ncontext of Industry 4.0, aiming to enhance interoperability in smart\nmanufacturing and reduce manual effort. We construct a \"semantic node\" data\nstructure to capture the semantic essence of textual data. Then, a system\npowered by large language models is designed and implemented to process\n\"semantic node\" and generate AAS instance models from textual technical data.\nOur evaluation demonstrates a 62-79% effective generation rate, indicating a\nsubstantial proportion of manual creation effort can be converted into easier\nvalidation effort, thereby reducing the time and cost in creating AAS instance\nmodels. In our evaluation, a comparative analysis of different LLMs and an\nin-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms\nprovide insights into the effectiveness of LLM systems for interpreting\ntechnical concepts. Our findings emphasize LLMs' capability in automating AAS\ninstance creation, enhancing semantic interoperability, and contributing to the\nbroader field of semantic interoperability for digital twins in industrial\napplications. The prototype implementation and evaluation results are released\non our GitHub Repository with the link: https://github.com/YuchenXia/AASbyLLM\n","authors":["Yuchen Xia","Zhewen Xiao","Nasser Jazdi","Michael Weyrich"],"pdf_url":"https://arxiv.org/pdf/2403.17209v1.pdf","comment":"Pre-print, submitted to IEEE ACCESS, under peer-review"},{"id":"http://arxiv.org/abs/2403.17089v1","updated":"2024-03-25T18:25:10Z","published":"2024-03-25T18:25:10Z","title":"GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI\n  collaboration","summary":"  The advent of ChatGPT and similar large language models (LLMs) has\nrevolutionized the human-AI interaction and information-seeking process.\nLeveraging LLMs as an alternative to search engines, users can now access\nsummarized information tailored to their queries, significantly reducing the\ncognitive load associated with navigating vast information resources. This\nshift underscores the potential of LLMs in redefining information access\nparadigms. Drawing on the foundation of task-focused information retrieval and\nLLMs' task planning ability, this research extends the scope of LLM\ncapabilities beyond routine task automation to support users in navigating\nlong-term and significant life tasks. It introduces the GOLF framework\n(Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability\nto assist in significant life decisions through goal orientation and long-term\nplanning. The methodology encompasses a comprehensive simulation study to test\nthe framework's efficacy, followed by model and human evaluations to develop a\ndataset benchmark for long-term life tasks, and experiments across different\nmodels and settings. By shifting the focus from short-term tasks to the broader\nspectrum of long-term life goals, this research underscores the transformative\npotential of LLMs in enhancing human decision-making processes and task\nmanagement, marking a significant step forward in the evolution of human-AI\ncollaboration.\n","authors":["Ben Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16948v1","updated":"2024-03-25T17:10:39Z","published":"2024-03-25T17:10:39Z","title":"Reinforcement Learning-based Recommender Systems with Large Language\n  Models for State Reward and Action Modeling","summary":"  Reinforcement Learning (RL)-based recommender systems have demonstrated\npromising performance in meeting user expectations by learning to make accurate\nnext-item recommendations from historical user-item interactions. However,\nexisting offline RL-based sequential recommendation methods face the challenge\nof obtaining effective user feedback from the environment. Effectively modeling\nthe user state and shaping an appropriate reward for recommendation remains a\nchallenge. In this paper, we leverage language understanding capabilities and\nadapt large language models (LLMs) as an environment (LE) to enhance RL-based\nrecommenders. The LE is learned from a subset of user-item interaction data,\nthus reducing the need for large training data, and can synthesise user\nfeedback for offline data by: (i) acting as a state model that produces high\nquality states that enrich the user representation, and (ii) functioning as a\nreward model to accurately capture nuanced user preferences on actions.\nMoreover, the LE allows to generate positive actions that augment the limited\noffline training data. We propose a LE Augmentation (LEA) method to further\nimprove recommendation performance by optimising jointly the supervised\ncomponent and the RL policy, using the augmented actions and historical user\nsignals. We use LEA, the state and reward models in conjunction with\nstate-of-the-art RL recommenders and report experimental results on two\npublicly available datasets.\n","authors":["Jie Wang","Alexandros Karatzoglou","Ioannis Arapakis","Joemon M. Jose"],"pdf_url":"https://arxiv.org/pdf/2403.16948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16656v1","updated":"2024-03-25T11:47:53Z","published":"2024-03-25T11:47:53Z","title":"Graph Augmentation for Recommendation","summary":"  Graph augmentation with contrastive learning has gained significant attention\nin the field of recommendation systems due to its ability to learn expressive\nuser representations, even when labeled data is limited. However, directly\napplying existing GCL models to real-world recommendation environments poses\nchallenges. There are two primary issues to address. Firstly, the lack of\nconsideration for data noise in contrastive learning can result in noisy\nself-supervised signals, leading to degraded performance. Secondly, many\nexisting GCL approaches rely on graph neural network (GNN) architectures, which\ncan suffer from over-smoothing problems due to non-adaptive message passing. To\naddress these challenges, we propose a principled framework called GraphAug.\nThis framework introduces a robust data augmentor that generates denoised\nself-supervised signals, enhancing recommender systems. The GraphAug framework\nincorporates a graph information bottleneck (GIB)-regularized augmentation\nparadigm, which automatically distills informative self-supervision information\nand adaptively adjusts contrastive view generation. Through rigorous\nexperimentation on real-world datasets, we thoroughly assessed the performance\nof our novel GraphAug model. The outcomes consistently unveil its superiority\nover existing baseline methods. The source code for our model is publicly\navailable at: https://github.com/HKUDS/GraphAug.\n","authors":["Qianru Zhang","Lianghao Xia","Xuheng Cai","Siuming Yiu","Chao Huang","Christian S. Jensen"],"pdf_url":"https://arxiv.org/pdf/2403.16656v1.pdf","comment":"13 pages and accepted by ICDE 2024"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.16809v1","updated":"2024-03-25T14:32:28Z","published":"2024-03-25T14:32:28Z","title":"An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems","summary":"  The increasing prevalence of Cyber-Physical Systems and the Internet of\nThings (CPS-IoT) applications and Foundation Models are enabling new\napplications that leverage real-time control of the environment. For example,\nreal-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems\ncan reduce its usage when not needed for the comfort of human occupants, hence\nreducing energy consumption. Collecting real-time feedback on human preferences\nin such human-in-the-loop (HITL) systems, however, is difficult in practice. We\npropose the use of large language models (LLMs) to deal with the challenges of\ndynamic environments and difficult-to-obtain data in CPS optimization. In this\npaper, we present a case study that employs LLM agents to mimic the behaviors\nand thermal preferences of various population groups (e.g. young families, the\nelderly) in a shopping mall. The aggregated thermal preferences are integrated\ninto an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which\nemploys the LLM as a dynamic simulation of the physical environment to learn\nhow to balance between energy savings and occupant comfort. Our results show\nthat LLMs are capable of simulating complex population movements within large\nopen spaces. Besides, AitL-RL demonstrates superior performance compared to the\npopular existing policy of set point control, suggesting that adaptive and\npersonalized decision-making is critical for efficient optimization in CPS-IoT\napplications. Through this case study, we demonstrate the potential of\nintegrating advanced Foundation Models like LLMs into CPS-IoT to enhance system\nadaptability and efficiency. The project's code can be found on our GitHub\nrepository.\n","authors":["Hanqing Yang","Marie Siew","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16809v1.pdf","comment":"Accepted at International Workshop on Foundation Models for\n  Cyber-Physical Systems & Internet of Things (FMSys) 2024, Co-located at\n  CPS-IoT Week 2024"},{"id":"http://arxiv.org/abs/2403.16798v1","updated":"2024-03-25T14:17:38Z","published":"2024-03-25T14:17:38Z","title":"Cluster-Based Normalization Layer for Neural Networks","summary":"  Deep learning faces significant challenges during the training of neural\nnetworks, including internal covariate shift, label shift, vanishing/exploding\ngradients, overfitting, and computational complexity. While conventional\nnormalization methods, such as Batch Normalization, aim to tackle some of these\nissues, they often depend on assumptions that constrain their adaptability.\nMixture Normalization faces computational hurdles in its pursuit of handling\nmultiple Gaussian distributions. This paper introduces Cluster-Based\nNormalization (CB-Norm) in two variants - Supervised Cluster-Based\nNormalization (SCB-Norm) and Unsupervised Cluster-Based Normalization\n(UCB-Norm) - proposing a groundbreaking one-step normalization approach.\nCB-Norm leverages a Gaussian mixture model to specifically address challenges\nrelated to gradient stability and learning acceleration. For SCB-Norm, a\nsupervised variant, the novel mechanism involves introducing predefined data\npartitioning, termed clusters, to normalize activations based on the assigned\ncluster. This cluster-driven approach creates a space that conforms to a\nGaussian mixture model. On the other hand, UCB-Norm, an unsupervised\ncounterpart, dynamically clusters neuron activations during training, adapting\nto task-specific challenges without relying on predefined data partitions\n(clusters). This dual approach ensures flexibility in addressing diverse\nlearning scenarios. CB-Norm innovatively uses a one-step normalization\napproach, where parameters of each mixture component (cluster in activation\nspace) serve as weights for deep neural networks. This adaptive clustering\nprocess tackles both clustering and resolution of deep neural network tasks\nconcurrently during training, signifying a notable advancement in the field.\n","authors":["Bilal Faye","Hanane Azzag","Mustapha Lebbah"],"pdf_url":"https://arxiv.org/pdf/2403.16798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16790v1","updated":"2024-03-25T14:05:52Z","published":"2024-03-25T14:05:52Z","title":"Iso-Diffusion: Improving Diffusion Probabilistic Models Using the\n  Isotropy of the Additive Gaussian Noise","summary":"  Denoising Diffusion Probabilistic Models (DDPMs) have accomplished much in\nthe realm of generative AI. Despite their high performance, there is room for\nimprovement, especially in terms of sample fidelity by utilizing statistical\nproperties that impose structural integrity, such as isotropy. Minimizing the\nmean squared error between the additive and predicted noise alone does not\nimpose constraints on the predicted noise to be isotropic. Thus, we were\nmotivated to utilize the isotropy of the additive noise as a constraint on the\nobjective function to enhance the fidelity of DDPMs. Our approach is simple and\ncan be applied to any DDPM variant. We validate our approach by presenting\nexperiments conducted on four synthetic 2D datasets as well as on unconditional\nimage generation. As demonstrated by the results, the incorporation of this\nconstraint improves the fidelity metrics, Precision and Density for the 2D\ndatasets as well as for the unconditional image generation.\n","authors":["Dilum Fernando","Dhananjaya jayasundara","Roshan Godaliyadda","Chaminda Bandara","Parakrama Ekanayake","Vijitha Herath"],"pdf_url":"https://arxiv.org/pdf/2403.16790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16782v1","updated":"2024-03-25T13:57:45Z","published":"2024-03-25T13:57:45Z","title":"The Anatomy of Adversarial Attacks: Concept-based XAI Dissection","summary":"  Adversarial attacks (AAs) pose a significant threat to the reliability and\nrobustness of deep neural networks. While the impact of these attacks on model\npredictions has been extensively studied, their effect on the learned\nrepresentations and concepts within these models remains largely unexplored. In\nthis work, we perform an in-depth analysis of the influence of AAs on the\nconcepts learned by convolutional neural networks (CNNs) using eXplainable\nartificial intelligence (XAI) techniques. Through an extensive set of\nexperiments across various network architectures and targeted AA techniques, we\nunveil several key findings. First, AAs induce substantial alterations in the\nconcept composition within the feature space, introducing new concepts or\nmodifying existing ones. Second, the adversarial perturbation itself can be\nlinearly decomposed into a set of latent vector components, with a subset of\nthese being responsible for the attack's success. Notably, we discover that\nthese components are target-specific, i.e., are similar for a given target\nclass throughout different AA techniques and starting classes. Our findings\nprovide valuable insights into the nature of AAs and their impact on learned\nrepresentations, paving the way for the development of more robust and\ninterpretable deep learning models, as well as effective defenses against\nadversarial threats.\n","authors":["Georgii Mikriukov","Gesina Schwalbe","Franz Motzkus","Korinna Bade"],"pdf_url":"https://arxiv.org/pdf/2403.16782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16776v1","updated":"2024-03-25T13:52:48Z","published":"2024-03-25T13:52:48Z","title":"Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases","summary":"  Anatomical atlases are widely used for population analysis. Conditional\natlases target a particular sub-population defined via certain conditions (e.g.\ndemographics or pathologies) and allow for the investigation of fine-grained\nanatomical differences - such as morphological changes correlated with age.\nExisting approaches use either registration-based methods that are unable to\nhandle large anatomical variations or generative models, which can suffer from\ntraining instabilities and hallucinations. To overcome these limitations, we\nuse latent diffusion models to generate deformation fields, which transform a\ngeneral population atlas into one representing a specific sub-population. By\ngenerating a deformation field and registering the conditional atlas to a\nneighbourhood of images, we ensure structural plausibility and avoid\nhallucinations, which can occur during direct image synthesis. We compare our\nmethod to several state-of-the-art atlas generation methods in experiments\nusing 5000 brain as well as whole-body MR images from UK Biobank. Our method\ngenerates highly realistic atlases with smooth transformations and high\nanatomical fidelity, outperforming the baselines.\n","authors":["Sophie Starck","Vasiliki Sideri-Lampretsa","Bernhard Kainz","Martin Menten","Tamara Mueller","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2403.16776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16771v1","updated":"2024-03-25T13:50:11Z","published":"2024-03-25T13:50:11Z","title":"Synthetic Data Generation and Joint Learning for Robust Code-Mixed\n  Translation","summary":"  The widespread online communication in a modern multilingual world has\nprovided opportunities to blend more than one language (aka code-mixed\nlanguage) in a single utterance. This has resulted a formidable challenge for\nthe computational models due to the scarcity of annotated data and presence of\nnoise. A potential solution to mitigate the data scarcity problem in\nlow-resource setup is to leverage existing data in resource-rich language\nthrough translation. In this paper, we tackle the problem of code-mixed\n(Hinglish and Bengalish) to English machine translation. First, we\nsynthetically develop HINMIX, a parallel corpus of Hinglish to English, with\n~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation\nbased joint-training model that learns to handle noise in the real-world\ncode-mixed text by parameter sharing across clean and noisy words. Further, we\nshow the adaptability of RCMT in a zero-shot setup for Bengalish to English\ntranslation. Our evaluation and comprehensive analyses qualitatively and\nquantitatively demonstrate the superiority of RCMT over state-of-the-art\ncode-mixed and robust translation methods.\n","authors":[" Kartik","Sanjana Soni","Anoop Kunchukuttan","Tanmoy Chakraborty","Md Shad Akhtar"],"pdf_url":"https://arxiv.org/pdf/2403.16771v1.pdf","comment":"9 pages, 2 figures, to be published in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16768v1","updated":"2024-03-25T13:46:09Z","published":"2024-03-25T13:46:09Z","title":"DeepKnowledge: Generalisation-Driven Deep Learning Testing","summary":"  Despite their unprecedented success, DNNs are notoriously fragile to small\nshifts in data distribution, demanding effective testing techniques that can\nassess their dependability. Despite recent advances in DNN testing, there is a\nlack of systematic testing approaches that assess the DNN's capability to\ngeneralise and operate comparably beyond data in their training distribution.\nWe address this gap with DeepKnowledge, a systematic testing methodology for\nDNN-based systems founded on the theory of knowledge generalisation, which aims\nto enhance DNN robustness and reduce the residual risk of 'black box' models.\nConforming to this theory, DeepKnowledge posits that core computational DNN\nunits, termed Transfer Knowledge neurons, can generalise under domain shift.\nDeepKnowledge provides an objective confidence measurement on testing\nactivities of DNN given data distribution shifts and uses this information to\ninstrument a generalisation-informed test adequacy criterion to check the\ntransfer knowledge capacity of a test set. Our empirical evaluation of several\nDNNs, across multiple datasets and state-of-the-art adversarial generation\ntechniques demonstrates the usefulness and effectiveness of DeepKnowledge and\nits ability to support the engineering of more dependable DNNs. We report\nimprovements of up to 10 percentage points over state-of-the-art coverage\ncriteria for detecting adversarial attacks on several benchmarks, including\nMNIST, SVHN, and CIFAR.\n","authors":["Sondess Missaoui","Simos Gerasimou","Nikolaos Matragkas"],"pdf_url":"https://arxiv.org/pdf/2403.16768v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2403.11722v2","updated":"2024-03-25T13:34:40Z","published":"2024-03-18T12:22:11Z","title":"Time Series Compression using Quaternion Valued Neural Networks and\n  Quaternion Backpropagation","summary":"  We propose a novel quaternionic time-series compression methodology where we\ndivide a long time-series into segments of data, extract the min, max, mean and\nstandard deviation of these chunks as representative features and encapsulate\nthem in a quaternion, yielding a quaternion valued time-series. This\ntime-series is processed using quaternion valued neural network layers, where\nwe aim to preserve the relation between these features through the usage of the\nHamilton product. To train this quaternion neural network, we derive quaternion\nbackpropagation employing the GHR calculus, which is required for a valid\nproduct and chain rule in quaternion space. Furthermore, we investigate the\nconnection between the derived update rules and automatic differentiation. We\napply our proposed compression method on the Tennessee Eastman Dataset, where\nwe perform fault classification using the compressed data in two settings: a\nfully supervised one and in a semi supervised, contrastive learning setting.\nBoth times, we were able to outperform real valued counterparts as well as two\nbaseline models: one with the uncompressed time-series as the input and the\nother with a regular downsampling using the mean. Further, we could improve the\nclassification benchmark set by SimCLR-TS from 81.43% to 83.90%.\n","authors":["Johannes Pöppelbaum","Andreas Schwung"],"pdf_url":"https://arxiv.org/pdf/2403.11722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04599v2","updated":"2024-03-25T13:30:37Z","published":"2024-02-07T05:47:31Z","title":"Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via\n  Temporal-Viewpoint Alignment","summary":"  Video sequences exhibit significant nuisance variations (undesired effects)\nof speed of actions, temporal locations, and subjects' poses, leading to\ntemporal-viewpoint misalignment when comparing two sets of frames or evaluating\nthe similarity of two sequences. Thus, we propose Joint tEmporal and cAmera\nviewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D\nskeleton sequences whose camera and subjects' poses can be easily manipulated\nin 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where\nmatching well temporal blocks (temporal chunks that make up a sequence) of\nsupport-query sequence pairs (by factoring out nuisance variations) is\nessential due to limited samples of novel classes. Given a query sequence, we\ncreate its several views by simulating several camera locations. For a support\nsequence, we match it with view-simulated query sequences, as in the popular\nDynamic Time Warping (DTW). Specifically, each support temporal block can be\nmatched to the query temporal block with the same or adjacent (next) temporal\nindex, and adjacent camera views to achieve joint local temporal-viewpoint\nwarping. JEANIE selects the smallest distance among matching paths with\ndifferent temporal-viewpoint warping patterns, an advantage over DTW which only\nperforms temporal alignment. We also propose an unsupervised FSAR akin to\nclustering of sequences with JEANIE as a distance measure. JEANIE achieves\nstate-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D\nMultiview Activity II on supervised and unsupervised FSAR, and their\nmeta-learning inspired fusion.\n","authors":["Lei Wang","Jun Liu","Liang Zheng","Tom Gedeon","Piotr Koniusz"],"pdf_url":"https://arxiv.org/pdf/2402.04599v2.pdf","comment":"Accepted by the International Journal of Computer Vision (IJCV). An\n  extension of our ACCV'22 paper [arXiv:arXiv:2210.16820] which was\n  distinguished by the Sang Uk Lee Best Student Paper Award"},{"id":"http://arxiv.org/abs/2402.02423v2","updated":"2024-03-25T13:20:46Z","published":"2024-02-04T09:40:22Z","title":"Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement\n  Learning with Diverse Human Feedback","summary":"  Reinforcement Learning with Human Feedback (RLHF) has received significant\nattention for performing tasks without the need for costly manual reward design\nby aligning human preferences. It is crucial to consider diverse human feedback\ntypes and various learning methods in different environments. However,\nquantifying progress in RLHF with diverse feedback is challenging due to the\nlack of standardized annotation platforms and widely used unified benchmarks.\nTo bridge this gap, we introduce Uni-RLHF, a comprehensive system\nimplementation tailored for RLHF. It aims to provide a complete workflow from\nreal human feedback, fostering progress in the development of practical\nproblems. Uni-RLHF contains three packages: 1) a universal multi-feedback\nannotation platform, 2) large-scale crowdsourced feedback datasets, and 3)\nmodular offline RLHF baseline implementations. Uni-RLHF develops a\nuser-friendly annotation interface tailored to various feedback types,\ncompatible with a wide range of mainstream RL environments. We then establish a\nsystematic pipeline of crowdsourced annotations, resulting in large-scale\nannotated datasets comprising more than 15 million steps across 30+ popular\ntasks. Through extensive experiments, the results in the collected datasets\ndemonstrate competitive performance compared to those from well-designed manual\nrewards. We evaluate various design choices and offer insights into their\nstrengths and potential areas of improvement. We wish to build valuable\nopen-source platforms, datasets, and baselines to facilitate the development of\nmore robust and reliable RLHF solutions based on realistic human feedback. The\nwebsite is available at https://uni-rlhf.github.io/.\n","authors":["Yifu Yuan","Jianye Hao","Yi Ma","Zibin Dong","Hebin Liang","Jinyi Liu","Zhixin Feng","Kai Zhao","Yan Zheng"],"pdf_url":"https://arxiv.org/pdf/2402.02423v2.pdf","comment":"Published as a conference paper at ICLR 2024. The website is\n  available at https://uni-rlhf.github.io/"},{"id":"http://arxiv.org/abs/2402.07310v2","updated":"2024-03-25T12:58:45Z","published":"2024-02-11T21:16:42Z","title":"BioNeRF: Biologically Plausible Neural Radiance Fields for View\n  Synthesis","summary":"  This paper presents BioNeRF, a biologically plausible architecture that\nmodels scenes in a 3D representation and synthesizes new views through radiance\nfields. Since NeRF relies on the network weights to store the scene's\n3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism\nthat fuses inputs from multiple sources into a memory-like structure, improving\nthe storing capacity and extracting more intrinsic and correlated information.\nBioNeRF also mimics a behavior observed in pyramidal cells concerning\ncontextual information, in which the memory is provided as the context and\ncombined with the inputs of two subsequent neural models, one responsible for\nproducing the volumetric densities and the other the colors used to render the\nscene. Experimental results show that BioNeRF outperforms state-of-the-art\nresults concerning a quality measure that encodes human perception in two\ndatasets: real-world images and synthetic data.\n","authors":["Leandro A. Passos","Douglas Rodrigues","Danilo Jodas","Kelton A. P. Costa","Ahsan Adeel","João Paulo Papa"],"pdf_url":"https://arxiv.org/pdf/2402.07310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17919v3","updated":"2024-03-25T12:52:42Z","published":"2024-01-31T15:33:37Z","title":"LOCOST: State-Space Models for Long Document Abstractive Summarization","summary":"  State-space models are a low-complexity alternative to transformers for\nencoding long sequences and capturing long-term dependencies. We propose\nLOCOST: an encoder-decoder architecture based on state-space models for\nconditional text generation with long context inputs. With a computational\ncomplexity of $O(L \\log L)$, this architecture can handle significantly longer\nsequences than state-of-the-art models that are based on sparse attention\npatterns. We evaluate our model on a series of long document abstractive\nsummarization tasks. The model reaches a performance level that is 93-96%\ncomparable to the top-performing sparse transformers of the same size while\nsaving up to 50% memory during training and up to 87% during inference.\nAdditionally, LOCOST effectively handles input texts exceeding 600K tokens at\ninference time, setting new state-of-the-art results on full-book summarization\nand opening new perspectives for long input processing.\n","authors":["Florian Le Bronnec","Song Duong","Mathieu Ravaut","Alexandre Allauzen","Nancy F. Chen","Vincent Guigue","Alberto Lumbreras","Laure Soulier","Patrick Gallinari"],"pdf_url":"https://arxiv.org/pdf/2401.17919v3.pdf","comment":"9 pages, 5 figures, 7 tables, EACL 2024 conference"},{"id":"http://arxiv.org/abs/2403.16707v1","updated":"2024-03-25T12:44:52Z","published":"2024-03-25T12:44:52Z","title":"One-Shot Domain Incremental Learning","summary":"  Domain incremental learning (DIL) has been discussed in previous studies on\ndeep neural network models for classification. In DIL, we assume that samples\non new domains are observed over time. The models must classify inputs on all\ndomains. In practice, however, we may encounter a situation where we need to\nperform DIL under the constraint that the samples on the new domain are\nobserved only infrequently. Therefore, in this study, we consider the extreme\ncase where we have only one sample from the new domain, which we call one-shot\nDIL. We first empirically show that existing DIL methods do not work well in\none-shot DIL. We have analyzed the reason for this failure through various\ninvestigations. According to our analysis, we clarify that the difficulty of\none-shot DIL is caused by the statistics in the batch normalization layers.\nTherefore, we propose a technique regarding these statistics and demonstrate\nthe effectiveness of our technique through experiments on open datasets.\n","authors":["Yasushi Esaki","Satoshi Koide","Takuro Kutsuna"],"pdf_url":"https://arxiv.org/pdf/2403.16707v1.pdf","comment":"accepted at IEEE International Joint Conference on Neural Networks\n  (IJCNN) 2024"},{"id":"http://arxiv.org/abs/2403.16695v1","updated":"2024-03-25T12:26:32Z","published":"2024-03-25T12:26:32Z","title":"Assessing the Performance of Deep Learning for Automated Gleason Grading\n  in Prostate Cancer","summary":"  Prostate cancer is a dominant health concern calling for advanced diagnostic\ntools. Utilizing digital pathology and artificial intelligence, this study\nexplores the potential of 11 deep neural network architectures for automated\nGleason grading in prostate carcinoma focusing on comparing traditional and\nrecent architectures. A standardized image classification pipeline, based on\nthe AUCMEDI framework, facilitated robust evaluation using an in-house dataset\nconsisting of 34,264 annotated tissue tiles. The results indicated varying\nsensitivity across architectures, with ConvNeXt demonstrating the strongest\nperformance. Notably, newer architectures achieved superior performance, even\nthough with challenges in differentiating closely related Gleason grades. The\nConvNeXt model was capable of learning a balance between complexity and\ngeneralizability. Overall, this study lays the groundwork for enhanced Gleason\ngrading systems, potentially improving diagnostic efficiency for prostate\ncancer.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Daniel Hieber","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Frank Kramer","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16689v1","updated":"2024-03-25T12:23:39Z","published":"2024-03-25T12:23:39Z","title":"Synapse: Learning Preferential Concepts from Visual Demonstrations","summary":"  This paper addresses the problem of preference learning, which aims to learn\nuser-specific preferences (e.g., \"good parking spot\", \"convenient drop-off\nlocation\") from visual input. Despite its similarity to learning factual\nconcepts (e.g., \"red cube\"), preference learning is a fundamentally harder\nproblem due to its subjective nature and the paucity of person-specific\ntraining data. We address this problem using a new framework called Synapse,\nwhich is a neuro-symbolic approach designed to efficiently learn preferential\nconcepts from limited demonstrations. Synapse represents preferences as\nneuro-symbolic programs in a domain-specific language (DSL) that operates over\nimages, and leverages a novel combination of visual parsing, large language\nmodels, and program synthesis to learn programs representing individual\npreferences. We evaluate Synapse through extensive experimentation including a\nuser case study focusing on mobility-related concepts in mobile robotics and\nautonomous driving. Our evaluation demonstrates that Synapse significantly\noutperforms existing baselines as well as its own ablations. The code and other\ndetails can be found on the project website https://amrl.cs.utexas.edu/synapse .\n","authors":["Sadanand Modak","Noah Patton","Isil Dillig","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2403.16689v1.pdf","comment":"23 pages, 7 figures; Preprint"},{"id":"http://arxiv.org/abs/2403.16681v1","updated":"2024-03-25T12:15:55Z","published":"2024-03-25T12:15:55Z","title":"A note on generalization bounds for losses with finite moments","summary":"  This paper studies the truncation method from Alquier [1] to derive\nhigh-probability PAC-Bayes bounds for unbounded losses with heavy tails.\nAssuming that the $p$-th moment is bounded, the resulting bounds interpolate\nbetween a slow rate $1 / \\sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p\n\\to \\infty$ and the loss is essentially bounded. Moreover, the paper derives a\nhigh-probability PAC-Bayes bound for losses with a bounded variance. This bound\nhas an exponentially better dependence on the confidence parameter and the\ndependency measure than previous bounds in the literature. Finally, the paper\nextends all results to guarantees in expectation and single-draw PAC-Bayes. In\norder to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded\nlosses from [2] in these settings.\n","authors":["Borja Rodríguez-Gálvez","Omar Rivasplata","Ragnar Thobaben","Mikael Skoglund"],"pdf_url":"https://arxiv.org/pdf/2403.16681v1.pdf","comment":"9 pages: 5 of main text, 1 of references, and 3 of appendices"},{"id":"http://arxiv.org/abs/2403.16680v1","updated":"2024-03-25T12:15:47Z","published":"2024-03-25T12:15:47Z","title":"Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics","summary":"  Learning physical simulations has been an essential and central aspect of\nmany recent research efforts in machine learning, particularly for\nNavier-Stokes-based fluid mechanics. Classic numerical solvers have\ntraditionally been computationally expensive and challenging to use in inverse\nproblems, whereas Neural solvers aim to address both concerns through machine\nlearning. We propose a general formulation for continuous convolutions using\nseparable basis functions as a superset of existing methods and evaluate a\nlarge set of basis functions in the context of (a) a compressible 1D SPH\nsimulation, (b) a weakly compressible 2D SPH simulation, and (c) an\nincompressible 2D SPH Simulation. We demonstrate that even and odd symmetries\nincluded in the basis functions are key aspects of stability and accuracy. Our\nbroad evaluation shows that Fourier-based continuous convolutions outperform\nall other architectures regarding accuracy and generalization. Finally, using\nthese Fourier-based networks, we show that prior inductive biases, such as\nwindow functions, are no longer necessary. An implementation of our approach,\nas well as complete datasets and solver implementations, is available at\nhttps://github.com/tum-pbs/SFBC.\n","authors":["Rene Winchenbach","Nils Thuerey"],"pdf_url":"https://arxiv.org/pdf/2403.16680v1.pdf","comment":"Published at International Conference on Learning Representation\n  (ICLR) 2024, 54 pages, 39 figures"},{"id":"http://arxiv.org/abs/2403.16678v1","updated":"2024-03-25T12:15:42Z","published":"2024-03-25T12:15:42Z","title":"DeepGleason: a System for Automated Gleason Grading of Prostate Cancer\n  using Deep Neural Networks","summary":"  Advances in digital pathology and artificial intelligence (AI) offer\npromising opportunities for clinical decision support and enhancing diagnostic\nworkflows. Previous studies already demonstrated AI's potential for automated\nGleason grading, but lack state-of-the-art methodology and model reusability.\nTo address this issue, we propose DeepGleason: an open-source deep neural\nnetwork based image classification system for automated Gleason grading using\nwhole-slide histopathology images from prostate tissue sections. Implemented\nwith the standardized AUCMEDI framework, our tool employs a tile-wise\nclassification approach utilizing fine-tuned image preprocessing techniques in\ncombination with a ConvNeXt architecture which was compared to various\nstate-of-the-art architectures. The neural network model was trained and\nvalidated on an in-house dataset of 34,264 annotated tiles from 369 prostate\ncarcinoma slides. We demonstrated that DeepGleason is capable of highly\naccurate and reliable Gleason grading with a macro-averaged F1-score of 0.806,\nAUC of 0.991, and Accuracy of 0.974. The internal architecture comparison\nrevealed that the ConvNeXt model was superior performance-wise on our dataset\nto established and other modern architectures like transformers. Furthermore,\nwe were able to outperform the current state-of-the-art in tile-wise\nfine-classification with a sensitivity and specificity of 0.94 and 0.98 for\nbenign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs\nGleason 4 & 5 classification, respectively. Our tool contributes to the wider\nadoption of AI-based Gleason grading within the research community and paves\nthe way for broader clinical application of deep learning models in digital\npathology. DeepGleason is open-source and publicly available for research\napplication in the following Git repository:\nhttps://github.com/frankkramer-lab/DeepGleason.\n","authors":["Dominik Müller","Philip Meyer","Lukas Rentschler","Robin Manz","Jonas Bäcker","Samantha Cramer","Christoph Wengenmayr","Bruno Märkl","Ralf Huss","Iñaki Soto-Rey","Johannes Raffler"],"pdf_url":"https://arxiv.org/pdf/2403.16678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16677v1","updated":"2024-03-25T12:14:48Z","published":"2024-03-25T12:14:48Z","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression","summary":"  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on perceptual quality measures at lower bitrates. We\nextensively evaluate transfer cost reduction by including the peculiarity of\nintermittently available network connections in low earth orbit. Lastly, we\ntest the feasibility of our system for standardized nanosatellite form factors.\nWe demonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n","authors":["Alireza Furutanpey","Qiyang Zhang","Philipp Raith","Tobias Pfandzelter","Shangguang Wang","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2403.16677v1.pdf","comment":"18 pages, double column, 19 figures, 7 tables, Initial Submission to\n  IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2403.16674v1","updated":"2024-03-25T12:13:20Z","published":"2024-03-25T12:13:20Z","title":"Understanding the Functional Roles of Modelling Components in Spiking\n  Neural Networks","summary":"  Spiking neural networks (SNNs), inspired by the neural circuits of the brain,\nare promising in achieving high computational efficiency with biological\nfidelity. Nevertheless, it is quite difficult to optimize SNNs because the\nfunctional roles of their modelling components remain unclear. By designing and\nevaluating several variants of the classic model, we systematically investigate\nthe functional roles of key modelling components, leakage, reset, and\nrecurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive\nexperiments, we demonstrate how these components influence the accuracy,\ngeneralization, and robustness of SNNs. Specifically, we find that the leakage\nplays a crucial role in balancing memory retention and robustness, the reset\nmechanism is essential for uninterrupted temporal processing and computational\nefficiency, and the recurrence enriches the capability to model complex\ndynamics at a cost of robustness degradation. With these interesting\nobservations, we provide optimization suggestions for enhancing the performance\nof SNNs in different scenarios. This work deepens the understanding of how SNNs\nwork, which offers valuable guidance for the development of more effective and\nrobust neuromorphic models.\n","authors":["Huifeng Yin","Hanle Zheng","Jiayi Mao","Siyuan Ding","Xing Liu","Mingkun Xu","Yifan Hu","Jing Pei","Lei Deng"],"pdf_url":"https://arxiv.org/pdf/2403.16674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02930v2","updated":"2024-03-25T12:07:13Z","published":"2024-03-05T12:48:29Z","title":"A Second Look on BASS -- Boosting Abstractive Summarization with Unified\n  Semantic Graphs -- A Replication Study","summary":"  We present a detailed replication study of the BASS framework, an abstractive\nsummarization system based on the notion of Unified Semantic Graphs. Our\ninvestigation includes challenges in replicating key components and an ablation\nstudy to systematically isolate error sources rooted in replicating novel\ncomponents. Our findings reveal discrepancies in performance compared to the\noriginal work. We highlight the significance of paying careful attention even\nto reasonably omitted details for replicating advanced frameworks like BASS,\nand emphasize key practices for writing replicable papers.\n","authors":["Osman Alperen Koraş","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2403.02930v2.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Advances in Information Retrieval, 46th European Conference on\n  Information Retrieval, ECIR 2024. 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.14587v2","updated":"2024-03-25T12:00:19Z","published":"2024-03-21T17:42:45Z","title":"An Analysis of Linear Time Series Forecasting Models","summary":"  Despite their simplicity, linear models perform well at time series\nforecasting, even when pitted against deeper and more expensive models. A\nnumber of variations to the linear model have been proposed, often including\nsome form of feature normalisation that improves model generalisation. In this\npaper we analyse the sets of functions expressible using these linear model\narchitectures. In so doing we show that several popular variants of linear\nmodels for time series forecasting are equivalent and functionally\nindistinguishable from standard, unconstrained linear regression. We\ncharacterise the model classes for each linear variant. We demonstrate that\neach model can be reinterpreted as unconstrained linear regression over a\nsuitably augmented feature set, and therefore admit closed-form solutions when\nusing a mean-squared loss function. We provide experimental evidence that the\nmodels under inspection learn nearly identical solutions, and finally\ndemonstrate that the simpler closed form solutions are superior forecasters\nacross 72% of test settings.\n","authors":["William Toner","Luke Darlow"],"pdf_url":"https://arxiv.org/pdf/2403.14587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12581v2","updated":"2024-03-25T11:58:04Z","published":"2023-08-24T05:49:58Z","title":"A Huber Loss Minimization Approach to Byzantine Robust Federated\n  Learning","summary":"  Federated learning systems are susceptible to adversarial attacks. To combat\nthis, we introduce a novel aggregator based on Huber loss minimization, and\nprovide a comprehensive theoretical analysis. Under independent and identically\ndistributed (i.i.d) assumption, our approach has several advantages compared to\nexisting methods. Firstly, it has optimal dependence on $\\epsilon$, which\nstands for the ratio of attacked clients. Secondly, our approach does not need\nprecise knowledge of $\\epsilon$. Thirdly, it allows different clients to have\nunequal data sizes. We then broaden our analysis to include non-i.i.d data,\nsuch that clients have slightly different distributions.\n","authors":["Puning Zhao","Fei Yu","Zhiguo Wan"],"pdf_url":"https://arxiv.org/pdf/2308.12581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16656v1","updated":"2024-03-25T11:47:53Z","published":"2024-03-25T11:47:53Z","title":"Graph Augmentation for Recommendation","summary":"  Graph augmentation with contrastive learning has gained significant attention\nin the field of recommendation systems due to its ability to learn expressive\nuser representations, even when labeled data is limited. However, directly\napplying existing GCL models to real-world recommendation environments poses\nchallenges. There are two primary issues to address. Firstly, the lack of\nconsideration for data noise in contrastive learning can result in noisy\nself-supervised signals, leading to degraded performance. Secondly, many\nexisting GCL approaches rely on graph neural network (GNN) architectures, which\ncan suffer from over-smoothing problems due to non-adaptive message passing. To\naddress these challenges, we propose a principled framework called GraphAug.\nThis framework introduces a robust data augmentor that generates denoised\nself-supervised signals, enhancing recommender systems. The GraphAug framework\nincorporates a graph information bottleneck (GIB)-regularized augmentation\nparadigm, which automatically distills informative self-supervision information\nand adaptively adjusts contrastive view generation. Through rigorous\nexperimentation on real-world datasets, we thoroughly assessed the performance\nof our novel GraphAug model. The outcomes consistently unveil its superiority\nover existing baseline methods. The source code for our model is publicly\navailable at: https://github.com/HKUDS/GraphAug.\n","authors":["Qianru Zhang","Lianghao Xia","Xuheng Cai","Siuming Yiu","Chao Huang","Christian S. Jensen"],"pdf_url":"https://arxiv.org/pdf/2403.16656v1.pdf","comment":"13 pages and accepted by ICDE 2024"},{"id":"http://arxiv.org/abs/2403.16654v1","updated":"2024-03-25T11:42:01Z","published":"2024-03-25T11:42:01Z","title":"A Novel Loss Function-based Support Vector Machine for Binary\n  Classification","summary":"  The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss\nSVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the\ndegree of penalty for the correctly classified samples within the margin. This\noversight affects the generalization ability of the SVM classifier to some\nextent. To address this limitation, from the perspective of confidence margin,\nwe propose a novel Slide loss function ($\\ell_s$) to construct the support\nvector machine classifier($\\ell_s$-SVM). By introducing the concept of proximal\nstationary point, and utilizing the property of Lipschitz continuity, we derive\nthe first-order optimality conditions for $\\ell_s$-SVM. Based on this, we\ndefine the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM. To\nefficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method\nof multipliers with the working set ($\\ell_s$-ADMM), and provide the\nconvergence analysis. The numerical experiments on real world datasets confirm\nthe robustness and effectiveness of the proposed method.\n","authors":["Yan Li","Liping Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11922v3","updated":"2024-03-25T11:39:57Z","published":"2024-02-19T08:11:26Z","title":"Spatio-Temporal Few-Shot Learning via Diffusive Neural Network\n  Generation","summary":"  Spatio-temporal modeling is foundational for smart city applications, yet it\nis often hindered by data scarcity in many cities and regions. To bridge this\ngap, we propose a novel generative pre-training framework, GPD, for\nspatio-temporal few-shot learning with urban knowledge transfer. Unlike\nconventional approaches that heavily rely on common feature extraction or\nintricate few-shot learning designs, our solution takes a novel approach by\nperforming generative pre-training on a collection of neural network parameters\noptimized with data from source cities. We recast spatio-temporal few-shot\nlearning as pre-training a generative diffusion model, which generates tailored\nneural networks guided by prompts, allowing for adaptability to diverse data\ndistributions and city-specific characteristics. GPD employs a\nTransformer-based denoising diffusion model, which is model-agnostic to\nintegrate with powerful spatio-temporal neural networks. By addressing\nchallenges arising from data gaps and the complexity of generalizing knowledge\nacross cities, our framework consistently outperforms state-of-the-art\nbaselines on multiple real-world datasets for tasks such as traffic speed\nprediction and crowd flow prediction. The implementation of our approach is\navailable: https://github.com/tsinghua-fib-lab/GPD.\n","authors":["Yuan Yuan","Chenyang Shao","Jingtao Ding","Depeng Jin","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2402.11922v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16644v1","updated":"2024-03-25T11:29:32Z","published":"2024-03-25T11:29:32Z","title":"Bridging the Sim-to-Real Gap with Bayesian Inference","summary":"  We present SIM-FSVGD for learning robot dynamics from data. As opposed to\ntraditional methods, SIM-FSVGD leverages low-fidelity physical priors, e.g., in\nthe form of simulators, to regularize the training of neural network models.\nWhile learning accurate dynamics already in the low data regime, SIM-FSVGD\nscales and excels also when more data is available. We empirically show that\nlearning with implicit physical priors results in accurate mean model\nestimation as well as precise uncertainty quantification. We demonstrate the\neffectiveness of SIM-FSVGD in bridging the sim-to-real gap on a\nhigh-performance RC racecar system. Using model-based RL, we demonstrate a\nhighly dynamic parking maneuver with drifting, using less than half the data\ncompared to the state of the art.\n","authors":["Jonas Rothfuss","Bhavya Sukhija","Lenart Treven","Florian Dörfler","Stelian Coros","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2403.16644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16640v1","updated":"2024-03-25T11:28:52Z","published":"2024-03-25T11:28:52Z","title":"Multi-Scale Texture Loss for CT denoising with GANs","summary":"  Generative Adversarial Networks (GANs) have proved as a powerful framework\nfor denoising applications in medical imaging. However, GAN-based denoising\nalgorithms still suffer from limitations in capturing complex relationships\nwithin the images. In this regard, the loss function plays a crucial role in\nguiding the image generation process, encompassing how much a synthetic image\ndiffers from a real image. To grasp highly complex and non-linear textural\nrelationships in the training process, this work presents a loss function that\nleverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence\nMatrix (GLCM). Although the recent advances in deep learning have demonstrated\nsuperior performance in classification and detection tasks, we hypothesize that\nits information content can be valuable when integrated into GANs' training. To\nthis end, we propose a differentiable implementation of the GLCM suited for\ngradient-based optimization. Our approach also introduces a self-attention\nlayer that dynamically aggregates the multi-scale texture information extracted\nfrom the images. We validate our approach by carrying out extensive experiments\nin the context of low-dose CT denoising, a challenging application that aims to\nenhance the quality of noisy CT scans. We utilize three publicly available\ndatasets, including one simulated and two real datasets. The results are\npromising as compared to other well-established loss functions, being also\nconsistent across three different GAN architectures. The code is available at:\nhttps://github.com/FrancescoDiFeola/DenoTextureLoss\n","authors":["Francesco Di Feola","Lorenzo Tronchin","Valerio Guarrasi","Paolo Soda"],"pdf_url":"https://arxiv.org/pdf/2403.16640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16630v1","updated":"2024-03-25T11:20:23Z","published":"2024-03-25T11:20:23Z","title":"A comparative analysis of embedding models for patent similarity","summary":"  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n","authors":["Grazia Sveva Ascione","Valerio Sterzi"],"pdf_url":"https://arxiv.org/pdf/2403.16630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03472v2","updated":"2024-03-25T10:58:22Z","published":"2023-08-07T11:02:44Z","title":"Improving the forecast accuracy of wind power by leveraging multiple\n  hierarchical structure","summary":"  Renewable energy generation is of utmost importance for global\ndecarbonization. Forecasting renewable energies, particularly wind energy, is\nchallenging due to the inherent uncertainty in wind energy generation, which\ndepends on weather conditions. Recent advances in hierarchical forecasting\nthrough reconciliation have demonstrated a significant increase in the quality\nof wind energy forecasts for short-term periods. We leverage the\ncross-sectional and temporal hierarchical structure of turbines in wind farms\nand build cross-temporal hierarchies to further investigate how integrated\ncross-sectional and temporal dimensions can add value to forecast accuracy in\nwind farms. We found that cross-temporal reconciliation was superior to\nindividual cross-sectional reconciliation at multiple temporal aggregations.\nAdditionally, machine learning based forecasts that were cross-temporally\nreconciled demonstrated high accuracy at coarser temporal granularities, which\nmay encourage adoption for short-term wind forecasts. Empirically, we provide\ninsights for decision-makers on the best methods for forecasting high-frequency\nwind data across different forecasting horizons and levels.\n","authors":["Lucas English","Mahdi Abolghasemi"],"pdf_url":"https://arxiv.org/pdf/2308.03472v2.pdf","comment":"41 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.16612v1","updated":"2024-03-25T10:42:48Z","published":"2024-03-25T10:42:48Z","title":"Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting","summary":"  Seasonal forecasting is a crucial task when it comes to detecting the extreme\nheat and colds that occur due to climate change. Confidence in the predictions\nshould be reliable since a small increase in the temperatures in a year has a\nbig impact on the world. Calibration of the neural networks provides a way to\nensure our confidence in the predictions. However, calibrating regression\nmodels is an under-researched topic, especially in forecasters. We calibrate a\nUNet++ based architecture, which was shown to outperform physics-based models\nin temperature anomalies. We show that with a slight trade-off between\nprediction error and calibration error, it is possible to get more reliable and\nsharper forecasts. We believe that calibration should be an important part of\nsafety-critical machine learning applications such as weather forecasters.\n","authors":["Busra Asan","Abdullah Akgul","Alper Unal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2403.16612v1.pdf","comment":"Accepted as a workshop paper at \"ICLR 2024 Tackling Climate Change\n  with Machine Learning\""},{"id":"http://arxiv.org/abs/2403.16610v1","updated":"2024-03-25T10:40:04Z","published":"2024-03-25T10:40:04Z","title":"Distributed collaborative anomalous sound detection by embedding sharing","summary":"  To develop a machine sound monitoring system, a method for detecting\nanomalous sound is proposed. In this paper, we explore a method for multiple\nclients to collaboratively learn an anomalous sound detection model while\nkeeping their raw data private from each other. In the context of industrial\nmachine anomalous sound detection, each client possesses data from different\nmachines or different operational states, making it challenging to learn\nthrough federated learning or split learning. In our proposed method, each\nclient calculates embeddings using a common pre-trained model developed for\nsound data classification, and these calculated embeddings are aggregated on\nthe server to perform anomalous sound detection through outlier exposure.\nExperiments showed that our proposed method improves the AUC of anomalous sound\ndetection by an average of 6.8%.\n","authors":["Kota Dohi","Yohei Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2403.16610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16607v1","updated":"2024-03-25T10:38:17Z","published":"2024-03-25T10:38:17Z","title":"Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction\n  and Defect-Focus","summary":"  Addressing the challenge of data scarcity in industrial domains, transfer\nlearning emerges as a pivotal paradigm. This work introduces Style Filter, a\ntailored methodology for industrial contexts. By selectively filtering source\ndomain data before knowledge transfer, Style Filter reduces the quantity of\ndata while maintaining or even enhancing the performance of transfer learning\nstrategy. Offering label-free operation, minimal reliance on prior knowledge,\nindependence from specific models, and re-utilization, Style Filter is\nevaluated on authentic industrial datasets, highlighting its effectiveness when\nemployed before conventional transfer strategies in the deep learning domain.\nThe results underscore the effectiveness of Style Filter in real-world\nindustrial applications.\n","authors":["Chen Li","Ruijie Ma","Xiang Qian","Xiaohao Wang","Xinghui Li"],"pdf_url":"https://arxiv.org/pdf/2403.16607v1.pdf","comment":"17 pages, 11 figures,4 tables"},{"id":"http://arxiv.org/abs/2403.16594v1","updated":"2024-03-25T10:13:52Z","published":"2024-03-25T10:13:52Z","title":"EDUE: Expert Disagreement-Guided One-Pass Uncertainty Estimation for\n  Medical Image Segmentation","summary":"  Deploying deep learning (DL) models in medical applications relies on\npredictive performance and other critical factors, such as conveying\ntrustworthy predictive uncertainty. Uncertainty estimation (UE) methods provide\npotential solutions for evaluating prediction reliability and improving the\nmodel confidence calibration. Despite increasing interest in UE, challenges\npersist, such as the need for explicit methods to capture aleatoric uncertainty\nand align uncertainty estimates with real-life disagreements among domain\nexperts. This paper proposes an Expert Disagreement-Guided Uncertainty\nEstimation (EDUE) for medical image segmentation. By leveraging variability in\nground-truth annotations from multiple raters, we guide the model during\ntraining and incorporate random sampling-based strategies to enhance\ncalibration confidence. Our method achieves 55% and 23% improvement in\ncorrelation on average with expert disagreements at the image and pixel levels,\nrespectively, better calibration, and competitive segmentation performance\ncompared to the state-of-the-art deep ensembles, requiring only a single\nforward pass.\n","authors":["Kudaibergen Abutalip","Numan Saeed","Ikboljon Sobirov","Vincent Andrearczyk","Adrien Depeursinge","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.16594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16591v1","updated":"2024-03-25T10:06:45Z","published":"2024-03-25T10:06:45Z","title":"Deciphering the Interplay between Local Differential Privacy, Average\n  Bayesian Privacy, and Maximum Bayesian Privacy","summary":"  The swift evolution of machine learning has led to emergence of various\ndefinitions of privacy due to the threats it poses to privacy, including the\nconcept of local differential privacy (LDP). Although widely embraced and\nutilized across numerous domains, this conventional approach to measure privacy\nstill exhibits certain limitations, spanning from failure to prevent\ninferential disclosure to lack of consideration for the adversary's background\nknowledge. In this comprehensive study, we introduce Bayesian privacy and delve\ninto the intricate relationship between local differential privacy and its\nBayesian counterparts, unveiling novel insights into utility-privacy\ntrade-offs. We introduce a framework that encapsulates both attack and defense\nstrategies, highlighting their interplay and effectiveness. Our theoretical\ncontributions are anchored in the rigorous definitions and relationships\nbetween Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP),\nencapsulated by equations $\\epsilon_{p,a} \\leq\n\\frac{1}{\\sqrt{2}}\\sqrt{(\\epsilon_{p,m} + \\epsilon)\\cdot(e^{\\epsilon_{p,m} +\n\\epsilon} - 1)}$ and the equivalence between $\\xi$-MBP and $2\\xi$-LDP\nestablished under uniform prior distribution. These relationships fortify our\nunderstanding of the privacy guarantees provided by various mechanisms, leading\nto the realization that a mechanism satisfying $\\xi$-LDP also confers\n$\\xi$-MBP, and vice versa. Our work not only lays the groundwork for future\nempirical exploration but also promises to enhance the design of\nprivacy-preserving algorithms that do not compromise on utility, thereby\nfostering the development of trustworthy machine learning solutions.\n","authors":["Xiaojin Zhang","Yulin Fei","Wei Chen","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2403.16591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16582v1","updated":"2024-03-25T09:49:42Z","published":"2024-03-25T09:49:42Z","title":"In the Search for Optimal Multi-view Learning Models for Crop\n  Classification with Global Remote Sensing Data","summary":"  Crop classification is of critical importance due to its role in studying\ncrop pattern changes, resource management, and carbon sequestration. When\nemploying data-driven techniques for its prediction, utilizing various temporal\ndata sources is necessary. Deep learning models have proven to be effective for\nthis task by mapping time series data to high-level representation for\nprediction. However, they face substantial challenges when dealing with\nmultiple input patterns. The literature offers limited guidance for Multi-View\nLearning (MVL) scenarios, as it has primarily focused on exploring fusion\nstrategies with specific encoders and validating them in local regions. In\ncontrast, we investigate the impact of simultaneous selection of the fusion\nstrategy and the encoder architecture evaluated on a global-scale cropland and\ncrop-type classifications. We use a range of five fusion strategies (Input,\nFeature, Decision, Ensemble, Hybrid) and five temporal encoder architectures\n(LSTM, GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The\nvalidation is on the CropHarvest dataset that provides optical, radar, and\nweather time series, and topographic information as input data. We found that\nin scenarios with a limited number of labeled samples, a unique configuration\nis insufficient for all the cases. Instead, a specialized combination,\nincluding encoder and fusion strategy, should be meticulously sought. To\nstreamline this search process, we suggest initially identifying the optimal\nencoder architecture tailored for a particular fusion strategy, and then\ndetermining the most suitable fusion strategy for the classification task. We\nprovide a technical framework for researchers exploring crop classification or\nrelated tasks through a MVL approach.\n","authors":["Francisco Mena","Diego Arenas","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2403.16582v1.pdf","comment":"submitted to journal"},{"id":"http://arxiv.org/abs/2403.10615v2","updated":"2024-03-25T09:42:13Z","published":"2024-03-15T18:26:33Z","title":"LightIt: Illumination Modeling and Control for Diffusion Models","summary":"  We introduce LightIt, a method for explicit illumination control for image\ngeneration. Recent generative methods lack lighting control, which is crucial\nto numerous artistic aspects of image generation such as setting the overall\nmood or cinematic appearance. To overcome these limitations, we propose to\ncondition the generation on shading and normal maps. We model the lighting with\nsingle bounce shading, which includes cast shadows. We first train a shading\nestimation module to generate a dataset of real-world images and shading pairs.\nThen, we train a control network using the estimated shading and normals as\ninput. Our method demonstrates high-quality image generation and lighting\ncontrol in numerous scenes. Additionally, we use our generated dataset to train\nan identity-preserving relighting model, conditioned on an image and a target\nshading. Our method is the first that enables the generation of images with\ncontrollable, consistent lighting and performs on par with specialized\nrelighting state-of-the-art methods.\n","authors":["Peter Kocsis","Julien Philip","Kalyan Sunkavalli","Matthias Nießner","Yannick Hold-Geoffroy"],"pdf_url":"https://arxiv.org/pdf/2403.10615v2.pdf","comment":"Project page: https://peter-kocsis.github.io/LightIt/ Video:\n  https://youtu.be/cCfSBD5aPLI"},{"id":"http://arxiv.org/abs/2403.16576v1","updated":"2024-03-25T09:41:49Z","published":"2024-03-25T09:41:49Z","title":"Antigen-Specific Antibody Design via Direct Energy-based Preference\n  Optimization","summary":"  Antibody design, a crucial task with significant implications across various\ndisciplines such as therapeutics and biology, presents considerable challenges\ndue to its intricate nature. In this paper, we tackle antigen-specific antibody\ndesign as a protein sequence-structure co-design problem, considering both\nrationality and functionality. Leveraging a pre-trained conditional diffusion\nmodel that jointly models sequences and structures of\ncomplementarity-determining regions (CDR) in antibodies with equivariant neural\nnetworks, we propose direct energy-based preference optimization to guide the\ngeneration of antibodies with both rational structures and considerable binding\naffinities to given antigens. Our method involves fine-tuning the pre-trained\ndiffusion model using a residue-level decomposed energy preference.\nAdditionally, we employ gradient surgery to address conflicts between various\ntypes of energy, such as attraction and repulsion. Experiments on RAbD\nbenchmark show that our approach effectively optimizes the energy of generated\nantibodies and achieves state-of-the-art performance in designing high-quality\nantibodies with low total energy and high binding affinity, demonstrating the\nsuperiority of our approach.\n","authors":["Xiangxin Zhou","Dongyu Xue","Ruizhe Chen","Zaixiang Zheng","Liang Wang","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2403.16576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16571v1","updated":"2024-03-25T09:36:51Z","published":"2024-03-25T09:36:51Z","title":"NSINA: A News Corpus for Sinhala","summary":"  The introduction of large language models (LLMs) has advanced natural\nlanguage processing (NLP), but their effectiveness is largely dependent on\npre-training resources. This is especially evident in low-resource languages,\nsuch as Sinhala, which face two primary challenges: the lack of substantial\ntraining data and limited benchmarking datasets. In response, this study\nintroduces NSINA, a comprehensive news corpus of over 500,000 articles from\npopular Sinhala news websites, along with three NLP tasks: news media\nidentification, news category prediction, and news headline generation. The\nrelease of NSINA aims to provide a solution to challenges in adapting LLMs to\nSinhala, offering valuable resources and benchmarks for improving NLP in the\nSinhala language. NSINA is the largest news corpus for Sinhala, available up to\ndate.\n","authors":["Hansi Hettiarachchi","Damith Premasiri","Lasitha Uyangodage","Tharindu Ranasinghe"],"pdf_url":"https://arxiv.org/pdf/2403.16571v1.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.16569v1","updated":"2024-03-25T09:36:10Z","published":"2024-03-25T09:36:10Z","title":"Revealing Vulnerabilities of Neural Networks in Parameter Learning and\n  Defense Against Explanation-Aware Backdoors","summary":"  Explainable Artificial Intelligence (XAI) strategies play a crucial part in\nincreasing the understanding and trustworthiness of neural networks.\nNonetheless, these techniques could potentially generate misleading\nexplanations. Blinding attacks can drastically alter a machine learning\nalgorithm's prediction and explanation, providing misleading information by\nadding visually unnoticeable artifacts into the input, while maintaining the\nmodel's accuracy. It poses a serious challenge in ensuring the reliability of\nXAI methods. To ensure the reliability of XAI methods poses a real challenge,\nwe leverage statistical analysis to highlight the changes in CNN weights within\na CNN following blinding attacks. We introduce a method specifically designed\nto limit the effectiveness of such attacks during the evaluation phase,\navoiding the need for extra training. The method we suggest defences against\nmost modern explanation-aware adversarial attacks, achieving an approximate\ndecrease of ~99\\% in the Attack Success Rate (ASR) and a ~91\\% reduction in the\nMean Square Error (MSE) between the original explanation and the defended\n(post-attack) explanation across three unique types of attacks.\n","authors":["Md Abdul Kadir","GowthamKrishna Addluri","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2403.16569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16561v1","updated":"2024-03-25T09:24:05Z","published":"2024-03-25T09:24:05Z","title":"FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning","summary":"  Federated Learning (FL) heavily depends on label quality for its performance.\nHowever, the label distribution among individual clients is always both noisy\nand heterogeneous. The high loss incurred by client-specific samples in\nheterogeneous label noise poses challenges for distinguishing between\nclient-specific and noisy label samples, impacting the effectiveness of\nexisting label noise learning approaches. To tackle this issue, we propose\nFedFixer, where the personalized model is introduced to cooperate with the\nglobal model to effectively select clean client-specific samples. In the dual\nmodels, updating the personalized model solely at a local level can lead to\noverfitting on noisy data due to limited samples, consequently affecting both\nthe local and global models' performance. To mitigate overfitting, we address\nthis concern from two perspectives. Firstly, we employ a confidence regularizer\nto alleviate the impact of unconfident predictions caused by label noise.\nSecondly, a distance regularizer is implemented to constrain the disparity\nbetween the personalized and global models. We validate the effectiveness of\nFedFixer through extensive experiments on benchmark datasets. The results\ndemonstrate that FedFixer can perform well in filtering noisy label samples on\ndifferent clients, especially in highly heterogeneous label noise scenarios.\n","authors":["Xinyuan Ji","Zhaowei Zhu","Wei Xi","Olga Gadyatskaya","Zilong Song","Yong Cai","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16561v1.pdf","comment":"accepted by AAA24"},{"id":"http://arxiv.org/abs/2403.16557v1","updated":"2024-03-25T09:16:59Z","published":"2024-03-25T09:16:59Z","title":"Accelerating Federated Learning by Selecting Beneficial Herd of Local\n  Gradients","summary":"  Federated Learning (FL) is a distributed machine learning framework in\ncommunication network systems. However, the systems' Non-Independent and\nIdentically Distributed (Non-IID) data negatively affect the convergence\nefficiency of the global model, since only a subset of these data samples are\nbeneficial for model convergence. In pursuit of this subset, a reliable\napproach involves determining a measure of validity to rank the samples within\nthe dataset. In this paper, We propose the BHerd strategy which selects a\nbeneficial herd of local gradients to accelerate the convergence of the FL\nmodel. Specifically, we map the distribution of the local dataset to the local\ngradients and use the Herding strategy to obtain a permutation of the set of\ngradients, where the more advanced gradients in the permutation are closer to\nthe average of the set of gradients. These top portion of the gradients will be\nselected and sent to the server for global aggregation. We conduct experiments\non different datasets, models and scenarios by building a prototype system, and\nexperimental results demonstrate that our BHerd strategy is effective in\nselecting beneficial local gradients to mitigate the effects brought by the\nNon-IID dataset, thus accelerating model convergence.\n","authors":["Ping Luo","Xiaoge Deng","Ziqing Wen","Tao Sun","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.16557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14714v4","updated":"2024-03-25T08:58:39Z","published":"2023-10-23T08:51:05Z","title":"BatteryML:An Open-source platform for Machine Learning on Battery\n  Degradation","summary":"  Battery degradation remains a pivotal concern in the energy storage domain,\nwith machine learning emerging as a potent tool to drive forward insights and\nsolutions. However, this intersection of electrochemical science and machine\nlearning poses complex challenges. Machine learning experts often grapple with\nthe intricacies of battery science, while battery researchers face hurdles in\nadapting intricate models tailored to specific datasets. Beyond this, a\ncohesive standard for battery degradation modeling, inclusive of data formats\nand evaluative benchmarks, is conspicuously absent. Recognizing these\nimpediments, we present BatteryML - a one-step, all-encompass, and open-source\nplatform designed to unify data preprocessing, feature extraction, and the\nimplementation of both traditional and state-of-the-art models. This\nstreamlined approach promises to enhance the practicality and efficiency of\nresearch applications. BatteryML seeks to fill this void, fostering an\nenvironment where experts from diverse specializations can collaboratively\ncontribute, thus elevating the collective understanding and advancement of\nbattery research.The code for our project is publicly available on GitHub at\nhttps://github.com/microsoft/BatteryML.\n","authors":["Han Zhang","Xiaofan Gui","Shun Zheng","Ziheng Lu","Yuqi Li","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2310.14714v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02760v2","updated":"2024-03-25T08:57:47Z","published":"2023-11-05T20:33:18Z","title":"Causal Question Answering with Reinforcement Learning","summary":"  Causal questions inquire about causal relationships between different events\nor phenomena. They are important for a variety of use cases, including virtual\nassistants and search engines. However, many current approaches to causal\nquestion answering cannot provide explanations or evidence for their answers.\nHence, in this paper, we aim to answer causal questions with a causality graph,\na large-scale dataset of causal relations between noun phrases along with the\nrelations' provenance data. Inspired by recent, successful applications of\nreinforcement learning to knowledge graph tasks, such as link prediction and\nfact-checking, we explore the application of reinforcement learning on a\ncausality graph for causal question answering. We introduce an\nActor-Critic-based agent which learns to search through the graph to answer\ncausal questions. We bootstrap the agent with a supervised learning procedure\nto deal with large action spaces and sparse rewards. Our evaluation shows that\nthe agent successfully prunes the search space to answer binary causal\nquestions by visiting less than 30 nodes per question compared to over 3,000\nnodes by a naive breadth-first search. Our ablation study indicates that our\nsupervised learning strategy provides a strong foundation upon which our\nreinforcement learning agent improves. The paths returned by our agent explain\nthe mechanisms by which a cause produces an effect. Moreover, for each edge on\na path, our causality graph provides its original source allowing for easy\nverification of paths.\n","authors":["Lukas Blübaum","Stefan Heindorf"],"pdf_url":"https://arxiv.org/pdf/2311.02760v2.pdf","comment":"Accepted at WWW 2024"},{"id":"http://arxiv.org/abs/2402.09132v3","updated":"2024-03-25T08:46:02Z","published":"2024-02-14T12:28:38Z","title":"Exploring the Adversarial Capabilities of Large Language Models","summary":"  The proliferation of large language models (LLMs) has sparked widespread and\ngeneral interest due to their strong language generation capabilities, offering\ngreat potential for both industry and research. While previous research delved\ninto the security and privacy issues of LLMs, the extent to which these models\ncan exhibit adversarial behavior remains largely unexplored. Addressing this\ngap, we investigate whether common publicly available LLMs have inherent\ncapabilities to perturb text samples to fool safety measures, so-called\nadversarial examples resp.~attacks. More specifically, we investigate whether\nLLMs are inherently able to craft adversarial examples out of benign samples to\nfool existing safe rails. Our experiments, which focus on hate speech\ndetection, reveal that LLMs succeed in finding adversarial perturbations,\neffectively undermining hate speech detection systems. Our findings carry\nsignificant implications for (semi-)autonomous systems relying on LLMs,\nhighlighting potential challenges in their interaction with existing systems\nand safety measures.\n","authors":["Lukas Struppek","Minh Hieu Le","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2402.09132v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16542v1","updated":"2024-03-25T08:35:19Z","published":"2024-03-25T08:35:19Z","title":"Differentially Private Online Federated Learning with Correlated Noise","summary":"  We propose a novel differentially private algorithm for online federated\nlearning that employs temporally correlated noise to improve the utility while\nensuring the privacy of the continuously released models. To address challenges\nstemming from DP noise and local updates with streaming noniid data, we develop\na perturbed iterate analysis to control the impact of the DP noise on the\nutility. Moreover, we demonstrate how the drift errors from local updates can\nbe effectively managed under a quasi-strong convexity condition. Subject to an\n$(\\epsilon, \\delta)$-DP budget, we establish a dynamic regret bound over the\nentire time horizon that quantifies the impact of key parameters and the\nintensity of changes in dynamic environments. Numerical experiments validate\nthe efficacy of the proposed algorithm.\n","authors":["Jiaojiao Zhang","Linglingzhi Zhu","Mikael Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.16542v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.16523v1","updated":"2024-03-25T08:06:08Z","published":"2024-03-25T08:06:08Z","title":"Causal Discovery from Poisson Branching Structural Causal Model Using\n  High-Order Cumulant with Path Analysis","summary":"  Count data naturally arise in many fields, such as finance, neuroscience, and\nepidemiology, and discovering causal structure among count data is a crucial\ntask in various scientific and industrial scenarios. One of the most common\ncharacteristics of count data is the inherent branching structure described by\na binomial thinning operator and an independent Poisson distribution that\ncaptures both branching and noise. For instance, in a population count\nscenario, mortality and immigration contribute to the count, where survival\nfollows a Bernoulli distribution, and immigration follows a Poisson\ndistribution. However, causal discovery from such data is challenging due to\nthe non-identifiability issue: a single causal pair is Markov equivalent, i.e.,\n$X\\rightarrow Y$ and $Y\\rightarrow X$ are distributed equivalent. Fortunately,\nin this work, we found that the causal order from $X$ to its child $Y$ is\nidentifiable if $X$ is a root vertex and has at least two directed paths to\n$Y$, or the ancestor of $X$ with the most directed path to $X$ has a directed\npath to $Y$ without passing $X$. Specifically, we propose a Poisson Branching\nStructure Causal Model (PB-SCM) and perform a path analysis on PB-SCM using\nhigh-order cumulants. Theoretical results establish the connection between the\npath and cumulant and demonstrate that the path information can be obtained\nfrom the cumulant. With the path information, causal order is identifiable\nunder some graphical conditions. A practical algorithm for learning causal\nstructure under PB-SCM is proposed and the experiments demonstrate and verify\nthe effectiveness of the proposed method.\n","authors":["Jie Qiao","Yu Xiang","Zhengming Chen","Ruichu Cai","Zhifeng Hao"],"pdf_url":"https://arxiv.org/pdf/2403.16523v1.pdf","comment":"Accepted by AAAI-2024"},{"id":"http://arxiv.org/abs/2306.08929v2","updated":"2024-03-25T08:00:38Z","published":"2023-06-15T08:02:07Z","title":"On the resilience of Collaborative Learning-based Recommender Systems\n  Against Community Detection Attack","summary":"  Collaborative-learning-based recommender systems emerged following the\nsuccess of collaborative learning techniques such as Federated Learning (FL)\nand Gossip Learning (GL). In these systems, users participate in the training\nof a recommender system while maintaining their history of consumed items on\ntheir devices. While these solutions seemed appealing for preserving the\nprivacy of the participants at first glance, recent studies have revealed that\ncollaborative learning can be vulnerable to various privacy attacks. In this\npaper, we study the resilience of collaborative learning-based recommender\nsystems against a novel privacy attack called Community Detection Attack (CDA).\nThis attack enables an adversary to identify community members based on a\nchosen set of items (eg., identifying users interested in specific\npoints-of-interest). Through experiments on three real recommendation datasets\nusing two state-of-the-art recommendation models, we evaluate the sensitivity\nof an FL-based recommender system as well as two flavors of Gossip\nLearning-based recommender systems to CDA. The results show that across all\nmodels and datasets, the FL setting is more vulnerable to CDA compared to\nGossip settings. Furthermore, we assess two off-the-shelf mitigation\nstrategies, namely differential privacy (DP) and a \\emph{Share less} policy,\nwhich consists of sharing a subset of less sensitive model parameters. The\nfindings indicate a more favorable privacy-utility trade-off for the\n\\emph{Share less} strategy, particularly in FedRecs.\n","authors":["Yacine Belal","Sonia Ben Mokhtar","Mohamed Maouche","Anthony Simonet-Boulogne"],"pdf_url":"https://arxiv.org/pdf/2306.08929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16509v1","updated":"2024-03-25T07:48:34Z","published":"2024-03-25T07:48:34Z","title":"Human Understanding AI Paper Challenge 2024 -- Dataset Design","summary":"  In 2024, we will hold a research paper competition (the third Human\nUnderstanding AI Paper Challenge) for the research and development of\nartificial intelligence technologies to understand human daily life. This\ndocument introduces the datasets that will be provided to participants in the\ncompetition, and summarizes the issues to consider in data processing and\nlearning model development.\n","authors":["Se Won Oh","Hyuntae Jeong","Jeong Mook Lim","Seungeun Chung","Kyoung Ju Noh"],"pdf_url":"https://arxiv.org/pdf/2403.16509v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.16497v1","updated":"2024-03-25T07:29:18Z","published":"2024-03-25T07:29:18Z","title":"PathoTune: Adapting Visual Foundation Model to Pathological Specialists","summary":"  As natural image understanding moves towards the pretrain-finetune era,\nresearch in pathology imaging is concurrently evolving. Despite the predominant\nfocus on pretraining pathological foundation models, how to adapt foundation\nmodels to downstream tasks is little explored. For downstream adaptation, we\npropose the existence of two domain gaps, i.e., the Foundation-Task Gap and the\nTask-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework\ndesigned to efficiently adapt pathological or even visual foundation models to\npathology-specific tasks via multi-modal prompt tuning. The proposed framework\nleverages Task-specific Visual Prompts and Task-specific Textual Prompts to\nidentify task-relevant features, along with Instance-specific Visual Prompts\nfor encoding single pathological image features. Results across multiple\ndatasets at both patch-level and WSI-level demonstrate its superior performance\nover single-modality prompt tuning approaches. Significantly, PathoTune\nfacilitates the direct adaptation of natural visual foundation models to\npathological tasks, drastically outperforming pathological foundation models\nwith simple linear probing. The code will be available upon acceptance.\n","authors":["Jiaxuan Lu","Fang Yan","Xiaofan Zhang","Yue Gao","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16497v1.pdf","comment":"Submitted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.16495v1","updated":"2024-03-25T07:23:23Z","published":"2024-03-25T07:23:23Z","title":"LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural\n  Network for Traffic Flow Forecasting","summary":"  Accurate traffic forecasting is a fundamental problem in intelligent\ntransportation systems and learning long-range traffic representations with key\ninformation through spatiotemporal graph neural networks (STGNNs) is a basic\nassumption of current traffic flow prediction models. However, due to\nstructural limitations, existing STGNNs can only utilize short-range traffic\nflow data; therefore, the models cannot adequately learn the complex trends and\nperiodic features in traffic flow. Besides, it is challenging to extract the\nkey temporal information from the long historical traffic series and obtain a\ncompact representation. To solve the above problems, we propose a novel LSTTN\n(Long-Short Term Transformer-based Network) framework comprehensively\nconsidering the long- and short-term features in historical traffic flow.\nFirst, we employ a masked subseries Transformer to infer the content of masked\nsubseries from a small portion of unmasked subseries and their temporal context\nin a pretraining manner, forcing the model to efficiently learn compressed and\ncontextual subseries temporal representations from long historical series.\nThen, based on the learned representations, long-term trend is extracted by\nusing stacked 1D dilated convolution layers, and periodic features are\nextracted by dynamic graph convolution layers. For the difficulties in making\ntime-step level prediction, LSTTN adopts a short-term trend extractor to learn\nfine-grained short-term temporal features. Finally, LSTTN fuses the long-term\ntrend, periodic features and short-term features to obtain the prediction\nresults. Experiments on four real-world datasets show that in 60-minute-ahead\nlong-term forecasting, the LSTTN model achieves a minimum improvement of 5.63\\%\nand a maximum improvement of 16.78\\% over baseline models. The source code is\navailable at https://github.com/GeoX-Lab/LSTTN.\n","authors":["Qinyao Luo","Silu He","Xing Han","Yuhan Wang","Haifeng Li"],"pdf_url":"https://arxiv.org/pdf/2403.16495v1.pdf","comment":"15 pages, 10 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.16482v1","updated":"2024-03-25T07:08:01Z","published":"2024-03-25T07:08:01Z","title":"Determined Multi-Label Learning via Similarity-Based Prompt","summary":"  In multi-label classification, each training instance is associated with\nmultiple class labels simultaneously. Unfortunately, collecting the fully\nprecise class labels for each training instance is time- and labor-consuming\nfor real-world applications. To alleviate this problem, a novel labeling\nsetting termed \\textit{Determined Multi-Label Learning} (DMLL) is proposed,\naiming to effectively alleviate the labeling cost inherent in multi-label\ntasks. In this novel labeling setting, each training instance is associated\nwith a \\textit{determined label} (either \"Yes\" or \"No\"), which indicates\nwhether the training instance contains the provided class label. The provided\nclass label is randomly and uniformly selected from the whole candidate labels\nset. Besides, each training instance only need to be determined once, which\nsignificantly reduce the annotation cost of the labeling task for multi-label\ndatasets. In this paper, we theoretically derive an risk-consistent estimator\nto learn a multi-label classifier from these determined-labeled training data.\nAdditionally, we introduce a similarity-based prompt learning method for the\nfirst time, which minimizes the risk-consistent loss of large-scale pre-trained\nmodels to learn a supplemental prompt with richer semantic information.\nExtensive experimental validation underscores the efficacy of our approach,\ndemonstrating superior performance compared to existing state-of-the-art\nmethods.\n","authors":["Meng Wei","Zhongnian Li","Peng Ying","Yong Zhou","Xinzheng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16482v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.11834v2","updated":"2024-03-25T07:00:29Z","published":"2023-12-19T04:02:50Z","title":"Multi-agent reinforcement learning using echo-state network and its\n  application to pedestrian dynamics","summary":"  In recent years, simulations of pedestrians using the multi-agent\nreinforcement learning (MARL) have been studied. This study considered the\nroads on a grid-world environment, and implemented pedestrians as MARL agents\nusing an echo-state network and the least squares policy iteration method.\nUnder this environment, the ability of these agents to learn to move forward by\navoiding other agents was investigated. Specifically, we considered two types\nof tasks: the choice between a narrow direct route and a broad detour, and the\nbidirectional pedestrian flow in a corridor. The simulations results indicated\nthat the learning was successful when the density of the agents was not that\nhigh.\n","authors":["Hisato Komatsu"],"pdf_url":"https://arxiv.org/pdf/2312.11834v2.pdf","comment":"26 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.06606v2","updated":"2024-03-25T06:57:57Z","published":"2024-03-11T10:50:53Z","title":"Distributionally Generative Augmentation for Fair Facial Attribute\n  Classification","summary":"  Facial Attribute Classification (FAC) holds substantial promise in widespread\napplications. However, FAC models trained by traditional methodologies can be\nunfair by exhibiting accuracy inconsistencies across varied data\nsubpopulations. This unfairness is largely attributed to bias in data, where\nsome spurious attributes (e.g., Male) statistically correlate with the target\nattribute (e.g., Smiling). Most of existing fairness-aware methods rely on the\nlabels of spurious attributes, which may be unavailable in practice. This work\nproposes a novel, generation-based two-stage framework to train a fair FAC\nmodel on biased data without additional annotation. Initially, we identify the\npotential spurious attributes based on generative models. Notably, it enhances\ninterpretability by explicitly showing the spurious attributes in image space.\nFollowing this, for each image, we first edit the spurious attributes with a\nrandom degree sampled from a uniform distribution, while keeping target\nattribute unchanged. Then we train a fair FAC model by fostering model\ninvariance to these augmentation. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our method in promoting fairness in\nFAC without compromising accuracy. Codes are in\nhttps://github.com/heqianpei/DiGA.\n","authors":["Fengda Zhang","Qianpei He","Kun Kuang","Jiashuo Liu","Long Chen","Chao Wu","Jun Xiao","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06606v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16469v1","updated":"2024-03-25T06:50:25Z","published":"2024-03-25T06:50:25Z","title":"Learning from Reduced Labels for Long-Tailed Data","summary":"  Long-tailed data is prevalent in real-world classification tasks and heavily\nrelies on supervised information, which makes the annotation process\nexceptionally labor-intensive and time-consuming. Unfortunately, despite being\na common approach to mitigate labeling costs, existing weakly supervised\nlearning methods struggle to adequately preserve supervised information for\ntail samples, resulting in a decline in accuracy for the tail classes. To\nalleviate this problem, we introduce a novel weakly supervised labeling setting\ncalled Reduced Label. The proposed labeling setting not only avoids the decline\nof supervised information for the tail samples, but also decreases the labeling\ncosts associated with long-tailed data. Additionally, we propose an\nstraightforward and highly efficient unbiased framework with strong theoretical\nguarantees to learn from these Reduced Labels. Extensive experiments conducted\non benchmark datasets including ImageNet validate the effectiveness of our\napproach, surpassing the performance of state-of-the-art weakly supervised\nmethods.\n","authors":["Meng Wei","Zhongnian Li","Yong Zhou","Xinzheng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16469v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.16464v1","updated":"2024-03-25T06:46:27Z","published":"2024-03-25T06:46:27Z","title":"Training Generative Adversarial Network-Based Vocoder with Limited Data\n  Using Augmentation-Conditional Discriminator","summary":"  A generative adversarial network (GAN)-based vocoder trained with an\nadversarial discriminator is commonly used for speech synthesis because of its\nfast, lightweight, and high-quality characteristics. However, this data-driven\nmodel requires a large amount of training data incurring high data-collection\ncosts. This fact motivates us to train a GAN-based vocoder on limited data. A\npromising solution is to augment the training data to avoid overfitting.\nHowever, a standard discriminator is unconditional and insensitive to\ndistributional changes caused by data augmentation. Thus, augmented speech\n(which can be extraordinary) may be considered real speech. To address this\nissue, we propose an augmentation-conditional discriminator (AugCondD) that\nreceives the augmentation state as input in addition to speech, thereby\nassessing the input speech according to the augmentation state, without\ninhibiting the learning of the original non-augmented distribution.\nExperimental results indicate that AugCondD improves speech quality under\nlimited data conditions while achieving comparable speech quality under\nsufficient data conditions. Audio samples are available at\nhttps://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/.\n","authors":["Takuhiro Kaneko","Hirokazu Kameoka","Kou Tanaka"],"pdf_url":"https://arxiv.org/pdf/2403.16464v1.pdf","comment":"Accepted to ICASSP 2024. Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/"},{"id":"http://arxiv.org/abs/2403.16460v1","updated":"2024-03-25T06:43:28Z","published":"2024-03-25T06:43:28Z","title":"FedAC: A Adaptive Clustered Federated Learning Framework for\n  Heterogeneous Data","summary":"  Clustered federated learning (CFL) is proposed to mitigate the performance\ndeterioration stemming from data heterogeneity in federated learning (FL) by\ngrouping similar clients for cluster-wise model training. However, current CFL\nmethods struggle due to inadequate integration of global and intra-cluster\nknowledge and the absence of an efficient online model similarity metric, while\ntreating the cluster count as a fixed hyperparameter limits flexibility and\nrobustness. In this paper, we propose an adaptive CFL framework, named FedAC,\nwhich (1) efficiently integrates global knowledge into intra-cluster learning\nby decoupling neural networks and utilizing distinct aggregation methods for\neach submodule, significantly enhancing performance; (2) includes a\ncosteffective online model similarity metric based on dimensionality reduction;\n(3) incorporates a cluster number fine-tuning module for improved adaptability\nand scalability in complex, heterogeneous environments. Extensive experiments\nshow that FedAC achieves superior empirical performance, increasing the test\naccuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets,\nrespectively, under different non-IID settings compared to SOTA methods.\n","authors":["Yuxin Zhang","Haoyu Chen","Zheng Lin","Zhe Chen","Jin Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.16460v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16459v1","updated":"2024-03-25T06:42:02Z","published":"2024-03-25T06:42:02Z","title":"On the rates of convergence for learning with convolutional neural\n  networks","summary":"  We study the approximation and learning capacities of convolutional neural\nnetworks (CNNs). Our first result proves a new approximation bound for CNNs\nwith certain constraint on the weights. Our second result gives a new analysis\non the covering number of feed-forward neural networks, which include CNNs as\nspecial cases. The analysis carefully takes into account the size of the\nweights and hence gives better bounds than existing literature in some\nsituations. Using these two results, we are able to derive rates of convergence\nfor estimators based on CNNs in many learning problems. In particular, we\nestablish minimax optimal convergence rates of the least squares based on CNNs\nfor learning smooth functions in the nonparametric regression setting. For\nbinary classification, we derive convergence rates for CNN classifiers with\nhinge loss and logistic loss. It is also shown that the obtained rates are\nminimax optimal in several settings.\n","authors":["Yunfei Yang","Han Feng","Ding-Xuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.16459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16430v5","updated":"2024-03-25T06:32:49Z","published":"2023-12-27T06:34:54Z","title":"Preference as Reward, Maximum Preference Optimization with Importance\n  Sampling","summary":"  Preference learning is a key technology for aligning language models with\nhuman values. Reinforcement Learning from Human Feedback (RLHF) is a\nmodel-based algorithm to optimize preference learning, which first fits a\nreward model for preference scores and then optimizes the generating policy\nwith an on-policy PPO algorithm to maximize the reward. The processing of RLHF\nis complex, time-consuming, and unstable. The Direct Preference Optimization\n(DPO) algorithm uses an off-policy algorithm to directly optimize the\ngenerating policy and eliminates the need for a reward model. DPO is more\ndata-efficient and stable. However, DPO has a drawback of overfitting to the\npreference data and ignoring the KL-regularization term when the preference is\ndeterministic. Identity mapping Preference Optimization(IPO) uses a\nroot-finding MSE loss to incorporate KL-regularization. However, both DPO and\nIPO fail to properly address the KL-regularization term because the support of\nthe preference distribution is not equal to the reference distribution. In this\npaper, we propose a simple and intuitive off-policy preference optimization\nalgorithm from an importance sampling view, which we call Maximum Preference\nOptimization (MPO). MPO incorporates the off-policy KL-regularization term,\nmaking regularization truly effective. MPO achieves the best of both worlds by\ncombining the objectives of RLHF and IPO while being an off-policy algorithm.\nFurthermore, MPO eliminates the need for a reward model and reference policy,\nsimplifying the learning process and reducing memory usage.\n","authors":["Zaifan Jiang","Xing Huang","Chao Wei"],"pdf_url":"https://arxiv.org/pdf/2312.16430v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16451v1","updated":"2024-03-25T06:30:54Z","published":"2024-03-25T06:30:54Z","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","summary":"  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n","authors":["Xiang-Li Lu","Hwai-Jung Hsu","Che-Wei Chou","H. T. Kung","Chen-Hsin Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08282v3","updated":"2024-03-25T06:21:37Z","published":"2023-10-12T12:39:08Z","title":"Data driven modeling for self-similar dynamics","summary":"  Multiscale modeling of complex systems is crucial for understanding their\nintricacies. Data-driven multiscale modeling has emerged as a promising\napproach to tackle challenges associated with complex systems. On the other\nhand, self-similarity is prevalent in complex systems, hinting that large-scale\ncomplex systems can be modeled at a reduced cost. In this paper, we introduce a\nmultiscale neural network framework that incorporates self-similarity as prior\nknowledge, facilitating the modeling of self-similar dynamical systems. For\ndeterministic dynamics, our framework can discern whether the dynamics are\nself-similar. For uncertain dynamics, it can compare and determine which\nparameter set is closer to self-similarity. The framework allows us to extract\nscale-invariant kernels from the dynamics for modeling at any scale. Moreover,\nour method can identify the power law exponents in self-similar systems.\nPreliminary tests on the Ising model yielded critical exponents consistent with\ntheoretical expectations, providing valuable insights for addressing critical\nphase transitions in non-equilibrium systems.\n","authors":["Ruyi Tao","Ningning Tao","Yi-zhuang You","Jiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08282v3.pdf","comment":"10 pages,7 figures,1 table"},{"id":"http://arxiv.org/abs/2403.16442v1","updated":"2024-03-25T06:05:50Z","published":"2024-03-25T06:05:50Z","title":"If CLIP Could Talk: Understanding Vision-Language Model Representations\n  Through Their Preferred Concept Descriptions","summary":"  Recent works often assume that Vision-Language Model (VLM) representations\nare based on visual attributes like shape. However, it is unclear to what\nextent VLMs prioritize this information to represent concepts. We propose\nExtract and Explore (EX2), a novel approach to characterize important textual\nfeatures for VLMs. EX2 uses reinforcement learning to align a large language\nmodel with VLM preferences and generates descriptions that incorporate the\nimportant features for the VLM. Then, we inspect the descriptions to identify\nthe features that contribute to VLM representations. We find that spurious\ndescriptions have a major role in VLM representations despite providing no\nhelpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,\namong informative descriptions, VLMs rely significantly on non-visual\nattributes like habitat to represent visual concepts. Also, our analysis\nreveals that different VLMs prioritize different attributes in their\nrepresentations. Overall, we show that VLMs do not simply match images to scene\ndescriptions and that non-visual or even spurious descriptions significantly\ninfluence their representations.\n","authors":["Reza Esfandiarpoor","Cristina Menghini","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2403.16442v1.pdf","comment":"Code: https://github.com/BatsResearch/ex2"},{"id":"http://arxiv.org/abs/2403.16439v1","updated":"2024-03-25T05:58:33Z","published":"2024-03-25T05:58:33Z","title":"Producing and Leveraging Online Map Uncertainty in Trajectory Prediction","summary":"  High-definition (HD) maps have played an integral role in the development of\nmodern autonomous vehicle (AV) stacks, albeit with high associated labeling and\nmaintenance costs. As a result, many recent works have proposed methods for\nestimating HD maps online from sensor data, enabling AVs to operate outside of\npreviously-mapped regions. However, current online map estimation approaches\nare developed in isolation of their downstream tasks, complicating their\nintegration in AV stacks. In particular, they do not produce uncertainty or\nconfidence estimates. In this work, we extend multiple state-of-the-art online\nmap estimation methods to additionally estimate uncertainty and show how this\nenables more tightly integrating online mapping with trajectory forecasting. In\ndoing so, we find that incorporating uncertainty yields up to 50% faster\ntraining convergence and up to 15% better prediction performance on the\nreal-world nuScenes driving dataset.\n","authors":["Xunjiang Gu","Guanyu Song","Igor Gilitschenski","Marco Pavone","Boris Ivanovic"],"pdf_url":"https://arxiv.org/pdf/2403.16439v1.pdf","comment":"14 pages, 14 figures, 6 tables. CVPR 2024"},{"id":"http://arxiv.org/abs/2312.03009v2","updated":"2024-03-25T05:04:04Z","published":"2023-12-04T19:01:19Z","title":"I-PHYRE: Interactive Physical Reasoning","summary":"  Current evaluation protocols predominantly assess physical reasoning in\nstationary scenes, creating a gap in evaluating agents' abilities to interact\nwith dynamic events. While contemporary methods allow agents to modify initial\nscene configurations and observe consequences, they lack the capability to\ninteract with events in real time. To address this, we introduce I-PHYRE, a\nframework that challenges agents to simultaneously exhibit intuitive physical\nreasoning, multi-step planning, and in-situ intervention. Here, intuitive\nphysical reasoning refers to a quick, approximate understanding of physics to\naddress complex problems; multi-step denotes the need for extensive sequence\nplanning in I-PHYRE, considering each intervention can significantly alter\nsubsequent choices; and in-situ implies the necessity for timely object\nmanipulation within a scene, where minor timing deviations can result in task\nfailure. We formulate four game splits to scrutinize agents' learning and\ngeneralization of essential principles of interactive physical reasoning,\nfostering learning through interaction with representative scenarios. Our\nexploration involves three planning strategies and examines several supervised\nand reinforcement agents' zero-shot generalization proficiency on I-PHYRE. The\noutcomes highlight a notable gap between existing learning algorithms and human\nperformance, emphasizing the imperative for more research in enhancing agents\nwith interactive physical reasoning capabilities. The environment and baselines\nwill be made publicly available.\n","authors":["Shiqian Li","Kewen Wu","Chi Zhang","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.03009v2.pdf","comment":"21 pages, ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16418v1","updated":"2024-03-25T04:43:47Z","published":"2024-03-25T04:43:47Z","title":"An incremental MaxSAT-based model to learn balanced rules","summary":"  The increasing advancements in the field of machine learning have led to the\ndevelopment of numerous applications that effectively address a wide range of\nproblems with accurate predictions. However, in certain cases, accuracy alone\nmay not be sufficient. Many real-world problems also demand explanations and\ninterpretability behind the predictions. One of the most popular interpretable\nmodels that are classification rules. This work aims to propose an incremental\nmodel for learning interpretable and balanced rules based on MaxSAT, called\nIMLIB. This new model was based on two other approaches, one based on SAT and\nthe other on MaxSAT. The one based on SAT limits the size of each generated\nrule, making it possible to balance them. We suggest that such a set of rules\nseem more natural to be understood compared to a mixture of large and small\nrules. The approach based on MaxSAT, called IMLI, presents a technique to\nincrease performance that involves learning a set of rules by incrementally\napplying the model in a dataset. Finally, IMLIB and IMLI are compared using\ndiverse databases. IMLIB obtained results comparable to IMLI in terms of\naccuracy, generating more balanced rules with smaller sizes.\n","authors":["Antônio Carlos Souza Ferreira Júnior","Thiago Alves Rocha"],"pdf_url":"https://arxiv.org/pdf/2403.16418v1.pdf","comment":"16 pages, 5 tables, submitted to BRACIS 2023 (Brazilian Conference on\n  Intelligent Systems), accepted version published in Intelligent Systems,\n  LNCS, vol 14195"},{"id":"http://arxiv.org/abs/2312.05654v3","updated":"2024-03-25T04:32:19Z","published":"2023-12-09T19:42:36Z","title":"Spectral methods for Neural Integral Equations","summary":"  Neural integral equations are deep learning models based on the theory of\nintegral equations, where the model consists of an integral operator and the\ncorresponding equation (of the second kind) which is learned through an\noptimization procedure. This approach allows to leverage the nonlocal\nproperties of integral operators in machine learning, but it is computationally\nexpensive. In this article, we introduce a framework for neural integral\nequations based on spectral methods that allows us to learn an operator in the\nspectral domain, resulting in a cheaper computational cost, as well as in high\ninterpolation accuracy. We study the properties of our methods and show various\ntheoretical guarantees regarding the approximation capabilities of the model,\nand convergence to solutions of the numerical methods. We provide numerical\nexperiments to demonstrate the practical effectiveness of the resulting model.\n","authors":["Emanuele Zappala"],"pdf_url":"https://arxiv.org/pdf/2312.05654v3.pdf","comment":"15 pages, 3 figures and 2 tables. v3: Missing hypotheses for the\n  framework have been now added"},{"id":"http://arxiv.org/abs/2403.14689v2","updated":"2024-03-25T04:21:13Z","published":"2024-03-13T22:38:08Z","title":"Developing and Deploying Industry Standards for Artificial Intelligence\n  in Education (AIED): Challenges, Strategies, and Future Directions","summary":"  The adoption of Artificial Intelligence in Education (AIED) holds the promise\nof revolutionizing educational practices by offering personalized learning\nexperiences, automating administrative and pedagogical tasks, and reducing the\ncost of content creation. However, the lack of standardized practices in the\ndevelopment and deployment of AIED solutions has led to fragmented ecosystems,\nwhich presents challenges in interoperability, scalability, and ethical\ngovernance. This article aims to address the critical need to develop and\nimplement industry standards in AIED, offering a comprehensive analysis of the\ncurrent landscape, challenges, and strategic approaches to overcome these\nobstacles. We begin by examining the various applications of AIED in various\neducational settings and identify key areas lacking in standardization,\nincluding system interoperability, ontology mapping, data integration,\nevaluation, and ethical governance. Then, we propose a multi-tiered framework\nfor establishing robust industry standards for AIED. In addition, we discuss\nmethodologies for the iterative development and deployment of standards,\nincorporating feedback loops from real-world applications to refine and adapt\nstandards over time. The paper also highlights the role of emerging\ntechnologies and pedagogical theories in shaping future standards for AIED.\nFinally, we outline a strategic roadmap for stakeholders to implement these\nstandards, fostering a cohesive and ethical AIED ecosystem. By establishing\ncomprehensive industry standards, such as those by IEEE Artificial Intelligence\nStandards Committee (AISC) and International Organization for Standardization\n(ISO), we can accelerate and scale AIED solutions to improve educational\noutcomes, ensuring that technological advances align with the principles of\ninclusivity, fairness, and educational excellence.\n","authors":["Richard Tong","Haoyang Li","Joleen Liang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.14689v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2310.01777v2","updated":"2024-03-25T04:04:05Z","published":"2023-10-03T03:56:26Z","title":"SEA: Sparse Linear Attention with Estimated Attention Mask","summary":"  The transformer architecture has driven breakthroughs in recent years on\ntasks which require modeling pairwise relationships between sequential\nelements, as is the case in natural language understanding. However, long\nseqeuences pose a problem due to the quadratic complexity of the attention\noperation. Previous research has aimed to lower the complexity by sparsifying\nor linearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix and often\nrequire complete retraining from scratch. Furthermore, previous sparse and\nlinear approaches lose interpretability if they cannot produce full attention\nmatrices. To address these challenges, we propose SEA: Sparse linear attention\nwith an Estimated Attention mask. SEA estimates the attention matrix with\nlinear complexity via kernel-based linear attention, then subsequently creates\na sparse attention matrix with a top-k selection to perform a sparse attention\noperation. For language modeling tasks (Wikitext2), previous linear and sparse\nattention methods show roughly two-fold worse perplexity scores over the\nquadratic OPT-1.3B baseline, while SEA achieves better perplexity than\nOPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable\nattention matrix. We believe that our work will have a large practical impact,\nas it opens the possibility of running large transformers on resource-limited\ndevices with less memory.\n","authors":["Heejun Lee","Jina Kim","Jeffrey Willette","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2310.01777v2.pdf","comment":"9 main pages"},{"id":"http://arxiv.org/abs/2403.16405v1","updated":"2024-03-25T03:44:36Z","published":"2024-03-25T03:44:36Z","title":"Ensemble Adversarial Defense via Integration of Multiple Dispersed Low\n  Curvature Models","summary":"  The integration of an ensemble of deep learning models has been extensively\nexplored to enhance defense against adversarial attacks. The diversity among\nsub-models increases the attack cost required to deceive the majority of the\nensemble, thereby improving the adversarial robustness. While existing\napproaches mainly center on increasing diversity in feature representations or\ndispersion of first-order gradients with respect to input, the limited\ncorrelation between these diversity metrics and adversarial robustness\nconstrains the performance of ensemble adversarial defense. In this work, we\naim to enhance ensemble diversity by reducing attack transferability. We\nidentify second-order gradients, which depict the loss curvature, as a key\nfactor in adversarial robustness. Computing the Hessian matrix involved in\nsecond-order gradients is computationally expensive. To address this, we\napproximate the Hessian-vector product using differential approximation. Given\nthat low curvature provides better robustness, our ensemble model was designed\nto consider the influence of curvature among different sub-models. We introduce\na novel regularizer to train multiple more-diverse low-curvature network\nmodels. Extensive experiments across various datasets demonstrate that our\nensemble model exhibits superior robustness against a range of attacks,\nunderscoring the effectiveness of our approach.\n","authors":["Kaikang Zhao","Xi Chen","Wei Huang","Liuxin Ding","Xianglong Kong","Fan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16405v1.pdf","comment":"Accepted to The 2024 International Joint Conference on Neural\n  Networks (IJCNN)"},{"id":"http://arxiv.org/abs/2403.16398v1","updated":"2024-03-25T03:26:01Z","published":"2024-03-25T03:26:01Z","title":"Rethinking the Representation in Federated Unsupervised Learning with\n  Non-IID Data","summary":"  Federated learning achieves effective performance in modeling decentralized\ndata. In practice, client data are not well-labeled, which makes it potential\nfor federated unsupervised learning (FUSL) with non-IID data. However, the\nperformance of existing FUSL methods suffers from insufficient representations,\ni.e., (1) representation collapse entanglement among local and global models,\nand (2) inconsistent representation spaces among local models. The former\nindicates that representation collapse in local model will subsequently impact\nthe global model and other local models. The latter means that clients model\ndata representation with inconsistent parameters due to the deficiency of\nsupervision signals. In this work, we propose FedU2 which enhances generating\nuniform and unified representation in FUSL with non-IID data. Specifically,\nFedU2 consists of flexible uniform regularizer (FUR) and efficient unified\naggregator (EUA). FUR in each client avoids representation collapse via\ndispersing samples uniformly, and EUA in server promotes unified representation\nby constraining consistent client model updating. To extensively validate the\nperformance of FedU2, we conduct both cross-device and cross-silo evaluation\nexperiments on two benchmark datasets, i.e., CIFAR10 and CIFAR100.\n","authors":["Xinting Liao","Weiming Liu","Chaochao Chen","Pengyang Zhou","Fengyuan Yu","Huabin Zhu","Binhui Yao","Tao Wang","Xiaolin Zheng","Yanchao Tan"],"pdf_url":"https://arxiv.org/pdf/2403.16398v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2309.02836v2","updated":"2024-03-25T03:17:30Z","published":"2023-09-06T08:48:03Z","title":"BigVSAN: Enhancing GAN-based Neural Vocoders with Slicing Adversarial\n  Network","summary":"  Generative adversarial network (GAN)-based vocoders have been intensively\nstudied because they can synthesize high-fidelity audio waveforms faster than\nreal-time. However, it has been reported that most GANs fail to obtain the\noptimal projection for discriminating between real and fake data in the feature\nspace. In the literature, it has been demonstrated that slicing adversarial\nnetwork (SAN), an improved GAN training framework that can find the optimal\nprojection, is effective in the image generation task. In this paper, we\ninvestigate the effectiveness of SAN in the vocoding task. For this purpose, we\npropose a scheme to modify least-squares GAN, which most GAN-based vocoders\nadopt, so that their loss functions satisfy the requirements of SAN. Through\nour experiments, we demonstrate that SAN can improve the performance of\nGAN-based vocoders, including BigVGAN, with small modifications. Our code is\navailable at https://github.com/sony/bigvsan.\n","authors":["Takashi Shibuya","Yuhta Takida","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2309.02836v2.pdf","comment":"Accepted at ICASSP 2024. Equation (5) in the previous version is\n  wrong. We modified it"},{"id":"http://arxiv.org/abs/2403.16393v1","updated":"2024-03-25T03:17:27Z","published":"2024-03-25T03:17:27Z","title":"Concurrent Linguistic Error Detection (CLED) for Large Language Models","summary":"  The wide adoption of Large language models (LLMs) makes their dependability a\npressing concern. Detection of errors is the first step to mitigating their\nimpact on a system and thus, efficient error detection for LLMs is an important\nissue. In many settings, the LLM is considered as a black box with no access to\nthe internal nodes; this prevents the use of many error detection schemes that\nneed access to the model's internal nodes. An interesting observation is that\nthe output of LLMs in error-free operation should be valid and normal text.\nTherefore, when the text is not valid or differs significantly from normal\ntext, it is likely that there is an error. Based on this observation we propose\nto perform Concurrent Linguistic Error Detection (CLED); this scheme extracts\nsome linguistic features of the text generated by the LLM and feeds them to a\nconcurrent classifier that detects errors. Since the proposed error detection\nmechanism only relies on the outputs of the model, then it can be used on LLMs\nin which there is no access to the internal nodes. The proposed CLED scheme has\nbeen evaluated on the T5 model when used for news summarization and on the\nOPUS-MT model when used for translation. In both cases, the same set of\nlinguistic features has been used for error detection to illustrate the\napplicability of the proposed scheme beyond a specific case. The results show\nthat CLED can detect most of the errors at a low overhead penalty. The use of\nthe concurrent classifier also enables a trade-off between error detection\neffectiveness and its associated overhead, so providing flexibility to a\ndesigner.\n","authors":["Jinhua Zhu","Javier Conde","Zhen Gao","Pedro Reviriego","Shanshan Liu","Fabrizio Lombardi"],"pdf_url":"https://arxiv.org/pdf/2403.16393v1.pdf","comment":"11 pages, 6 figures, 30 references"},{"id":"http://arxiv.org/abs/2403.16391v1","updated":"2024-03-25T03:13:56Z","published":"2024-03-25T03:13:56Z","title":"Physics-informed RL for Maximal Safety Probability Estimation","summary":"  Accurate risk quantification and reachability analysis are crucial for safe\ncontrol and learning, but sampling from rare events, risky states, or long-term\ntrajectories can be prohibitively costly. Motivated by this, we study how to\nestimate the long-term safety probability of maximally safe actions without\nsufficient coverage of samples from risky states and long-term trajectories.\nThe use of maximal safety probability in control and learning is expected to\navoid conservative behaviors due to over-approximation of risk. Here, we first\nshow that long-term safety probability, which is multiplicative in time, can be\nconverted into additive costs and be solved using standard reinforcement\nlearning methods. We then derive this probability as solutions of partial\ndifferential equations (PDEs) and propose Physics-Informed Reinforcement\nLearning (PIRL) algorithm. The proposed method can learn using sparse rewards\nbecause the physics constraints help propagate risk information through\nneighbors. This suggests that, for the purpose of extracting more information\nfor efficient learning, physics constraints can serve as an alternative to\nreward shaping. The proposed method can also estimate long-term risk using\nshort-term samples and deduce the risk of unsampled states. This feature is in\nstark contrast with the unconstrained deep RL that demands sufficient data\ncoverage. These merits of the proposed method are demonstrated in numerical\nsimulation.\n","authors":["Hikaru Hoshino","Yorie Nakahira"],"pdf_url":"https://arxiv.org/pdf/2403.16391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15365v2","updated":"2024-03-25T03:06:08Z","published":"2024-03-22T17:33:11Z","title":"A Transfer Attack to Image Watermarks","summary":"  Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.\n","authors":["Yuepeng Hu","Zhengyuan Jiang","Moyang Guo","Neil Gong"],"pdf_url":"https://arxiv.org/pdf/2403.15365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16377v1","updated":"2024-03-25T02:47:29Z","published":"2024-03-25T02:47:29Z","title":"Real-time Adaptation for Condition Monitoring Signal Prediction using\n  Label-aware Neural Processes","summary":"  Building a predictive model that rapidly adapts to real-time condition\nmonitoring (CM) signals is critical for engineering systems/units.\nUnfortunately, many current methods suffer from a trade-off between\nrepresentation power and agility in online settings. For instance, parametric\nmethods that assume an underlying functional form for CM signals facilitate\nefficient online prediction updates. However, this simplification leads to\nvulnerability to model specifications and an inability to capture complex\nsignals. On the other hand, approaches based on over-parameterized or\nnon-parametric models can excel at explaining complex nonlinear signals, but\nreal-time updates for such models pose a challenging task. In this paper, we\npropose a neural process-based approach that addresses this trade-off. It\nencodes available observations within a CM signal into a representation space\nand then reconstructs the signal's history and evolution for prediction. Once\ntrained, the model can encode an arbitrary number of observations without\nrequiring retraining, enabling on-the-spot real-time predictions along with\nquantified uncertainty and can be readily updated as more online data is\ngathered. Furthermore, our model is designed to incorporate qualitative\ninformation (i.e., labels) from individual units. This integration not only\nenhances individualized predictions for each unit but also enables joint\ninference for both signals and their associated labels. Numerical studies on\nboth synthetic and real-world data in reliability engineering highlight the\nadvantageous features of our model in real-time adaptation, enhanced signal\nprediction with uncertainty quantification, and joint prediction for labels and\nsignals.\n","authors":["Seokhyun Chung","Raed Al Kontar"],"pdf_url":"https://arxiv.org/pdf/2403.16377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09506v2","updated":"2024-03-25T02:45:35Z","published":"2024-03-14T15:53:04Z","title":"Don't Judge by the Look: Towards Motion Coherent Video Representation","summary":"  Current training pipelines in object recognition neglect Hue Jittering when\ndoing data augmentation as it not only brings appearance changes that are\ndetrimental to classification, but also the implementation is inefficient in\npractice. In this study, we investigate the effect of hue variance in the\ncontext of video understanding and find this variance to be beneficial since\nstatic appearances are less important in videos that contain motion\ninformation. Based on this observation, we propose a data augmentation method\nfor video understanding, named Motion Coherent Augmentation (MCA), that\nintroduces appearance variation in videos and implicitly encourages the model\nto prioritize motion patterns, rather than static appearances. Concretely, we\npropose an operation SwapMix to efficiently modify the appearance of video\nsamples, and introduce Variation Alignment (VA) to resolve the distribution\nshift caused by SwapMix, enforcing the model to learn appearance invariant\nrepresentations. Comprehensive empirical evaluation across various\narchitectures and different datasets solidly validates the effectiveness and\ngeneralization ability of MCA, and the application of VA in other augmentation\nmethods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.\n","authors":["Yitian Zhang","Yue Bai","Huan Wang","Yizhou Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2403.09506v2.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2403.16374v1","updated":"2024-03-25T02:38:34Z","published":"2024-03-25T02:38:34Z","title":"ProIn: Learning to Predict Trajectory Based on Progressive Interactions\n  for Autonomous Driving","summary":"  Accurate motion prediction of pedestrians, cyclists, and other surrounding\nvehicles (all called agents) is very important for autonomous driving. Most\nexisting works capture map information through an one-stage interaction with\nmap by vector-based attention, to provide map constraints for social\ninteraction and multi-modal differentiation. However, these methods have to\nencode all required map rules into the focal agent's feature, so as to retain\nall possible intentions' paths while at the meantime to adapt to potential\nsocial interaction. In this work, a progressive interaction network is proposed\nto enable the agent's feature to progressively focus on relevant maps, in order\nto better learn agents' feature representation capturing the relevant map\nconstraints. The network progressively encode the complex influence of map\nconstraints into the agent's feature through graph convolutions at the\nfollowing three stages: after historical trajectory encoder, after social\ninteraction, and after multi-modal differentiation. In addition, a weight\nallocation mechanism is proposed for multi-modal training, so that each mode\ncan obtain learning opportunities from a single-mode ground truth. Experiments\nhave validated the superiority of progressive interactions to the existing\none-stage interaction, and demonstrate the effectiveness of each component.\nEncouraging results were obtained in the challenging benchmarks.\n","authors":["Yinke Dong","Haifeng Yuan","Hongkun Liu","Wei Jing","Fangzhen Li","Hongmin Liu","Bin Fan"],"pdf_url":"https://arxiv.org/pdf/2403.16374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.10438v4","updated":"2024-03-25T02:37:09Z","published":"2022-04-21T23:59:17Z","title":"EVOTER: Evolution of Transparent Explainable Rule-sets","summary":"  Most AI systems are black boxes generating reasonable outputs for given\ninputs. Some domains, however, have explainability and trustworthiness\nrequirements that cannot be directly met by these approaches. Various methods\nhave therefore been developed to interpret black-box models after training.\nThis paper advocates an alternative approach where the models are transparent\nand explainable to begin with. This approach, EVOTER, evolves rule-sets based\non simple logical expressions. The approach is evaluated in several\nprediction/classification and prescription/policy search domains with and\nwithout a surrogate. It is shown to discover meaningful rule sets that perform\nsimilarly to black-box models. The rules can provide insight into the domain,\nand make biases hidden in the data explicit. It may also be possible to edit\nthem directly to remove biases and add constraints. EVOTER thus forms a\npromising foundation for building trustworthy AI systems for real-world\napplications in the future.\n","authors":["Hormoz Shahrzad","Babak Hodjat","Risto Miikkulainen"],"pdf_url":"https://arxiv.org/pdf/2204.10438v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16372v1","updated":"2024-03-25T02:32:43Z","published":"2024-03-25T02:32:43Z","title":"SignSGD with Federated Voting","summary":"  Distributed learning is commonly used for accelerating model training by\nharnessing the computational capabilities of multiple-edge devices. However, in\npractical applications, the communication delay emerges as a bottleneck due to\nthe substantial information exchange required between workers and a central\nparameter server. SignSGD with majority voting (signSGD-MV) is an effective\ndistributed learning algorithm that can significantly reduce communication\ncosts by one-bit quantization. However, due to heterogeneous computational\ncapabilities, it fails to converge when the mini-batch sizes differ among\nworkers. To overcome this, we propose a novel signSGD optimizer with\n\\textit{federated voting} (signSGD-FV). The idea of federated voting is to\nexploit learnable weights to perform weighted majority voting. The server\nlearns the weights assigned to the edge devices in an online fashion based on\ntheir computational capabilities. Subsequently, these weights are employed to\ndecode the signs of the aggregated local gradients in such a way to minimize\nthe sign decoding error probability. We provide a unified convergence rate\nanalysis framework applicable to scenarios where the estimated weights are\nknown to the parameter server either perfectly or imperfectly. We demonstrate\nthat the proposed signSGD-FV algorithm has a theoretical convergence guarantee\neven when edge devices use heterogeneous mini-batch sizes. Experimental results\nshow that signSGD-FV outperforms signSGD-MV, exhibiting a faster convergence\nrate, especially in heterogeneous mini-batch sizes.\n","authors":["Chanho Park","H. Vincent Poor","Namyoon Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16369v1","updated":"2024-03-25T02:17:54Z","published":"2024-03-25T02:17:54Z","title":"Learning Action-based Representations Using Invariance","summary":"  Robust reinforcement learning agents using high-dimensional observations must\nbe able to identify relevant state features amidst many exogeneous distractors.\nA representation that captures controllability identifies these state elements\nby determining what affects agent control. While methods such as inverse\ndynamics and mutual information capture controllability for a limited number of\ntimesteps, capturing long-horizon elements remains a challenging problem.\nMyopic controllability can capture the moment right before an agent crashes\ninto a wall, but not the control-relevance of the wall while the agent is still\nsome distance away. To address this we introduce action-bisimulation encoding,\na method inspired by the bisimulation invariance pseudometric, that extends\nsingle-step controllability with a recursive invariance constraint. By doing\nthis, action-bisimulation learns a multi-step controllability metric that\nsmoothly discounts distant state features that are relevant for control. We\ndemonstrate that action-bisimulation pretraining on reward-free, uniformly\nrandom data improves sample efficiency in several environments, including a\nphotorealistic 3D simulation domain, Habitat. Additionally, we provide\ntheoretical analysis and qualitative results demonstrating the information\ncaptured by action-bisimulation.\n","authors":["Max Rudolph","Caleb Chuck","Kevin Black","Misha Lvovsky","Scott Niekum","Amy Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15048v2","updated":"2024-03-25T02:08:01Z","published":"2024-03-22T09:13:09Z","title":"Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning","summary":"  Large-scale Text-to-Image (TTI) models have become a common approach for\ngenerating training data in various generative fields. However, visual\nhallucinations, which contain perceptually critical defects, remain a concern,\nespecially in non-photorealistic styles like cartoon characters. We propose a\nnovel visual hallucination detection system for cartoon character images\ngenerated by TTI models. Our approach leverages pose-aware in-context visual\nlearning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB\nimages and pose information. By incorporating pose guidance from a fine-tuned\npose estimator, we enable VLMs to make more accurate decisions. Experimental\nresults demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images. This\nresearch advances TTI models by mitigating visual hallucinations, expanding\ntheir potential in non-photorealistic domains.\n","authors":["Bumsoo Kim","Wonseop Shin","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2403.15048v2.pdf","comment":"11 pages, 12 figures, 1 table, Project page:\n  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/"},{"id":"http://arxiv.org/abs/2403.16365v1","updated":"2024-03-25T02:03:38Z","published":"2024-03-25T02:03:38Z","title":"Generating Potent Poisons and Backdoors from Scratch with Guided\n  Diffusion","summary":"  Modern neural networks are often trained on massive datasets that are web\nscraped with minimal human inspection. As a result of this insecure curation\npipeline, an adversary can poison or backdoor the resulting model by uploading\nmalicious data to the internet and waiting for a victim to scrape and train on\nit. Existing approaches for creating poisons and backdoors start with randomly\nsampled clean data, called base samples, and then modify those samples to craft\npoisons. However, some base samples may be significantly more amenable to\npoisoning than others. As a result, we may be able to craft more potent poisons\nby carefully choosing the base samples. In this work, we use guided diffusion\nto synthesize base samples from scratch that lead to significantly more potent\npoisons and backdoors than previous state-of-the-art attacks. Our Guided\nDiffusion Poisoning (GDP) base samples can be combined with any downstream\npoisoning or backdoor attack to boost its effectiveness. Our implementation\ncode is publicly available at: https://github.com/hsouri/GDP .\n","authors":["Hossein Souri","Arpit Bansal","Hamid Kazemi","Liam Fowl","Aniruddha Saha","Jonas Geiping","Andrew Gordon Wilson","Rama Chellappa","Tom Goldstein","Micah Goldblum"],"pdf_url":"https://arxiv.org/pdf/2403.16365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01391v2","updated":"2024-03-25T01:55:36Z","published":"2023-04-03T21:42:42Z","title":"Counterfactual Learning on Graphs: A Survey","summary":"  Graph-structured data are pervasive in the real-world such as social\nnetworks, molecular graphs and transaction networks. Graph neural networks\n(GNNs) have achieved great success in representation learning on graphs,\nfacilitating various downstream tasks. However, GNNs have several drawbacks\nsuch as lacking interpretability, can easily inherit the bias of data and\ncannot model casual relations. Recently, counterfactual learning on graphs has\nshown promising results in alleviating these drawbacks. Various approaches have\nbeen proposed for counterfactual fairness, explainability, link prediction and\nother applications on graphs. To facilitate the development of this promising\ndirection, in this survey, we categorize and comprehensively review papers on\ngraph counterfactual learning. We divide existing methods into four categories\nbased on problems studied. For each category, we provide background and\nmotivating examples, a general framework summarizing existing works and a\ndetailed review of these works. We point out promising future research\ndirections at the intersection of graph-structured data, counterfactual\nlearning, and real-world applications. To offer a comprehensive view of\nresources for future studies, we compile a collection of open-source\nimplementations, public datasets, and commonly-used evaluation metrics. This\nsurvey aims to serve as a ``one-stop-shop'' for building a unified\nunderstanding of graph counterfactual learning categories and current\nresources. We also maintain a repository for papers and resources and will keep\nupdating the repository\nhttps://github.com/TimeLovercc/Awesome-Graph-Causal-Learning.\n","authors":["Zhimeng Guo","Teng Xiao","Zongyu Wu","Charu Aggarwal","Hui Liu","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2304.01391v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16354v1","updated":"2024-03-25T01:12:57Z","published":"2024-03-25T01:12:57Z","title":"ChatDBG: An AI-Powered Debugging Assistant","summary":"  This paper presents ChatDBG, the first AI-powered debugging assistant.\nChatDBG integrates large language models (LLMs) to significantly enhance the\ncapabilities and user-friendliness of conventional debuggers. ChatDBG lets\nprogrammers engage in a collaborative dialogue with the debugger, allowing them\nto pose complex questions about program state, perform root cause analysis for\ncrashes or assertion failures, and explore open-ended queries like \"why is x\nnull?\". To handle these queries, ChatDBG grants the LLM autonomy to take the\nwheel and drive debugging by issuing commands to navigate through stacks and\ninspect program state; it then reports its findings and yields back control to\nthe programmer. Our ChatDBG prototype integrates with standard debuggers\nincluding LLDB, GDB, and WinDBG for native code and Pdb for Python. Our\nevaluation across a diverse set of code, including C/C++ code with known bugs\nand a suite of Python code including standalone scripts and Jupyter notebooks,\ndemonstrates that ChatDBG can successfully analyze root causes, explain bugs,\nand generate accurate fixes for a wide range of real-world errors. For the\nPython programs, a single query led to an actionable bug fix 67% of the time;\none additional follow-up query increased the success rate to 85%. ChatDBG has\nseen rapid uptake; it has already been downloaded nearly 30,000 times.\n","authors":["Kyla Levin","Nicolas van Kempen","Emery D. Berger","Stephen N. Freund"],"pdf_url":"https://arxiv.org/pdf/2403.16354v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2310.00290v5","updated":"2024-03-25T01:07:23Z","published":"2023-09-30T07:46:47Z","title":"Universality of almost periodicity in bounded discrete time series","summary":"  We consider arbitrary bounded discrete time series. From its statistical\nfeature, without any use of the Fourier transform, we find an almost periodic\nfunction which suitably characterizes the corresponding time series.\n","authors":["Chikara Nakayama","Tsuyoshi Yoneda"],"pdf_url":"https://arxiv.org/pdf/2310.00290v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16347v1","updated":"2024-03-25T00:50:27Z","published":"2024-03-25T00:50:27Z","title":"ChatGPT Incorrectness Detection in Software Reviews","summary":"  We conducted a survey of 135 software engineering (SE) practitioners to\nunderstand how they use Generative AI-based chatbots like ChatGPT for SE tasks.\nWe find that they want to use ChatGPT for SE tasks like software library\nselection but often worry about the truthfulness of ChatGPT responses. We\ndeveloped a suite of techniques and a tool called CID (ChatGPT Incorrectness\nDetector) to automatically test and detect the incorrectness in ChatGPT\nresponses. CID is based on the iterative prompting to ChatGPT by asking it\ncontextually similar but textually divergent questions (using an approach that\nutilizes metamorphic relationships in texts). The underlying principle in CID\nis that for a given question, a response that is different from other responses\n(across multiple incarnations of the question) is likely an incorrect response.\nIn a benchmark study of library selection, we show that CID can detect\nincorrect responses from ChatGPT with an F1-score of 0.74 - 0.75.\n","authors":["Minaoar Hossain Tanzil","Junaed Younus Khan","Gias Uddin"],"pdf_url":"https://arxiv.org/pdf/2403.16347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06626v2","updated":"2024-03-25T00:45:30Z","published":"2023-01-16T22:30:53Z","title":"Masked Vector Quantization","summary":"  Generative models with discrete latent representations have recently\ndemonstrated an impressive ability to learn complex high-dimensional data\ndistributions. However, their performance relies on a long sequence of tokens\nper instance and a large number of codebook entries, resulting in long sampling\ntimes and considerable computation to fit the categorical posterior. To address\nthese issues, we propose the Masked Vector Quantization (MVQ) framework which\nincreases the representational capacity of each code vector by learning mask\nconfigurations via a stochastic winner-takes-all training regime called\nMultiple Hypothese Dropout (MH-Dropout). On ImageNet 64$\\times$64, MVQ reduces\nFID in existing vector quantization architectures by up to $68\\%$ at 2 tokens\nper instance and $57\\%$ at 5 tokens. These improvements widen as codebook\nentries is reduced and allows for $7\\textit{--}45\\times$ speed-up in token\nsampling during inference. As an additional benefit, we find that smaller\nlatent spaces lead to MVQ identifying transferable visual representations where\nmultiple can be smoothly combined.\n","authors":["David D. Nguyen","David Leibowitz","Surya Nepal","Salil S. Kanhere"],"pdf_url":"https://arxiv.org/pdf/2301.06626v2.pdf","comment":"A newer version of this manuscript was archived under 2312.11735"},{"id":"http://arxiv.org/abs/2403.16336v1","updated":"2024-03-25T00:21:34Z","published":"2024-03-25T00:21:34Z","title":"Predictive Inference in Multi-environment Scenarios","summary":"  We address the challenge of constructing valid confidence intervals and sets\nin problems of prediction across multiple environments. We investigate two\ntypes of coverage suitable for these problems, extending the jackknife and\nsplit-conformal methods to show how to obtain distribution-free coverage in\nsuch non-traditional, hierarchical data-generating scenarios. Our contributions\nalso include extensions for settings with non-real-valued responses and a\ntheory of consistency for predictive inference in these general problems. We\ndemonstrate a novel resizing method to adapt to problem difficulty, which\napplies both to existing approaches for predictive inference with hierarchical\ndata and the methods we develop; this reduces prediction set sizes using\nlimited information from the test environment, a key to the methods' practical\nperformance, which we evaluate through neurochemical sensing and species\nclassification datasets.\n","authors":["John C. Duchi","Suyash Gupta","Kuanhao Jiang","Pragya Sur"],"pdf_url":"https://arxiv.org/pdf/2403.16336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08774v2","updated":"2024-03-25T00:18:35Z","published":"2023-10-12T23:46:08Z","title":"PhyloGFN: Phylogenetic inference with generative flow networks","summary":"  Phylogenetics is a branch of computational biology that studies the\nevolutionary relationships among biological entities. Its long history and\nnumerous applications notwithstanding, inference of phylogenetic trees from\nsequence data remains challenging: the high complexity of tree space poses a\nsignificant obstacle for the current combinatorial and probabilistic\ntechniques. In this paper, we adopt the framework of generative flow networks\n(GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and\nBayesian phylogenetic inference. Because GFlowNets are well-suited for sampling\ncomplex combinatorial structures, they are a natural choice for exploring and\nsampling from the multimodal posterior distribution over tree topologies and\nevolutionary distances. We demonstrate that our amortized posterior sampler,\nPhyloGFN, produces diverse and high-quality evolutionary hypotheses on real\nbenchmark datasets. PhyloGFN is competitive with prior works in marginal\nlikelihood estimation and achieves a closer fit to the target distribution than\nstate-of-the-art variational inference methods. Our code is available at\nhttps://github.com/zmy1116/phylogfn.\n","authors":["Mingyang Zhou","Zichao Yan","Elliot Layne","Nikolay Malkin","Dinghuai Zhang","Moksh Jain","Mathieu Blanchette","Yoshua Bengio"],"pdf_url":"https://arxiv.org/pdf/2310.08774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16335v1","updated":"2024-03-25T00:17:43Z","published":"2024-03-25T00:17:43Z","title":"MEDDAP: Medical Dataset Enhancement via Diversified Augmentation\n  Pipeline","summary":"  The effectiveness of Deep Neural Networks (DNNs) heavily relies on the\nabundance and accuracy of available training data. However, collecting and\nannotating data on a large scale is often both costly and time-intensive,\nparticularly in medical cases where practitioners are already occupied with\ntheir duties. Moreover, ensuring that the model remains robust across various\nscenarios of image capture is crucial in medical domains, especially when\ndealing with ultrasound images that vary based on the settings of different\ndevices and the manual operation of the transducer. To address this challenge,\nwe introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion\n(SD) models to augment existing small datasets by automatically generating new\ninformative labeled samples. Pretrained checkpoints for SD are typically based\non natural images, and training them for medical images requires significant\nGPU resources due to their heavy parameters. To overcome this challenge, we\nintroduce USLoRA (Ultrasound Low-Rank Adaptation), a novel fine-tuning method\ntailored specifically for ultrasound applications. USLoRA allows for selective\nfine-tuning of weights within SD, requiring fewer than 0.1\\% of parameters\ncompared to fully fine-tuning only the UNet portion of SD. To enhance dataset\ndiversity, we incorporate different adjectives into the generation process\nprompts, thereby desensitizing the classifiers to intensity changes across\ndifferent images. This approach is inspired by clinicians' decision-making\nprocesses regarding breast tumors, where tumor shape often plays a more crucial\nrole than intensity. In conclusion, our pipeline not only outperforms\nclassifiers trained on the original dataset but also demonstrates superior\nperformance when encountering unseen datasets. The source code is available at\nhttps://github.com/yasamin-med/MEDDAP.\n","authors":["Yasamin Medghalchi","Niloufar Zakariaei","Arman Rahmim","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2403.16335v1.pdf","comment":"submitted to miccai 2024 submitted to miccai 2024 Submitted to\n  MICCAI-2024"},{"id":"http://arxiv.org/abs/2403.16334v1","updated":"2024-03-25T00:15:34Z","published":"2024-03-25T00:15:34Z","title":"Graphs Generalization under Distribution Shifts","summary":"  Traditional machine learning methods heavily rely on the independent and\nidentically distribution assumption, which imposes limitations when the test\ndistribution deviates from the training distribution. To address this crucial\nissue, out-of-distribution (OOD) generalization, which aims to achieve\nsatisfactory generalization performance when faced with unknown distribution\nshifts, has made a significant process. However, the OOD method for\ngraph-structured data currently lacks clarity and remains relatively unexplored\ndue to two primary challenges. Firstly, distribution shifts on graphs often\noccur simultaneously on node attributes and graph topology. Secondly, capturing\ninvariant information amidst diverse distribution shifts proves to be a\nformidable challenge. To overcome these obstacles, in this paper, we introduce\na novel framework, namely Graph Learning Invariant Domain genERation (GLIDER).\nThe goal is to (1) diversify variations across domains by modeling the\npotential seen or unseen variations of attribute distribution and topological\nstructure and (2) minimize the discrepancy of the variation in a representation\nspace where the target is to predict semantic labels. Extensive experiment\nresults indicate that our model outperforms baseline methods on node-level OOD\ngeneralization across domains in distribution shift on node features and\ntopological structures simultaneously.\n","authors":["Qin Tian","Wenjun Wang","Chen Zhao","Minglai Shao","Wang Zhang","Dong Li"],"pdf_url":"https://arxiv.org/pdf/2403.16334v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2304.02970v5","updated":"2024-03-25T08:50:42Z","published":"2023-04-06T09:54:06Z","title":"Unraveling Instance Associations: A Closer Look for Audio-Visual\n  Segmentation","summary":"  Audio-visual segmentation (AVS) is a challenging task that involves\naccurately segmenting sounding objects based on audio-visual cues. The\neffectiveness of audio-visual learning critically depends on achieving accurate\ncross-modal alignment between sound and visual objects. Successful audio-visual\nlearning requires two essential components: 1) a challenging dataset with\nhigh-quality pixel-level multi-class annotated images associated with audio\nfiles, and 2) a model that can establish strong links between audio information\nand its corresponding visual object. However, these requirements are only\npartially addressed by current methods, with training sets containing biased\naudio-visual data, and models that generalise poorly beyond this biased\ntraining set. In this work, we propose a new cost-effective strategy to build\nchallenging and relatively unbiased high-quality audio-visual segmentation\nbenchmarks. We also propose a new informative sample mining method for\naudio-visual supervised contrastive learning to leverage discriminative\ncontrastive samples to enforce cross-modal understanding. We show empirical\nresults that demonstrate the effectiveness of our benchmark. Furthermore,\nexperiments conducted on existing AVS datasets and on our new benchmark show\nthat our method achieves state-of-the-art (SOTA) segmentation accuracy.\n","authors":["Yuanhong Chen","Yuyuan Liu","Hu Wang","Fengbei Liu","Chong Wang","Helen Frazer","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2304.02970v5.pdf","comment":"Code is available at https://github.com/cyh-0/CAVP"},{"id":"http://arxiv.org/abs/2403.10066v2","updated":"2024-03-25T06:27:57Z","published":"2024-03-15T07:16:07Z","title":"Contrastive Pre-Training with Multi-View Fusion for No-Reference Point\n  Cloud Quality Assessment","summary":"  No-reference point cloud quality assessment (NR-PCQA) aims to automatically\nevaluate the perceptual quality of distorted point clouds without available\nreference, which have achieved tremendous improvements due to the utilization\nof deep neural networks. However, learning-based NR-PCQA methods suffer from\nthe scarcity of labeled data and usually perform suboptimally in terms of\ngeneralization. To solve the problem, we propose a novel contrastive\npre-training framework tailored for PCQA (CoPA), which enables the pre-trained\nmodel to learn quality-aware representations from unlabeled data. To obtain\nanchors in the representation space, we project point clouds with different\ndistortions into images and randomly mix their local patches to form mixed\nimages with multiple distortions. Utilizing the generated anchors, we constrain\nthe pre-training process via a quality-aware contrastive loss following the\nphilosophy that perceptual quality is closely related to both content and\ndistortion. Furthermore, in the model fine-tuning stage, we propose a\nsemantic-guided multi-view fusion module to effectively integrate the features\nof projected images from multiple perspectives. Extensive experiments show that\nour method outperforms the state-of-the-art PCQA methods on popular benchmarks.\nFurther investigations demonstrate that CoPA can also benefit existing\nlearning-based PCQA models.\n","authors":["Ziyu Shan","Yujie Zhang","Qi Yang","Haichen Yang","Yiling Xu","Jenq-Neng Hwang","Xiaozhong Xu","Shan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18107v2","updated":"2024-03-25T05:28:20Z","published":"2024-02-28T06:54:35Z","title":"Multimodal Interaction Modeling via Self-Supervised Multi-Task Learning\n  for Review Helpfulness Prediction","summary":"  In line with the latest research, the task of identifying helpful reviews\nfrom a vast pool of user-generated textual and visual data has become a\nprominent area of study. Effective modal representations are expected to\npossess two key attributes: consistency and differentiation. Current methods\ndesigned for Multimodal Review Helpfulness Prediction (MRHP) face limitations\nin capturing distinctive information due to their reliance on uniform\nmultimodal annotation. The process of adding varied multimodal annotations is\nnot only time-consuming but also labor-intensive. To tackle these challenges,\nwe propose an auto-generated scheme based on multi-task learning to generate\npseudo labels. This approach allows us to simultaneously train for the global\nmultimodal interaction task and the separate cross-modal interaction subtasks,\nenabling us to learn and leverage both consistency and differentiation\neffectively. Subsequently, experimental results validate the effectiveness of\npseudo labels, and our approach surpasses previous textual and multimodal\nbaseline models on two widely accessible benchmark datasets, providing a\nsolution to the MRHP problem.\n","authors":["HongLin Gong","Mengzhao Jia","Liqiang Jing"],"pdf_url":"https://arxiv.org/pdf/2402.18107v2.pdf","comment":"10 pages,4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.15048v2","updated":"2024-03-25T02:08:01Z","published":"2024-03-22T09:13:09Z","title":"Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning","summary":"  Large-scale Text-to-Image (TTI) models have become a common approach for\ngenerating training data in various generative fields. However, visual\nhallucinations, which contain perceptually critical defects, remain a concern,\nespecially in non-photorealistic styles like cartoon characters. We propose a\nnovel visual hallucination detection system for cartoon character images\ngenerated by TTI models. Our approach leverages pose-aware in-context visual\nlearning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB\nimages and pose information. By incorporating pose guidance from a fine-tuned\npose estimator, we enable VLMs to make more accurate decisions. Experimental\nresults demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images. This\nresearch advances TTI models by mitigating visual hallucinations, expanding\ntheir potential in non-photorealistic domains.\n","authors":["Bumsoo Kim","Wonseop Shin","Kyuchul Lee","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2403.15048v2.pdf","comment":"11 pages, 12 figures, 1 table, Project page:\n  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/"},{"id":"http://arxiv.org/abs/2308.09911v2","updated":"2024-03-25T01:54:41Z","published":"2023-08-19T05:34:13Z","title":"Noisy-Correspondence Learning for Text-to-Image Person Re-identification","summary":"  Text-to-image person re-identification (TIReID) is a compelling topic in the\ncross-modal community, which aims to retrieve the target person based on a\ntextual query. Although numerous TIReID methods have been proposed and achieved\npromising performance, they implicitly assume the training image-text pairs are\ncorrectly aligned, which is not always the case in real-world scenarios. In\npractice, the image-text pairs inevitably exist under-correlated or even\nfalse-correlated, a.k.a noisy correspondence (NC), due to the low quality of\nthe images and annotation errors. To address this problem, we propose a novel\nRobust Dual Embedding method (RDE) that can learn robust visual-semantic\nassociations even with NC. Specifically, RDE consists of two main components:\n1) A Confident Consensus Division (CCD) module that leverages the dual-grained\ndecisions of dual embedding modules to obtain a consensus set of clean training\ndata, which enables the model to learn correct and reliable visual-semantic\nassociations. 2) A Triplet Alignment Loss (TAL) relaxes the conventional\nTriplet Ranking loss with the hardest negative samples to a log-exponential\nupper bound over all negative ones, thus preventing the model collapse under NC\nand can also focus on hard-negative samples for promising performance. We\nconduct extensive experiments on three public benchmarks, namely CUHK-PEDES,\nICFG-PEDES, and RSTPReID, to evaluate the performance and robustness of our\nRDE. Our method achieves state-of-the-art results both with and without\nsynthetic noisy correspondences on all three datasets. Code is available at\nhttps://github.com/QinYang79/RDE.\n","authors":["Yang Qin","Yingke Chen","Dezhong Peng","Xi Peng","Joey Tianyi Zhou","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2308.09911v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17005v1","updated":"2024-03-25T17:59:40Z","published":"2024-03-25T17:59:40Z","title":"TRIP: Temporal Residual Learning with Image Noise Prior for\n  Image-to-Video Diffusion Models","summary":"  Recent advances in text-to-video generation have demonstrated the utility of\npowerful diffusion models. Nevertheless, the problem is not trivial when\nshaping diffusion models to animate static image (i.e., image-to-video\ngeneration). The difficulty originates from the aspect that the diffusion\nprocess of subsequent animated frames should not only preserve the faithful\nalignment with the given image but also pursue temporal coherence among\nadjacent frames. To alleviate this, we present TRIP, a new recipe of\nimage-to-video diffusion paradigm that pivots on image noise prior derived from\nstatic image to jointly trigger inter-frame relational reasoning and ease the\ncoherent temporal modeling via temporal residual learning. Technically, the\nimage noise prior is first attained through one-step backward diffusion process\nbased on both static image and noised video latent codes. Next, TRIP executes a\nresidual-like dual-path scheme for noise prediction: 1) a shortcut path that\ndirectly takes image noise prior as the reference noise of each frame to\namplify the alignment between the first frame and subsequent frames; 2) a\nresidual path that employs 3D-UNet over noised video and static image latent\ncodes to enable inter-frame relational reasoning, thereby easing the learning\nof the residual noise for each frame. Furthermore, both reference and residual\nnoise of each frame are dynamically merged via attention mechanism for final\nvideo generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT\ndatasets demonstrate the effectiveness of our TRIP for image-to-video\ngeneration. Please see our project page at https://trip-i2v.github.io/TRIP/.\n","authors":["Zhongwei Zhang","Fuchen Long","Yingwei Pan","Zhaofan Qiu","Ting Yao","Yang Cao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17005v1.pdf","comment":"CVPR 2024; Project page: https://trip-i2v.github.io/TRIP/"},{"id":"http://arxiv.org/abs/2403.17004v1","updated":"2024-03-25T17:59:35Z","published":"2024-03-25T17:59:35Z","title":"SD-DiT: Unleashing the Power of Self-supervised Discrimination in\n  Diffusion Transformer","summary":"  Diffusion Transformer (DiT) has emerged as the new trend of generative\ndiffusion models on image generation. In view of extremely slow convergence in\ntypical DiT, recent breakthroughs have been driven by mask strategy that\nsignificantly improves the training efficiency of DiT with additional\nintra-image contextual learning. Despite this progress, mask strategy still\nsuffers from two inherent limitations: (a) training-inference discrepancy and\n(b) fuzzy relations between mask reconstruction & generative diffusion process,\nresulting in sub-optimal training of DiT. In this work, we address these\nlimitations by novelly unleashing the self-supervised discrimination knowledge\nto boost DiT training. Technically, we frame our DiT in a teacher-student\nmanner. The teacher-student discriminative pairs are built on the diffusion\nnoises along the same Probability Flow Ordinary Differential Equation (PF-ODE).\nInstead of applying mask reconstruction loss over both DiT encoder and decoder,\nwe decouple DiT encoder and decoder to separately tackle discriminative and\ngenerative objectives. In particular, by encoding discriminative pairs with\nstudent and teacher DiT encoders, a new discriminative loss is designed to\nencourage the inter-image alignment in the self-supervised embedding space.\nAfter that, student samples are fed into student DiT decoder to perform the\ntypical generative diffusion task. Extensive experiments are conducted on\nImageNet dataset, and our method achieves a competitive balance between\ntraining cost and generative capacity.\n","authors":["Rui Zhu","Yingwei Pan","Yehao Li","Ting Yao","Zhenglong Sun","Tao Mei","Chang Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17004v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17001v1","updated":"2024-03-25T17:59:31Z","published":"2024-03-25T17:59:31Z","title":"VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation","summary":"  Recent innovations on text-to-3D generation have featured Score Distillation\nSampling (SDS), which enables the zero-shot learning of implicit 3D models\n(NeRF) by directly distilling prior knowledge from 2D diffusion models.\nHowever, current SDS-based models still struggle with intricate text prompts\nand commonly result in distorted 3D models with unrealistic textures or\ncross-view inconsistency issues. In this work, we introduce a novel Visual\nPrompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the\nvisual appearance knowledge in 2D visual prompt to boost text-to-3D generation.\nInstead of solely supervising SDS with text prompt, VP3D first capitalizes on\n2D diffusion model to generate a high-quality image from input text, which\nsubsequently acts as visual prompt to strengthen SDS optimization with explicit\nvisual appearance. Meanwhile, we couple the SDS optimization with additional\ndifferentiable reward function that encourages rendering images of 3D models to\nbetter visually align with 2D visual prompt and semantically match with text\nprompt. Through extensive experiments, we show that the 2D Visual Prompt in our\nVP3D significantly eases the learning of visual appearance of 3D models and\nthus leads to higher visual fidelity with more detailed textures. It is also\nappealing in view that when replacing the self-generating visual prompt with a\ngiven reference image, VP3D is able to trigger a new task of stylized\ntext-to-3D generation. Our project page is available at\nhttps://vp3d-cvpr24.github.io.\n","authors":["Yang Chen","Yingwei Pan","Haibo Yang","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17001v1.pdf","comment":"CVPR 2024; Project page: https://vp3d-cvpr24.github.io"},{"id":"http://arxiv.org/abs/2403.17000v1","updated":"2024-03-25T17:59:26Z","published":"2024-03-25T17:59:26Z","title":"Learning Spatial Adaptation and Temporal Coherence in Diffusion Models\n  for Video Super-Resolution","summary":"  Diffusion models are just at a tipping point for image super-resolution task.\nNevertheless, it is not trivial to capitalize on diffusion models for video\nsuper-resolution which necessitates not only the preservation of visual\nappearance from low-resolution to high-resolution videos, but also the temporal\nconsistency across video frames. In this paper, we propose a novel approach,\npursuing Spatial Adaptation and Temporal Coherence (SATeCo), for video\nsuper-resolution. SATeCo pivots on learning spatial-temporal guidance from\nlow-resolution videos to calibrate both latent-space high-resolution video\ndenoising and pixel-space video reconstruction. Technically, SATeCo freezes all\nthe parameters of the pre-trained UNet and VAE, and only optimizes two\ndeliberately-designed spatial feature adaptation (SFA) and temporal feature\nalignment (TFA) modules, in the decoder of UNet and VAE. SFA modulates frame\nfeatures via adaptively estimating affine parameters for each pixel,\nguaranteeing pixel-wise guidance for high-resolution frame synthesis. TFA\ndelves into feature interaction within a 3D local window (tubelet) through\nself-attention, and executes cross-attention between tubelet and its\nlow-resolution counterpart to guide temporal feature alignment. Extensive\nexperiments conducted on the REDS4 and Vid4 datasets demonstrate the\neffectiveness of our approach.\n","authors":["Zhikai Chen","Fuchen Long","Zhaofan Qiu","Ting Yao","Wengang Zhou","Jiebo Luo","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17000v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16985v1","updated":"2024-03-25T17:46:51Z","published":"2024-03-25T17:46:51Z","title":"Towards Low-Latency and Energy-Efficient Hybrid P2P-CDN Live Video\n  Streaming","summary":"  Streaming segmented videos over the Hypertext Transfer Protocol (HTTP) is an\nincreasingly popular approach in both live and video-on-demand (VoD)\napplications. However, designing a scalable and adaptable framework that\nreduces servers energy consumption and supports low latency and high quality\nservices, particularly for live video streaming scenarios, is still challenging\nfor Over-The-Top (OTT) service providers. To address such challenges, this\npaper introduces a new hybrid P2P-CDN framework that leverages new networking\nand computing paradigms, i.e., Network Function Virtualization (NFV) and edge\ncomputing for live video streaming. The proposed framework introduces a\nmulti-layer architecture and a tree of possible actions therein (an action\ntree), taking into account all available resources from peers, edge, and CDN\nservers to efficiently distribute video fetching and transcoding tasks across a\nhybrid P2P-CDN network, consequently enhancing the users latency and video\nquality. We also discuss our testbed designed to validate the framework and\ncompare it with baseline methods. The experimental results indicate that the\nproposed framework improves user Quality of Experience (QoE), reduces client\nserving latency, and improves edge server energy consumption compared to\nbaseline approaches.\n","authors":["Reza Farahani","Christian Timmerer","Hermann Hellwagner"],"pdf_url":"https://arxiv.org/pdf/2403.16985v1.pdf","comment":"6 pages, 3 figures, Special Issue on Sustainable Multimedia\n  Communications and Services, IEEE MMTC Communications"},{"id":"http://arxiv.org/abs/2403.16951v1","updated":"2024-03-25T17:12:43Z","published":"2024-03-25T17:12:43Z","title":"Network-Assisted Delivery of Adaptive Video Streaming Services through\n  CDN, SDN, and MEC","summary":"  Multimedia applications, mainly video streaming services, are currently the\ndominant source of network load worldwide. In recent Video-on-Demand (VoD) and\nlive video streaming services, traditional streaming delivery techniques have\nbeen replaced by adaptive solutions based on the HTTP protocol. Current trends\ntoward high-resolution (e.g., 8K) and/or low-latency VoD and live video\nstreaming pose new challenges to end-to-end (E2E) bandwidth demand and have\nstringent delay requirements. To do this, video providers typically rely on\nContent Delivery Networks (CDNs) to ensure that they provide scalable video\nstreaming services. To support future streaming scenarios involving millions of\nusers, it is necessary to increase the CDNs' efficiency. It is widely agreed\nthat these requirements may be satisfied by adopting emerging networking\ntechniques to present Network-Assisted Video Streaming (NAVS) methods.\nMotivated by this, this thesis goes one step beyond traditional pure\nclient-based HAS algorithms by incorporating (an) in-network component(s) with\na broader view of the network to present completely transparent NAVS solutions\nfor HAS clients.\n","authors":["Reza Farahani"],"pdf_url":"https://arxiv.org/pdf/2403.16951v1.pdf","comment":"PhD thesis defended in 22.08.2023\n  (https://netlibrary.aau.at/obvuklhs/content/titleinfo/9173622)"}]},"2024-03-24T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2311.01623v3","updated":"2024-03-24T23:13:06Z","published":"2023-11-03T16:58:10Z","title":"VQPy: An Object-Oriented Approach to Modern Video Analytics","summary":"  Video analytics is widely used in contemporary systems and services. At the\nforefront of video analytics are video queries that users develop to find\nobjects of particular interest. Building upon the insight that video objects\n(e.g., human, animals, cars, etc.), the center of video analytics, are similar\nin spirit to objects modeled by traditional object-oriented languages, we\npropose to develop an object-oriented approach to video analytics. This\napproach, named VQPy, consists of a frontend$\\unicode{x2015}$a Python variant\nwith constructs that make it easy for users to express video objects and their\ninteractions$\\unicode{x2015}$as well as an extensible backend that can\nautomatically construct and optimize pipelines based on video objects. We have\nimplemented and open-sourced VQPy, which has been productized in Cisco as part\nof its DeepVision framework.\n","authors":["Shan Yu","Zhenting Zhu","Yu Chen","Hanchen Xu","Pengzhan Zhao","Yang Wang","Arthi Padmanabhan","Hugo Latapie","Harry Xu"],"pdf_url":"https://arxiv.org/pdf/2311.01623v3.pdf","comment":"MLSys'24"},{"id":"http://arxiv.org/abs/2311.11202v2","updated":"2024-03-24T22:02:47Z","published":"2023-11-19T02:34:12Z","title":"Unmasking and Improving Data Credibility: A Study with Datasets for\n  Training Harmless Language Models","summary":"  Language models have shown promise in various tasks but can be affected by\nundesired data during training, fine-tuning, or alignment. For example, if some\nunsafe conversations are wrongly annotated as safe ones, the model fine-tuned\non these samples may be harmful. Therefore, the correctness of annotations,\ni.e., the credibility of the dataset, is important. This study focuses on the\ncredibility of real-world datasets, including the popular benchmarks Jigsaw\nCivil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that\ncan be used for training a harmless language model. Given the cost and\ndifficulty of cleaning these datasets by humans, we introduce a systematic\nframework for evaluating the credibility of datasets, identifying label errors,\nand evaluating the influence of noisy labels in the curated language data,\nspecifically focusing on unsafe comments and conversation classification. With\nthe framework, we find and fix an average of 6.16% label errors in 11 datasets\nconstructed from the above benchmarks. The data credibility and downstream\nlearning performance can be remarkably improved by directly fixing label\nerrors, indicating the significance of cleaning existing real-world datasets.\nWe provide an open-source tool, Docta, for data cleaning at\nhttps://github.com/Docta-ai/docta.\n","authors":["Zhaowei Zhu","Jialu Wang","Hao Cheng","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.11202v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16303v1","updated":"2024-03-24T21:29:39Z","published":"2024-03-24T21:29:39Z","title":"Large Language Models in Biomedical and Health Informatics: A\n  Bibliometric Review","summary":"  Large Language Models (LLMs) have rapidly become important tools in\nBiomedical and Health Informatics (BHI), enabling new ways to analyze data,\ntreat patients, and conduct research. This bibliometric review aims to provide\na panoramic view of how LLMs have been used in BHI by examining research\narticles and collaboration networks from 2022 to 2023. It further explores how\nLLMs can improve Natural Language Processing (NLP) applications in various BHI\nareas like medical diagnosis, patient engagement, electronic health record\nmanagement, and personalized medicine. To do this, our bibliometric review\nidentifies key trends, maps out research networks, and highlights major\ndevelopments in this fast-moving field. Lastly, it discusses the ethical\nconcerns and practical challenges of using LLMs in BHI, such as data privacy\nand reliable medical recommendations. Looking ahead, we consider how LLMs could\nfurther transform biomedical research as well as healthcare delivery and\npatient outcomes. This comprehensive review serves as a resource for\nstakeholders in healthcare, including researchers, clinicians, and\npolicymakers, to understand the current state and future potential of LLMs in\nBHI.\n","authors":["Huizi Yu","Lizhou Fan","Lingyao Li","Jiayan Zhou","Zihui Ma","Lu Xian","Wenyue Hua","Sijia He","Mingyu Jin","Yongfeng Zhang","Ashvin Gandhi","Xin Ma"],"pdf_url":"https://arxiv.org/pdf/2403.16303v1.pdf","comment":"50 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.16295v1","updated":"2024-03-24T21:02:35Z","published":"2024-03-24T21:02:35Z","title":"LexDrafter: Terminology Drafting for Legislative Documents using\n  Retrieval Augmented Generation","summary":"  With the increase in legislative documents at the EU, the number of new terms\nand their definitions is increasing as well. As per the Joint Practical Guide\nof the European Parliament, the Council and the Commission, terms used in legal\ndocuments shall be consistent, and identical concepts shall be expressed\nwithout departing from their meaning in ordinary, legal, or technical language.\nThus, while drafting a new legislative document, having a framework that\nprovides insights about existing definitions and helps define new terms based\non a document's context will support such harmonized legal definitions across\ndifferent regulations and thus avoid ambiguities. In this paper, we present\nLexDrafter, a framework that assists in drafting Definitions articles for\nlegislative documents using retrieval augmented generation (RAG) and existing\nterm definitions present in different legislative documents. For this,\ndefinition elements are built by extracting definitions from existing\ndocuments. Using definition elements and RAG, a Definitions article can be\nsuggested on demand for a legislative document that is being drafted. We\ndemonstrate and evaluate the functionality of LexDrafter using a collection of\nEU documents from the energy domain. The code for LexDrafter framework is\navailable at https://github.com/achouhan93/LexDrafter.\n","authors":["Ashish Chouhan","Michael Gertz"],"pdf_url":"https://arxiv.org/pdf/2403.16295v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2304.02017v7","updated":"2024-03-24T19:15:22Z","published":"2023-03-27T21:27:58Z","title":"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its\n  Applications, Advantages, Limitations, and Future Directions in Natural\n  Language Processing","summary":"  Large language models have revolutionized the field of artificial\nintelligence and have been used in various applications. Among these models,\nChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,\nit stands out as a powerful tool that has been widely adopted. ChatGPT has been\nsuccessfully applied in numerous areas, including chatbots, content generation,\nlanguage translation, personalized recommendations, and even medical diagnosis\nand treatment. Its success in these applications can be attributed to its\nability to generate human-like responses, understand natural language, and\nadapt to different contexts. Its versatility and accuracy make it a powerful\ntool for natural language processing (NLP). However, there are also limitations\nto ChatGPT, such as its tendency to produce biased responses and its potential\nto perpetuate harmful language patterns. This article provides a comprehensive\noverview of ChatGPT, its applications, advantages, and limitations.\nAdditionally, the paper emphasizes the importance of ethical considerations\nwhen using this robust tool in real-world scenarios. Finally, This paper\ncontributes to ongoing discussions surrounding artificial intelligence and its\nimpact on vision and NLP domains by providing insights into prompt engineering\ntechniques.\n","authors":["Walid Hariri"],"pdf_url":"https://arxiv.org/pdf/2304.02017v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16265v1","updated":"2024-03-24T18:59:38Z","published":"2024-03-24T18:59:38Z","title":"Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved\n  Phrase Graphs","summary":"  We study the patent phrase similarity inference task, which measures the\nsemantic similarity between two patent phrases. As patent documents employ\nlegal and highly technical language, existing semantic textual similarity\nmethods that use localized contextual information do not perform satisfactorily\nin inferring patent phrase similarity. To address this, we introduce a\ngraph-augmented approach to amplify the global contextual information of the\npatent phrases. For each patent phrase, we construct a phrase graph that links\nto its focal patents and a list of patents that are either cited by or cite\nthese focal patents. The augmented phrase embedding is then derived from\ncombining its localized contextual embedding with its global embedding within\nthe phrase graph. We further propose a self-supervised learning objective that\ncapitalizes on the retrieved topology to refine both the contextualized\nembedding and the graph parameters in an end-to-end manner. Experimental\nresults from a unique patent phrase similarity dataset demonstrate that our\napproach significantly enhances the representation of patent phrases, resulting\nin marked improvements in similarity inference in a self-supervised fashion.\nSubstantial improvements are also observed in the supervised setting,\nunderscoring the potential benefits of leveraging retrieved phrase graph\naugmentation.\n","authors":["Zhuoyi Peng","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.16265v1.pdf","comment":"Findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2305.14310v3","updated":"2024-03-24T18:03:10Z","published":"2023-05-23T17:48:21Z","title":"Navigating Prompt Complexity for Zero-Shot Classification: A Study of\n  Large Language Models in Computational Social Science","summary":"  Instruction-tuned Large Language Models (LLMs) have exhibited impressive\nlanguage understanding and the capacity to generate responses that follow\nspecific prompts. However, due to the computational demands associated with\ntraining these models, their applications often adopt a zero-shot setting. In\nthis paper, we evaluate the zero-shot performance of two publicly accessible\nLLMs, ChatGPT and OpenAssistant, in the context of six Computational Social\nScience classification tasks, while also investigating the effects of various\nprompting strategies. Our experiments investigate the impact of prompt\ncomplexity, including the effect of incorporating label definitions into the\nprompt; use of synonyms for label names; and the influence of integrating past\nmemories during foundation model training. The findings indicate that in a\nzero-shot setting, current LLMs are unable to match the performance of smaller,\nfine-tuned baseline transformer models (such as BERT-large). Additionally, we\nfind that different prompting strategies can significantly affect\nclassification accuracy, with variations in accuracy and F1 scores exceeding\n10\\%.\n","authors":["Yida Mu","Ben P. Wu","William Thorne","Ambrose Robinson","Nikolaos Aletras","Carolina Scarton","Kalina Bontcheva","Xingyi Song"],"pdf_url":"https://arxiv.org/pdf/2305.14310v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16248v1","updated":"2024-03-24T17:39:51Z","published":"2024-03-24T17:39:51Z","title":"Large Language Models Offer an Alternative to the Traditional Approach\n  of Topic Modelling","summary":"  Topic modelling, as a well-established unsupervised technique, has found\nextensive use in automatically detecting significant topics within a corpus of\ndocuments. However, classic topic modelling approaches (e.g., LDA) have certain\ndrawbacks, such as the lack of semantic understanding and the presence of\noverlapping topics. In this work, we investigate the untapped potential of\nlarge language models (LLMs) as an alternative for uncovering the underlying\ntopics within extensive text corpora. To this end, we introduce a framework\nthat prompts LLMs to generate topics from a given set of documents and\nestablish evaluation protocols to assess the clustering efficacy of LLMs. Our\nfindings indicate that LLMs with appropriate prompts can stand out as a viable\nalternative, capable of generating relevant topic titles and adhering to human\nguidelines to refine and merge topics. Through in-depth experiments and\nevaluation, we summarise the advantages and constraints of employing LLMs in\ntopic extraction.\n","authors":["Yida Mu","Chun Dong","Kalina Bontcheva","Xingyi Song"],"pdf_url":"https://arxiv.org/pdf/2403.16248v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16247v1","updated":"2024-03-24T17:39:36Z","published":"2024-03-24T17:39:36Z","title":"Improving Sequence-to-Sequence Models for Abstractive Text Summarization\n  Using Meta Heuristic Approaches","summary":"  As human society transitions into the information age, reduction in our\nattention span is a contingency, and people who spend time reading lengthy news\narticles are decreasing rapidly and the need for succinct information is higher\nthan ever before. Therefore, it is essential to provide a quick overview of\nimportant news by concisely summarizing the top news article and the most\nintuitive headline. When humans try to make summaries, they extract the\nessential information from the source and add useful phrases and grammatical\nannotations from the original extract. Humans have a unique ability to create\nabstractions. However, automatic summarization is a complicated problem to\nsolve. The use of sequence-to-sequence (seq2seq) models for neural abstractive\ntext summarization has been ascending as far as prevalence. Numerous innovative\nstrategies have been proposed to develop the current seq2seq models further,\npermitting them to handle different issues like saliency, familiarity, and\nhuman lucidness and create excellent synopses. In this article, we aimed toward\nenhancing the present architectures and models for abstractive text\nsummarization. The modifications have been aimed at fine-tuning\nhyper-parameters, attempting specific encoder-decoder combinations. We examined\nmany experiments on an extensively used CNN/DailyMail dataset to check the\neffectiveness of various models.\n","authors":["Aditya Saxena","Ashutosh Ranjan"],"pdf_url":"https://arxiv.org/pdf/2403.16247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14119v2","updated":"2024-03-24T17:16:53Z","published":"2024-03-21T04:08:29Z","title":"C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via\n  Text Feature Dispersion","summary":"  In deep learning, test-time adaptation has gained attention as a method for\nmodel fine-tuning without the need for labeled data. A prime exemplification is\nthe recently proposed test-time prompt tuning for large-scale vision-language\nmodels such as CLIP. Unfortunately, these prompts have been mainly developed to\nimprove accuracy, overlooking the importance of calibration, which is a crucial\naspect for quantifying prediction uncertainty. However, traditional calibration\nmethods rely on substantial amounts of labeled data, making them impractical\nfor test-time scenarios. To this end, this paper explores calibration during\ntest-time prompt tuning by leveraging the inherent properties of CLIP. Through\na series of observations, we find that the prompt choice significantly affects\nthe calibration in CLIP, where the prompts leading to higher text feature\ndispersion result in better-calibrated predictions. Introducing the Average\nText Feature Dispersion (ATFD), we establish its relationship with calibration\nerror and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),\nfor optimizing prompts during test-time with enhanced calibration. Through\nextensive experiments on different CLIP architectures and datasets, we show\nthat C-TPT can effectively improve the calibration of test-time prompt tuning\nwithout needing labeled data. The code is publicly accessible at\nhttps://github.com/hee-suk-yoon/C-TPT.\n","authors":["Hee Suk Yoon","Eunseop Yoon","Joshua Tian Jin Tee","Mark Hasegawa-Johnson","Yingzhen Li","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.14119v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2310.16450v3","updated":"2024-03-24T17:14:11Z","published":"2023-10-25T08:13:02Z","title":"CLEX: Continuous Length Extrapolation for Large Language Models","summary":"  Transformer-based Large Language Models (LLMs) are pioneering advances in\nmany natural language processing tasks, however, their exceptional capabilities\nare restricted within the preset context window of Transformer. Position\nEmbedding (PE) scaling methods, while effective in extending the context window\nto a specific length, demonstrate either notable limitations in their\nextrapolation abilities or sacrificing partial performance within the context\nwindow. Length extrapolation methods, although theoretically capable of\nextending the context window beyond the training sequence length, often\nunderperform in practical long-context applications. To address these\nchallenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We\ngeneralise the PE scaling approaches to model the continuous dynamics by\nordinary differential equations over the length scaling factor, thereby\novercoming the constraints of current PE scaling methods designed for specific\nlengths. Moreover, by extending the dynamics to desired context lengths beyond\nthe training sequence length, CLEX facilitates the length extrapolation with\nimpressive performance in practical tasks. We demonstrate that CLEX can be\nseamlessly incorporated into LLMs equipped with Rotary Position Embedding, such\nas LLaMA and GPT-NeoX, with negligible impact on training and inference\nlatency. Experimental results reveal that CLEX can effectively extend the\ncontext window to over 4x or almost 8x training length, with no deterioration\nin performance. Furthermore, when evaluated on the practical LongBench\nbenchmark, our model trained on a 4k length exhibits competitive performance\nagainst state-of-the-art open-source models trained on context lengths up to\n32k. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.\n","authors":["Guanzheng Chen","Xin Li","Zaiqiao Meng","Shangsong Liang","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2310.16450v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2304.04806v3","updated":"2024-03-24T16:46:06Z","published":"2023-04-10T18:31:26Z","title":"Examining Temporalities on Stance Detection towards COVID-19 Vaccination","summary":"  Previous studies have highlighted the importance of vaccination as an\neffective strategy to control the transmission of the COVID-19 virus. It is\ncrucial for policymakers to have a comprehensive understanding of the public's\nstance towards vaccination on a large scale. However, attitudes towards\nCOVID-19 vaccination, such as pro-vaccine or vaccine hesitancy, have evolved\nover time on social media. Thus, it is necessary to account for possible\ntemporal shifts when analysing these stances. This study aims to examine the\nimpact of temporal concept drift on stance detection towards COVID-19\nvaccination on Twitter. To this end, we evaluate a range of transformer-based\nmodels using chronological (splitting the training, validation, and test sets\nin order of time) and random splits (randomly splitting these three sets) of\nsocial media data. Our findings reveal significant discrepancies in model\nperformance between random and chronological splits in several existing\nCOVID-19-related datasets; specifically, chronological splits significantly\nreduce the accuracy of stance classification. Therefore, real-world stance\ndetection approaches need to be further refined to incorporate temporal factors\nas a key consideration.\n","authors":["Yida Mu","Mali Jin","Kalina Bontcheva","Xingyi Song"],"pdf_url":"https://arxiv.org/pdf/2304.04806v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16204v1","updated":"2024-03-24T15:57:24Z","published":"2024-03-24T15:57:24Z","title":"SQL-Encoder: Improving NL2SQL In-Context Learning Through a\n  Context-Aware Encoder","summary":"  Detecting structural similarity between queries is essential for selecting\nexamples in in-context learning models. However, assessing structural\nsimilarity based solely on the natural language expressions of queries, without\nconsidering SQL queries, presents a significant challenge. This paper explores\nthe significance of this similarity metric and proposes a model for accurately\nestimating it. To achieve this, we leverage a dataset comprising 170k question\npairs, meticulously curated to train a similarity prediction model. Our\ncomprehensive evaluation demonstrates that the proposed model adeptly captures\nthe structural similarity between questions, as evidenced by improvements in\nKendall-Tau distance and precision@k metrics. Notably, our model outperforms\nstrong competitive embedding models from OpenAI and Cohere. Furthermore,\ncompared to these competitive models, our proposed encoder enhances the\ndownstream performance of NL2SQL models in 1-shot in-context learning scenarios\nby 1-2\\% for GPT-3.5-turbo, 4-8\\% for CodeLlama-7B, and 2-3\\% for\nCodeLlama-13B.\n","authors":["Mohammadreza Pourreza","Davood Rafiei","Yuxi Feng","Raymond Li","Zhenan Fan","Weiwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16694v2","updated":"2024-03-24T15:41:21Z","published":"2024-02-26T16:09:00Z","title":"HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual\n  Natural Language Generalization","summary":"  Large language models (LLMs) have made significant progress in generating\ncodes from textual prompts. However, existing benchmarks have mainly\nconcentrated on translating English prompts to multilingual codes or have been\nconstrained to very limited natural languages (NLs). These benchmarks have\noverlooked the vast landscape of massively multilingual NL to multilingual\ncode, leaving a critical gap in the evaluation of multilingual LLMs. In\nresponse, we introduce HumanEval-XL, a massively multilingual code generation\nbenchmark specifically crafted to address this deficiency. HumanEval-XL\nestablishes connections between 23 NLs and 12 programming languages (PLs), and\ncomprises of a collection of 22,080 prompts with an average of 8.33 test cases.\nBy ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a\ncomprehensive evaluation platform for multilingual LLMs, allowing the\nassessment of the understanding of different NLs. Our work serves as a\npioneering step towards filling the void in evaluating NL generalization in the\narea of multilingual code generation. We make our evaluation code and data\npublicly available at \\url{https://github.com/FloatAI/humaneval-xl}.\n","authors":["Qiwei Peng","Yekun Chai","Xuhong Li"],"pdf_url":"https://arxiv.org/pdf/2402.16694v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16187v1","updated":"2024-03-24T15:09:55Z","published":"2024-03-24T15:09:55Z","title":"ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language\n  Models","summary":"  Parameter-efficient fine-tuning (PEFT) is widely studied for its\neffectiveness and efficiency in the era of large language models. Low-rank\nadaptation (LoRA) has demonstrated commendable performance as a popular and\nrepresentative method. However, it is implemented with a fixed intrinsic rank\nthat might not be the ideal setting for the downstream tasks. Recognizing the\nneed for more flexible downstream task adaptation, we extend the methodology of\nLoRA to an innovative approach we call allocating low-rank adaptation (ALoRA)\nthat enables dynamic adjustments to the intrinsic rank during the adaptation\nprocess. First, we propose a novel method, AB-LoRA, that can effectively\nestimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we\ngradually prune abundant and negatively impacting LoRA ranks and allocate the\npruned LoRA budgets to important Transformer modules needing higher ranks. We\nhave conducted experiments on various tasks, and the experimental results\ndemonstrate that our ALoRA method can outperform the recent baselines with\ncomparable tunable parameters.\n","authors":["Zequan Liu","Jiawen Lyn","Wei Zhu","Xing Tian","Yvette Graham"],"pdf_url":"https://arxiv.org/pdf/2403.16187v1.pdf","comment":"Accepted by NAACL-2024"},{"id":"http://arxiv.org/abs/2403.16176v1","updated":"2024-03-24T14:35:44Z","published":"2024-03-24T14:35:44Z","title":"Subspace Defense: Discarding Adversarial Perturbations by Learning a\n  Subspace for Clean Signals","summary":"  Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks\nthat place carefully crafted perturbations on normal examples to fool DNNs. To\nbetter understand such attacks, a characterization of the features carried by\nadversarial examples is needed. In this paper, we tackle this challenge by\ninspecting the subspaces of sample features through spectral analysis. We first\nempirically show that the features of either clean signals or adversarial\nperturbations are redundant and span in low-dimensional linear subspaces\nrespectively with minimal overlap, and the classical low-dimensional subspace\nprojection can suppress perturbation features out of the subspace of clean\nsignals. This makes it possible for DNNs to learn a subspace where only\nfeatures of clean signals exist while those of perturbations are discarded,\nwhich can facilitate the distinction of adversarial examples. To prevent the\nresidual perturbations that is inevitable in subspace learning, we propose an\nindependence criterion to disentangle clean signals from perturbations.\nExperimental results show that the proposed strategy enables the model to\ninherently suppress adversaries, which not only boosts model robustness but\nalso motivates new directions of effective adversarial defense.\n","authors":["Rui Zheng","Yuhao Zhou","Zhiheng Xi","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2403.16176v1.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2302.14534v2","updated":"2024-03-24T14:34:53Z","published":"2023-02-28T12:44:10Z","title":"Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face","summary":"  We present Spacerini, a tool that integrates the Pyserini toolkit for\nreproducible information retrieval research with Hugging Face to enable the\nseamless construction and deployment of interactive search engines. Spacerini\nmakes state-of-the-art sparse and dense retrieval models more accessible to\nnon-IR practitioners while minimizing deployment effort. This is useful for NLP\nresearchers who want to better understand and validate their research by\nperforming qualitative analyses of training corpora, for IR researchers who\nwant to demonstrate new retrieval models integrated into the growing Pyserini\necosystem, and for third parties reproducing the work of other researchers.\nSpacerini is open source and includes utilities for loading, preprocessing,\nindexing, and deploying search engines locally and remotely. We demonstrate a\nportfolio of 13 search engines created with Spacerini for different use cases.\n","authors":["Christopher Akiki","Odunayo Ogundepo","Aleksandra Piktus","Xinyu Zhang","Akintunde Oladipo","Jimmy Lin","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2302.14534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16167v1","updated":"2024-03-24T14:21:06Z","published":"2024-03-24T14:21:06Z","title":"Exploiting Semantic Reconstruction to Mitigate Hallucinations in\n  Vision-Language Models","summary":"  Hallucinations in vision-language models pose a significant challenge to\ntheir reliability, particularly in the generation of long captions. Current\nmethods fall short of accurately identifying and mitigating these\nhallucinations. To address this issue, we introduce ESREAL, a novel\nunsupervised learning framework designed to suppress the generation of\nhallucinations through accurate localization and penalization of hallucinated\ntokens. Initially, ESREAL creates a reconstructed image based on the generated\ncaption and aligns its corresponding regions with those of the original image.\nThis semantic reconstruction aids in identifying both the presence and type of\ntoken-level hallucinations within the generated caption. Subsequently, ESREAL\ncomputes token-level hallucination scores by assessing the semantic similarity\nof aligned regions based on the type of hallucination. Finally, ESREAL employs\na proximal policy optimization algorithm, where it selectively penalizes\nhallucinated tokens according to their token-level hallucination scores. Our\nframework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2\nby 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved\nsolely through signals derived from the image itself, without the need for any\nimage-text pairs.\n","authors":["Minchan Kim","Minyeong Kim","Junik Bae","Suhwan Choi","Sungkyung Kim","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2403.16167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16158v1","updated":"2024-03-24T13:51:05Z","published":"2024-03-24T13:51:05Z","title":"Korean Bio-Medical Corpus (KBMC) for Medical Named Entity Recognition","summary":"  Named Entity Recognition (NER) plays a pivotal role in medical Natural\nLanguage Processing (NLP). Yet, there has not been an open-source medical NER\ndataset specifically for the Korean language. To address this, we utilized\nChatGPT to assist in constructing the KBMC (Korean Bio-Medical Corpus), which\nwe are now presenting to the public. With the KBMC dataset, we noticed an\nimpressive 20% increase in medical NER performance compared to models trained\non general Korean NER datasets. This research underscores the significant\nbenefits and importance of using specialized tools and datasets, like ChatGPT,\nto enhance language processing in specialized fields such as healthcare.\n","authors":["Sungjoo Byun","Jiseung Hong","Sumin Park","Dongjun Jang","Jean Seo","Minseok Kim","Chaeyoung Oh","Hyopil Shin"],"pdf_url":"https://arxiv.org/pdf/2403.16158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02541v3","updated":"2024-03-24T13:39:45Z","published":"2023-04-05T16:03:42Z","title":"PWESuite: Phonetic Word Embeddings and Tasks They Facilitate","summary":"  Mapping words into a fixed-dimensional vector space is the backbone of modern\nNLP. While most word embedding methods successfully encode semantic\ninformation, they overlook phonetic information that is crucial for many tasks.\nWe develop three methods that use articulatory features to build phonetically\ninformed word embeddings. To address the inconsistent evaluation of existing\nphonetic word embedding methods, we also contribute a task suite to fairly\nevaluate past, current, and future methods. We evaluate both (1) intrinsic\naspects of phonetic word embeddings, such as word retrieval and correlation\nwith sound similarity, and (2) extrinsic performance on tasks such as rhyme and\ncognate detection and sound analogies. We hope our task suite will promote\nreproducibility and inspire future phonetic embedding research.\n","authors":["Vilém Zouhar","Kalvin Chang","Chenxuan Cui","Nathaniel Carlson","Nathaniel Robinson","Mrinmaya Sachan","David Mortensen"],"pdf_url":"https://arxiv.org/pdf/2304.02541v3.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.16142v1","updated":"2024-03-24T13:28:27Z","published":"2024-03-24T13:28:27Z","title":"What Happens to a Dataset Transformed by a Projection-based Concept\n  Removal Method?","summary":"  We investigate the behavior of methods that use linear projections to remove\ninformation about a concept from a language representation, and we consider the\nquestion of what happens to a dataset transformed by such a method. A\ntheoretical analysis and experiments on real-world and synthetic data show that\nthese methods inject strong statistical dependencies into the transformed\ndatasets. After applying such a method, the representation space is highly\nstructured: in the transformed space, an instance tends to be located near\ninstances of the opposite label. As a consequence, the original labeling can in\nsome cases be reconstructed by applying an anti-clustering method.\n","authors":["Richard Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.16142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16139v1","updated":"2024-03-24T13:21:58Z","published":"2024-03-24T13:21:58Z","title":"A Little Leak Will Sink a Great Ship: Survey of Transparency for Large\n  Language Models from Start to Finish","summary":"  Large Language Models (LLMs) are trained on massive web-crawled corpora. This\nposes risks of leakage, including personal information, copyrighted texts, and\nbenchmark datasets. Such leakage leads to undermining human trust in AI due to\npotential unauthorized generation of content or overestimation of performance.\nWe establish the following three criteria concerning the leakage issues: (1)\nleakage rate: the proportion of leaked data in training data, (2) output rate:\nthe ease of generating leaked data, and (3) detection rate: the detection\nperformance of leaked versus non-leaked data. Despite the leakage rate being\nthe origin of data leakage issues, it is not understood how it affects the\noutput rate and detection rate. In this paper, we conduct an experimental\nsurvey to elucidate the relationship between the leakage rate and both the\noutput rate and detection rate for personal information, copyrighted texts, and\nbenchmark data. Additionally, we propose a self-detection approach that uses\nfew-shot learning in which LLMs detect whether instances are present or absent\nin their training data, in contrast to previous methods that do not employ\nexplicit learning. To explore the ease of generating leaked information, we\ncreate a dataset of prompts designed to elicit personal information,\ncopyrighted text, and benchmarks from LLMs. Our experiments reveal that LLMs\nproduce leaked information in most cases despite less such data in their\ntraining set. This indicates even small amounts of leaked data can greatly\naffect outputs. Our self-detection method showed superior performance compared\nto existing detection methods.\n","authors":["Masahiro Kaneko","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2403.16139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16129v1","updated":"2024-03-24T12:58:48Z","published":"2024-03-24T12:58:48Z","title":"A Survey on Lexical Ambiguity Detection and Word Sense Disambiguation","summary":"  This paper explores techniques that focus on understanding and resolving\nambiguity in language within the field of natural language processing (NLP),\nhighlighting the complexity of linguistic phenomena such as polysemy and\nhomonymy and their implications for computational models. Focusing extensively\non Word Sense Disambiguation (WSD), it outlines diverse approaches ranging from\ndeep learning techniques to leveraging lexical resources and knowledge graphs\nlike WordNet. The paper introduces cutting-edge methodologies like word sense\nextension (WSE) and neuromyotonic approaches, enhancing disambiguation accuracy\nby predicting new word senses. It examines specific applications in biomedical\ndisambiguation and language specific optimisation and discusses the\nsignificance of cognitive metaphors in discourse analysis. The research\nidentifies persistent challenges in the field, such as the scarcity of sense\nannotated corpora and the complexity of informal clinical texts. It concludes\nby suggesting future directions, including using large language models, visual\nWSD, and multilingual WSD systems, emphasising the ongoing evolution in\naddressing lexical complexities in NLP. This thinking perspective highlights\nthe advancement in this field to enable computers to understand language more\naccurately.\n","authors":["Miuru Abeysiriwardana","Deshan Sumanathilaka"],"pdf_url":"https://arxiv.org/pdf/2403.16129v1.pdf","comment":"6 pages, 5 figures, 3 tables, Accepted by 20th IEEE International\n  Colloquium on Signal Processing & its Applications (CSPA 2024)"},{"id":"http://arxiv.org/abs/2403.16127v1","updated":"2024-03-24T12:49:30Z","published":"2024-03-24T12:49:30Z","title":"WangchanLion and WangchanX MRC Eval","summary":"  This technical report describes the development of WangchanLion, an\ninstruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in\nthe Thai language. Our model is based on SEA-LION and a collection of\ninstruction following datasets. To promote open research and reproducibility,\nwe publically release all training data, code, and the final model weights\nunder the Apache-2 license. To assess the contextual understanding capability,\nwe conducted extensive experimental studies using two Thai MRC datasets, XQuAD\nand Iapp_wiki_qa_squad. Experimental results demonstrate the model's ability to\ncomprehend the context and produce an answer faithful to the reference one in\n0-shot and 1-shot settings. In addition, our evaluation goes beyond the\ntraditional MRC. We propose a new evaluation scheme assessing the answer's\ncorrectness, helpfulness, conciseness, and contextuality. Evaluation results\nprovide insight into how we can improve our model in the future. Our code is\npublic at https://github.com/vistec-AI/WangchanLion.\n","authors":["Wannaphong Phatthiyaphaibun","Surapon Nonesung","Patomporn Payoungkhamdee","Peerat Limkonchotiwat","Can Udomcharoenchaikit","Ekapol Chuangsuwanich","Sarana Nutanong"],"pdf_url":"https://arxiv.org/pdf/2403.16127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16099v1","updated":"2024-03-24T11:29:55Z","published":"2024-03-24T11:29:55Z","title":"A Multi-Label Dataset of French Fake News: Human and Machine Insights","summary":"  We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of\nFrench press considered unreliable by expert agencies, annotated using 11\nlabels by 8 annotators. By collecting more labels than usual, by more\nannotators than is typically done, we can identify features that humans\nconsider as characteristic of fake news, and compare them to the predictions of\nautomated classifiers. We present a topic and genre analysis using Gate Cloud,\nindicative of the prevalence of satire-like text in the corpus. We then use the\nsubjectivity analyzer VAGO, and a neural version of it, to clarify the link\nbetween ascriptions of the label Subjective and ascriptions of the label Fake\nNews. The annotated dataset is available online at the following url:\nhttps://github.com/obs-info/obsinfox\n  Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion,\nExaggeration, French Press\n","authors":["Benjamin Icard","François Maine","Morgane Casanova","Géraud Faye","Julien Chanson","Guillaume Gadek","Ghislain Atemezing","François Bancilhon","Paul Égré"],"pdf_url":"https://arxiv.org/pdf/2403.16099v1.pdf","comment":"Paper to appear in the Proceedings of the 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation\n  (LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2403.16087v1","updated":"2024-03-24T10:57:08Z","published":"2024-03-24T10:57:08Z","title":"LLMs as Compiler for Arabic Programming Language","summary":"  In this paper we introduce APL (Arabic Programming Language) that uses Large\nlanguage models (LLM) as semi-compiler to covert Arabic text code to python\ncode then run the code. Designing a full pipeline from the structure of the APL\ntext then a prompt (using prompt engineering) then running the prodcued python\ncode using PyRunner. This project has a three parts first python library, a\nplayground with simple interface and this research paper.\n","authors":["Serry Sibaee","Omar Najar","Lahouri Ghouti","Anis Koubaa"],"pdf_url":"https://arxiv.org/pdf/2403.16087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16084v1","updated":"2024-03-24T10:43:21Z","published":"2024-03-24T10:43:21Z","title":"Argument Quality Assessment in the Age of Instruction-Following Large\n  Language Models","summary":"  The computational treatment of arguments on controversial issues has been\nsubject to extensive NLP research, due to its envisioned impact on opinion\nformation, decision making, writing education, and the like. A critical task in\nany such application is the assessment of an argument's quality - but it is\nalso particularly challenging. In this position paper, we start from a brief\nsurvey of argument quality research, where we identify the diversity of quality\nnotions and the subjectiveness of their perception as the main hurdles towards\nsubstantial progress on argument quality assessment. We argue that the\ncapabilities of instruction-following large language models (LLMs) to leverage\nknowledge across contexts enable a much more reliable assessment. Rather than\njust fine-tuning LLMs towards leaderboard chasing on assessment tasks, they\nneed to be instructed systematically with argumentation theories and scenarios\nas well as with ways to solve argument-related problems. We discuss the\nreal-world opportunities and ethical issues emerging thereby.\n","authors":["Henning Wachsmuth","Gabriella Lapesa","Elena Cabrio","Anne Lauscher","Joonsuk Park","Eva Maria Vecchi","Serena Villata","Timon Ziegenbein"],"pdf_url":"https://arxiv.org/pdf/2403.16084v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.04369v3","updated":"2024-03-24T10:08:13Z","published":"2024-03-07T09:57:42Z","title":"From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge\n  Prediction","summary":"  Confusing charge prediction is a challenging task in legal AI, which involves\npredicting confusing charges based on fact descriptions. While existing charge\nprediction methods have shown impressive performance, they face significant\nchallenges when dealing with confusing charges, such as Snatch and Robbery. In\nthe legal domain, constituent elements play a pivotal role in distinguishing\nconfusing charges. Constituent elements are fundamental behaviors underlying\ncriminal punishment and have subtle distinctions among charges. In this paper,\nwe introduce a novel From Graph to Word Bag (FWGB) approach, which introduces\ndomain knowledge regarding constituent elements to guide the model in making\njudgments on confusing charges, much like a judge's reasoning process.\nSpecifically, we first construct a legal knowledge graph containing constituent\nelements to help select keywords for each charge, forming a word bag.\nSubsequently, to guide the model's attention towards the differentiating\ninformation for each charge within the context, we expand the attention\nmechanism and introduce a new loss function with attention supervision through\nwords in the word bag. We construct the confusing charges dataset from\nreal-world judicial documents. Experiments demonstrate the effectiveness of our\nmethod, especially in maintaining exceptional performance in imbalanced label\ndistributions.\n","authors":["Ang Li","Qiangchao Chen","Yiquan Wu","Ming Cai","Xiang Zhou","Fei Wu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2403.04369v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18340v2","updated":"2024-03-24T09:09:00Z","published":"2023-10-22T02:32:53Z","title":"UrbanCLIP: Learning Text-enhanced Urban Region Profiling with\n  Contrastive Language-Image Pretraining from the Web","summary":"  Urban region profiling from web-sourced data is of utmost importance for\nurban planning and sustainable development. We are witnessing a rising trend of\nLLMs for various fields, especially dealing with multi-modal data research such\nas vision-language learning, where the text modality serves as a supplement\ninformation for the image. Since textual modality has never been introduced\ninto modality combinations in urban region profiling, we aim to answer two\nfundamental questions in this paper: i) Can textual modality enhance urban\nregion profiling? ii) and if so, in what ways and with regard to which aspects?\nTo answer the questions, we leverage the power of Large Language Models (LLMs)\nand introduce the first-ever LLM-enhanced framework that integrates the\nknowledge of textual modality into urban imagery profiling, named LLM-enhanced\nUrban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP).\nSpecifically, it first generates a detailed textual description for each\nsatellite image by an open-source Image-to-Text LLM. Then, the model is trained\non the image-text pairs, seamlessly unifying natural language supervision for\nurban visual representation learning, jointly with contrastive loss and\nlanguage modeling loss. Results on predicting three urban indicators in four\nmajor Chinese metropolises demonstrate its superior performance, with an\naverage improvement of 6.1% on R^2 compared to the state-of-the-art methods.\nOur code and the image-language dataset will be released upon paper\nnotification.\n","authors":["Yibo Yan","Haomin Wen","Siru Zhong","Wei Chen","Haodong Chen","Qingsong Wen","Roger Zimmermann","Yuxuan Liang"],"pdf_url":"https://arxiv.org/pdf/2310.18340v2.pdf","comment":"Accepted by The Web Conference 2024"},{"id":"http://arxiv.org/abs/2403.16056v1","updated":"2024-03-24T07:48:05Z","published":"2024-03-24T07:48:05Z","title":"Qibo: A Large Language Model for Traditional Chinese Medicine","summary":"  In the field of Artificial Intelligence, Large Language Models (LLMs) have\ndemonstrated significant advances in user intent understanding and response in\na number of specialized domains, including medicine, law, and finance. However,\nin the unique domain of traditional Chinese medicine (TCM), the performance\nenhancement of LLMs is challenged by the essential differences between its\ntheories and modern medicine, as well as the lack of specialized corpus\nresources. In this paper, we aim to construct and organize a professional\ncorpus in the field of TCM, to endow the large model with professional\nknowledge that is characteristic of TCM theory, and to successfully develop the\nQibo model based on LLaMA, which is the first LLM in the field of TCM to\nundergo a complete training process from pre-training to Supervised Fine-Tuning\n(SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for\nevaluating the performance of LLMs, which is a specialized tool for evaluating\nthe performance of LLMs in the TCM domain. This tool will provide an important\nbasis for quantifying and comparing the understanding and application\ncapabilities of different models in the field of traditional Chinese medicine,\nand provide guidance for future research directions and practical applications\nof intelligent assistants for traditional Chinese medicine. Finally, we\nconducted sufficient experiments to prove that Qibo has good performance in the\nfield of traditional Chinese medicine.\n","authors":["Heyi Zhang","Xin Wang","Zhaopeng Meng","Yongzhe Jia","Dawei Xu"],"pdf_url":"https://arxiv.org/pdf/2403.16056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09282v4","updated":"2024-03-24T07:06:19Z","published":"2024-02-14T16:10:45Z","title":"Leveraging Large Language Models for Enhanced NLP Task Performance\n  through Knowledge Distillation and Optimized Training Strategies","summary":"  Emerging Large Language Models (LLMs) like GPT-4 have revolutionized Natural\nLanguage Processing (NLP), showing potential in traditional tasks such as Named\nEntity Recognition (NER). Our study explores a three-phase training strategy\nthat harnesses GPT-4's capabilities to enhance the BERT model's performance on\nNER. Initially, GPT-4 annotates a subset of the CONLL2003 and additional BBC\ndataset without fine-tuning. We then train BERT using a mix of original and\nLLM-annotated data, analyzing the efficacy of LLM annotations against\ntraditional methods. The second phase involves comparative experiments with\ndifferent training regimens, assessing the synergy between distilled and\noriginal data. We observe that sequential strategies, particularly a simple mix\nof training first with distilled data followed by original data, significantly\nboost performance. In the third phase, we investigate various data blending\ntechniques, including sigmoid and power decay functions, to optimize the\ntraining process further. Our results indicate that a strategic mix of\ndistilled and original data markedly elevates the NER capabilities of BERT. Our\napproach presents a scalable methodology that reduces manual annotation costs\nand increases efficiency, making it especially pertinent in resource-limited\nand closed-network environments. The study concludes that while the 'Simple\nMix' strategy yields the best results, understanding its underlying mechanisms\nrequires further research. Future work will also focus on refining prompt\ndesigns and enhancing annotation selection processes, aiming to extend our\nmethodology to diverse NLP tasks.\n","authors":["Yining Huang","Keke Tang","Meilian Chen"],"pdf_url":"https://arxiv.org/pdf/2402.09282v4.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.16038v1","updated":"2024-03-24T06:49:07Z","published":"2024-03-24T06:49:07Z","title":"Monotonic Paraphrasing Improves Generalization of Language Model\n  Prompting","summary":"  Performance of large language models (LLMs) may vary with different prompts\nor instructions of even the same task. One commonly recognized factor for this\nphenomenon is the model's familiarity with the given prompt or instruction,\nwhich is typically estimated by its perplexity. However, finding the prompt\nwith the lowest perplexity is challenging, given the enormous space of possible\nprompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara),\nan end-to-end decoding strategy that paraphrases given prompts or instructions\ninto their lower perplexity counterparts based on an ensemble of a paraphrase\nLM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or\ninstruction executor) that constrains the generation for lower perplexity. The\nensemble decoding process can efficiently paraphrase the original prompt\nwithout altering its semantic meaning, while monotonically decreasing the\nperplexity of each generation as calculated by the target LM. We explore in\ndetail both greedy and search-based decoding as two alternative decoding\nschemes of MonoPara. Notably, MonoPara does not require any training and can\nmonotonically lower the perplexity of the paraphrased prompt or instruction,\nleading to improved performance of zero-shot LM prompting as evaluated on a\nwide selection of tasks. In addition, MonoPara is also shown to effectively\nimprove LMs' generalization on perturbed and unseen task instructions.\n","authors":["Qin Liu","Fei Wang","Nan Xu","Tianyi Yan","Tao Meng","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2403.16038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10647v3","updated":"2024-03-24T06:46:19Z","published":"2024-01-19T11:48:09Z","title":"Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language\n  Models","summary":"  In the rapidly advancing field of artificial intelligence, the concept of\nRed-Teaming or Jailbreaking large language models (LLMs) has emerged as a\ncrucial area of study. This approach is especially significant in terms of\nassessing and enhancing the safety and robustness of these models. This paper\ninvestigates the intricate consequences of such modifications through model\nediting, uncovering a complex relationship between enhancing model accuracy and\npreserving its ethical integrity. Our in-depth analysis reveals a striking\nparadox: while injecting accurate information is crucial for model reliability,\nit can paradoxically destabilize the model's foundational framework, resulting\nin unpredictable and potentially unsafe behaviors. Additionally, we propose a\nbenchmark dataset NicheHazardQA to investigate this unsafe behavior both within\nthe same and cross topical domain. This aspect of our research sheds light on\nhow the edits, impact the model's safety metrics and guardrails. Our findings\nshow that model editing serves as a cost-effective tool for topical red-teaming\nby methodically applying targeted edits and evaluating the resultant model\nbehavior.\n","authors":["Rima Hazra","Sayan Layek","Somnath Banerjee","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2401.10647v3.pdf","comment":"Under review.\n  {https://huggingface.co/datasets/SoftMINER-Group/NicheHazardQA}"},{"id":"http://arxiv.org/abs/2307.07697v6","updated":"2024-03-24T06:42:47Z","published":"2023-07-15T03:31:38Z","title":"Think-on-Graph: Deep and Responsible Reasoning of Large Language Model\n  on Knowledge Graph","summary":"  Although large language models (LLMs) have achieved significant success in\nvarious tasks, they often struggle with hallucination problems, especially in\nscenarios requiring deep and responsible reasoning. These issues could be\npartially addressed by introducing external knowledge graphs (KG) in LLM\nreasoning. In this paper, we propose a new LLM-KG integrating paradigm\n``$\\hbox{LLM}\\otimes\\hbox{KG}$'' which treats the LLM as an agent to\ninteractively explore related entities and relations on KGs and perform\nreasoning based on the retrieved knowledge. We further implement this paradigm\nby introducing a new approach called Think-on-Graph (ToG), in which the LLM\nagent iteratively executes beam search on KG, discovers the most promising\nreasoning paths, and returns the most likely reasoning results. We use a number\nof well-designed experiments to examine and illustrate the following advantages\nof ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has\nthe ability of knowledge traceability and knowledge correctability by\nleveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible\nplug-and-play framework for different LLMs, KGs and prompting strategies\nwithout any additional training cost; 4) the performance of ToG with small LLM\nmodels could exceed large LLM such as GPT-4 in certain scenarios and this\nreduces the cost of LLM deployment and application. As a training-free method\nwith lower computational cost and better generality, ToG achieves overall SOTA\nin 6 out of 9 datasets where most previous SOTAs rely on additional training.\n","authors":["Jiashuo Sun","Chengjin Xu","Lumingyuan Tang","Saizhuo Wang","Chen Lin","Yeyun Gong","Lionel M. Ni","Heung-Yeung Shum","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2307.07697v6.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2403.16033v1","updated":"2024-03-24T06:28:54Z","published":"2024-03-24T06:28:54Z","title":"Node Classification via Semantic-Structural Attention-Enhanced Graph\n  Convolutional Networks","summary":"  Graph data, also known as complex network data, is omnipresent across various\ndomains and applications. Prior graph neural network models primarily focused\non extracting task-specific structural features through supervised learning\nobjectives, but they fell short in capturing the inherent semantic and\nstructural features of the entire graph. In this paper, we introduce the\nsemantic-structural attention-enhanced graph convolutional network (SSA-GCN),\nwhich not only models the graph structure but also extracts generalized\nunsupervised features to enhance vertex classification performance. The\nSSA-GCN's key contributions lie in three aspects: firstly, it derives semantic\ninformation through unsupervised feature extraction from a knowledge graph\nperspective; secondly, it obtains structural information through unsupervised\nfeature extraction from a complex network perspective; and finally, it\nintegrates these features through a cross-attention mechanism. By leveraging\nthese features, we augment the graph convolutional network, thereby enhancing\nthe model's generalization capabilities. Our experiments on the Cora and\nCiteSeer datasets demonstrate the performance improvements achieved by our\nproposed method. Furthermore, our approach also exhibits excellent accuracy\nunder privacy settings, making it a robust and effective solution for graph\ndata analysis.\n","authors":["Hongyin Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.16033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11311v2","updated":"2024-03-24T06:21:27Z","published":"2024-03-17T19:12:26Z","title":"Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding","summary":"  Deep multimodal semantic understanding that goes beyond the mere superficial\ncontent relation mining has received increasing attention in the realm of\nartificial intelligence. The challenges of collecting and annotating\nhigh-quality multi-modal data have underscored the significance of few-shot\nlearning. In this paper, we focus on two critical tasks under this context:\nfew-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis\n(MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware\nPrompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on\nthe unified vision-language model (VLM). Specifically, we design three experts\nof soft prompts: a text prompt and an image prompt that extract\nmodality-specific features to enrich the single-modal representation, and a\nunified prompt to assist multi-modal interaction. Additionally, we reorganize\nTransformer layers into several blocks and introduce cross-modal prompt\nattention between adjacent blocks, which smoothens the transition from\nsingle-modal representation to multi-modal fusion. On both MSD and MSA datasets\nin few-shot setting, our proposed model not only surpasses the 8.2B model\nInstructBLIP with merely 2% parameters (150M), but also significantly\noutperforms other widely-used prompt methods on VLMs or task-specific methods.\n","authors":["Zichen Wu","Hsiu-Yuan Huang","Fanyi Qu","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11311v2.pdf","comment":"LREC-COLING 2024, Long Paper"},{"id":"http://arxiv.org/abs/2403.16008v1","updated":"2024-03-24T04:34:34Z","published":"2024-03-24T04:34:34Z","title":"CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral\n  Therapy-based Mental Health Question Answering","summary":"  The recent advancements in artificial intelligence highlight the potential of\nlanguage models in psychological health support. While models trained on data\nfrom mental health service platform have achieved preliminary success,\nchallenges persist in areas such as data scarcity, quality, and ensuring a\nsolid foundation in psychological techniques. To address these challenges, this\nstudy introduces a novel approach to enhance the precision and efficacy of\npsychological support through large language models. Specifically, we design a\nspecific prompt derived from principles of Cognitive Behavioral Therapy (CBT)\nand have generated the CBT QA dataset, specifically for Chinese psychological\nhealth Q&A based on CBT structured intervention strategies. Unlike previous\nmethods, our dataset emphasizes professional and structured response. Utilizing\nthis dataset, we fine-tuned the large language model, giving birth to CBT-LLM,\nthe large-scale language model specifically designed for Cognitive Behavioral\nTherapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in\ngenerating structured, professional, and highly relevant responses in\npsychological health support tasks, showcasing its practicality and quality.\nThe model is available on Hugging Face:\nhttps://huggingface.co/Hongbin37/CBT-LLM.\n","authors":["Hongbin Na"],"pdf_url":"https://arxiv.org/pdf/2403.16008v1.pdf","comment":"Accepted at COLING 2024"},{"id":"http://arxiv.org/abs/2309.13339v3","updated":"2024-03-24T04:17:28Z","published":"2023-09-23T11:21:12Z","title":"Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models\n  through Logic","summary":"  Recent advancements in large language models have showcased their remarkable\ngeneralizability across various domains. However, their reasoning abilities\nstill have significant room for improvement, especially when confronted with\nscenarios requiring multi-step reasoning. Although large language models\npossess extensive knowledge, their reasoning often fails to effectively utilize\nthis knowledge to establish a coherent thinking paradigm. These models\nsometimes show hallucinations as their reasoning procedures are unconstrained\nby logical principles. Aiming at improving the zero-shot chain-of-thought\nreasoning ability of large language models, we propose LoT (Logical Thoughts),\na self-improvement prompting framework that leverages principles rooted in\nsymbolic logic, particularly Reductio ad Absurdum, to systematically verify and\nrectify the reasoning processes step by step. Experimental evaluations\nconducted on language tasks in diverse domains, including arithmetic,\ncommonsense, symbolic, causal inference, and social problems, demonstrate the\nefficacy of enhanced reasoning by logic. The implementation code for LoT can be\naccessed at: \\url{https://github.com/xf-zhao/LoT}.\n","authors":["Xufeng Zhao","Mengdi Li","Wenhao Lu","Cornelius Weber","Jae Hee Lee","Kun Chu","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2309.13339v3.pdf","comment":"Accepted in COLING 2024. Code see https://github.com/xf-zhao/LoT"},{"id":"http://arxiv.org/abs/2403.15992v1","updated":"2024-03-24T03:10:07Z","published":"2024-03-24T03:10:07Z","title":"BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval","summary":"  The burgeoning integration of 3D medical imaging into healthcare has led to a\nsubstantial increase in the workload of medical professionals. To assist\nclinicians in their diagnostic processes and alleviate their workload, the\ndevelopment of a robust system for retrieving similar case studies presents a\nviable solution. While the concept holds great promise, the field of 3D medical\ntext-image retrieval is currently limited by the absence of robust evaluation\nbenchmarks and curated datasets. To remedy this, our study presents a\ngroundbreaking dataset, BIMCV-R (This dataset will be released upon\nacceptance.), which includes an extensive collection of 8,069 3D CT volumes,\nencompassing over 2 million slices, paired with their respective radiological\nreports. Expanding upon the foundational work of our dataset, we craft a\nretrieval strategy, MedFinder. This approach employs a dual-stream network\narchitecture, harnessing the potential of large language models to advance the\nfield of medical image retrieval beyond existing text-image retrieval\nsolutions. It marks our preliminary step towards developing a system capable of\nfacilitating text-to-image, image-to-text, and keyword-based retrieval tasks.\n","authors":["Yinda Chen","Che Liu","Xiaoyu Liu","Rossella Arcucci","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.15992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06461v3","updated":"2024-03-24T01:20:49Z","published":"2024-01-12T09:15:20Z","title":"Between Lines of Code: Unraveling the Distinct Patterns of Machine and\n  Human Programmers","summary":"  Large language models have catalyzed an unprecedented wave in code\ngeneration. While achieving significant advances, they blur the distinctions\nbetween machine- and human-authored source code, causing integrity and\nauthenticity issues of software artifacts. Previous methods such as DetectGPT\nhave proven effective in discerning machine-generated texts, but they do not\nidentify and harness the unique patterns of machine-generated code. Thus, its\napplicability falters when applied to code. In this paper, we carefully study\nthe specific patterns that characterize machine- and human-authored code.\nThrough a rigorous analysis of code attributes such as lexical diversity,\nconciseness, and naturalness, we expose unique patterns inherent to each\nsource. We particularly notice that the syntactic segmentation of code is a\ncritical factor in identifying its provenance. Based on our findings, we\npropose DetectCodeGPT, a novel method for detecting machine-generated code,\nwhich improves DetectGPT by capturing the distinct stylized patterns of code.\nDiverging from conventional techniques that depend on external LLMs for\nperturbations, DetectCodeGPT perturbs the code corpus by strategically\ninserting spaces and newlines, ensuring both efficacy and efficiency.\nExperiment results show that our approach significantly outperforms\nstate-of-the-art techniques in detecting machine-generated code.\n","authors":["Yuling Shi","Hongyu Zhang","Chengcheng Wan","Xiaodong Gu"],"pdf_url":"https://arxiv.org/pdf/2401.06461v3.pdf","comment":"code available at https://github.com/YerbaPage/DetectCodeGPT"},{"id":"http://arxiv.org/abs/2309.11576v2","updated":"2024-03-24T00:06:49Z","published":"2023-09-20T18:27:19Z","title":"Examining the Limitations of Computational Rumor Detection Models\n  Trained on Static Datasets","summary":"  A crucial aspect of a rumor detection model is its ability to generalize,\nparticularly its ability to detect emerging, previously unknown rumors. Past\nresearch has indicated that content-based (i.e., using solely source posts as\ninput) rumor detection models tend to perform less effectively on unseen\nrumors. At the same time, the potential of context-based models remains largely\nuntapped. The main contribution of this paper is in the in-depth evaluation of\nthe performance gap between content and context-based models specifically on\ndetecting new, unseen rumors. Our empirical findings demonstrate that\ncontext-based models are still overly dependent on the information derived from\nthe rumors' source post and tend to overlook the significant role that\ncontextual information can play. We also study the effect of data split\nstrategies on classifier performance. Based on our experimental results, the\npaper also offers practical suggestions on how to minimize the effects of\ntemporal concept drift in static datasets during the training of rumor\ndetection methods.\n","authors":["Yida Mu","Xingyi Song","Kalina Bontcheva","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2309.11576v2.pdf","comment":"Accepted at LREC-COLING 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2307.09985v3","updated":"2024-03-24T15:53:57Z","published":"2023-07-19T13:44:32Z","title":"Our Model Achieves Excellent Performance on MovieLens: What Does it\n  Mean?","summary":"  A typical benchmark dataset for recommender system (RecSys) evaluation\nconsists of user-item interactions generated on a platform within a time\nperiod. The interaction generation mechanism partially explains why a user\ninteracts with (e.g., like, purchase, rate) an item, and the context of when a\nparticular interaction happened. In this study, we conduct a meticulous\nanalysis of the MovieLens dataset and explain the potential impact of using the\ndataset for evaluating recommendation algorithms. We make a few main findings\nfrom our analysis. First, there are significant differences in user\ninteractions at the different stages when a user interacts with the MovieLens\nplatform. The early interactions largely define the user portrait which affects\nthe subsequent interactions. Second, user interactions are highly affected by\nthe candidate movies that are recommended by the platform's internal\nrecommendation algorithm(s). Third, changing the order of user interactions\nmakes it more difficult for sequential algorithms to capture the progressive\ninteraction process. We further discuss the discrepancy between the interaction\ngeneration mechanism that is employed by the MovieLens system and that of\ntypical real-world recommendation scenarios. In summary, the MovieLens platform\ndemonstrates an efficient and effective way of collecting user preferences to\naddress cold-starts. However, models that achieve excellent recommendation\naccuracy on the MovieLens dataset may not demonstrate superior performance in\npractice, for at least two kinds of differences: (i) the differences in the\ncontexts of user-item interaction generation, and (ii) the differences in user\nknowledge about the item collections. While results on MovieLens can be useful\nas a reference, they should not be solely relied upon as the primary\njustification for the effectiveness of a recommendation system model.\n","authors":["Yu-chen Fan","Yitong Ji","Jie Zhang","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2307.09985v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.14534v2","updated":"2024-03-24T14:34:53Z","published":"2023-02-28T12:44:10Z","title":"Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face","summary":"  We present Spacerini, a tool that integrates the Pyserini toolkit for\nreproducible information retrieval research with Hugging Face to enable the\nseamless construction and deployment of interactive search engines. Spacerini\nmakes state-of-the-art sparse and dense retrieval models more accessible to\nnon-IR practitioners while minimizing deployment effort. This is useful for NLP\nresearchers who want to better understand and validate their research by\nperforming qualitative analyses of training corpora, for IR researchers who\nwant to demonstrate new retrieval models integrated into the growing Pyserini\necosystem, and for third parties reproducing the work of other researchers.\nSpacerini is open source and includes utilities for loading, preprocessing,\nindexing, and deploying search engines locally and remotely. We demonstrate a\nportfolio of 13 search engines created with Spacerini for different use cases.\n","authors":["Christopher Akiki","Odunayo Ogundepo","Aleksandra Piktus","Xinyu Zhang","Akintunde Oladipo","Jimmy Lin","Martin Potthast"],"pdf_url":"https://arxiv.org/pdf/2302.14534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16151v1","updated":"2024-03-24T13:44:32Z","published":"2024-03-24T13:44:32Z","title":"Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior\n  Detection","summary":"  The online community has increasingly been inundated by a toxic wave of\nharmful comments. In response to this growing challenge, we introduce a\ntwo-stage ultra-low-cost multimodal harmful behavior detection method designed\nto identify harmful comments and images with high precision and recall rates.\nWe first utilize the CLIP-ViT model to transform tweets and images into\nembeddings, effectively capturing the intricate interplay of semantic meaning\nand subtle contextual clues within texts and images. Then in the second stage,\nthe system feeds these embeddings into a conventional machine learning\nclassifier like SVM or logistic regression, enabling the system to be trained\nrapidly and to perform inference at an ultra-low cost. By converting tweets\ninto rich multimodal embeddings through the CLIP-ViT model and utilizing them\nto train conventional machine learning classifiers, our system is not only\ncapable of detecting harmful textual information with near-perfect performance,\nachieving precision and recall rates above 99\\% but also demonstrates the\nability to zero-shot harmful images without additional training, thanks to its\nmultimodal embedding input. This capability empowers our system to identify\nunseen harmful images without requiring extensive and costly image datasets.\nAdditionally, our system quickly adapts to new harmful content; if a new\nharmful content pattern is identified, we can fine-tune the classifier with the\ncorresponding tweets' embeddings to promptly update the system. This makes it\nwell suited to addressing the ever-evolving nature of online harmfulness,\nproviding online communities with a robust, generalizable, and cost-effective\ntool to safeguard their communities.\n","authors":["Albert Lu","Stephen Cranefield"],"pdf_url":"https://arxiv.org/pdf/2403.16151v1.pdf","comment":"to be appear in International Workshop on Coordination,\n  Organizations, Institutions, Norms and Ethics for Governance of Multi-Agent\n  Systems"},{"id":"http://arxiv.org/abs/2403.16135v1","updated":"2024-03-24T13:06:05Z","published":"2024-03-24T13:06:05Z","title":"Complementary Recommendation in E-commerce: Definition, Approaches, and\n  Future Directions","summary":"  In recent years, complementary recommendation has received extensive\nattention in the e-commerce domain. In this paper, we comprehensively summarize\nand compare 34 representative studies conducted between 2009 and 2024. Firstly,\nwe compare the data and methods used for modeling complementary relationships\nbetween products, including simple complementarity and more complex scenarios\nsuch as asymmetric complementarity, the coexistence of substitution and\ncomplementarity relationships between products, and varying degrees of\ncomplementarity between different pairs of products. Next, we classify and\ncompare the models based on the research problems of complementary\nrecommendation, such as diversity, personalization, and cold-start.\nFurthermore, we provide a comparative analysis of experimental results from\ndifferent studies conducted on the same dataset, which helps identify the\nstrengths and weaknesses of the research. Compared to previous surveys, this\npaper provides a more updated and comprehensive summary of the research,\ndiscusses future research directions, and contributes to the advancement of\nthis field.\n","authors":["Linyue Li","Zhijuan Du"],"pdf_url":"https://arxiv.org/pdf/2403.16135v1.pdf","comment":"20 pages,9 figures"},{"id":"http://arxiv.org/abs/2403.16085v1","updated":"2024-03-24T10:45:55Z","published":"2024-03-24T10:45:55Z","title":"RankingSHAP -- Listwise Feature Attribution Explanations for Ranking\n  Models","summary":"  Feature attributions are a commonly used explanation type, when we want to\nposthoc explain the prediction of a trained model. Yet, they are not very well\nexplored in IR. Importantly, feature attribution has rarely been rigorously\ndefined, beyond attributing the most important feature the highest value. What\nit means for a feature to be more important than others is often left vague.\nConsequently, most approaches focus on just selecting the most important\nfeatures and under utilize or even ignore the relative importance within\nfeatures. In this work, we rigorously define the notion of feature attribution\nfor ranking models, and list essential properties that a valid attribution\nshould have. We then propose RankingSHAP as a concrete instantiation of a\nlist-wise ranking attribution method. Contrary to current explanation\nevaluation schemes that focus on selections, we propose two novel evaluation\nparadigms for evaluating attributions over learning-to-rank models. We evaluate\nRankingSHAP for commonly used learning-to-rank datasets to showcase the more\nnuanced use of an attribution method while highlighting the limitations of\nselection-based explanations. In a simulated experiment we design an\ninterpretable model to demonstrate how list-wise ranking attributes can be used\nto investigate model decisions and evaluate the explanations qualitatively.\nBecause of the contrastive nature of the ranking task, our understanding of\nranking model decisions can substantially benefit from feature attribution\nexplanations like RankingSHAP.\n","authors":["Maria Heuss","Maarten de Rijke","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2403.16085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16037v1","updated":"2024-03-24T06:41:33Z","published":"2024-03-24T06:41:33Z","title":"Knowledge-aware Dual-side Attribute-enhanced Recommendation","summary":"  \\textit{Knowledge-aware} recommendation methods (KGR) based on \\textit{graph\nneural networks} (GNNs) and \\textit{contrastive learning} (CL) have achieved\npromising performance. However, they fall short in modeling fine-grained user\npreferences and further fail to leverage the \\textit{preference-attribute\nconnection} to make predictions, leading to sub-optimal performance. To address\nthe issue, we propose a method named \\textit{\\textbf{K}nowledge-aware\n\\textbf{D}ual-side \\textbf{A}ttribute-enhanced \\textbf{R}ecommendation} (KDAR).\nSpecifically, we build \\textit{user preference representations} and\n\\textit{attribute fusion representations} upon the attribute information in\nknowledge graphs, which are utilized to enhance \\textit{collaborative\nfiltering} (CF) based user and item representations, respectively. To\ndiscriminate the contribution of each attribute in these two types of\nattribute-based representations, a \\textit{multi-level collaborative alignment\ncontrasting} mechanism is proposed to align the importance of attributes with\nCF signals. Experimental results on four benchmark datasets demonstrate the\nsuperiority of KDAR over several state-of-the-art baselines. Further analyses\nverify the effectiveness of our method. The code of KDAR is released at:\n\\href{https://github.com/TJTP/KDAR}{https://github.com/TJTP/KDAR}.\n","authors":["Taotian Pang","Xingyu Lou","Fei Zhao","Zhen Wu","Kuiyao Dong","Qiuying Peng","Yue Qi","Xinyu Dai"],"pdf_url":"https://arxiv.org/pdf/2403.16037v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.16331v1","updated":"2024-03-24T23:50:15Z","published":"2024-03-24T23:50:15Z","title":"Modeling Analog Dynamic Range Compressors using Deep Learning and\n  State-space Models","summary":"  We describe a novel approach for developing realistic digital models of\ndynamic range compressors for digital audio production by analyzing their\nanalog prototypes. While realistic digital dynamic compressors are potentially\nuseful for many applications, the design process is challenging because the\ncompressors operate nonlinearly over long time scales. Our approach is based on\nthe structured state space sequence model (S4), as implementing the state-space\nmodel (SSM) has proven to be efficient at learning long-range dependencies and\nis promising for modeling dynamic range compressors. We present in this paper a\ndeep learning model with S4 layers to model the Teletronix LA-2A analog dynamic\nrange compressor. The model is causal, executes efficiently in real time, and\nachieves roughly the same quality as previous deep-learning models but with\nfewer parameters.\n","authors":["Hanzhi Yin","Gang Cheng","Christian J. Steinmetz","Ruibin Yuan","Richard M. Stern","Roger B. Dannenberg"],"pdf_url":"https://arxiv.org/pdf/2403.16331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15168v3","updated":"2024-03-24T23:32:50Z","published":"2023-10-23T17:59:52Z","title":"Ghost on the Shell: An Expressive Representation of General 3D Shapes","summary":"  The creation of photorealistic virtual worlds requires the accurate modeling\nof 3D surface geometry for a wide range of objects. For this, meshes are\nappealing since they 1) enable fast physics-based rendering with realistic\nmaterial and lighting, 2) support physical simulation, and 3) are\nmemory-efficient for modern graphics pipelines. Recent work on reconstructing\nand statistically modeling 3D shape, however, has critiqued meshes as being\ntopologically inflexible. To capture a wide range of object shapes, any 3D\nrepresentation must be able to model solid, watertight, shapes as well as thin,\nopen, surfaces. Recent work has focused on the former, and methods for\nreconstructing open surfaces do not support fast reconstruction with material\nand lighting or unconditional generative modelling. Inspired by the observation\nthat open surfaces can be seen as islands floating on watertight surfaces, we\nparameterize open surfaces by defining a manifold signed distance field on\nwatertight templates. With this parameterization, we further develop a\ngrid-based and differentiable representation that parameterizes both watertight\nand non-watertight meshes of arbitrary topology. Our new representation, called\nGhost-on-the-Shell (G-Shell), enables two important applications:\ndifferentiable rasterization-based reconstruction from multiview images and\ngenerative modelling of non-watertight meshes. We empirically demonstrate that\nG-Shell achieves state-of-the-art performance on non-watertight mesh\nreconstruction and generation tasks, while also performing effectively for\nwatertight meshes.\n","authors":["Zhen Liu","Yao Feng","Yuliang Xiu","Weiyang Liu","Liam Paull","Michael J. Black","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2310.15168v3.pdf","comment":"ICLR 2024 Oral (v3: 30 pages, 19 figures, Project Page:\n  https://gshell3d.github.io/)"},{"id":"http://arxiv.org/abs/2403.16327v1","updated":"2024-03-24T23:22:02Z","published":"2024-03-24T23:22:02Z","title":"Artificial Neural Microcircuits as Building Blocks: Concept and\n  Challenges","summary":"  Artificial Neural Networks (ANNs) are one of the most widely employed forms\nof bio-inspired computation. However the current trend is for ANNs to be\nstructurally homogeneous. Furthermore, this structural homogeneity requires the\napplication of complex training and learning tools that produce application\nspecific ANNs, susceptible to pitfalls such as overfitting. In this paper, an\nnew approach is explored, inspired by the role played in biology by Neural\nMicrocircuits, the so called ``fundamental processing elements'' of organic\nnervous systems. How large neural networks, particularly Spiking Neural\nNetworks (SNNs) can be assembled using Artificial Neural Microcircuits (ANMs),\nintended as off-the-shelf components, is articulated; the results of initial\nwork to produce a catalogue of such Microcircuits though the use of Novelty\nSearch is shown; followed by efforts to expand upon this initial work,\nincluding a discussion of challenges uncovered during these efforts and\nexplorations of methods by which they might be overcome.\n","authors":["Andrew Walter","Shimeng Wu","Andy M. Tyrrell","Liam McDaid","Malachy McElholm","Nidhin Thandassery Sumithran","Jim Harkin","Martin A. Trefzer"],"pdf_url":"https://arxiv.org/pdf/2403.16327v1.pdf","comment":"12 pages, 31 figures, 3 tables, submitted to A-Life Journal for\n  review"},{"id":"http://arxiv.org/abs/2212.13332v3","updated":"2024-03-24T23:18:18Z","published":"2022-12-27T01:06:26Z","title":"Development and Evaluation of a Learning-based Model for Real-time\n  Haptic Texture Rendering","summary":"  Current Virtual Reality (VR) environments lack the rich haptic signals that\nhumans experience during real-life interactions, such as the sensation of\ntexture during lateral movement on a surface. Adding realistic haptic textures\nto VR environments requires a model that generalizes to variations of a user's\ninteraction and to the wide variety of existing textures in the world. Current\nmethodologies for haptic texture rendering exist, but they usually develop one\nmodel per texture, resulting in low scalability. We present a deep\nlearning-based action-conditional model for haptic texture rendering and\nevaluate its perceptual performance in rendering realistic texture vibrations\nthrough a multi part human user study. This model is unified over all materials\nand uses data from a vision-based tactile sensor (GelSight) to render the\nappropriate surface conditioned on the user's action in real time. For\nrendering texture, we use a high-bandwidth vibrotactile transducer attached to\na 3D Systems Touch device. The result of our user study shows that our\nlearning-based method creates high-frequency texture renderings with comparable\nor better quality than state-of-the-art methods without the need for learning a\nseparate model per texture. Furthermore, we show that the method is capable of\nrendering previously unseen textures using a single GelSight image of their\nsurface.\n","authors":["Negin Heravi","Heather Culbertson","Allison M. Okamura","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2212.13332v3.pdf","comment":"Accepted for publication in IEEE Transactions on Haptics 2024. 12\n  pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.01623v3","updated":"2024-03-24T23:13:06Z","published":"2023-11-03T16:58:10Z","title":"VQPy: An Object-Oriented Approach to Modern Video Analytics","summary":"  Video analytics is widely used in contemporary systems and services. At the\nforefront of video analytics are video queries that users develop to find\nobjects of particular interest. Building upon the insight that video objects\n(e.g., human, animals, cars, etc.), the center of video analytics, are similar\nin spirit to objects modeled by traditional object-oriented languages, we\npropose to develop an object-oriented approach to video analytics. This\napproach, named VQPy, consists of a frontend$\\unicode{x2015}$a Python variant\nwith constructs that make it easy for users to express video objects and their\ninteractions$\\unicode{x2015}$as well as an extensible backend that can\nautomatically construct and optimize pipelines based on video objects. We have\nimplemented and open-sourced VQPy, which has been productized in Cisco as part\nof its DeepVision framework.\n","authors":["Shan Yu","Zhenting Zhu","Yu Chen","Hanchen Xu","Pengzhan Zhao","Yang Wang","Arthi Padmanabhan","Hugo Latapie","Harry Xu"],"pdf_url":"https://arxiv.org/pdf/2311.01623v3.pdf","comment":"MLSys'24"},{"id":"http://arxiv.org/abs/2309.07133v2","updated":"2024-03-24T23:12:01Z","published":"2023-08-28T00:07:55Z","title":"Assessing cognitive function among older adults using machine learning\n  and wearable device data: a feasibility study","summary":"  Timely implementation of interventions to slow cognitive decline among older\nadults requires accurate monitoring to detect changes in cognitive function.\nData gathered using wearable devices that can continuously monitor factors\nknown to be associated with cognition could be used to train machine learning\nmodels and develop wearable-based cognitive monitoring systems. Using data from\nover 2,400 older adults in the National Health and Nutrition Examination Survey\n(NHANES) we developed prediction models to differentiate older adults with\nnormal cognition from those with poor cognition based on outcomes from three\ncognitive tests measuring different domains of cognitive function. During\nrepeated cross-validation, CatBoost, XGBoost, and Random Forest models\nperformed best when predicting cognition based on processing speed, working\nmemory, and attention (median AUCs >0.82) compared to immediate and delayed\nrecall (median AUCs >0.72) and categorical verbal fluency (median AUC >0.68).\nActivity and sleep parameters were also more strongly associated with\nprocessing speed, working memory, and attention compared to other cognitive\nsubdomains. Our work provides proof of concept that wearable-based cognitive\nmonitoring systems may be a viable alternative to traditional methods for\nmonitoring processing speeds, working memory, and attention. We further\nidentified novel metrics that could be targets in future causal studies seeking\nto better understand how sleep and activity parameters influence cognitive\nfunction among older adults.\n","authors":["Collin Sakal","Tingyou Li","Juan Li","Xinyue Li"],"pdf_url":"https://arxiv.org/pdf/2309.07133v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16317v1","updated":"2024-03-24T22:42:40Z","published":"2024-03-24T22:42:40Z","title":"Optimization on a Finer Scale: Bounded Local Subgradient Variation\n  Perspective","summary":"  We initiate the study of nonsmooth optimization problems under bounded local\nsubgradient variation, which postulates bounded difference between\n(sub)gradients in small local regions around points, in either average or\nmaximum sense. The resulting class of objective functions encapsulates the\nclasses of objective functions traditionally studied in optimization, which are\ndefined based on either Lipschitz continuity of the objective or\nH\\\"{o}lder/Lipschitz continuity of its gradient. Further, the defined class\ncontains functions that are neither Lipschitz continuous nor have a H\\\"{o}lder\ncontinuous gradient. When restricted to the traditional classes of optimization\nproblems, the parameters defining the studied classes lead to more fine-grained\ncomplexity bounds, recovering traditional oracle complexity bounds in the worst\ncase but generally leading to lower oracle complexity for functions that are\nnot ``worst case.'' Some highlights of our results are that: (i) it is possible\nto obtain complexity results for both convex and nonconvex problems with the\n(local or global) Lipschitz constant being replaced by a constant of local\nsubgradient variation and (ii) mean width of the subdifferential set around the\noptima plays a role in the complexity of nonsmooth optimization, particularly\nin parallel settings. A consequence of (ii) is that for any error parameter\n$\\epsilon > 0$, parallel oracle complexity of nonsmooth Lipschitz convex\noptimization is lower than its sequential oracle complexity by a factor\n$\\tilde{\\Omega}\\big(\\frac{1}{\\epsilon}\\big)$ whenever the objective function is\npiecewise linear with polynomially many pieces in the input size. This is\nparticularly surprising as existing parallel complexity lower bounds are based\non such classes of functions. The seeming contradiction is resolved by\nconsidering the region in which the algorithm is allowed to query the\nobjective.\n","authors":["Jelena Diakonikolas","Cristóbal Guzmán"],"pdf_url":"https://arxiv.org/pdf/2403.16317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11202v2","updated":"2024-03-24T22:02:47Z","published":"2023-11-19T02:34:12Z","title":"Unmasking and Improving Data Credibility: A Study with Datasets for\n  Training Harmless Language Models","summary":"  Language models have shown promise in various tasks but can be affected by\nundesired data during training, fine-tuning, or alignment. For example, if some\nunsafe conversations are wrongly annotated as safe ones, the model fine-tuned\non these samples may be harmful. Therefore, the correctness of annotations,\ni.e., the credibility of the dataset, is important. This study focuses on the\ncredibility of real-world datasets, including the popular benchmarks Jigsaw\nCivil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that\ncan be used for training a harmless language model. Given the cost and\ndifficulty of cleaning these datasets by humans, we introduce a systematic\nframework for evaluating the credibility of datasets, identifying label errors,\nand evaluating the influence of noisy labels in the curated language data,\nspecifically focusing on unsafe comments and conversation classification. With\nthe framework, we find and fix an average of 6.16% label errors in 11 datasets\nconstructed from the above benchmarks. The data credibility and downstream\nlearning performance can be remarkably improved by directly fixing label\nerrors, indicating the significance of cleaning existing real-world datasets.\nWe provide an open-source tool, Docta, for data cleaning at\nhttps://github.com/Docta-ai/docta.\n","authors":["Zhaowei Zhu","Jialu Wang","Hao Cheng","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.11202v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2302.05049v4","updated":"2024-03-24T22:00:52Z","published":"2023-02-10T04:46:22Z","title":"Principled Federated Domain Adaptation: Gradient Projection and\n  Auto-Weighting","summary":"  Federated Domain Adaptation (FDA) describes the federated learning (FL)\nsetting where source clients and a server work collaboratively to improve the\nperformance of a target client where limited data is available. The domain\nshift between the source and target domains, coupled with limited data of the\ntarget client, makes FDA a challenging problem, e.g., common techniques such as\nfederated averaging and fine-tuning fail due to domain shift and data scarcity.\nTo theoretically understand the problem, we introduce new metrics that\ncharacterize the FDA setting and a theoretical framework with novel theorems\nfor analyzing the performance of server aggregation rules. Further, we propose\na novel lightweight aggregation rule, Federated Gradient Projection\n($\\texttt{FedGP}$), which significantly improves the target performance with\ndomain shift and data scarcity. Moreover, our theory suggests an\n$\\textit{auto-weighting scheme}$ that finds the optimal combinations of the\nsource and target gradients. This scheme improves both $\\texttt{FedGP}$ and a\nsimpler heuristic aggregation rule. Extensive experiments verify the\ntheoretical insights and illustrate the effectiveness of the proposed methods\nin practice.\n","authors":["Enyi Jiang","Yibo Jacky Zhang","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2302.05049v4.pdf","comment":"ICLR 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.16143v1","updated":"2024-03-24T13:31:31Z","published":"2024-03-24T13:31:31Z","title":"CFAT: Unleashing TriangularWindows for Image Super-resolution","summary":"  Transformer-based models have revolutionized the field of image\nsuper-resolution (SR) by harnessing their inherent ability to capture complex\ncontextual features. The overlapping rectangular shifted window technique used\nin transformer architecture nowadays is a common practice in super-resolution\nmodels to improve the quality and robustness of image upscaling. However, it\nsuffers from distortion at the boundaries and has limited unique shifting\nmodes. To overcome these weaknesses, we propose a non-overlapping triangular\nwindow technique that synchronously works with the rectangular one to mitigate\nboundary-level distortion and allows the model to access more unique sifting\nmodes. In this paper, we propose a Composite Fusion Attention Transformer\n(CFAT) that incorporates triangular-rectangular window-based local attention\nwith a channel-based global attention technique in image super-resolution. As a\nresult, CFAT enables attention mechanisms to be activated on more image pixels\nand captures long-range, multi-scale features to improve SR performance. The\nextensive experimental results and ablation study demonstrate the effectiveness\nof CFAT in the SR domain. Our proposed model shows a significant 0.7 dB\nperformance improvement over other state-of-the-art SR architectures.\n","authors":["Abhisek Ray","Gaurav Kumar","Maheshkumar H. Kolekar"],"pdf_url":"https://arxiv.org/pdf/2403.16143v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16071v1","updated":"2024-03-24T09:18:21Z","published":"2024-03-24T09:18:21Z","title":"Landmark-Guided Cross-Speaker Lip Reading with Mutual Information\n  Regularization","summary":"  Lip reading, the process of interpreting silent speech from visual lip\nmovements, has gained rising attention for its wide range of realistic\napplications. Deep learning approaches greatly improve current lip reading\nsystems. However, lip reading in cross-speaker scenarios where the speaker\nidentity changes, poses a challenging problem due to inter-speaker variability.\nA well-trained lip reading system may perform poorly when handling a brand new\nspeaker. To learn a speaker-robust lip reading model, a key insight is to\nreduce visual variations across speakers, avoiding the model overfitting to\nspecific speakers. In this work, in view of both input visual clues and latent\nrepresentations based on a hybrid CTC/attention architecture, we propose to\nexploit the lip landmark-guided fine-grained visual clues instead of\nfrequently-used mouth-cropped images as input features, diminishing\nspeaker-specific appearance characteristics. Furthermore, a max-min mutual\ninformation regularization approach is proposed to capture speaker-insensitive\nlatent representations. Experimental evaluations on public lip reading datasets\ndemonstrate the effectiveness of the proposed approach under the intra-speaker\nand inter-speaker conditions.\n","authors":["Linzhi Wu","Xingyu Zhang","Yakun Zhang","Changyan Zheng","Tiejun Liu","Liang Xie","Ye Yan","Erwei Yin"],"pdf_url":"https://arxiv.org/pdf/2403.16071v1.pdf","comment":"To appear in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.11311v2","updated":"2024-03-24T06:21:27Z","published":"2024-03-17T19:12:26Z","title":"Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding","summary":"  Deep multimodal semantic understanding that goes beyond the mere superficial\ncontent relation mining has received increasing attention in the realm of\nartificial intelligence. The challenges of collecting and annotating\nhigh-quality multi-modal data have underscored the significance of few-shot\nlearning. In this paper, we focus on two critical tasks under this context:\nfew-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis\n(MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware\nPrompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on\nthe unified vision-language model (VLM). Specifically, we design three experts\nof soft prompts: a text prompt and an image prompt that extract\nmodality-specific features to enrich the single-modal representation, and a\nunified prompt to assist multi-modal interaction. Additionally, we reorganize\nTransformer layers into several blocks and introduce cross-modal prompt\nattention between adjacent blocks, which smoothens the transition from\nsingle-modal representation to multi-modal fusion. On both MSD and MSA datasets\nin few-shot setting, our proposed model not only surpasses the 8.2B model\nInstructBLIP with merely 2% parameters (150M), but also significantly\noutperforms other widely-used prompt methods on VLMs or task-specific methods.\n","authors":["Zichen Wu","Hsiu-Yuan Huang","Fanyi Qu","Yunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11311v2.pdf","comment":"LREC-COLING 2024, Long Paper"}]},"2024-03-23T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.15937v1","updated":"2024-03-23T21:54:18Z","published":"2024-03-23T21:54:18Z","title":"Model, Analyze, and Comprehend User Interactions and Various Attributes\n  within a Social Media Platform","summary":"  How can we effectively model, analyze, and comprehend user interactions and\nvarious attributes within a social media platform based on post-comment\nrelationship? In this study, we propose a novel graph-based approach to model\nand analyze user interactions within a social media platform based on\npost-comment relationship. We construct a user interaction graph from social\nmedia data and analyze it to gain insights into community dynamics, user\nbehavior, and content preferences. Our investigation reveals that while 56.05%\nof the active users are strongly connected within the community, only 0.8% of\nthem significantly contribute to its dynamics. Moreover, we observe temporal\nvariations in community activity, with certain periods experiencing heightened\nengagement. Additionally, our findings highlight a correlation between user\nactivity and popularity showing that more active users are generally more\npopular. Alongside these, a preference for positive and informative content is\nalso observed where 82.41% users preferred positive and informative content.\nOverall, our study provides a comprehensive framework for understanding and\nmanaging online communities, leveraging graph-based techniques to gain valuable\ninsights into user behavior and community dynamics.\n","authors":["Md Kaykobad Reza","S M Maksudul Alam","Yiran Luo","Youzhe Liu"],"pdf_url":"https://arxiv.org/pdf/2403.15937v1.pdf","comment":"9 Pages, 8 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2309.01157v2","updated":"2024-03-23T17:05:42Z","published":"2023-09-03T12:33:47Z","title":"Large Language Models for Generative Recommendation: A Survey and\n  Visionary Discussions","summary":"  Large language models (LLM) not only have revolutionized the field of natural\nlanguage processing (NLP) but also have the potential to reshape many other\nfields, e.g., recommender systems (RS). However, most of the related work\ntreats an LLM as a component of the conventional recommendation pipeline (e.g.,\nas a feature extractor), which may not be able to fully leverage the generative\npower of LLM. Instead of separating the recommendation process into multiple\nstages, such as score computation and re-ranking, this process can be\nsimplified to one stage with LLM: directly generating recommendations from the\ncomplete pool of items. This survey reviews the progress, methods, and future\ndirections of LLM-based generative recommendation by examining three questions:\n1) What generative recommendation is, 2) Why RS should advance to generative\nrecommendation, and 3) How to implement LLM-based generative recommendation for\nvarious RS tasks. We hope that this survey can provide the context and guidance\nneeded to explore this interesting and emerging topic.\n","authors":["Lei Li","Yongfeng Zhang","Dugang Liu","Li Chen"],"pdf_url":"https://arxiv.org/pdf/2309.01157v2.pdf","comment":"Published as a conference paper at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.13597v2","updated":"2024-03-23T17:05:15Z","published":"2024-03-20T13:44:30Z","title":"No more optimization rules: LLM-enabled policy-based multi-modal query\n  optimizer","summary":"  Large language model (LLM) has marked a pivotal moment in the field of\nmachine learning and deep learning. Recently its capability for query planning\nhas been investigated, including both single-modal and multi-modal queries.\nHowever, there is no work on the query optimization capability of LLM. As a\ncritical (or could even be the most important) step that significantly impacts\nthe execution performance of the query plan, such analysis and attempts should\nnot be missed. From another aspect, existing query optimizers are usually\nrule-based or rule-based + cost-based, i.e., they are dependent on manually\ncreated rules to complete the query plan rewrite/transformation. Given the fact\nthat modern optimizers include hundreds to thousands of rules, designing a\nmulti-modal query optimizer following a similar way is significantly\ntime-consuming since we will have to enumerate as many multi-modal optimization\nrules as possible, which has not been well addressed today. In this paper, we\ninvestigate the query optimization ability of LLM and use LLM to design LaPuda,\na novel LLM and Policy based multi-modal query optimizer. Instead of\nenumerating specific and detailed rules, LaPuda only needs a few abstract\npolicies to guide LLM in the optimization, by which much time and human effort\nare saved. Furthermore, to prevent LLM from making mistakes or negative\noptimization, we borrow the idea of gradient descent and propose a guided cost\ndescent (GCD) algorithm to perform the optimization, such that the optimization\ncan be kept in the correct direction. In our evaluation, our methods\nconsistently outperform the baselines in most cases. For example, the optimized\nplans generated by our methods result in 1~3x higher execution speed than those\nby the baselines.\n","authors":["Yifan Wang","Haodi Ma","Daisy Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.13597v2.pdf","comment":"Yifan and Haodi contribute equally to the work"},{"id":"http://arxiv.org/abs/2401.11624v5","updated":"2024-03-23T16:35:45Z","published":"2024-01-21T23:34:42Z","title":"In-context Learning with Retrieved Demonstrations for Language Models: A\n  Survey","summary":"  Language models, especially pre-trained large language models, have showcased\nremarkable abilities as few-shot in-context learners (ICL), adept at adapting\nto new tasks with just a few demonstrations in the input context. However, the\nmodel's ability to perform ICL is sensitive to the choice of the few-shot\ndemonstrations. Instead of using a fixed set of demonstrations, one recent\ndevelopment is to retrieve demonstrations tailored to each input query. The\nimplementation of demonstration retrieval is relatively straightforward,\nleveraging existing databases and retrieval systems. This not only improves the\nefficiency and scalability of the learning process but also has been shown to\nreduce biases inherent in manual example selection. In light of the encouraging\nresults and growing research in ICL with retrieved demonstrations, we conduct\nan extensive review of studies in this area. In this survey, we discuss and\ncompare different design choices for retrieval models, retrieval training\nprocedures, and inference algorithms.\n","authors":["Man Luo","Xin Xu","Yue Liu","Panupong Pasupat","Mehran Kazemi"],"pdf_url":"https://arxiv.org/pdf/2401.11624v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15765v1","updated":"2024-03-23T08:40:35Z","published":"2024-03-23T08:40:35Z","title":"Towards Human-Like Machine Comprehension: Few-Shot Relational Learning\n  in Visually-Rich Documents","summary":"  Key-value relations are prevalent in Visually-Rich Documents (VRDs), often\ndepicted in distinct spatial regions accompanied by specific color and font\nstyles. These non-textual cues serve as important indicators that greatly\nenhance human comprehension and acquisition of such relation triplets. However,\ncurrent document AI approaches often fail to consider this valuable prior\ninformation related to visual and spatial features, resulting in suboptimal\nperformance, particularly when dealing with limited examples. To address this\nlimitation, our research focuses on few-shot relational learning, specifically\ntargeting the extraction of key-value relation triplets in VRDs. Given the\nabsence of a suitable dataset for this task, we introduce two new few-shot\nbenchmarks built upon existing supervised benchmark datasets. Furthermore, we\npropose a variational approach that incorporates relational 2D-spatial priors\nand prototypical rectification techniques. This approach aims to generate\nrelation representations that are more aware of the spatial context and unseen\nrelation in a manner similar to human perception. Experimental results\ndemonstrate the effectiveness of our proposed method by showcasing its ability\nto outperform existing methods. This study also opens up new possibilities for\npractical applications.\n","authors":["Hao Wang","Tang Li","Chenhui Chu","Nengjun Zhu","Rui Wang","Pinpin Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.15765v1.pdf","comment":"13 pages, 7 figures, accepted by LERC-COLING2024"},{"id":"http://arxiv.org/abs/2403.15757v1","updated":"2024-03-23T08:03:50Z","published":"2024-03-23T08:03:50Z","title":"User-Side Realization","summary":"  Users are dissatisfied with services. Since the service is not tailor-made\nfor a user, it is natural for dissatisfaction to arise. The problem is, that\neven if users are dissatisfied, they often do not have the means to resolve\ntheir dissatisfaction. The user cannot alter the source code of the service,\nnor can they force the service provider to change. The user has no choice but\nto remain dissatisfied or quit the service. User-side realization offers\nproactive solutions to this problem by providing general algorithms to deal\nwith common problems on the user's side. These algorithms run on the user's\nside and solve the problems without having the service provider change the\nservice itself.\n","authors":["Ryoma Sato"],"pdf_url":"https://arxiv.org/pdf/2403.15757v1.pdf","comment":"Doctoral Thesis"},{"id":"http://arxiv.org/abs/2403.15667v1","updated":"2024-03-23T01:30:46Z","published":"2024-03-23T01:30:46Z","title":"QueryExplorer: An Interactive Query Generation Assistant for Search and\n  Exploration","summary":"  Formulating effective search queries remains a challenging task, particularly\nwhen users lack expertise in a specific domain or are not proficient in the\nlanguage of the content. Providing example documents of interest might be\neasier for a user. However, such query-by-example scenarios are prone to\nconcept drift, and the retrieval effectiveness is highly sensitive to the query\ngeneration method, without a clear way to incorporate user feedback. To enable\nexploration and to support Human-In-The-Loop experiments we propose\nQueryExplorer -- an interactive query generation, reformulation, and retrieval\ninterface with support for HuggingFace generation models and PyTerrier's\nretrieval pipelines and datasets, and extensive logging of human feedback. To\nallow users to create and modify effective queries, our demo supports\ncomplementary approaches of using LLMs interactively, assisting the user with\nedits and feedback at multiple stages of the query formulation process. With\nsupport for recording fine-grained interactions and user annotations,\nQueryExplorer can serve as a valuable experimental and research platform for\nannotation, qualitative evaluation, and conducting Human-in-the-Loop (HITL)\nexperiments for complex search tasks where users struggle to formulate queries.\n","authors":["Kaustubh D. Dhole","Shivam Bajaj","Ramraj Chandradevan","Eugene Agichtein"],"pdf_url":"https://arxiv.org/pdf/2403.15667v1.pdf","comment":"NAACL 2024 Demonstration Track"},{"id":"http://arxiv.org/abs/2403.15740v1","updated":"2024-03-23T06:36:32Z","published":"2024-03-23T06:36:32Z","title":"Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large\n  Language Models","summary":"  Web user data plays a central role in the ecosystem of pre-trained large\nlanguage models (LLMs) and their fine-tuned variants. Billions of data are\ncrawled from the web and fed to LLMs. How can \\textit{\\textbf{everyday web\nusers}} confirm if LLMs misuse their data without permission? In this work, we\nsuggest that users repeatedly insert personal passphrases into their documents,\nenabling LLMs to memorize them. These concealed passphrases in user documents,\nreferred to as \\textit{ghost sentences}, once they are identified in the\ngenerated content of LLMs, users can be sure that their data is used for\ntraining. To explore the effectiveness and usage of this copyrighting tool, we\ndefine the \\textit{user training data identification} task with ghost\nsentences. Multiple datasets from various sources at different scales are\ncreated and tested with LLMs of different sizes. For evaluation, we introduce a\nlast $k$ words verification manner along with two metrics: document and user\nidentification accuracy. In the specific case of instruction tuning of a 3B\nLLaMA model, 11 out of 16 users with ghost sentences identify their data within\nthe generation content. These 16 users contribute 383 examples to $\\sim$1.8M\ntraining documents. For continuing pre-training of a 1.1B TinyLlama model, 61\nout of 64 users with ghost sentences identify their data within the LLM output.\nThese 64 users contribute 1156 examples to $\\sim$10M training documents.\n","authors":["Shuai Zhao","Linchao Zhu","Ruijie Quan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.15740v1.pdf","comment":"Preprint, work in progress"}],"Multimedia":[{"id":"http://arxiv.org/abs/2309.11093v3","updated":"2024-03-23T13:33:06Z","published":"2023-09-20T06:54:55Z","title":"K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling","summary":"  Lyric translation, a field studied for over a century, is now attracting\ncomputational linguistics researchers. We identified two limitations in\nprevious studies. Firstly, lyric translation studies have predominantly focused\non Western genres and languages, with no previous study centering on K-pop\ndespite its popularity. Second, the field of lyric translation suffers from a\nlack of publicly available datasets; to the best of our knowledge, no such\ndataset exists. To broaden the scope of genres and languages in lyric\ntranslation studies, we introduce a novel singable lyric translation dataset,\napproximately 89\\% of which consists of K-pop song lyrics. This dataset aligns\nKorean and English lyrics line-by-line and section-by-section. We leveraged\nthis dataset to unveil unique characteristics of K-pop lyric translation,\ndistinguishing it from other extensively studied genres, and to construct a\nneural lyric translation model, thereby underscoring the importance of a\ndedicated dataset for singable lyric translations.\n","authors":["Haven Kim","Jongmin Jung","Dasaem Jeong","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2309.11093v3.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.15694v1","updated":"2024-03-23T03:06:19Z","published":"2024-03-23T03:06:19Z","title":"Group Benefits Instances Selection for Data Purification","summary":"  Manually annotating datasets for training deep models is very labor-intensive\nand time-consuming. To overcome such inferiority, directly leveraging web\nimages to conduct training data becomes a natural choice. Nevertheless, the\npresence of label noise in web data usually degrades the model performance.\nExisting methods for combating label noise are typically designed and tested on\nsynthetic noisy datasets. However, they tend to fail to achieve satisfying\nresults on real-world noisy datasets. To this end, we propose a method named\nGRIP to alleviate the noisy label problem for both synthetic and real-world\ndatasets. Specifically, GRIP utilizes a group regularization strategy that\nestimates class soft labels to improve noise robustness. Soft label supervision\nreduces overfitting on noisy labels and learns inter-class similarities to\nbenefit classification. Furthermore, an instance purification operation\nglobally identifies noisy labels by measuring the difference between each\ntraining sample and its class soft label. Through operations at both group and\ninstance levels, our approach integrates the advantages of noise-robust and\nnoise-cleaning methods and remarkably alleviates the performance degradation\ncaused by noisy labels. Comprehensive experimental results on synthetic and\nreal-world datasets demonstrate the superiority of GRIP over the existing\nstate-of-the-art methods.\n","authors":["Zhenhuang Cai","Chuanyi Zhang","Dan Huang","Yuanbo Chen","Xiuyun Guan","Yazhou Yao"],"pdf_url":"https://arxiv.org/pdf/2403.15694v1.pdf","comment":"accepted by IEEE Intelligent Systems"},{"id":"http://arxiv.org/abs/2403.15679v1","updated":"2024-03-23T02:09:23Z","published":"2024-03-23T02:09:23Z","title":"DS-NeRV: Implicit Neural Video Representation with Decomposed Static and\n  Dynamic Codes","summary":"  Implicit neural representations for video (NeRV) have recently become a novel\nway for high-quality video representation. However, existing works employ a\nsingle network to represent the entire video, which implicitly confuse static\nand dynamic information. This leads to an inability to effectively compress the\nredundant static information and lack the explicitly modeling of global\ntemporal-coherent dynamic details. To solve above problems, we propose DS-NeRV,\nwhich decomposes videos into sparse learnable static codes and dynamic codes\nwithout the need for explicit optical flow or residual supervision. By setting\ndifferent sampling rates for two codes and applying weighted sum and\ninterpolation sampling methods, DS-NeRV efficiently utilizes redundant static\ninformation while maintaining high-frequency details. Additionally, we design a\ncross-channel attention-based (CCA) fusion module to efficiently fuse these two\ncodes for frame decoding. Our approach achieves a high quality reconstruction\nof 31.2 PSNR with only 0.35M parameters thanks to separate static and dynamic\ncodes representation and outperforms existing NeRV methods in many downstream\ntasks. Our project website is at https://haoyan14.github.io/DS-NeRV.\n","authors":["Hao Yan","Zhihui Ke","Xiaobo Zhou","Tie Qiu","Xidong Shi","Dadong Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.15679v1.pdf","comment":"CVPR 2024. Project page at https://haoyan14.github.io/DS-NeRV"}]},"2024-03-26T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2402.13452v2","updated":"2024-03-26T17:59:14Z","published":"2024-02-21T01:11:28Z","title":"LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based\n  on Twitter Data","summary":"  Prior research on Twitter (now X) data has provided positive evidence of its\nutility in developing supplementary health surveillance systems. In this study,\nwe present a new framework to surveil public health, focusing on mental health\n(MH) outcomes. We hypothesize that locally posted tweets are indicative of\nlocal MH outcomes and collect tweets posted from 765 neighborhoods (census\nblock groups) in the USA. We pair these tweets from each neighborhood with the\ncorresponding MH outcome reported by the Center for Disease Control (CDC) to\ncreate a benchmark dataset, LocalTweets. With LocalTweets, we present the first\npopulation-level evaluation task for Twitter-based MH surveillance systems. We\nthen develop an efficient and effective method, LocalHealth, for predicting MH\noutcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the\nhighest F1-score and accuracy of 0.7429 and 79.78\\%, respectively, a 59\\%\nimprovement in F1-score over the GPT3.5 in zero-shot setting. We also utilize\nLocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,\nachieving an F1-score of 0.7291. Our work suggests that Twitter data can be\neffectively leveraged to simulate neighborhood-level MH outcomes.\n","authors":["Vijeta Deshpande","Minhwa Lee","Zonghai Yao","Zihao Zhang","Jason Brian Gibbons","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.13452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08763v3","updated":"2024-03-26T17:58:48Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17919v1","updated":"2024-03-26T17:55:02Z","published":"2024-03-26T17:55:02Z","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning","summary":"  The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.\n","authors":["Rui Pan","Xiang Liu","Shizhe Diao","Renjie Pi","Jipeng Zhang","Chi Han","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16248v2","updated":"2024-03-26T17:46:26Z","published":"2024-03-24T17:39:51Z","title":"Large Language Models Offer an Alternative to the Traditional Approach\n  of Topic Modelling","summary":"  Topic modelling, as a well-established unsupervised technique, has found\nextensive use in automatically detecting significant topics within a corpus of\ndocuments. However, classic topic modelling approaches (e.g., LDA) have certain\ndrawbacks, such as the lack of semantic understanding and the presence of\noverlapping topics. In this work, we investigate the untapped potential of\nlarge language models (LLMs) as an alternative for uncovering the underlying\ntopics within extensive text corpora. To this end, we introduce a framework\nthat prompts LLMs to generate topics from a given set of documents and\nestablish evaluation protocols to assess the clustering efficacy of LLMs. Our\nfindings indicate that LLMs with appropriate prompts can stand out as a viable\nalternative, capable of generating relevant topic titles and adhering to human\nguidelines to refine and merge topics. Through in-depth experiments and\nevaluation, we summarise the advantages and constraints of employing LLMs in\ntopic extraction.\n","authors":["Yida Mu","Chun Dong","Kalina Bontcheva","Xingyi Song"],"pdf_url":"https://arxiv.org/pdf/2403.16248v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17887v1","updated":"2024-03-26T17:20:04Z","published":"2024-03-26T17:20:04Z","title":"The Unreasonable Ineffectiveness of the Deeper Layers","summary":"  We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.\n","authors":["Andrey Gromov","Kushal Tirumala","Hassan Shapourian","Paolo Glorioso","Daniel A. Roberts"],"pdf_url":"https://arxiv.org/pdf/2403.17887v1.pdf","comment":"12 + 10 pages, 5 + 4 figures"},{"id":"http://arxiv.org/abs/2403.17860v1","updated":"2024-03-26T16:49:25Z","published":"2024-03-26T16:49:25Z","title":"Exploring LLMs as a Source of Targeted Synthetic Textual Data to\n  Minimize High Confidence Misclassifications","summary":"  Natural Language Processing (NLP) models optimized for predictive performance\noften make high confidence errors and suffer from vulnerability to adversarial\nand out-of-distribution data. Existing work has mainly focused on mitigation of\nsuch errors using either humans or an automated approach. In this study, we\nexplore the usage of large language models (LLMs) for data augmentation as a\npotential solution to the issue of NLP models making wrong predictions with\nhigh confidence during classification tasks. We compare the effectiveness of\nsynthetic data generated by LLMs with that of human data obtained via the same\nprocedure. For mitigation, humans or LLMs provide natural language\ncharacterizations of high confidence misclassifications to generate synthetic\ndata, which are then used to extend the training set. We conduct an extensive\nevaluation of our approach on three classification tasks and demonstrate its\neffectiveness in reducing the number of high confidence misclassifications\npresent in the model, all while maintaining the same level of accuracy.\nMoreover, we find that the cost gap between humans and LLMs surpasses an order\nof magnitude, as LLMs attain human-like performance while being more scalable.\n","authors":["Philip Lippmann","Matthijs Spaan","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17859v1","updated":"2024-03-26T16:48:13Z","published":"2024-03-26T16:48:13Z","title":"ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on\n  Historical American Newspaper Pages","summary":"  Question answering (QA) and Machine Reading Comprehension (MRC) tasks have\nsignificantly advanced in recent years due to the rapid development of deep\nlearning techniques and, more recently, large language models. At the same\ntime, many benchmark datasets have become available for QA and MRC tasks.\nHowever, most existing large-scale benchmark datasets have been created\npredominantly using synchronous document collections like Wikipedia or the Web.\nArchival document collections, such as historical newspapers, contain valuable\ninformation from the past that is still not widely used to train large language\nmodels. To further contribute to advancing QA and MRC tasks and to overcome the\nlimitation of previous datasets, we introduce ChroniclingAmericaQA, a\nlarge-scale dataset with 485K question-answer pairs created based on the\nhistorical newspaper collection Chronicling America. Our dataset is constructed\nfrom a subset of the Chronicling America newspaper collection spanning 120\nyears. One of the significant challenges for utilizing digitized historical\nnewspaper collections is the low quality of OCR text. Therefore, to enable\nrealistic testing of QA models, our dataset can be used in three different\nways: answering questions from raw and noisy content, answering questions from\ncleaner, corrected version of the content, as well as answering questions from\nscanned images of newspaper pages. This and the fact that ChroniclingAmericaQA\nspans the longest time period among available QA datasets make it quite a\nunique and useful resource.\n","authors":["Bhawna Piryani","Jamshid Mozafari","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.17859v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.17856v1","updated":"2024-03-26T16:45:27Z","published":"2024-03-26T16:45:27Z","title":"Verbing Weirds Language (Models): Evaluation of English Zero-Derivation\n  in Five LLMs","summary":"  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)\nis a hallmark of English morphology. In conversion, a word with one part of\nspeech is placed in a non-prototypical context, where it is coerced to behave\nas if it had a different part of speech. However, while this process affects a\nlarge part of the English lexicon, little work has been done to establish the\ndegree to which language models capture this type of generalization. This paper\nreports the first study on the behavior of large language models with reference\nto conversion. We design a task for testing lexical-syntactic flexibility --\nthe degree to which models can generalize over words in a construction with a\nnon-prototypical part of speech. This task is situated within a natural\nlanguage inference paradigm. We test the abilities of five language models --\ntwo proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral\n7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,\nfollowed by GPT-3.5, but that the open source language models are also able to\nperform it and that the 7B parameter Mistral displays as little difference\nbetween its baseline performance on the natural language inference task and the\nnon-prototypical syntactic category task, as the massive GPT-4.\n","authors":["David R. Mortensen","Valentina Izrailevitch","Yunze Xiao","Hinrich Schütze","Leonie Weissweiler"],"pdf_url":"https://arxiv.org/pdf/2403.17856v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.06795v2","updated":"2024-03-26T16:44:34Z","published":"2024-01-08T18:42:55Z","title":"AI and Generative AI for Research Discovery and Summarization","summary":"  AI and generative AI tools, including chatbots like ChatGPT that rely on\nlarge language models (LLMs), have burst onto the scene this year, creating\nincredible opportunities to increase work productivity and improve our lives.\nStatisticians and data scientists have begun experiencing the benefits from the\navailability of these tools in numerous ways, such as the generation of\nprogramming code from text prompts to analyze data or fit statistical models.\nOne area that these tools can make a substantial impact is in research\ndiscovery and summarization. Standalone tools and plugins to chatbots are being\ndeveloped that allow researchers to more quickly find relevant literature than\npre-2023 search tools. Furthermore, generative AI tools have improved to the\npoint where they can summarize and extract the key points from research\narticles in succinct language. Finally, chatbots based on highly parameterized\nLLMs can be used to simulate abductive reasoning, which provides researchers\nthe ability to make connections among related technical topics, which can also\nbe used for research discovery. We review the developments in AI and generative\nAI for research discovery and summarization, and propose directions where these\ntypes of tools are likely to head in the future that may be of interest to\nstatistician and data scientists.\n","authors":["Mark Glickman","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.06795v2.pdf","comment":"29 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.17853v1","updated":"2024-03-26T16:42:30Z","published":"2024-03-26T16:42:30Z","title":"Using Domain Knowledge to Guide Dialog Structure Induction via Neural\n  Probabilistic Soft Logic","summary":"  Dialog Structure Induction (DSI) is the task of inferring the latent dialog\nstructure (i.e., a set of dialog states and their temporal transitions) of a\ngiven goal-oriented dialog. It is a critical component for modern dialog system\ndesign and discourse analysis. Existing DSI approaches are often purely\ndata-driven, deploy models that infer latent states without access to domain\nknowledge, underperform when the training corpus is limited/noisy, or have\ndifficulty when test dialogs exhibit distributional shifts from the training\ndomain. This work explores a neural-symbolic approach as a potential solution\nto these problems. We introduce Neural Probabilistic Soft Logic Dialogue\nStructure Induction (NEUPSL DSI), a principled approach that injects symbolic\nknowledge into the latent space of a generative neural model. We conduct a\nthorough empirical investigation on the effect of NEUPSL DSI learning on hidden\nrepresentation quality, few-shot learning, and out-of-domain generalization\nperformance. Over three dialog structure induction datasets and across\nunsupervised and semi-supervised settings for standard and cross-domain\ngeneralization, the injection of symbolic knowledge using NEUPSL DSI provides a\nconsistent boost in performance over the canonical baselines.\n","authors":["Connor Pryor","Quan Yuan","Jeremiah Liu","Mehran Kazemi","Deepak Ramachandran","Tania Bedrax-Weiss","Lise Getoor"],"pdf_url":"https://arxiv.org/pdf/2403.17853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11278v3","updated":"2024-03-26T16:40:50Z","published":"2023-07-21T00:34:38Z","title":"Generator-Retriever-Generator Approach for Open-Domain Question\n  Answering","summary":"  Open-domain question answering (QA) tasks usually require the retrieval of\nrelevant information from a large corpus to generate accurate answers. We\npropose a novel approach called Generator-Retriever-Generator (GRG) that\ncombines document retrieval techniques with a large language model (LLM), by\nfirst prompting the model to generate contextual documents based on a given\nquestion. In parallel, a dual-encoder network retrieves documents that are\nrelevant to the question from an external corpus. The generated and retrieved\ndocuments are then passed to the second LLM, which generates the final answer.\nBy combining document retrieval and LLM generation, our approach addresses the\nchallenges of open-domain QA, such as generating informative and contextually\nrelevant answers. GRG outperforms the state-of-the-art generate-then-read and\nretrieve-then-read pipelines (GENREAD and RFiD) improving their performance by\nat least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets,\nrespectively. We provide code, datasets, and checkpoints at\nhttps://github.com/abdoelsayed2016/GRG.\n","authors":["Abdelrahman Abdallah","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2307.11278v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17848v1","updated":"2024-03-26T16:37:54Z","published":"2024-03-26T16:37:54Z","title":"ArabicaQA: A Comprehensive Dataset for Arabic Question Answering","summary":"  In this paper, we address the significant gap in Arabic natural language\nprocessing (NLP) resources by introducing ArabicaQA, the first large-scale\ndataset for machine reading comprehension and open-domain question answering in\nArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701\nunanswerable questions created by crowdworkers to look similar to answerable\nones, along with additional labels of open-domain questions marks a crucial\nadvancement in Arabic NLP resources. We also present AraDPR, the first dense\npassage retrieval model trained on the Arabic Wikipedia corpus, specifically\ndesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,\nour study includes extensive benchmarking of large language models (LLMs) for\nArabic question answering, critically evaluating their performance in the\nArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking\nof LLMs in Arabic question answering offer significant advancements in the\nfield of Arabic NLP. The dataset and code are publicly accessible for further\nresearch https://github.com/DataScienceUIBK/ArabicaQA.\n","authors":["Abdelrahman Abdallah","Mahmoud Kasem","Mahmoud Abdalla","Mohamed Mahmoud","Mohamed Elkasaby","Yasser Elbendary","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.17848v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2305.03123v2","updated":"2024-03-26T16:22:54Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for AI policy act, if designed by the governments.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v2.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2309.09800v3","updated":"2024-03-26T16:05:51Z","published":"2023-09-18T14:18:19Z","title":"AMuRD: Annotated Arabic-English Receipt Dataset for Key Information\n  Extraction and Classification","summary":"  The extraction of key information from receipts is a complex task that\ninvolves the recognition and extraction of text from scanned receipts. This\nprocess is crucial as it enables the retrieval of essential content and\norganizing it into structured documents for easy access and analysis. In this\npaper, we present AMuRD, a novel multilingual human-annotated dataset\nspecifically designed for information extraction from receipts. This dataset\ncomprises $47,720$ samples and addresses the key challenges in information\nextraction and item classification - the two critical aspects of data analysis\nin the retail industry. Each sample includes annotations for item names and\nattributes such as price, brand, and more. This detailed annotation facilitates\na comprehensive understanding of each item on the receipt. Furthermore, the\ndataset provides classification into $44$ distinct product categories. This\nclassification feature allows for a more organized and efficient analysis of\nthe items, enhancing the usability of the dataset for various applications. In\nour study, we evaluated various language model architectures, e.g., by\nfine-tuning LLaMA models on the AMuRD dataset. Our approach yielded exceptional\nresults, with an F1 score of 97.43\\% and accuracy of 94.99\\% in information\nextraction and classification, and an even higher F1 score of 98.51\\% and\naccuracy of 97.06\\% observed in specific tasks. The dataset and code are\npublicly accessible for further\nresearchhttps://github.com/Update-For-Integrated-Business-AI/AMuRD.\n","authors":["Abdelrahman Abdallah","Mahmoud Abdalla","Mohamed Elkasaby","Yasser Elbendary","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2309.09800v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03742v2","updated":"2024-03-26T16:03:57Z","published":"2023-08-07T17:46:49Z","title":"Training BERT Models to Carry Over a Coding System Developed on One\n  Corpus to Another","summary":"  This paper describes how we train BERT models to carry over a coding system\ndeveloped on the paragraphs of a Hungarian literary journal to another. The aim\nof the coding system is to track trends in the perception of literary\ntranslation around the political transformation in 1989 in Hungary. To evaluate\nnot only task performance but also the consistence of the annotation, moreover,\nto get better predictions from an ensemble, we use 10-fold crossvalidation.\nExtensive hyperparameter tuning is used to obtain the best possible results and\nfair comparisons. To handle label imbalance, we use loss functions and metrics\nrobust to it. Evaluation of the effect of domain shift is carried out by\nsampling a test set from the target domain. We establish the sample size by\nestimating the bootstrapped confidence interval via simulations. This way, we\nshow that our models can carry over one annotation system to the target domain.\nComparisons are drawn to provide insights such as learning multilabel\ncorrelations and confidence penalty improve resistance to domain shift, and\ndomain adaptation on OCR-ed text on another domain improves performance almost\nto the same extent as that on the corpus under study. See our code at\nhttps://codeberg.org/zsamboki/bert-annotator-ensemble.\n","authors":["Dalma Galambos","Pál Zsámboki"],"pdf_url":"https://arxiv.org/pdf/2308.03742v2.pdf","comment":"Camera-ready version, to be presented at the 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation\n  (LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2311.15964v2","updated":"2024-03-26T15:58:26Z","published":"2023-11-27T16:07:37Z","title":"Efficient Pre-training for Localized Instruction Generation of Videos","summary":"  Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.\n","authors":["Anil Batra","Davide Moltisanti","Laura Sevilla-Lara","Marcus Rohrbach","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2311.15964v2.pdf","comment":"This version has some missing experiments and elaborative technical\n  details"},{"id":"http://arxiv.org/abs/2403.17816v1","updated":"2024-03-26T15:53:02Z","published":"2024-03-26T15:53:02Z","title":"Graph Language Model (GLM): A new graph-based approach to detect social\n  instabilities","summary":"  This scientific report presents a novel methodology for the early prediction\nof important political events using News datasets. The methodology leverages\nnatural language processing, graph theory, clique analysis, and semantic\nrelationships to uncover hidden predictive signals within the data. Initially,\nwe designed a preliminary version of the method and tested it on a few events.\nThis analysis revealed limitations in the initial research phase. We then\nenhanced the model in two key ways: first, we added a filtration step to only\nconsider politically relevant news before further processing; second, we\nadjusted the input features to make the alert system more sensitive to\nsignificant spikes in the data. After finalizing the improved methodology, we\ntested it on eleven events including US protests, the Ukraine war, and French\nprotests. Results demonstrate the superiority of our approach compared to\nbaseline methods. Through targeted refinements, our model can now provide\nearlier and more accurate predictions of major political events based on subtle\npatterns in news data.\n","authors":["Wallyson Lemes de Oliveira","Vahid Shamsaddini","Ali Ghofrani","Rahul Singh Inda","Jithendra Sai Veeramaneni","Étienne Voutaz"],"pdf_url":"https://arxiv.org/pdf/2403.17816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17811v1","updated":"2024-03-26T15:50:37Z","published":"2024-03-26T15:50:37Z","title":"Are Compressed Language Models Less Subgroup Robust?","summary":"  To reduce the inference cost of large language models, model compression is\nincreasingly used to create smaller scalable models. However, little is known\nabout their robustness to minority subgroups defined by the labels and\nattributes of a dataset. In this paper, we investigate the effects of 18\ndifferent compression methods and settings on the subgroup robustness of BERT\nlanguage models. We show that worst-group performance does not depend on model\nsize alone, but also on the compression method used. Additionally, we find that\nmodel compression does not always worsen the performance on minority subgroups.\nAltogether, our analysis serves to further research into the subgroup\nrobustness of model compression.\n","authors":["Leonidas Gee","Andrea Zugarini","Novi Quadrianto"],"pdf_url":"https://arxiv.org/pdf/2403.17811v1.pdf","comment":"The 2023 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2023)"},{"id":"http://arxiv.org/abs/2401.11911v4","updated":"2024-03-26T15:47:14Z","published":"2024-01-22T12:54:04Z","title":"Blinded by Generated Contexts: How Language Models Merge Generated and\n  Retrieved Contexts for Open-Domain QA?","summary":"  While auxiliary information has become a key to enhancing Large Language\nModels (LLMs), relatively little is known about how LLMs merge these contexts,\nspecifically contexts generated by LLMs and those retrieved from external\nsources. To investigate this, we formulate a systematic framework to identify\nwhether LLMs' responses, derived from the integration of generated and\nretrieved contexts, are attributed to either generated or retrieved contexts.\nTo easily trace the origin of the response, we construct datasets with\nconflicting contexts, i.e., each question is paired with both generated and\nretrieved contexts, yet only one of them contains the correct answer. Our\nexperiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to\nfavor generated contexts, even when they provide incorrect information. We\nfurther identify two key factors contributing to this bias: i) contexts\ngenerated by LLMs typically show greater similarity to the questions,\nincreasing their likelihood of being selected; ii) the segmentation process\nused in retrieved contexts disrupts their completeness, thereby hindering their\nfull utilization in LLMs. Our analysis enhances the understanding of how LLMs\nmerge diverse contexts, offering valuable insights for advancing current\naugmentation methods for LLMs.\n","authors":["Hexiang Tan","Fei Sun","Wanli Yang","Yuanzhuo Wang","Qi Cao","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.11911v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17806v1","updated":"2024-03-26T15:44:58Z","published":"2024-03-26T15:44:58Z","title":"Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding\n  Model Mechanisms","summary":"  Many recent language model (LM) interpretability studies have adopted the\ncircuits framework, which aims to find the minimal computational subgraph, or\ncircuit, that explains LM behavior on a given task. Most studies determine\nwhich edges belong in a LM's circuit by performing causal interventions on each\nedge independently, but this scales poorly with model size. Edge attribution\npatching (EAP), gradient-based approximation to interventions, has emerged as a\nscalable but imperfect solution to this problem. In this paper, we introduce a\nnew method - EAP with integrated gradients (EAP-IG) - that aims to better\nmaintain a core property of circuits: faithfulness. A circuit is faithful if\nall model edges outside the circuit can be ablated without changing the model's\nperformance on the task; faithfulness is what justifies studying circuits,\nrather than the full model. Our experiments demonstrate that circuits found\nusing EAP are less faithful than those found using EAP-IG, even though both\nhave high node overlap with circuits found previously using causal\ninterventions. We conclude more generally that when using circuits to compare\nthe mechanisms models use to solve tasks, faithfulness, not overlap, is what\nshould be measured.\n","authors":["Michael Hanna","Sandro Pezzelle","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.17806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17804v1","updated":"2024-03-26T15:42:01Z","published":"2024-03-26T15:42:01Z","title":"Improving Text-to-Image Consistency via Automatic Prompt Optimization","summary":"  Impressive advances in text-to-image (T2I) generative models have yielded a\nplethora of high performing models which are able to generate aesthetically\nappealing, photorealistic images. Despite the progress, these models still\nstruggle to produce images that are consistent with the input prompt,\noftentimes failing to capture object quantities, relations and attributes\nproperly. Existing solutions to improve prompt-image consistency suffer from\nthe following challenges: (1) they oftentimes require model fine-tuning, (2)\nthey only focus on nearby prompt samples, and (3) they are affected by\nunfavorable trade-offs among image quality, representation diversity, and\nprompt-image consistency. In this paper, we address these challenges and\nintroduce a T2I optimization-by-prompting framework, OPT2I, which leverages a\nlarge language model (LLM) to improve prompt-image consistency in T2I models.\nOur framework starts from a user prompt and iteratively generates revised\nprompts with the goal of maximizing a consistency score. Our extensive\nvalidation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost\nthe initial consistency score by up to 24.9% in terms of DSG score while\npreserving the FID and increasing the recall between generated and real data.\nOur work paves the way toward building more reliable and robust T2I systems by\nharnessing the power of LLMs.\n","authors":["Oscar Mañas","Pietro Astolfi","Melissa Hall","Candace Ross","Jack Urbanek","Adina Williams","Aishwarya Agrawal","Adriana Romero-Soriano","Michal Drozdzal"],"pdf_url":"https://arxiv.org/pdf/2403.17804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07703v2","updated":"2024-03-26T15:31:34Z","published":"2023-11-13T19:41:34Z","title":"Measuring Entrainment in Spontaneous Code-switched Speech","summary":"  It is well-known that speakers who entrain to one another have more\nsuccessful conversations than those who do not. Previous research has shown\nthat interlocutors entrain on linguistic features in both written and spoken\nmonolingual domains. More recent work on code-switched communication has also\nshown preliminary evidence of entrainment on certain aspects of code-switching\n(CSW). However, such studies of entrainment in code-switched domains have been\nextremely few and restricted to human-machine textual interactions. Our work\nstudies code-switched spontaneous speech between humans, finding that (1)\npatterns of written and spoken entrainment in monolingual settings largely\ngeneralize to code-switched settings, and (2) some patterns of entrainment on\ncode-switching in dialogue agent-generated text generalize to spontaneous\ncode-switched speech. Our findings give rise to important implications for the\npotentially \"universal\" nature of entrainment as a communication phenomenon,\nand potential applications in inclusive and interactive speech technology.\n","authors":["Debasmita Bhattacharya","Siying Ding","Alayna Nguyen","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2311.07703v2.pdf","comment":"Edits: camera-ready manuscript for NAACL 2024"},{"id":"http://arxiv.org/abs/2403.01748v2","updated":"2024-03-26T15:26:21Z","published":"2024-03-04T05:55:01Z","title":"Decode Neural signal as Speech","summary":"  Decoding language from brain dynamics is an important open direction in the\nrealm of brain-computer interface (BCI), especially considering the rapid\ngrowth of large language models. Compared to invasive-based signals which\nrequire electrode implantation surgery, non-invasive neural signals (e.g. EEG,\nMEG) have attracted increasing attention considering their safety and\ngenerality. However, the exploration is not adequate in three aspects: 1)\nprevious methods mainly focus on EEG but none of the previous works address\nthis problem on MEG with better signal quality; 2) prior works have\npredominantly used ``teacher-forcing\" during generative decoding, which is\nimpractical; 3) prior works are mostly ``BART-based\" not fully auto-regressive,\nwhich performs better in other sequence tasks. In this paper, we explore the\nbrain-to-text translation of MEG signals in a speech-decoding formation. Here\nwe are the first to investigate a cross-attention-based ``whisper\" model for\ngenerating text directly from MEG signals without teacher forcing. Our model\nachieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \\&\nteacher-forcing on two major datasets (\\textit{GWilliams} and\n\\textit{Schoffelen}). This paper conducts a comprehensive review to understand\nhow speech decoding formation performs on the neural decoding tasks, including\npretraining initialization, training \\& evaluation set splitting, augmentation,\nand scaling law.\n","authors":["Yiqian Yang","Yiqun Duan","Qiang Zhang","Renjing Xu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.01748v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16167v2","updated":"2024-03-26T15:14:25Z","published":"2024-03-24T14:21:06Z","title":"Exploiting Semantic Reconstruction to Mitigate Hallucinations in\n  Vision-Language Models","summary":"  Hallucinations in vision-language models pose a significant challenge to\ntheir reliability, particularly in the generation of long captions. Current\nmethods fall short of accurately identifying and mitigating these\nhallucinations. To address this issue, we introduce ESREAL, a novel\nunsupervised learning framework designed to suppress the generation of\nhallucinations through accurate localization and penalization of hallucinated\ntokens. Initially, ESREAL creates a reconstructed image based on the generated\ncaption and aligns its corresponding regions with those of the original image.\nThis semantic reconstruction aids in identifying both the presence and type of\ntoken-level hallucinations within the generated caption. Subsequently, ESREAL\ncomputes token-level hallucination scores by assessing the semantic similarity\nof aligned regions based on the type of hallucination. Finally, ESREAL employs\na proximal policy optimization algorithm, where it selectively penalizes\nhallucinated tokens according to their token-level hallucination scores. Our\nframework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2\nby 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved\nsolely through signals derived from the image itself, without the need for any\nimage-text pairs.\n","authors":["Minchan Kim","Minyeong Kim","Junik Bae","Suhwan Choi","Sungkyung Kim","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2403.16167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17768v1","updated":"2024-03-26T14:54:48Z","published":"2024-03-26T14:54:48Z","title":"SciNews: From Scholarly Complexities to Public Narratives -- A Dataset\n  for Scientific News Report Generation","summary":"  Scientific news reports serve as a bridge, adeptly translating complex\nresearch articles into reports that resonate with the broader public. The\nautomated generation of such narratives enhances the accessibility of scholarly\ninsights. In this paper, we present a new corpus to facilitate this paradigm\ndevelopment. Our corpus comprises a parallel compilation of academic\npublications and their corresponding scientific news reports across nine\ndisciplines. To demonstrate the utility and reliability of our dataset, we\nconduct an extensive analysis, highlighting the divergences in readability and\nbrevity between scientific news narratives and academic manuscripts. We\nbenchmark our dataset employing state-of-the-art text generation models. The\nevaluation process involves both automatic and human evaluation, which lays the\ngroundwork for future explorations into the automated generation of scientific\nnews reports. The dataset and code related to this work are available at\nhttps://dongqi.me/projects/SciNews.\n","authors":["Dongqi Pu","Yifan Wang","Jia Loy","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2403.17768v1.pdf","comment":"LREC-COLING 2024 Main Conference Paper"},{"id":"http://arxiv.org/abs/2403.17760v1","updated":"2024-03-26T14:51:12Z","published":"2024-03-26T14:51:12Z","title":"Constructions Are So Difficult That Even Large Language Models Get Them\n  Right for the Wrong Reasons","summary":"  In this paper, we make a contribution that can be understood from two\nperspectives: from an NLP perspective, we introduce a small challenge dataset\nfor NLI with large lexical overlap, which minimises the possibility of models\ndiscerning entailment solely based on token distinctions, and show that GPT-4\nand Llama 2 fail it with strong bias. We then create further challenging\nsub-tasks in an effort to explain this failure. From a Computational\nLinguistics perspective, we identify a group of constructions with three\nclasses of adjectives which cannot be distinguished by surface features. This\nenables us to probe for LLM's understanding of these constructions in various\nways, and we find that they fail in a variety of ways to distinguish between\nthem, suggesting that they don't adequately represent their meaning or capture\nthe lexical properties of phrasal heads.\n","authors":["Shijia Zhou","Leonie Weissweiler","Taiqi He","Hinrich Schütze","David R. Mortensen","Lori Levin"],"pdf_url":"https://arxiv.org/pdf/2403.17760v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.11996v2","updated":"2024-03-26T14:46:04Z","published":"2024-03-18T17:30:27Z","title":"Accelerating Scientific Discovery with Generative Knowledge Extraction,\n  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning","summary":"  Leveraging generative Artificial Intelligence (AI), we have transformed a\ndataset comprising 1,000 scientific papers into an ontological knowledge graph.\nThrough an in-depth structural analysis, we have calculated node degrees,\nidentified communities and connectivities, and evaluated clustering\ncoefficients and betweenness centrality of pivotal nodes, uncovering\nfascinating knowledge architectures. The graph has an inherently scale-free\nnature, is highly connected, and can be used for graph reasoning by taking\nadvantage of transitive and isomorphic properties that reveal unprecedented\ninterdisciplinary relationships that can be used to answer queries, identify\ngaps in knowledge, propose never-before-seen material designs, and predict\nmaterial behaviors. We compute deep node embeddings for combinatorial node\nsimilarity ranking for use in a path sampling strategy links dissimilar\nconcepts that have previously not been related. One comparison revealed\nstructural parallels between biological materials and Beethoven's 9th Symphony,\nhighlighting shared patterns of complexity through isomorphic mapping. In\nanother example, the algorithm proposed a hierarchical mycelium-based composite\nbased on integrating path sampling with principles extracted from Kandinsky's\n'Composition VII' painting. The resulting material integrates an innovative set\nof concepts that include a balance of chaos/order, adjustable porosity,\nmechanical strength, and complex patterned chemical functionalization. We\nuncover other isomorphisms across science, technology and art, revealing a\nnuanced ontology of immanence that reveal a context-dependent heterarchical\ninterplay of constituents. Graph-based generative AI achieves a far higher\ndegree of novelty, explorative capacity, and technical detail, than\nconventional approaches and establishes a widely useful framework for\ninnovation by revealing hidden connections.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2403.11996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17752v1","updated":"2024-03-26T14:43:48Z","published":"2024-03-26T14:43:48Z","title":"Can multiple-choice questions really be useful in detecting the\n  abilities of LLMs?","summary":"  Multiple-choice questions (MCQs) are widely used in the evaluation of large\nlanguage models (LLMs) due to their simplicity and efficiency. However, there\nare concerns about whether MCQs can truly measure LLM's capabilities,\nparticularly in knowledge-intensive scenarios where long-form generation (LFG)\nanswers are required. The misalignment between the task and the evaluation\nmethod demands a thoughtful analysis of MCQ's efficacy, which we undertake in\nthis paper by evaluating nine LLMs on four question-answering (QA) datasets in\ntwo languages: Chinese and English. We identify a significant issue: LLMs\nexhibit an order sensitivity in bilingual MCQs, favoring answers located at\nspecific positions, i.e., the first position. We further quantify the gap\nbetween MCQs and long-form generation questions (LFGQs) by comparing their\ndirect outputs, token logits, and embeddings. Our results reveal a relatively\nlow correlation between answers from MCQs and LFGQs for identical questions.\nAdditionally, we propose two methods to quantify the consistency and confidence\nof LLMs' output, which can be generalized to other QA evaluation benchmarks.\nNotably, our analysis challenges the idea that the higher the consistency, the\ngreater the accuracy. We also find MCQs to be less reliable than LFGQs in terms\nof expected calibration error. Finally, the misalignment between MCQs and LFGQs\nis not only reflected in the evaluation performance but also in the embedding\nspace. Our code and models can be accessed at\nhttps://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.\n","authors":["Wangyue Li","Liangzhi Li","Tong Xiang","Xiao Liu","Wei Deng","Noa Garcia"],"pdf_url":"https://arxiv.org/pdf/2403.17752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17748v1","updated":"2024-03-26T14:40:10Z","published":"2024-03-26T14:40:10Z","title":"UCxn: Typologically Informed Annotation of Constructions Atop Universal\n  Dependencies","summary":"  The Universal Dependencies (UD) project has created an invaluable collection\nof treebanks with contributions in over 140 languages. However, the UD\nannotations do not tell the full story. Grammatical constructions that convey\nmeaning through a particular combination of several morphosyntactic elements --\nfor example, interrogative sentences with special markers and/or word orders --\nare not labeled holistically. We argue for (i) augmenting UD annotations with a\n'UCxn' annotation layer for such meaning-bearing grammatical constructions, and\n(ii) approaching this in a typologically informed way so that morphosyntactic\nstrategies can be compared across languages. As a case study, we consider five\nconstruction families in ten languages, identifying instances of each\nconstruction in UD treebanks through the use of morphosyntactic patterns. In\naddition to findings regarding these particular constructions, our study yields\nimportant insights on methodology for describing and identifying constructions\nin language-general and language-particular ways, and lays the foundation for\nfuture constructional enrichment of UD treebanks.\n","authors":["Leonie Weissweiler","Nina Böbel","Kirian Guiller","Santiago Herrera","Wesley Scivetti","Arthur Lorenzi","Nurit Melnik","Archna Bhatia","Hinrich Schütze","Lori Levin","Amir Zeldes","Joakim Nivre","William Croft","Nathan Schneider"],"pdf_url":"https://arxiv.org/pdf/2403.17748v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2310.02129v4","updated":"2024-03-26T14:38:23Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.12721v2","updated":"2024-03-26T14:32:34Z","published":"2024-03-19T13:30:47Z","title":"CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched\n  with Linguistic and Genre Annotation","summary":"  This paper presents a collection of highly comparable web corpora of\nSlovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian,\ncovering thereby the whole spectrum of official languages in the South Slavic\nlanguage space. The collection of these corpora comprises a total of 13 billion\ntokens of texts from 26 million documents. The comparability of the corpora is\nensured by a comparable crawling setup and the usage of identical crawling and\npost-processing technology. All the corpora were linguistically annotated with\nthe state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and\nenriched with document-level genre information via the Transformer-based\nmultilingual X-GENRE classifier, which further enhances comparability at the\nlevel of linguistic annotation and metadata enrichment. The genre-focused\nanalysis of the resulting corpora shows a rather consistent distribution of\ngenres throughout the seven corpora, with variations in the most prominent\ngenre categories being well-explained by the economic strength of each language\ncommunity. A comparison of the distribution of genre categories across the\ncorpora indicates that web corpora from less developed countries primarily\nconsist of news articles. Conversely, web corpora from economically more\ndeveloped countries exhibit a smaller proportion of news content, with a\ngreater presence of promotional and opinionated texts.\n","authors":["Nikola Ljubešić","Taja Kuzman"],"pdf_url":"https://arxiv.org/pdf/2403.12721v2.pdf","comment":"Accepted to the LREC-COLING 2024 conference"},{"id":"http://arxiv.org/abs/2307.05300v4","updated":"2024-03-26T14:32:33Z","published":"2023-07-11T14:45:19Z","title":"Unleashing the Emergent Cognitive Synergy in Large Language Models: A\n  Task-Solving Agent through Multi-Persona Self-Collaboration","summary":"  Human intelligence thrives on cognitive synergy, where collaboration among\ndifferent minds yield superior outcomes compared to isolated individuals. In\nthis work, we propose Solo Performance Prompting (SPP), which transforms a\nsingle LLM into a cognitive synergist by engaging in multi-turn\nself-collaboration with multiple personas. A cognitive synergist is an\nintelligent agent that collaboratively combines multiple minds' strengths and\nknowledge to enhance problem-solving in complex tasks. By dynamically\nidentifying and simulating different personas based on task inputs, SPP\nunleashes the potential of cognitive synergy in LLMs. Our in-depth analysis\nshows that assigning multiple fine-grained personas in LLMs improves\nproblem-solving abilities compared to using a single or fixed number of\npersonas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,\nCodenames Collaborative, and Logic Grid Puzzle, encompassing both\nknowledge-intensive and reasoning-intensive types. Unlike previous works, such\nas Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,\nexperimental results demonstrate that SPP effectively reduces factual\nhallucination, and maintains strong reasoning capabilities. Additionally,\ncomparative experiments show that cognitive synergy only emerges in GPT-4 and\ndoes not appear in less capable models, such as GPT-3.5-turbo and\nLlama2-13b-chat, which draws an interesting analogy to human development. Code,\ndata, and prompts can be found at:\nhttps://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.\n","authors":["Zhenhailong Wang","Shaoguang Mao","Wenshan Wu","Tao Ge","Furu Wei","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2307.05300v4.pdf","comment":"Accepted as a main conference paper at NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17733v1","updated":"2024-03-26T14:20:42Z","published":"2024-03-26T14:20:42Z","title":"Continual Few-shot Event Detection via Hierarchical Augmentation\n  Networks","summary":"  Traditional continual event detection relies on abundant labeled data for\ntraining, which is often impractical to obtain in real-world applications. In\nthis paper, we introduce continual few-shot event detection (CFED), a more\ncommonly encountered scenario when a substantial number of labeled samples are\nnot accessible. The CFED task is challenging as it involves memorizing previous\nevent types and learning new event types with few-shot samples. To mitigate\nthese challenges, we propose a memory-based framework: Hierarchical\nAugmentation Networks (HANet). To memorize previous event types with limited\nmemory, we incorporate prototypical augmentation into the memory set. For the\nissue of learning new event types in few-shot scenarios, we propose a\ncontrastive augmentation module for token representations. Despite comparing\nwith previous state-of-the-art methods, we also conduct comparisons with\nChatGPT. Experiment results demonstrate that our method significantly\noutperforms all of these methods in multiple continual few-shot event detection\ntasks.\n","authors":["Chenlong Zhang","Pengfei Cao","Yubo Chen","Kang Liu","Zhiqiang Zhang","Mengshu Sun","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.17733v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17727v1","updated":"2024-03-26T14:16:56Z","published":"2024-03-26T14:16:56Z","title":"FastPerson: Enhancing Video Learning through Effective Video\n  Summarization that Preserves Linguistic and Visual Contexts","summary":"  Quickly understanding lengthy lecture videos is essential for learners with\nlimited time and interest in various topics to improve their learning\nefficiency. To this end, video summarization has been actively researched to\nenable users to view only important scenes from a video. However, these studies\nfocus on either the visual or audio information of a video and extract\nimportant segments in the video. Therefore, there is a risk of missing\nimportant information when both the teacher's speech and visual information on\nthe blackboard or slides are important, such as in a lecture video. To tackle\nthis issue, we propose FastPerson, a video summarization approach that\nconsiders both the visual and auditory information in lecture videos.\nFastPerson creates summary videos by utilizing audio transcriptions along with\non-screen images and text, minimizing the risk of overlooking crucial\ninformation for learners. Further, it provides a feature that allows learners\nto switch between the summary and original videos for each chapter of the\nvideo, enabling them to adjust the pace of learning based on their interests\nand level of understanding. We conducted an evaluation with 40 participants to\nassess the effectiveness of our method and confirmed that it reduced viewing\ntime by 53\\% at the same level of comprehension as that when using traditional\nvideo playback methods.\n","authors":["Kazuki Kawamura","Jun Rekimoto"],"pdf_url":"https://arxiv.org/pdf/2403.17727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17706v1","updated":"2024-03-26T13:50:34Z","published":"2024-03-26T13:50:34Z","title":"Enhanced Short Text Modeling: Leveraging Large Language Models for Topic\n  Refinement","summary":"  Crafting effective topic models for brief texts, like tweets and news\nheadlines, is essential for capturing the swift shifts in social dynamics.\nTraditional topic models, however, often fall short in accurately representing\nthe semantic intricacies of short texts due to their brevity and lack of\ncontextual data. In our study, we harness the advanced capabilities of Large\nLanguage Models (LLMs) to introduce a novel approach termed \"Topic Refinement\".\nThis approach does not directly involve itself in the initial modeling of\ntopics but focuses on improving topics after they have been mined. By employing\nprompt engineering, we direct LLMs to eliminate off-topic words within a given\ntopic, ensuring that only contextually relevant words are preserved or\nsubstituted with ones that fit better semantically. This method emulates\nhuman-like scrutiny and improvement of topics, thereby elevating the semantic\nquality of the topics generated by various models. Our comprehensive evaluation\nacross three unique datasets has shown that our topic refinement approach\nsignificantly enhances the semantic coherence of topics.\n","authors":["Shuyu Chang","Rui Wang","Peng Ren","Haiping Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17706v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.17691v1","updated":"2024-03-26T13:32:32Z","published":"2024-03-26T13:32:32Z","title":"Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to\n  Inform GenAI Copyright Disputes","summary":"  The advent of Generative Artificial Intelligence (GenAI) models, including\nGitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content\ncreation, enabling non-professionals to produce high-quality content across\nvarious domains. This transformative technology has led to a surge of synthetic\ncontent and sparked legal disputes over copyright infringement. To address\nthese challenges, this paper introduces a novel approach that leverages the\nlearning capacity of GenAI models for copyright legal analysis, demonstrated\nwith GPT2 and Stable Diffusion models. Copyright law distinguishes between\noriginal expressions and generic ones (Sc\\`enes \\`a faire), protecting the\nformer and permitting reproduction of the latter. However, this distinction has\nhistorically been challenging to make consistently, leading to over-protection\nof copyrighted works. GenAI offers an unprecedented opportunity to enhance this\nlegal analysis by revealing shared patterns in preexisting works. We propose a\ndata-driven approach to identify the genericity of works created by GenAI,\nemploying \"data-driven bias\" to assess the genericity of expressive\ncompositions. This approach aids in copyright scope determination by utilizing\nthe capabilities of GenAI to identify and prioritize expressive elements and\nrank them according to their frequency in the model's dataset. The potential\nimplications of measuring expressive genericity for copyright law are profound.\nSuch scoring could assist courts in determining copyright scope during\nlitigation, inform the registration practices of Copyright Offices, allowing\nregistration of only highly original synthetic works, and help copyright owners\nsignal the value of their works and facilitate fairer licensing deals. More\ngenerally, this approach offers valuable insights to policymakers grappling\nwith adapting copyright law to the challenges posed by the era of GenAI.\n","authors":["Uri Hacohen","Adi Haviv","Shahar Sarfaty","Bruria Friedman","Niva Elkin-Koren","Roi Livni","Amit H Bermano"],"pdf_url":"https://arxiv.org/pdf/2403.17691v1.pdf","comment":"Presented at ACM CSLAW 2024"},{"id":"http://arxiv.org/abs/2305.14230v2","updated":"2024-03-26T13:16:37Z","published":"2023-05-23T16:46:18Z","title":"Exploring Representational Disparities Between Multilingual and\n  Bilingual Translation Models","summary":"  Multilingual machine translation has proven immensely useful for both\nparameter efficiency and overall performance across many language pairs via\ncomplete multilingual parameter sharing. However, some language pairs in\nmultilingual models can see worse performance than in bilingual models,\nespecially in the one-to-many translation setting. Motivated by their empirical\ndifferences, we examine the geometric differences in representations from\nbilingual models versus those from one-to-many multilingual models.\nSpecifically, we compute the isotropy of these representations using intrinsic\ndimensionality and IsoScore, in order to measure how the representations\nutilize the dimensions in their underlying vector space. Using the same\nevaluation data in both models, we find that for a given language pair, its\nmultilingual model decoder representations are consistently less isotropic and\noccupy fewer dimensions than comparable bilingual model decoder\nrepresentations. Additionally, we show that much of the anisotropy in\nmultilingual decoder representations can be attributed to modeling\nlanguage-specific information, therefore limiting remaining representational\ncapacity.\n","authors":["Neha Verma","Kenton Murray","Kevin Duh"],"pdf_url":"https://arxiv.org/pdf/2305.14230v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.02270v2","updated":"2024-03-26T13:14:52Z","published":"2024-03-04T17:57:18Z","title":"FENICE: Factuality Evaluation of summarization based on Natural language\n  Inference and Claim Extraction","summary":"  Recent advancements in text summarization, particularly with the advent of\nLarge Language Models (LLMs), have shown remarkable performance. However, a\nnotable challenge persists as a substantial number of automatically-generated\nsummaries exhibit factual inconsistencies, such as hallucinations. In response\nto this issue, various approaches for the evaluation of consistency for\nsummarization have emerged. Yet, these newly-introduced metrics face several\nlimitations, including lack of interpretability, focus on short document\nsummaries (e.g., news articles), and computational impracticality, especially\nfor LLM-based metrics. To address these shortcomings, we propose Factuality\nEvaluation of summarization based on Natural language Inference and Claim\nExtraction (FENICE), a more interpretable and efficient factuality-oriented\nmetric. FENICE leverages an NLI-based alignment between information in the\nsource document and a set of atomic facts, referred to as claims, extracted\nfrom the summary. Our metric sets a new state of the art on AGGREFACT, the\nde-facto benchmark for factuality evaluation. Moreover, we extend our\nevaluation to a more challenging setting by conducting a human annotation\nprocess of long-form summarization.\n","authors":["Alessandro Scirè","Karim Ghonim","Roberto Navigli"],"pdf_url":"https://arxiv.org/pdf/2403.02270v2.pdf","comment":"9 pages, long paper"},{"id":"http://arxiv.org/abs/2403.16915v2","updated":"2024-03-26T13:11:44Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.13737v3","updated":"2024-03-26T13:01:38Z","published":"2024-03-20T16:43:42Z","title":"EthioLLM: Multilingual Large Language Models for Ethiopian Languages\n  with Task Evaluation","summary":"  Large language models (LLMs) have gained popularity recently due to their\noutstanding performance in various downstream Natural Language Processing (NLP)\ntasks. However, low-resource languages are still lagging behind current\nstate-of-the-art (SOTA) developments in the field of NLP due to insufficient\nresources to train LLMs. Ethiopian languages exhibit remarkable linguistic\ndiversity, encompassing a wide array of scripts, and are imbued with profound\nreligious and cultural significance. This paper introduces EthioLLM --\nmultilingual large language models for five Ethiopian languages (Amharic,\nGe'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a\nnew benchmark dataset for various downstream NLP tasks. We evaluate the\nperformance of these models across five downstream NLP tasks. We open-source\nour multilingual language models, new benchmark datasets for various downstream\ntasks, and task-specific fine-tuned language models and discuss the performance\nof the models. Our dataset and models are available at the\nhttps://huggingface.co/EthioNLP repository.\n","authors":["Atnafu Lambebo Tonja","Israel Abebe Azime","Tadesse Destaw Belay","Mesay Gemeda Yigezu","Moges Ahmed Mehamed","Abinew Ali Ayele","Ebrahim Chekol Jibril","Michael Melese Woldeyohannis","Olga Kolesnikova","Philipp Slusallek","Dietrich Klakow","Shengwu Xiong","Seid Muhie Yimam"],"pdf_url":"https://arxiv.org/pdf/2403.13737v3.pdf","comment":"Accepted at LREC-Coling 2024"},{"id":"http://arxiv.org/abs/2403.14117v2","updated":"2024-03-26T12:53:14Z","published":"2024-03-21T04:03:16Z","title":"A Design Space for Intelligent and Interactive Writing Assistants","summary":"  In our era of rapid technological advancement, the research landscape for\nwriting assistants has become increasingly fragmented across various research\ncommunities. We seek to address this challenge by proposing a design space as a\nstructured way to examine and explore the multidimensional space of intelligent\nand interactive writing assistants. Through a large community collaboration, we\nexplore five aspects of writing assistants: task, user, technology,\ninteraction, and ecosystem. Within each aspect, we define dimensions (i.e.,\nfundamental components of an aspect) and codes (i.e., potential options for\neach dimension) by systematically reviewing 115 papers. Our design space aims\nto offer researchers and designers a practical tool to navigate, comprehend,\nand compare the various possibilities of writing assistants, and aid in the\nenvisioning and design of new writing assistants.\n","authors":["Mina Lee","Katy Ilonka Gero","John Joon Young Chung","Simon Buckingham Shum","Vipul Raheja","Hua Shen","Subhashini Venugopalan","Thiemo Wambsganss","David Zhou","Emad A. Alghamdi","Tal August","Avinash Bhat","Madiha Zahrah Choksi","Senjuti Dutta","Jin L. C. Guo","Md Naimul Hoque","Yewon Kim","Simon Knight","Seyed Parsa Neshaei","Agnia Sergeyuk","Antonette Shibani","Disha Shrivastava","Lila Shroff","Jessi Stark","Sarah Sterman","Sitong Wang","Antoine Bosselut","Daniel Buschek","Joseph Chee Chang","Sherol Chen","Max Kreminski","Joonsuk Park","Roy Pea","Eugenia H. Rho","Shannon Zejiang Shen","Pao Siangliulue"],"pdf_url":"https://arxiv.org/pdf/2403.14117v2.pdf","comment":"Published as a conference paper at CHI 2024"},{"id":"http://arxiv.org/abs/2403.17661v1","updated":"2024-03-26T12:47:39Z","published":"2024-03-26T12:47:39Z","title":"Language Models for Text Classification: Is In-Context Learning Enough?","summary":"  Recent foundational language models have shown state-of-the-art performance\nin many NLP tasks in zero- and few-shot settings. An advantage of these models\nover more standard approaches based on fine-tuning is the ability to understand\ninstructions written in natural language (prompts), which helps them generalise\nbetter to different tasks and domains without the need for specific training\ndata. This makes them suitable for addressing text classification problems for\ndomains with limited amounts of annotated instances. However, existing research\nis limited in scale and lacks understanding of how text generation models\ncombined with prompting techniques compare to more established methods for text\nclassification such as fine-tuning masked language models. In this paper, we\naddress this research gap by performing a large-scale evaluation study for 16\ntext classification datasets covering binary, multiclass, and multilabel\nproblems. In particular, we compare zero- and few-shot approaches of large\nlanguage models to fine-tuning smaller language models. We also analyse the\nresults by prompt, classification type, domain, and number of labels. In\ngeneral, the results show how fine-tuning smaller and more efficient language\nmodels can still outperform few-shot approaches of larger language models,\nwhich have room for improvement when it comes to text classification.\n","authors":["Aleksandra Edwards","Jose Camacho-Collados"],"pdf_url":"https://arxiv.org/pdf/2403.17661v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2308.10592v3","updated":"2024-03-26T12:31:35Z","published":"2023-08-21T09:47:31Z","title":"BAN-PL: a Novel Polish Dataset of Banned Harmful and Offensive Content\n  from Wykop.pl web service","summary":"  Since the Internet is flooded with hate, it is one of the main tasks for NLP\nexperts to master automated online content moderation. However, advancements in\nthis field require improved access to publicly available accurate and\nnon-synthetic datasets of social media content. For the Polish language, such\nresources are very limited. In this paper, we address this gap by presenting a\nnew open dataset of offensive social media content for the Polish language. The\ndataset comprises content from Wykop.pl, a popular online service often\nreferred to as the \"Polish Reddit\", reported by users and banned in the\ninternal moderation process. It contains a total of 691,662 posts and comments,\nevenly divided into two categories: \"harmful\" and \"neutral\" (\"non-harmful\").\nThe anonymized subset of the BAN-PL dataset consisting on 24,000 pieces (12,000\nfor each class), along with preprocessing scripts have been made publicly\navailable. Furthermore the paper offers valuable insights into real-life\ncontent moderation processes and delves into an analysis of linguistic features\nand content characteristics of the dataset. Moreover, a comprehensive\nanonymization procedure has been meticulously described and applied. The\nprevalent biases encountered in similar datasets, including post-moderation and\npre-selection biases, are also discussed.\n","authors":["Anna Kołos","Inez Okulska","Kinga Głąbińska","Agnieszka Karlińska","Emilia Wiśnios","Paweł Ellerik","Andrzej Prałat"],"pdf_url":"https://arxiv.org/pdf/2308.10592v3.pdf","comment":"Accepted for LREC-COLING 2024 Conference"},{"id":"http://arxiv.org/abs/2403.17647v1","updated":"2024-03-26T12:29:18Z","published":"2024-03-26T12:29:18Z","title":"Intrinsic Subgraph Generation for Interpretable Graph based Visual\n  Question Answering","summary":"  The large success of deep learning based methods in Visual Question Answering\n(VQA) has concurrently increased the demand for explainable methods. Most\nmethods in Explainable Artificial Intelligence (XAI) focus on generating\npost-hoc explanations rather than taking an intrinsic approach, the latter\ncharacterizing an interpretable model. In this work, we introduce an\ninterpretable approach for graph-based VQA and demonstrate competitive\nperformance on the GQA dataset. This approach bridges the gap between\ninterpretability and performance. Our model is designed to intrinsically\nproduce a subgraph during the question-answering process as its explanation,\nproviding insight into the decision making. To evaluate the quality of these\ngenerated subgraphs, we compare them against established post-hoc\nexplainability methods for graph neural networks, and perform a human\nevaluation. Moreover, we present quantitative metrics that correlate with the\nevaluations of human assessors, acting as automatic metrics for the generated\nexplanatory subgraphs. Our implementation is available at\nhttps://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.\n","authors":["Pascal Tilli","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17647v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17645v1","updated":"2024-03-26T12:27:32Z","published":"2024-03-26T12:27:32Z","title":"DANCER: Entity Description Augmented Named Entity Corrector for\n  Automatic Speech Recognition","summary":"  End-to-end automatic speech recognition (E2E ASR) systems often suffer from\nmistranscription of domain-specific phrases, such as named entities, sometimes\nleading to catastrophic failures in downstream tasks. A family of fast and\nlightweight named entity correction (NEC) models for ASR have recently been\nproposed, which normally build on phonetic-level edit distance algorithms and\nhave shown impressive NEC performance. However, as the named entity (NE) list\ngrows, the problems of phonetic confusion in the NE list are exacerbated; for\nexample, homophone ambiguities increase substantially. In view of this, we\nproposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER),\nwhich leverages entity descriptions to provide additional information to\nfacilitate mitigation of phonetic confusion for NEC on ASR transcription. To\nthis end, an efficient entity description augmented masked language model\n(EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to\nadapt swiftly to domain-specific entities for the NEC task. A series of\nexperiments conducted on the AISHELL-1 and Homophone datasets confirm the\neffectiveness of our modeling approach. DANCER outperforms a strong baseline,\nthe phonetic edit-distance-based NEC model (PED-NEC), by a character error rate\n(CER) reduction of about 7% relatively on AISHELL-1 for named entities. More\nnotably, when tested on Homophone that contain named entities of high phonetic\nconfusion, DANCER offers a more pronounced CER reduction of 46% relatively over\nPED-NEC for named entities.\n","authors":["Yi-Cheng Wang","Hsin-Wei Wang","Bi-Cheng Yan","Chi-Han Lin","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13334v2","updated":"2024-03-26T12:24:46Z","published":"2024-03-20T06:37:59Z","title":"Hyacinth6B: A large language model for Traditional Chinese","summary":"  This research's primary motivation of this study is to address the high\nhardware and computational demands typically associated with LLMs.Therefore,our\ngoal is to find a balance between model lightness and performance,striving to\nmaximize performance while using a comparatively lightweight model. Hyacinth6B\nwas developed with this objective in mind,aiming to fully leverage the core\ncapabilities of LLMs without incurring substantial resource costs, effectively\npushing the boundaries of smaller model's performance. The training approach\ninvolves parameter efficient finetuning using the LoRA method.\n","authors":["Chih-Wei Song","Yin-Te Tsai"],"pdf_url":"https://arxiv.org/pdf/2403.13334v2.pdf","comment":"14pages"},{"id":"http://arxiv.org/abs/2403.17640v1","updated":"2024-03-26T12:21:51Z","published":"2024-03-26T12:21:51Z","title":"REFeREE: A REference-FREE Model-Based Metric for Text Simplification","summary":"  Text simplification lacks a universal standard of quality, and annotated\nreference simplifications are scarce and costly. We propose to alleviate such\nlimitations by introducing REFeREE, a reference-free model-based metric with a\n3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage\nand can be applied to any quality standard as long as a small number of human\nannotations are available. Our experiments show that our metric outperforms\nexisting reference-based metrics in predicting overall ratings and reaches\ncompetitive and consistent performance in predicting specific ratings while\nrequiring no reference simplifications at inference time.\n","authors":["Yichen Huang","Ekaterina Kochmar"],"pdf_url":"https://arxiv.org/pdf/2403.17640v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17636v1","updated":"2024-03-26T12:11:29Z","published":"2024-03-26T12:11:29Z","title":"Mix-Initiative Response Generation with Dynamic Prefix Tuning","summary":"  Mixed initiative serves as one of the key factors in controlling conversation\ndirections. For a speaker, responding passively or leading proactively would\nresult in rather different responses. However, most dialogue systems focus on\ntraining a holistic response generation model without any distinction among\ndifferent initiatives. It leads to the cross-contamination problem, where the\nmodel confuses different initiatives and generates inappropriate responses.\nMoreover, obtaining plenty of human annotations for initiative labels can be\nexpensive. To address this issue, we propose a general mix-Initiative Dynamic\nPrefix Tuning framework (IDPT) to decouple different initiatives from the\ngeneration model, which learns initiative-aware prefixes in both supervised and\nunsupervised settings. Specifically, IDPT decouples initiative factors into\ndifferent prefix parameters and uses the attention mechanism to adjust the\nselection of initiatives in guiding generation dynamically. The prefix\nparameters can be tuned towards accurate initiative prediction as well as\nmix-initiative response generation. Extensive experiments on two public\ndialogue datasets show that the proposed IDPT outperforms previous baselines on\nboth automatic metrics and human evaluations. It also manages to generate\nappropriate responses with manipulated initiatives.\n","authors":["Yuxiang Nie","Heyan Huang","Xian-Ling Mao","Lizi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.17636v1.pdf","comment":"Accepted to the main conference of NAACL 2024"},{"id":"http://arxiv.org/abs/2310.12541v3","updated":"2024-03-26T12:04:44Z","published":"2023-10-19T07:46:54Z","title":"Large Language Model for Multi-objective Evolutionary Optimization","summary":"  Multiobjective evolutionary algorithms (MOEAs) are major methods for solving\nmultiobjective optimization problems (MOPs). Many MOEAs have been proposed in\nthe past decades, of which the search operators need a carefully handcrafted\ndesign with domain knowledge. Recently, some attempts have been made to replace\nthe manually designed operators in MOEAs with learning-based operators (e.g.,\nneural network models). However, much effort is still required for designing\nand training such models, and the learned operators might not generalize well\non new problems. To tackle the above challenges, this work investigates a novel\napproach that leverages the powerful large language model (LLM) to design MOEA\noperators. With proper prompt engineering, we successfully let a general LLM\nserve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a\nzero-shot manner. In addition, by learning from the LLM behavior, we further\ndesign an explicit white-box operator with randomness and propose a new version\nof decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on\ndifferent test benchmarks show that our proposed method can achieve competitive\nperformance with widely used MOEAs. It is also promising to see the operator\nonly learned from a few instances can have robust generalization performance on\nunseen problems with quite different patterns and settings. The results reveal\nthe potential benefits of using pre-trained LLMs in the design of MOEAs.To\nfoster reproducibility and accessibility, the source code is\nhttps://github.com/FeiLiu36/LLM4MOEA.\n","authors":["Fei Liu","Xi Lin","Zhenkun Wang","Shunyu Yao","Xialiang Tong","Mingxuan Yuan","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.12541v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15694v5","updated":"2024-03-26T11:52:59Z","published":"2023-10-24T10:05:32Z","title":"COPR: Continual Learning Human Preference through Optimal Policy\n  Regularization","summary":"  The technique of Reinforcement Learning from Human Feedback (RLHF) is a\ncommonly employed method to improve pre-trained Language Models (LM), enhancing\ntheir ability to conform to human preferences. Nevertheless, the current\nRLHF-based LMs necessitate full retraining each time novel queries or feedback\nare introduced, which becomes a challenging task because human preferences can\nvary between different domains or tasks. Retraining LMs poses practical\ndifficulties in many real-world situations due to the significant time and\ncomputational resources required, along with concerns related to data privacy.\nTo address this limitation, we propose a new method called Continual Optimal\nPolicy Regularization (COPR), in which we compute the distribution of optimal\npolicy bypassing the partition function and then regularize the current policy\nbased on the historically optimal distribution to mitigate Catastrophic\nForgetting (CF). COPR involves a single learning phase and doesn't necessitate\ncomplex reinforcement learning. Importantly, it shares the capability with RLHF\nto learn from unlabeled data by maintaining a scoring module, similar to reward\nmodel, making it flexible for continually learning without human feedback. Our\nexperimental results show that COPR outperforms strong Continuous Learning (CL)\nbaselines when it comes to consistently aligning with human preferences on\nincremental tasks and domains.\n","authors":["Han Zhang","Lin Gui","Yuanzhao Zhai","Hui Wang","Yu Lei","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2310.15694v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17612v1","updated":"2024-03-26T11:45:22Z","published":"2024-03-26T11:45:22Z","title":"\"You are an expert annotator\": Automatic Best-Worst-Scaling Annotations\n  for Emotion Intensity Modeling","summary":"  Labeling corpora constitutes a bottleneck to create models for new tasks or\ndomains. Large language models mitigate the issue with automatic corpus\nlabeling methods, particularly for categorical annotations. Some NLP tasks such\nas emotion intensity prediction, however, require text regression, but there is\nno work on automating annotations for continuous label assignments. Regression\nis considered more challenging than classification: The fact that humans\nperform worse when tasked to choose values from a rating scale lead to\ncomparative annotation methods, including best-worst scaling. This raises the\nquestion if large language model-based annotation methods show similar\npatterns, namely that they perform worse on rating scale annotation tasks than\non comparative annotation tasks. To study this, we automate emotion intensity\npredictions and compare direct rating scale predictions, pairwise comparisons\nand best-worst scaling. We find that the latter shows the highest reliability.\nA transformer regressor fine-tuned on these data performs nearly on par with a\nmodel trained on the original manual annotations.\n","authors":["Christopher Bagdon","Prathamesh Karmalker","Harsha Gurulingappa","Roman Klinger"],"pdf_url":"https://arxiv.org/pdf/2403.17612v1.pdf","comment":"accepted for publication in NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17611v1","updated":"2024-03-26T11:44:49Z","published":"2024-03-26T11:44:49Z","title":"Denoising Table-Text Retrieval for Open-Domain Question Answering","summary":"  In table-text open-domain question answering, a retriever system retrieves\nrelevant evidence from tables and text to answer questions. Previous studies in\ntable-text open-domain question answering have two common challenges: firstly,\ntheir retrievers can be affected by false-positive labels in training datasets;\nsecondly, they may struggle to provide appropriate evidence for questions that\nrequire reasoning across the table. To address these issues, we propose\nDenoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a\ndenoised training dataset with fewer false positive labels by discarding\ninstances with lower question-relevance scores measured through a false\npositive detection model. Subsequently, we integrate table-level ranking\ninformation into the retriever to assist in finding evidence for questions that\ndemand reasoning across the table. To encode this ranking information, we\nfine-tune a rank-aware column encoder to identify minimum and maximum values\nwithin a column. Experimental results demonstrate that DoTTeR significantly\noutperforms strong baselines on both retrieval recall and downstream QA tasks.\nOur code is available at https://github.com/deokhk/DoTTeR.\n","authors":["Deokhyung Kang","Baikjin Jung","Yunsu Kim","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2403.17611v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2305.14790v2","updated":"2024-03-26T11:29:21Z","published":"2023-05-24T06:43:23Z","title":"Advancing Topic Segmentation and Outline Generation in Chinese Texts:\n  The Paragraph-level Topic Representation, Corpus, and Benchmark","summary":"  Topic segmentation and outline generation strive to divide a document into\ncoherent topic sections and generate corresponding subheadings, unveiling the\ndiscourse topic structure of a document. Compared with sentence-level topic\nstructure, the paragraph-level topic structure can quickly grasp and understand\nthe overall context of the document from a higher level, benefitting many\ndownstream tasks such as summarization, discourse parsing, and information\nretrieval. However, the lack of large-scale, high-quality Chinese\nparagraph-level topic structure corpora restrained relative research and\napplications. To fill this gap, we build the Chinese paragraph-level topic\nrepresentation, corpus, and benchmark in this paper. Firstly, we propose a\nhierarchical paragraph-level topic structure representation with three layers\nto guide the corpus construction. Then, we employ a two-stage man-machine\ncollaborative annotation method to construct the largest Chinese\nParagraph-level Topic Structure corpus (CPTS), achieving high quality. We also\nbuild several strong baselines, including ChatGPT, to validate the\ncomputability of CPTS on two fundamental tasks (topic segmentation and outline\ngeneration) and preliminarily verified its usefulness for the downstream task\n(discourse parsing).\n","authors":["Feng Jiang","Weihao Liu","Xiaomin Chu","Peifeng Li","Qiaoming Zhu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2305.14790v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.13318v2","updated":"2024-03-26T11:26:04Z","published":"2023-09-23T09:24:05Z","title":"Spanish Resource Grammar version 2023","summary":"  We present the latest version of the Spanish Resource Grammar (SRG), a\ngrammar of Spanish implemented in the HPSG formalism. Such grammars encode a\ncomplex set of hypotheses about syntax making them a resource for empirical\ntesting of linguistic theory. They also encode a strict notion of\ngrammaticality which makes them a resource for natural language processing\napplications in computer-assisted language learning. This version of the SRG\nuses the recent version of the Freeling morphological analyzer and is released\nalong with an automatically created, manually verified treebank of 2,291\nsentences. We explain the treebanking process, emphasizing how it is different\nfrom treebanking with manual annotation and how it contributes to\nempirically-driven development of syntactic theory. The treebanks' high level\nof consistency and detail makes them a resource for training high-quality\nsemantic parsers and generally systems that benefit from precise and detailed\nsemantics. Finally, we present the grammar's coverage and overgeneration on 100\nsentences from a learner corpus, a new research line related to developing\nmethodologies for robust empirical evaluation of hypotheses in second language\nacquisition.\n","authors":["Olga Zamaraeva","Lorena S. Allegue","Carlos Gómez-Rodríguez"],"pdf_url":"https://arxiv.org/pdf/2309.13318v2.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.13518v2","updated":"2024-03-26T11:16:47Z","published":"2024-03-20T11:38:30Z","title":"Motion Generation from Fine-grained Textual Descriptions","summary":"  The task of text2motion is to generate human motion sequences from given\ntextual descriptions, where the model explores diverse mappings from natural\nlanguage instructions to human body movements. While most existing works are\nconfined to coarse-grained motion descriptions, e.g., \"A man squats.\",\nfine-grained descriptions specifying movements of relevant body parts are\nbarely explored. Models trained with coarse-grained texts may not be able to\nlearn mappings from fine-grained motion-related words to motion primitives,\nresulting in the failure to generate motions from unseen descriptions. In this\npaper, we build a large-scale language-motion dataset specializing in\nfine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with\nstep-by-step instructions with pseudo-code compulsory checks. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, making full use of\nfine-grained textual information. Our quantitative evaluation shows that\nFineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of\n0.38, compared with competitive baselines. According to the qualitative\nevaluation and case study, our model outperforms MotionDiffuse in generating\nspatially or chronologically composite motions, by learning the implicit\nmappings from fine-grained descriptions to the corresponding basic motions. We\nrelease our data at https://github.com/KunhangL/finemotiondiffuse.\n","authors":["Kunhang Li","Yansong Feng"],"pdf_url":"https://arxiv.org/pdf/2403.13518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08644v3","updated":"2024-03-26T11:13:56Z","published":"2024-02-13T18:24:08Z","title":"Tandem Transformers for Inference Efficient LLMs","summary":"  The autoregressive nature of conventional large language models (LLMs)\ninherently limits inference speed, as tokens are generated sequentially. While\nspeculative and parallel decoding techniques attempt to mitigate this, they\nface limitations: either relying on less accurate smaller models for generation\nor failing to fully leverage the base LLM's representations.\n  We introduce a novel architecture, Tandem transformers, to address these\nissues. This architecture uniquely combines (1) a small autoregressive model\nand (2) a large model operating in block mode (processing multiple tokens\nsimultaneously). The small model's predictive accuracy is substantially\nenhanced by granting it attention to the large model's richer representations.\nOn the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko\ndemonstrates a 3.3% improvement in next-token prediction accuracy over a\nstandalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter\nmodel with comparable downstream performance. We further incorporate the tandem\nmodel within the speculative decoding (SPEED) framework where the large model\nvalidates tokens from the small model. This ensures that the Tandem of\nPaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster\nthan using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream\ntask accuracy.\n","authors":["Aishwarya P S","Pranav Ajit Nair","Yashas Samaga","Toby Boyd","Sanjiv Kumar","Prateek Jain","Praneeth Netrapalli"],"pdf_url":"https://arxiv.org/pdf/2402.08644v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17599v1","updated":"2024-03-26T11:09:58Z","published":"2024-03-26T11:09:58Z","title":"Coimagining the Future of Voice Assistants with Cultural Sensitivity","summary":"  Voice assistants (VAs) are becoming a feature of our everyday life. Yet, the\nuser experience (UX) is often limited, leading to underuse, disengagement, and\nabandonment. Co-designing interactions for VAs with potential end-users can be\nuseful. Crowdsourcing this process online and anonymously may add value.\nHowever, most work has been done in the English-speaking West on dialogue data\nsets. We must be sensitive to cultural differences in language, social\ninteractions, and attitudes towards technology. Our aims were to explore the\nvalue of co-designing VAs in the non-Western context of Japan and demonstrate\nthe necessity of cultural sensitivity. We conducted an online elicitation study\n(N = 135) where Americans (n = 64) and Japanese people (n = 71) imagined\ndialogues (N = 282) and activities (N = 73) with future VAs. We discuss the\nimplications for coimagining interactions with future VAs, offer design\nguidelines for the Japanese and English-speaking US contexts, and suggest\nopportunities for cultural plurality in VA design and scholarship.\n","authors":["Katie Seaborn","Yuto Sawa","Mizuki Watanabe"],"pdf_url":"https://arxiv.org/pdf/2403.17599v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2403.14438v2","updated":"2024-03-26T11:02:32Z","published":"2024-03-21T14:44:03Z","title":"A Multimodal Approach to Device-Directed Speech Detection with Large\n  Language Models","summary":"  Interactions with virtual assistants typically start with a predefined\ntrigger phrase followed by the user command. To make interactions with the\nassistant more intuitive, we explore whether it is feasible to drop the\nrequirement that users must begin each command with a trigger phrase. We\nexplore this task in three ways: First, we train classifiers using only\nacoustic information obtained from the audio waveform. Second, we take the\ndecoder outputs of an automatic speech recognition (ASR) system, such as 1-best\nhypotheses, as input features to a large language model (LLM). Finally, we\nexplore a multimodal system that combines acoustic and lexical features, as\nwell as ASR decoder signals in an LLM. Using multimodal information yields\nrelative equal-error-rate improvements over text-only and audio-only models of\nup to 39% and 61%. Increasing the size of the LLM and training with low-rank\nadaption leads to further relative EER reductions of up to 18% on our dataset.\n","authors":["Dominik Wagner","Alexander Churchill","Siddharth Sigtia","Panayiotis Georgiou","Matt Mirsamadi","Aarshee Mishra","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2403.14438v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03632"},{"id":"http://arxiv.org/abs/2402.11537v2","updated":"2024-03-26T10:45:40Z","published":"2024-02-18T10:36:05Z","title":"Deciphering the Impact of Pretraining Data on Large Language Models\n  through Machine Unlearning","summary":"  Through pretraining on a corpus with various sources, Large Language Models\n(LLMs) have gained impressive performance. However, the impact of each\ncomponent of the pretraining corpus remains opaque. As a result, the\norganization of the pretraining corpus is still empirical and may deviate from\nthe optimal. To address this issue, we systematically analyze the impact of 48\ndatasets from 5 major categories of pretraining data of LLMs and measure their\nimpacts on LLMs using benchmarks about nine major categories of model\ncapabilities. Our analyses provide empirical results about the contribution of\nmultiple corpora on the performances of LLMs, along with their joint impact\npatterns, including complementary, orthogonal, and correlational relationships.\nWe also identify a set of ``high-impact data'' such as Books that is\nsignificantly related to a set of model capabilities. These findings provide\ninsights into the organization of data to support more efficient pretraining of\nLLMs.\n","authors":["Yang Zhao","Li Du","Xiao Ding","Kai Xiong","Zhouhao Sun","Jun Shi","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2402.11537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17582v1","updated":"2024-03-26T10:45:11Z","published":"2024-03-26T10:45:11Z","title":"Towards a Zero-Data, Controllable, Adaptive Dialog System","summary":"  Conversational Tree Search (V\\\"ath et al., 2023) is a recent approach to\ncontrollable dialog systems, where domain experts shape the behavior of a\nReinforcement Learning agent through a dialog tree. The agent learns to\nefficiently navigate this tree, while adapting to information needs, e.g.,\ndomain familiarity, of different users. However, the need for additional\ntraining data hinders deployment in new domains. To address this, we explore\napproaches to generate this data directly from dialog trees. We improve the\noriginal approach, and show that agents trained on synthetic data can achieve\ncomparable dialog success to models trained on human data, both when using a\ncommercial Large Language Model for generation, or when using a smaller\nopen-source model, running on a single GPU. We further demonstrate the\nscalability of our approach by collecting and testing on two new datasets:\nONBOARD, a new domain helping foreign residents moving to a new city, and the\nmedical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and\nhead symptoms. Finally, we perform human testing, where no statistically\nsignificant differences were found in either objective or subjective measures\nbetween models trained on human and generated data.\n","authors":["Dirk Väth","Lindsey Vanderlyn","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08274v4","updated":"2024-03-26T10:36:31Z","published":"2023-12-13T16:43:41Z","title":"High-throughput Biomedical Relation Extraction for Semi-Structured Web\n  Articles Empowered by Large Language Models","summary":"  Objective: To develop a high-throughput biomedical relation extraction system\nthat takes advantage of the large language models'(LLMs) reading comprehension\nability and biomedical world knowledge in a scalable and evidential manner.\nMethods: We formulate the relation extraction task as binary classifications\nfor large language models. Specifically, LLMs make the decision based on the\nexternal corpus and its world knowledge, giving the reason for the judgment for\nfactual verification. This method is tailored for semi-structured web articles,\nwherein we designate the main title as the tail entity and explicitly\nincorporate it into the context, and the potential head entities are matched\nbased on a biomedical thesaurus. Moreover, lengthy contents are sliced into\ntext chunks, embedded, and retrieved with additional embedding models. Results:\nUsing an open-source LLM, we extracted 248659 relation triplets of three\ndistinct relation types from three reputable biomedical websites. To assess the\nefficacy of the basic pipeline employed for biomedical relation extraction, we\ncurated a benchmark dataset annotated by a medical expert. Evaluation results\nindicate that the pipeline exhibits performance comparable to that of GPT-4.\nCase studies further illuminate challenges faced by contemporary LLMs in the\ncontext of biomedical relation extraction for semi-structured web articles.\nConclusion: The proposed method has demonstrated its effectiveness in\nleveraging the strengths of LLMs for high-throughput biomedical relation\nextraction. Its adaptability is evident, as it can be seamlessly extended to\ndiverse semi-structured biomedical websites, facilitating the extraction of\nvarious types of biomedical relations with ease.\n","authors":["Songchi Zhou","Sheng Yu"],"pdf_url":"https://arxiv.org/pdf/2312.08274v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15885v2","updated":"2024-03-26T10:26:04Z","published":"2024-03-23T16:45:22Z","title":"STEntConv: Predicting Disagreement with Stance Detection and a Signed\n  Graph Convolutional Network","summary":"  The rise of social media platforms has led to an increase in polarised online\ndiscussions, especially on political and socio-cultural topics such as\nelections and climate change. We propose a simple and novel unsupervised method\nto predict whether the authors of two posts agree or disagree, leveraging user\nstances about named entities obtained from their posts. We present STEntConv, a\nmodel which builds a graph of users and named entities weighted by stance and\ntrains a Signed Graph Convolutional Network (SGCN) to detect disagreement\nbetween comment and reply posts. We run experiments and ablation studies and\nshow that including this information improves disagreement detection\nperformance on a dataset of Reddit posts for a range of controversial subreddit\ntopics, without the need for platform-specific features or user history.\n","authors":["Isabelle Lorge","Li Zhang","Xiaowen Dong","Janet B. Pierrehumbert"],"pdf_url":"https://arxiv.org/pdf/2403.15885v2.pdf","comment":"Accepted for the 2024 Joint International Conference on Computational\n  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"},{"id":"http://arxiv.org/abs/2403.17564v1","updated":"2024-03-26T10:14:12Z","published":"2024-03-26T10:14:12Z","title":"Task-Oriented Paraphrase Analytics","summary":"  Since paraphrasing is an ill-defined task, the term \"paraphrasing\" covers\ntext transformation tasks with different characteristics. Consequently,\nexisting paraphrasing studies have applied quite different (explicit and\nimplicit) criteria as to when a pair of texts is to be considered a paraphrase,\nall of which amount to postulating a certain level of semantic or lexical\nsimilarity. In this paper, we conduct a literature review and propose a\ntaxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using\nclassifiers trained to identify the tasks that a given paraphrasing instance\nfits, we find that the distributions of task-specific instances in the known\nparaphrase corpora vary substantially. This means that the use of these\ncorpora, without the respective paraphrase conditions being clearly defined\n(which is the normal case), must lead to incomparable and misleading results.\n","authors":["Marcel Gohsen","Matthias Hagen","Martin Potthast","Benno Stein"],"pdf_url":"https://arxiv.org/pdf/2403.17564v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2304.02541v4","updated":"2024-03-26T10:13:06Z","published":"2023-04-05T16:03:42Z","title":"PWESuite: Phonetic Word Embeddings and Tasks They Facilitate","summary":"  Mapping words into a fixed-dimensional vector space is the backbone of modern\nNLP. While most word embedding methods successfully encode semantic\ninformation, they overlook phonetic information that is crucial for many tasks.\nWe develop three methods that use articulatory features to build phonetically\ninformed word embeddings. To address the inconsistent evaluation of existing\nphonetic word embedding methods, we also contribute a task suite to fairly\nevaluate past, current, and future methods. We evaluate both (1) intrinsic\naspects of phonetic word embeddings, such as word retrieval and correlation\nwith sound similarity, and (2) extrinsic performance on tasks such as rhyme and\ncognate detection and sound analogies. We hope our task suite will promote\nreproducibility and inspire future phonetic embedding research.\n","authors":["Vilém Zouhar","Kalvin Chang","Chenxuan Cui","Nathaniel Carlson","Nathaniel Robinson","Mrinmaya Sachan","David Mortensen"],"pdf_url":"https://arxiv.org/pdf/2304.02541v4.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17556v1","updated":"2024-03-26T10:04:24Z","published":"2024-03-26T10:04:24Z","title":"m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt","summary":"  Multilingual translation supports multiple translation directions by\nprojecting all languages in a shared space, but the translation quality is\nundermined by the difference between languages in the text-only modality,\nespecially when the number of languages is large. To bridge this gap, we\nintroduce visual context as the universal language-independent representation\nto facilitate multilingual translation. In this paper, we propose a framework\nto leverage the multimodal prompt to guide the Multimodal Multilingual neural\nMachine Translation (m3P), which aligns the representations of different\nlanguages with the same meaning and generates the conditional vision-language\nmemory for translation. We construct a multilingual multimodal instruction\ndataset (InstrMulti102) to support 102 languages. Our method aims to minimize\nthe representation distance of different languages by regarding the image as a\ncentral language. Experimental results show that m3P outperforms previous\ntext-only baselines and multilingual multimodal methods by a large margin.\nFurthermore, the probing experiments validate the effectiveness of our method\nin enhancing translation under the low-resource and massively multilingual\nscenario.\n","authors":["Jian Yang","Hongcheng Guo","Yuwei Yin","Jiaqi Bai","Bing Wang","Jiaheng Liu","Xinnian Liang","Linzheng Cahi","Liqun Yang","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2403.17556v1.pdf","comment":"COLING 2024"},{"id":"http://arxiv.org/abs/2403.17553v1","updated":"2024-03-26T10:01:01Z","published":"2024-03-26T10:01:01Z","title":"RuBia: A Russian Language Bias Detection Dataset","summary":"  Warning: this work contains upsetting or disturbing content.\n  Large language models (LLMs) tend to learn the social and cultural biases\npresent in the raw pre-training data. To test if an LLM's behavior is fair,\nfunctional datasets are employed, and due to their purpose, these datasets are\nhighly language and culture-specific. In this paper, we address a gap in the\nscope of multilingual bias evaluation by presenting a bias detection dataset\nspecifically designed for the Russian language, dubbed as RuBia. The RuBia\ndataset is divided into 4 domains: gender, nationality, socio-economic status,\nand diverse, each of the domains is further divided into multiple fine-grained\nsubdomains. Every example in the dataset consists of two sentences with the\nfirst reinforcing a potentially harmful stereotype or trope and the second\ncontradicting it. These sentence pairs were first written by volunteers and\nthen validated by native-speaking crowdsourcing workers. Overall, there are\nnearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To\nillustrate the dataset's purpose, we conduct a diagnostic evaluation of\nstate-of-the-art or near-state-of-the-art LLMs and discuss the LLMs'\npredisposition to social biases.\n","authors":["Veronika Grigoreva","Anastasiia Ivanova","Ilseyar Alimova","Ekaterina Artemova"],"pdf_url":"https://arxiv.org/pdf/2403.17553v1.pdf","comment":"accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17552v1","updated":"2024-03-26T09:59:45Z","published":"2024-03-26T09:59:45Z","title":"Naive Bayes-based Context Extension for Large Language Models","summary":"  Large Language Models (LLMs) have shown promising in-context learning\nabilities. However, conventional In-Context Learning (ICL) approaches are often\nimpeded by length limitations of transformer architecture, which pose\nchallenges when attempting to effectively integrate supervision from a\nsubstantial number of demonstration examples. In this paper, we introduce a\nnovel framework, called Naive Bayes-based Context Extension (NBCE), to enable\nexisting LLMs to perform ICL with an increased number of demonstrations by\nsignificantly expanding their context size. Importantly, this expansion does\nnot require fine-tuning or dependence on particular model architectures, all\nthe while preserving linear efficiency. NBCE initially splits the context into\nequal-sized windows fitting the target LLM's maximum length. Then, it\nintroduces a voting mechanism to select the most relevant window, regarded as\nthe posterior context. Finally, it employs Bayes' theorem to generate the test\ntask. Our experimental results demonstrate that NBCE substantially enhances\nperformance, particularly as the number of demonstration examples increases,\nconsistently outperforming alternative methods. The NBCE code will be made\npublicly accessible. The code NBCE is available at:\nhttps://github.com/amurtadha/NBCE-master\n","authors":["Jianlin Su","Murtadha Ahmed"," Wenbo","Luo Ao","Mingren Zhu","Yunfeng Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17552v1.pdf","comment":"Accepted to main NAACL 2024"},{"id":"http://arxiv.org/abs/2403.17546v1","updated":"2024-03-26T09:51:43Z","published":"2024-03-26T09:51:43Z","title":"Decoding excellence: Mapping the demand for psychological traits of\n  operations and supply chain professionals through text mining","summary":"  The current study proposes an innovative methodology for the profiling of\npsychological traits of Operations Management (OM) and Supply Chain Management\n(SCM) professionals. We use innovative methods and tools of text mining and\nsocial network analysis to map the demand for relevant skills from a set of job\ndescriptions, with a focus on psychological characteristics. The proposed\napproach aims to evaluate the market demand for specific traits by combining\nrelevant psychological constructs, text mining techniques, and an innovative\nmeasure, namely, the Semantic Brand Score. We apply the proposed methodology to\na dataset of job descriptions for OM and SCM professionals, with the objective\nof providing a mapping of their relevant required skills, including\npsychological characteristics. In addition, the analysis is then detailed by\nconsidering the region of the organization that issues the job description, its\norganizational size, and the seniority level of the open position in order to\nunderstand their nuances. Finally, topic modeling is used to examine key\ncomponents and their relative significance in job descriptions. By employing a\nnovel methodology and considering contextual factors, we provide an innovative\nunderstanding of the attitudinal traits that differentiate professionals. This\nresearch contributes to talent management, recruitment practices, and\nprofessional development initiatives, since it provides new figures and\nperspectives to improve the effectiveness and success of Operations Management\nand Supply Chain Management professionals.\n","authors":["S. Di Luozzo","A. Fronzetti Colladon","M. M. Schiraldi"],"pdf_url":"https://arxiv.org/pdf/2403.17546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17545v1","updated":"2024-03-26T09:49:35Z","published":"2024-03-26T09:49:35Z","title":"A Gaze-grounded Visual Question Answering Dataset for Clarifying\n  Ambiguous Japanese Questions","summary":"  Situated conversations, which refer to visual information as visual question\nanswering (VQA), often contain ambiguities caused by reliance on directive\ninformation. This problem is exacerbated because some languages, such as\nJapanese, often omit subjective or objective terms. Such ambiguities in\nquestions are often clarified by the contexts in conversational situations,\nsuch as joint attention with a user or user gaze information. In this study, we\npropose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous\nquestions using gaze information by focusing on a clarification process\ncomplemented by gaze information. We also propose a method that utilizes gaze\ntarget estimation results to improve the accuracy of GazeVQA tasks. Our\nexperimental results showed that the proposed method improved the performance\nin some cases of a VQA system on GazeVQA and identified some typical problems\nof GazeVQA tasks that need to be improved.\n","authors":["Shun Inadumi","Seiya Kawano","Akishige Yuguchi","Yasutomo Kawanishi","Koichiro Yoshino"],"pdf_url":"https://arxiv.org/pdf/2403.17545v1.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17540v1","updated":"2024-03-26T09:43:15Z","published":"2024-03-26T09:43:15Z","title":"Large Language Models Are State-of-the-Art Evaluator for Grammatical\n  Error Correction","summary":"  Large Language Models (LLMs) have been reported to outperform existing\nautomatic evaluation metrics in some tasks, such as text summarization and\nmachine translation. However, there has been a lack of research on LLMs as\nevaluators in grammatical error correction (GEC). In this study, we investigate\nthe performance of LLMs in GEC evaluation by employing prompts designed to\nincorporate various evaluation criteria inspired by previous research. Our\nextensive experimental results demonstrate that GPT-4 achieved Kendall's rank\ncorrelation of 0.662 with human judgments, surpassing all existing methods.\nFurthermore, in recent GEC evaluations, we have underscored the significance of\nthe LLMs scale and particularly emphasized the importance of fluency among\nevaluation criteria.\n","authors":["Masamune Kobayashi","Masato Mita","Mamoru Komachi"],"pdf_url":"https://arxiv.org/pdf/2403.17540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17536v1","updated":"2024-03-26T09:41:21Z","published":"2024-03-26T09:41:21Z","title":"ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent\n  Classifier and Slot Filler","summary":"  State-of-the-art intent classification (IC) and slot filling (SF) methods\noften rely on data-intensive deep learning models, limiting their practicality\nfor industry applications. Large language models on the other hand,\nparticularly instruction-tuned models (Instruct-LLMs), exhibit remarkable\nzero-shot performance across various natural language tasks. This study\nevaluates Instruct-LLMs on popular benchmark datasets for IC and SF,\nemphasizing their capacity to learn from fewer examples. We introduce\nILLUMINER, an approach framing IC and SF as language generation tasks for\nInstruct-LLMs, with a more efficient SF-prompting method compared to prior\nwork. A comprehensive comparison with multiple baselines shows that our\napproach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint\nIC+SF method and in-context learning with GPT3.5 (175B), particularly in slot\nfilling by 11.1--32.2 percentage points. Additionally, our in-depth ablation\nstudy demonstrates that parameter-efficient fine-tuning requires less than 6%\nof training data to yield comparable performance with traditional full-weight\nfine-tuning.\n","authors":["Paramita Mirza","Viju Sudhi","Soumya Ranjan Sahoo","Sinchana Ramakanth Bhat"],"pdf_url":"https://arxiv.org/pdf/2403.17536v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17534v1","updated":"2024-03-26T09:39:53Z","published":"2024-03-26T09:39:53Z","title":"Sparse Logistic Regression with High-order Features for Automatic\n  Grammar Rule Extraction from Treebanks","summary":"  Descriptive grammars are highly valuable, but writing them is time-consuming\nand difficult. Furthermore, while linguists typically use corpora to create\nthem, grammar descriptions often lack quantitative data. As for formal\ngrammars, they can be challenging to interpret. In this paper, we propose a new\nmethod to extract and explore significant fine-grained grammar patterns and\npotential syntactic grammar rules from treebanks, in order to create an\neasy-to-understand corpus-based grammar. More specifically, we extract\ndescriptions and rules across different languages for two linguistic phenomena,\nagreement and word order, using a large search space and paying special\nattention to the ranking order of the extracted rules. For that, we use a\nlinear classifier to extract the most salient features that predict the\nlinguistic phenomena under study. We associate statistical information to each\nrule, and we compare the ranking of the model's results to those of other\nquantitative and statistical measures. Our method captures both well-known and\nless well-known significant grammar rules in Spanish, French, and Wolof.\n","authors":["Santiago Herrera","Caio Corro","Sylvain Kahane"],"pdf_url":"https://arxiv.org/pdf/2403.17534v1.pdf","comment":"Published in LREC-Coling 2024 proceedings"},{"id":"http://arxiv.org/abs/2403.17528v1","updated":"2024-03-26T09:31:55Z","published":"2024-03-26T09:31:55Z","title":"Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual\n  Applications","summary":"  Prior work on multilingual sentence embedding has demonstrated that the\nefficient use of natural language inference (NLI) data to build\nhigh-performance models can outperform conventional methods. However, the\npotential benefits from the recent ``exponential'' growth of language models\nwith billions of parameters have not yet been fully explored. In this paper, we\nintroduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based\nmultilingual sentence embedding, by extending Sentence T5, an existing\nmonolingual model. By employing the low-rank adaptation (LoRA) technique, we\nhave achieved a successful scaling of the model's size to 5.7 billion\nparameters. We conducted experiments to evaluate the performance of sentence\nembedding and verified that the method outperforms the NLI-based prior\napproach. Furthermore, we also have confirmed a positive correlation between\nthe size of the model and its performance. It was particularly noteworthy that\nlanguages with fewer resources or those with less linguistic similarity to\nEnglish benefited more from the parameter increase. Our model is available at\nhttps://huggingface.co/pkshatech/m-ST5.\n","authors":["Chihiro Yano","Akihiko Fukuchi","Shoko Fukasawa","Hideyuki Tachibana","Yotaro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2403.17528v1.pdf","comment":"Accepted in LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.08281v4","updated":"2024-03-26T09:29:51Z","published":"2024-03-13T06:18:48Z","title":"Mastering Text, Code and Math Simultaneously via Fusing Highly\n  Specialized Language Models","summary":"  Underlying data distributions of natural language, programming code, and\nmathematical symbols vary vastly, presenting a complex challenge for large\nlanguage models (LLMs) that strive to achieve high performance across all three\ndomains simultaneously. Achieving a very high level of proficiency for an LLM\nwithin a specific domain often requires extensive training with relevant\ncorpora, which is typically accompanied by a sacrifice in performance in other\ndomains. In this paper, we propose to fuse models that are already\nhighly-specialized directly. The proposed fusing framework, UltraFuser,\nconsists of three distinct specialists that are already sufficiently trained on\nlanguage, coding, and mathematics. A token-level gating mechanism is introduced\nto blend the specialists' outputs. A two-stage training strategy accompanied by\nbalanced sampling is designed to ensure stability. To effectively train the\nfused model, we further construct a high-quality supervised instruction tuning\ndataset, UltraChat 2, which includes text, code, and mathematical content. This\ndataset comprises approximately 300,000 instructions and covers a wide range of\ntopics in each domain. Experiments show that our model could simultaneously\nachieve mastery of the three crucial domains.\n","authors":["Ning Ding","Yulin Chen","Ganqu Cui","Xingtai Lv","Weilin Zhao","Ruobing Xie","Bowen Zhou","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.08281v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17524v1","updated":"2024-03-26T09:25:57Z","published":"2024-03-26T09:25:57Z","title":"Provably Secure Disambiguating Neural Linguistic Steganography","summary":"  Recent research in provably secure neural linguistic steganography has\noverlooked a crucial aspect: the sender must detokenize stegotexts to avoid\nraising suspicion from the eavesdropper. The segmentation ambiguity problem,\nwhich arises when using language models based on subwords, leads to occasional\ndecoding failures in all neural language steganography implementations based on\nthese models. Current solutions to this issue involve altering the probability\ndistribution of candidate words, rendering them incompatible with provably\nsecure steganography. We propose a novel secure disambiguation method named\nSyncPool, which effectively addresses the segmentation ambiguity problem. We\ngroup all tokens with prefix relationships in the candidate pool before the\nsteganographic embedding algorithm runs to eliminate uncertainty among\nambiguous tokens. To enable the receiver to synchronize the sampling process of\nthe sender, a shared cryptographically-secure pseudorandom number generator\n(CSPRNG) is deployed to select a token from the ambiguity pool. SyncPool does\nnot change the size of the candidate pool or the distribution of tokens and\nthus is applicable to provably secure language steganography methods. We\nprovide theoretical proofs and experimentally demonstrate the applicability of\nour solution to various languages and models, showing its potential to\nsignificantly improve the reliability and security of neural linguistic\nsteganography systems.\n","authors":["Yuang Qi","Kejiang Chen","Kai Zeng","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17516v1","updated":"2024-03-26T09:18:59Z","published":"2024-03-26T09:18:59Z","title":"MapGuide: A Simple yet Effective Method to Reconstruct Continuous\n  Language from Brain Activities","summary":"  Decoding continuous language from brain activity is a formidable yet\npromising field of research. It is particularly significant for aiding people\nwith speech disabilities to communicate through brain signals. This field\naddresses the complex task of mapping brain signals to text. The previous best\nattempt reverse-engineered this process in an indirect way: it began by\nlearning to encode brain activity from text and then guided text generation by\naligning with predicted brain responses. In contrast, we propose a simple yet\neffective method that guides text reconstruction by directly comparing them\nwith the predicted text embeddings mapped from brain activities. Comprehensive\nexperiments reveal that our method significantly outperforms the current\nstate-of-the-art model, showing average improvements of 77% and 54% on BLEU and\nMETEOR scores. We further validate the proposed modules through detailed\nablation studies and case analyses and highlight a critical correlation: the\nmore precisely we map brain activities to text embeddings, the better the text\nreconstruction results. Such insight can simplify the task of reconstructing\nlanguage from brain activities for future work, emphasizing the importance of\nimproving brain-to-text-embedding mapping techniques.\n","authors":["Xinpei Zhao","Jingyuan Sun","Shaonan Wang","Jing Ye","Xiaohan Zhang","Chengqing Zong"],"pdf_url":"https://arxiv.org/pdf/2403.17516v1.pdf","comment":"Accepted to NAACL 2024 main conference"},{"id":"http://arxiv.org/abs/2305.16031v2","updated":"2024-03-26T09:16:36Z","published":"2023-05-25T13:08:10Z","title":"Efficient Document Embeddings via Self-Contrastive Bregman Divergence\n  Learning","summary":"  Learning quality document embeddings is a fundamental problem in natural\nlanguage processing (NLP), information retrieval (IR), recommendation systems,\nand search engines. Despite recent advances in the development of\ntransformer-based models that produce sentence embeddings with self-contrastive\nlearning, the encoding of long documents (Ks of words) is still challenging\nwith respect to both efficiency and quality considerations. Therefore, we train\nLongfomer-based document encoders using a state-of-the-art unsupervised\ncontrastive learning method (SimCSE). Further on, we complement the baseline\nmethod -- siamese neural network -- with additional convex neural networks\nbased on functional Bregman divergence aiming to enhance the quality of the\noutput document representations. We show that overall the combination of a\nself-contrastive siamese network and our proposed neural Bregman network\noutperforms the baselines in two linear classification settings on three long\ndocument topic classification tasks from the legal and biomedical domains.\n","authors":["Daniel Saggau","Mina Rezaei","Bernd Bischl","Ilias Chalkidis"],"pdf_url":"https://arxiv.org/pdf/2305.16031v2.pdf","comment":"5 pages, short paper at Findings of ACL 2023"},{"id":"http://arxiv.org/abs/2403.17497v1","updated":"2024-03-26T08:58:28Z","published":"2024-03-26T08:58:28Z","title":"Sharing the Cost of Success: A Game for Evaluating and Learning\n  Collaborative Multi-Agent Instruction Giving and Following Policies","summary":"  In collaborative goal-oriented settings, the participants are not only\ninterested in achieving a successful outcome, but do also implicitly negotiate\nthe effort they put into the interaction (by adapting to each other). In this\nwork, we propose a challenging interactive reference game that requires two\nplayers to coordinate on vision and language observations. The learning signal\nin this game is a score (given after playing) that takes into account the\nachieved goal and the players' assumed efforts during the interaction. We show\nthat a standard Proximal Policy Optimization (PPO) setup achieves a high\nsuccess rate when bootstrapped with heuristic partner behaviors that implement\ninsights from the analysis of human-human interactions. And we find that a\npairing of neural partners indeed reduces the measured joint effort when\nplaying together repeatedly. However, we observe that in comparison to a\nreasonable heuristic pairing there is still room for improvement -- which\ninvites further research in the direction of cost-sharing in collaborative\ninteractions.\n","authors":["Philipp Sadler","Sherzod Hakimov","David Schlangen"],"pdf_url":"https://arxiv.org/pdf/2403.17497v1.pdf","comment":"9 pages, Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17491v1","updated":"2024-03-26T08:47:23Z","published":"2024-03-26T08:47:23Z","title":"DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation","summary":"  The method of training language models based on domain datasets has obtained\nsignificant achievements in the task of generating scientific paper abstracts.\nHowever, such models face problems of generalization and expensive training\ncosts. The use of large language models (LLMs) to solve the task of generating\npaper abstracts saves the cost of model training. However, due to the\nhallucination problem of LLM, it is often necessary to improve the reliability\nof the results through multi-round query prompt approach such as Graph of\nThoughts (GoT), which also brings additional reasoning costs. In this paper, we\npropose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages\nof the existing GoT prompt approach, but also dynamically adjust the graph\nstructure according to data characteristics while reducing model reasoning\ncost. Experimental results show that our method's cost-effectiveness in\nabstract generation tasks is only 43.7% to 56.4% of other multi-round query\nprompt approaches. Our code is available at https://github.com/JayceNing/DGoT.\n","authors":["Xinyu Ning","Yutong Zhao","Yitong Liu","Hongwen Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17491v1.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.14974v3","updated":"2024-03-26T08:46:07Z","published":"2023-09-25T09:21:25Z","title":"Detecting Sexual Content at the Sentence Level in First Millennium Latin\n  Texts","summary":"  In this study, we propose to evaluate the use of deep learning methods for\nsemantic classification at the sentence level to accelerate the process of\ncorpus building in the field of humanities and linguistics, a traditional and\ntime-consuming task. We introduce a novel corpus comprising around 2500\nsentences spanning from 300 BCE to 900 CE including sexual semantics (medical,\nerotica, etc.). We evaluate various sentence classification approaches and\ndifferent input embedding layers, and show that all consistently outperform\nsimple token-based searches. We explore the integration of idiolectal and\nsociolectal metadata embeddings (centuries, author, type of writing), but find\nthat it leads to overfitting. Our results demonstrate the effectiveness of this\napproach, achieving high precision and true positive rates (TPR) of\nrespectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset\nsize on the model performances (420 instead of 2013), and show that, while our\nmodels perform worse, they still offer a high enough precision and TPR, even\nwithout MLM, respectively 69% and 51%. Given the result, we provide an analysis\nof the attention mechanism as a supporting added value for humanists in order\nto produce more data.\n","authors":["Thibault Clérice"],"pdf_url":"https://arxiv.org/pdf/2309.14974v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17486v1","updated":"2024-03-26T08:32:39Z","published":"2024-03-26T08:32:39Z","title":"KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with\n  Adaptive Angular margin Contrastive Learning","summary":"  Previous work on multimodal sentence embedding has proposed multimodal\ncontrastive learning and achieved promising results. However, by taking the\nrest of the batch as negative samples without reviewing when forming\ncontrastive pairs, those studies encountered many suspicious and noisy negative\nexamples, significantly affecting the methods' overall performance. In this\nwork, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning\nof Sentence Embeddings), a novel approach that enhances the discrimination and\ngeneralizability of multimodal representation and inherits the knowledge from\nthe teacher model to learn the difference between positive and negative\ninstances and via that, can detect noisy and wrong negative samples effectively\nbefore they are calculated in the contrastive objective. Furthermore, to\novercome the limitation of modeling the variation within negative pairs, we\nintroduce a new contrastive objective, AdapACSE (Adaptive Angular Margin\nSupervised Contrastive Learning for Multimodal sentence embeddings), that\nenhances the discriminative representation by strengthening the margin within\nthe angular space while capturing varying semantics within the negative.\nExperimental results on widely used Semantic Textual Similarity (STS)\nbenchmarks demonstrate the effectiveness of our approach.\n","authors":["Cong-Duy Nguyen","Thong Nguyen","Xiaobao Wu","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2403.17486v1.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2309.11888v2","updated":"2024-03-26T08:04:36Z","published":"2023-09-21T08:45:41Z","title":"High-order Joint Constituency and Dependency Parsing","summary":"  This work revisits the topic of jointly parsing constituency and dependency\ntrees, i.e., to produce compatible constituency and dependency trees\nsimultaneously for input sentences, which is attractive considering that the\ntwo types of trees are complementary in representing syntax. The original work\nof Zhou and Zhao (2019) performs joint parsing only at the inference phase.\nThey train two separate parsers under the multi-task learning framework (i.e.,\none shared encoder and two independent decoders). They design an ad-hoc dynamic\nprogramming-based decoding algorithm of $O(n^5)$ time complexity for finding\noptimal compatible tree pairs. Compared to their work, we make progress in\nthree aspects: (1) adopting a much more efficient decoding algorithm of\n$O(n^4)$ time complexity, (2) exploring joint modeling at the training phase,\ninstead of only at the inference phase, (3) proposing high-order scoring\ncomponents to promote constituent-dependency interaction. We conduct\nexperiments and analysis on seven languages, covering both rich-resource and\nlow-resource scenarios. Results and analysis show that joint modeling leads to\na modest overall performance boost over separate modeling, but substantially\nimproves the complete matching ratio of whole trees, thanks to the explicit\nmodeling of tree compatibility.\n","authors":["Yanggan Gu","Yang Hou","Zhefeng Wang","Xinyu Duan","Zhenghua Li"],"pdf_url":"https://arxiv.org/pdf/2309.11888v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17445v1","updated":"2024-03-26T07:23:46Z","published":"2024-03-26T07:23:46Z","title":"Incorporating Exponential Smoothing into MLP: A Simple but Effective\n  Sequence Model","summary":"  Modeling long-range dependencies in sequential data is a crucial step in\nsequence learning. A recently developed model, the Structured State Space (S4),\ndemonstrated significant effectiveness in modeling long-range sequences.\nHowever, It is unclear whether the success of S4 can be attributed to its\nintricate parameterization and HiPPO initialization or simply due to State\nSpace Models (SSMs). To further investigate the potential of the deep SSMs, we\nstart with exponential smoothing (ETS), a simple SSM, and propose a stacked\narchitecture by directly incorporating it into an element-wise MLP. We augment\nsimple ETS with additional parameters and complex field to reduce the inductive\nbias. Despite increasing less than 1\\% of parameters of element-wise MLP, our\nmodels achieve comparable results to S4 on the LRA benchmark.\n","authors":["Jiqun Chu","Zuoquan Lin"],"pdf_url":"https://arxiv.org/pdf/2403.17445v1.pdf","comment":"12 pages, 5 tables, 3 figures"},{"id":"http://arxiv.org/abs/2403.16662v2","updated":"2024-03-26T07:13:15Z","published":"2024-03-25T11:56:29Z","title":"RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking\n  on Russia-Ukraine Conflict","summary":"  Fact-checking is the task of verifying the factuality of a given claim by\nexamining the available evidence. High-quality evidence plays a vital role in\nenhancing fact-checking systems and facilitating the generation of explanations\nthat are understandable to humans. However, the provision of both sufficient\nand relevant evidence for explainable fact-checking systems poses a challenge.\nTo tackle this challenge, we propose a method based on a Large Language Model\nto automatically retrieve and summarize evidence from the Web. Furthermore, we\nconstruct RU22Fact, a novel multilingual explainable fact-checking dataset on\nthe Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world\nclaims, optimized evidence, and referenced explanation. To establish a baseline\nfor our dataset, we also develop an end-to-end explainable fact-checking system\nto verify claims and generate explanations. Experimental results demonstrate\nthe prospect of optimized evidence in increasing fact-checking performance and\nalso indicate the possibility of further progress in the end-to-end claim\nverification and explanation generation tasks.\n","authors":["Yirong Zeng","Xiao Ding","Yi Zhao","Xiangyu Li","Jie Zhang","Chao Yao","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2403.16662v2.pdf","comment":"12 pages, 3 figures, accepted by lrec-coling2024"},{"id":"http://arxiv.org/abs/2403.14633v2","updated":"2024-03-26T07:12:40Z","published":"2024-02-16T23:18:19Z","title":"Born With a Silver Spoon? Investigating Socioeconomic Bias in Large\n  Language Models","summary":"  Socioeconomic bias in society exacerbates disparities, influencing access to\nopportunities and resources based on individuals' economic and social\nbackgrounds. This pervasive issue perpetuates systemic inequalities, hindering\nthe pursuit of inclusive progress as a society. In this paper, we investigate\nthe presence of socioeconomic bias, if any, in large language models. To this\nend, we introduce a novel dataset SilverSpoon, consisting of 3000 samples that\nillustrate hypothetical scenarios that involve underprivileged people\nperforming ethically ambiguous actions due to their circumstances, and ask\nwhether the action is ethically justified. Further, this dataset has a\ndual-labeling scheme and has been annotated by people belonging to both ends of\nthe socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of\nsocioeconomic bias expressed in large language models and the variation of this\ndegree as a function of model size. We also perform qualitative analysis to\nanalyze the nature of this bias. Our analysis reveals that while humans\ndisagree on which situations require empathy toward the underprivileged, most\nlarge language models are unable to empathize with the socioeconomically\nunderprivileged regardless of the situation. To foster further research in this\ndomain, we make SilverSpoon and our evaluation harness publicly available.\n","authors":["Smriti Singh","Shuvam Keshari","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2403.14633v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17431v1","updated":"2024-03-26T06:57:23Z","published":"2024-03-26T06:57:23Z","title":"Robust and Scalable Model Editing for Large Language Models","summary":"  Large language models (LLMs) can make predictions using parametric\nknowledge--knowledge encoded in the model weights--or contextual\nknowledge--knowledge presented in the context. In many scenarios, a desirable\nbehavior is that LLMs give precedence to contextual knowledge when it conflicts\nwith the parametric knowledge, and fall back to using their parametric\nknowledge when the context is irrelevant. This enables updating and correcting\nthe model's knowledge by in-context editing instead of retraining. Previous\nworks have shown that LLMs are inclined to ignore contextual knowledge and fail\nto reliably fall back to parametric knowledge when presented with irrelevant\ncontext. In this work, we discover that, with proper prompting methods,\ninstruction-finetuned LLMs can be highly controllable by contextual knowledge\nand robust to irrelevant context. Utilizing this feature, we propose EREN (Edit\nmodels by REading Notes) to improve the scalability and robustness of LLM\nediting. To better evaluate the robustness of model editors, we collect a new\ndataset, that contains irrelevant questions that are more challenging than the\nones in existing datasets. Empirical results show that our method outperforms\ncurrent state-of-the-art methods by a large margin. Unlike existing techniques,\nit can integrate knowledge from multiple edits, and correctly respond to\nsyntactically similar but semantically unrelated inputs (and vice versa). The\nsource code can be found at https://github.com/thunlp/EREN.\n","authors":["Yingfa Chen","Zhengyan Zhang","Xu Han","Chaojun Xiao","Zhiyuan Liu","Chen Chen","Kuai Li","Tao Yang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2403.17431v1.pdf","comment":"LREC-COLING 2024 paper, 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2207.01262v2","updated":"2024-03-26T06:54:43Z","published":"2022-07-04T08:54:43Z","title":"Understanding Performance of Long-Document Ranking Models through\n  Comprehensive Evaluation and Leaderboarding","summary":"  We evaluated 20+ Transformer models for ranking of long documents (including\nrecent LongP models trained with FlashAttention) and compared them with simple\nFirstP baselines (applying the same model to input truncated to the first 512\ntokens). We used MS MARCO Documents v1 as a primary training set and evaluated\nmodels in the zero-shot scenario as well as after fine-tuning on other\ncollections.\n  In our initial experiments with standard collections we found that\nlong-document models underperformed FirstP or outperformed it by at most 5% on\naverage in terms of MRR or NDCG. We then conjectured that this was not due to\nmodels inability to process long context but rather due to a positional bias of\nrelevant passages, which tended to be among the first 512 document tokens. We\nfound evidence that this bias was, indeed, present in at least two test sets,\nwhich motivated us to create a new collection MS MARCO FarRelevant where the\nrelevant passages were not present among the first 512 tokens.\n  Unlike standard collections where we observed both little benefit from\nincorporating longer contexts and limited variability in model performance\n(within a few %), experiments on MS MARCO FarRelevant uncovered dramatic\ndifferences among models. FirstP models performed roughly at the\nrandom-baseline level in both zero-shot and fine-tuning scenarios. Simple\naggregation models (e.g., MaxP) had good zero-shot accuracy but benefited\nlittle from fine-tuning. Most other models had poor zero-shot performance\n(sometimes at a random baseline level) but outstripped MaxP by as much 13-28\\%\nafter finetuning. Thus, positional bias not only diminishes benefits of\nprocessing longer document contexts but also leads to model overfitting to this\nbias and performing poorly in a zero-shot setting when a distribution of\nrelevant passages changes substantially.\n  We make our software and MS MARCO FarRelevant available.\n","authors":["Leonid Boytsov","David Akinpelu","Tianyi Lin","Fangwei Gao","Yutian Zhao","Jeffrey Huang","Eric Nyberg"],"pdf_url":"https://arxiv.org/pdf/2207.01262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17428v1","updated":"2024-03-26T06:50:04Z","published":"2024-03-26T06:50:04Z","title":"Aligning Large Language Models for Enhancing Psychiatric Interviews\n  through Symptom Delineation and Summarization","summary":"  Recent advancements in Large Language Models (LLMs) have accelerated their\nusage in various domains. Given the fact that psychiatric interviews are\ngoal-oriented and structured dialogues between the professional interviewer and\nthe interviewee, it is one of the most underexplored areas where LLMs can\ncontribute substantial value. Here, we explore the use of LLMs for enhancing\npsychiatric interviews, by analyzing counseling data from North Korean\ndefectors with traumatic events and mental health issues. Specifically, we\ninvestigate whether LLMs can (1) delineate the part of the conversation that\nsuggests psychiatric symptoms and name the symptoms, and (2) summarize\nstressors and symptoms, based on the interview dialogue transcript. Here, the\ntranscript data was labeled by mental health experts for training and\nevaluation of LLMs. Our experimental results show that appropriately prompted\nLLMs can achieve high performance on both the symptom delineation task and the\nsummarization task. This research contributes to the nascent field of applying\nLLMs to psychiatric interview and demonstrates their potential effectiveness in\naiding mental health practitioners.\n","authors":["Jae-hee So","Joonhwan Chang","Eunji Kim","Junho Na","JiYeon Choi","Jy-yong Sohn","Byung-Hoon Kim","Sang Hui Chu"],"pdf_url":"https://arxiv.org/pdf/2403.17428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17413v1","updated":"2024-03-26T06:12:21Z","published":"2024-03-26T06:12:21Z","title":"LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error\n  Correction","summary":"  Over-correction is a critical problem in Chinese grammatical error correction\n(CGEC) task. Recent work using model ensemble methods based on voting can\neffectively mitigate over-correction and improve the precision of the GEC\nsystem. However, these methods still require the output of several GEC systems\nand inevitably lead to reduced error recall. In this light, we propose the\nLM-Combiner, a rewriting model that can directly modify the over-correction of\nGEC system outputs without a model ensemble. Specifically, we train the model\non an over-correction dataset constructed through the proposed K-fold cross\ninference method, which allows it to directly generate filtered sentences by\ncombining the original and the over-corrected text. In the inference stage, we\ndirectly take the original sentences and the output results of other systems as\ninput and then obtain the filtered sentences through LM-Combiner. Experiments\non the FCGEC dataset show that our proposed method effectively alleviates the\nover-correction of the original system (+18.2 Precision) while ensuring the\nerror recall remains unchanged. Besides, we find that LM-Combiner still has a\ngood rewriting performance even with small parameters and few training data,\nand thus can cost-effectively mitigate the over-correction of black-box GEC\nsystems (e.g., ChatGPT).\n","authors":["Yixuan Wang","Baoxin Wang","Yijun Liu","Dayong Wu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2403.17413v1.pdf","comment":"Accepted to COLING 2024"},{"id":"http://arxiv.org/abs/2403.17411v1","updated":"2024-03-26T06:11:07Z","published":"2024-03-26T06:11:07Z","title":"PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large\n  Language Models","summary":"  Prompt compression is an innovative method for efficiently condensing input\nprompts while preserving essential information. To facilitate quick-start\nservices, user-friendly interfaces, and compatibility with common datasets and\nmetrics, we present the Prompt Compression Toolkit (PCToolkit). This toolkit is\na unified plug-and-play solution for compressing prompts in Large Language\nModels (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and\nmetrics for comprehensive performance evaluation. PCToolkit boasts a modular\ndesign, allowing for easy integration of new datasets and metrics through\nportable and user-friendly interfaces. In this paper, we outline the key\ncomponents and functionalities of PCToolkit. We conducted evaluations of the\ncompressors within PCToolkit across various natural language tasks, including\nreconstruction, summarization, mathematical problem-solving, question\nanswering, few-shot learning, synthetic tasks, code completion, boolean\nexpressions, multiple choice questions, and lies recognition.\n","authors":["Jinyi Li","Yihuai Lan","Lei Wang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17411v1.pdf","comment":"For open-source repository, see\n  https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression"},{"id":"http://arxiv.org/abs/2403.17407v1","updated":"2024-03-26T05:55:21Z","published":"2024-03-26T05:55:21Z","title":"Transcribing Bengali Text with Regional Dialects to IPA using District\n  Guided Tokens","summary":"  Accurate transcription of Bengali text to the International Phonetic Alphabet\n(IPA) is a challenging task due to the complex phonology of the language and\ncontext-dependent sound changes. This challenge is even more for regional\nBengali dialects due to unavailability of standardized spelling conventions for\nthese dialects, presence of local and foreign words popular in those regions\nand phonological diversity across different regions. This paper presents an\napproach to this sequence-to-sequence problem by introducing the District\nGuided Tokens (DGT) technique on a new dataset spanning six districts of\nBangladesh. The key idea is to provide the model with explicit information\nabout the regional dialect or \"district\" of the input text before generating\nthe IPA transcription. This is achieved by prepending a district token to the\ninput sequence, effectively guiding the model to understand the unique phonetic\npatterns associated with each district. The DGT technique is applied to\nfine-tune several transformer-based models, on this new dataset. Experimental\nresults demonstrate the effectiveness of DGT, with the ByT5 model achieving\nsuperior performance over word-based models like mT5, BanglaT5, and umT5. This\nis attributed to ByT5's ability to handle a high percentage of\nout-of-vocabulary words in the test set. The proposed approach highlights the\nimportance of incorporating regional dialect information into ubiquitous\nnatural language processing systems for languages with diverse phonological\nvariations. The following work was a result of the \"Bhashamul\" challenge, which\nis dedicated to solving the problem of Bengali text with regional dialects to\nIPA transcription https://www.kaggle.com/competitions/regipa/. The training and\ninference notebooks are available through the competition link.\n","authors":["S M Jishanul Islam","Sadia Ahmmed","Sahid Hossain Mustakim"],"pdf_url":"https://arxiv.org/pdf/2403.17407v1.pdf","comment":"This work became the champion of the Bhashamul challenge"},{"id":"http://arxiv.org/abs/2403.17385v1","updated":"2024-03-26T05:11:51Z","published":"2024-03-26T05:11:51Z","title":"ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity\n  Recognition","summary":"  In this work, we revisit the problem of semi-supervised named entity\nrecognition (NER) focusing on extremely light supervision, consisting of a\nlexicon containing only 10 examples per class. We introduce ELLEN, a simple,\nfully modular, neuro-symbolic method that blends fine-tuned language models\nwith linguistic rules. These rules include insights such as ''One Sense Per\nDiscourse'', using a Masked Language Model as an unsupervised NER, leveraging\npart-of-speech tags to identify and eliminate unlabeled entities as false\nnegatives, and other intuitions about classifier confidence scores in local and\nglobal context. ELLEN achieves very strong performance on the CoNLL-2003\ndataset when using the minimal supervision from the lexicon above. It also\noutperforms most existing (and considerably more complex) semi-supervised NER\nmethods under the same supervision settings commonly used in the literature\n(i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a\nzero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and\nachieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also\nachieves over 75% of the performance of a strong, fully supervised model\ntrained on gold data. Our code is available at:\nhttps://github.com/hriaz17/ELLEN.\n","authors":["Haris Riaz","Razvan-Gabriel Dumitru","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2403.17385v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2112.06166v2","updated":"2024-03-26T04:26:18Z","published":"2021-12-12T06:25:15Z","title":"Topic Detection and Tracking with Time-Aware Document Embeddings","summary":"  The time at which a message is communicated is a vital piece of metadata in\nmany real-world natural language processing tasks such as Topic Detection and\nTracking (TDT). TDT systems aim to cluster a corpus of news articles by event,\nand in that context, stories that describe the same event are likely to have\nbeen written at around the same time. Prior work on time modeling for TDT takes\nthis into account, but does not well capture how time interacts with the\nsemantic nature of the event. For example, stories about a tropical storm are\nlikely to be written within a short time interval, while stories about a movie\nrelease may appear over weeks or months. In our work, we design a neural method\nthat fuses temporal and textual information into a single representation of\nnews documents for event detection. We fine-tune these time-aware document\nembeddings with a triplet loss architecture, integrate the model into\ndownstream TDT systems, and evaluate the systems on two benchmark TDT data sets\nin English. In the retrospective setting, we apply clustering algorithms to the\ntime-aware embeddings and show substantial improvements over baselines on the\nNews2013 data set. In the online streaming setting, we add our document encoder\nto an existing state-of-the-art TDT pipeline and demonstrate that it can\nbenefit the overall performance. We conduct ablation studies on the time\nrepresentation and fusion algorithm strategies, showing that our proposed model\noutperforms alternative strategies. Finally, we probe the model to examine how\nit handles recurring events more effectively than previous TDT systems.\n","authors":["Hang Jiang","Doug Beeferman","Weiquan Mao","Deb Roy"],"pdf_url":"https://arxiv.org/pdf/2112.06166v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2308.06463v2","updated":"2024-03-26T04:23:12Z","published":"2023-08-12T04:05:57Z","title":"GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher","summary":"  Safety lies at the core of the development of Large Language Models (LLMs).\nThere is ample work on aligning LLMs with human ethics and preferences,\nincluding data filtering in pretraining, supervised fine-tuning, reinforcement\nlearning from human feedback, and red teaming, etc. In this study, we discover\nthat chat in cipher can bypass the safety alignment techniques of LLMs, which\nare mainly conducted in natural languages. We propose a novel framework\nCipherChat to systematically examine the generalizability of safety alignment\nto non-natural languages -- ciphers. CipherChat enables humans to chat with\nLLMs through cipher prompts topped with system role descriptions and few-shot\nenciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,\nincluding ChatGPT and GPT-4 for different representative human ciphers across\n11 safety domains in both English and Chinese. Experimental results show that\ncertain ciphers succeed almost 100% of the time to bypass the safety alignment\nof GPT-4 in several safety domains, demonstrating the necessity of developing\nsafety alignment for non-natural languages. Notably, we identify that LLMs seem\nto have a ''secret cipher'', and propose a novel SelfCipher that uses only role\nplay and several demonstrations in natural language to evoke this capability.\nSelfCipher surprisingly outperforms existing human ciphers in almost all cases.\nOur code and data will be released at https://github.com/RobustNLP/CipherChat.\n","authors":["Youliang Yuan","Wenxiang Jiao","Wenxuan Wang","Jen-tse Huang","Pinjia He","Shuming Shi","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2308.06463v2.pdf","comment":"Accepted by ICLR 2024. 21 pages, 3 figures, 13 tables"},{"id":"http://arxiv.org/abs/2403.09963v2","updated":"2024-03-26T04:08:47Z","published":"2024-03-15T02:04:35Z","title":"Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias\n  in Factual Knowledge Extraction","summary":"  Recent research shows that pre-trained language models (PLMs) suffer from\n\"prompt bias\" in factual knowledge extraction, i.e., prompts tend to introduce\nbiases toward specific labels. Prompt bias presents a significant challenge in\nassessing the factual knowledge within PLMs. Therefore, this paper aims to\nimprove the reliability of existing benchmarks by thoroughly investigating and\nmitigating prompt bias. We show that: 1) all prompts in the experiments exhibit\nnon-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt\ndisplaying significantly higher levels of bias; 2) prompt bias can amplify\nbenchmark accuracy unreasonably by overfitting the test datasets, especially on\nimbalanced datasets like LAMA. Based on these findings, we propose a\nrepresentation-based approach to mitigate the prompt bias during inference\ntime. Specifically, we first estimate the biased representation using\nprompt-only querying, and then remove it from the model's internal\nrepresentations to generate the debiased representations, which are used to\nproduce the final debiased outputs. Experiments across various prompts, PLMs,\nand benchmarks show that our approach can not only correct the overfitted\nperformance caused by prompt bias, but also significantly improve the prompt\nretrieval capability (up to 10% absolute performance gain). These results\nindicate that our approach effectively alleviates prompt bias in knowledge\nevaluation, thereby enhancing the reliability of benchmark assessments.\nHopefully, our plug-and-play approach can be a golden standard to strengthen\nPLMs toward reliable knowledge bases. Code and data are released in\nhttps://github.com/FelliYang/PromptBias.\n","authors":["Ziyang Xu","Keqin Peng","Liang Ding","Dacheng Tao","Xiliang Lu"],"pdf_url":"https://arxiv.org/pdf/2403.09963v2.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2403.17368v1","updated":"2024-03-26T04:07:08Z","published":"2024-03-26T04:07:08Z","title":"ChatGPT Rates Natural Language Explanation Quality Like Humans: But on\n  Which Scales?","summary":"  As AI becomes more integral in our lives, the need for transparency and\nresponsibility grows. While natural language explanations (NLEs) are vital for\nclarifying the reasoning behind AI decisions, evaluating them through human\njudgments is complex and resource-intensive due to subjectivity and the need\nfor fine-grained ratings. This study explores the alignment between ChatGPT and\nhuman assessments across multiple scales (i.e., binary, ternary, and 7-Likert\nscale). We sample 300 data instances from three NLE datasets and collect 900\nhuman annotations for both informativeness and clarity scores as the text\nquality measurement. We further conduct paired comparison experiments under\ndifferent ranges of subjectivity scores, where the baseline comes from 8,346\nhuman annotations. Our results show that ChatGPT aligns better with humans in\nmore coarse-grained scales. Also, paired comparisons and dynamic prompting\n(i.e., providing semantically similar examples in the prompt) improve the\nalignment. This research advances our understanding of large language models'\ncapabilities to assess the text explanation quality in different configurations\nfor responsible AI development.\n","authors":["Fan Huang","Haewoon Kwak","Kunwoo Park","Jisun An"],"pdf_url":"https://arxiv.org/pdf/2403.17368v1.pdf","comment":"Accpeted by LREC-COLING 2024 main conference, long paper"},{"id":"http://arxiv.org/abs/2403.17363v1","updated":"2024-03-26T03:58:52Z","published":"2024-03-26T03:58:52Z","title":"Extracting Biomedical Entities from Noisy Audio Transcripts","summary":"  Automatic Speech Recognition (ASR) technology is fundamental in transcribing\nspoken language into text, with considerable applications in the clinical\nrealm, including streamlining medical transcription and integrating with\nElectronic Health Record (EHR) systems. Nevertheless, challenges persist,\nespecially when transcriptions contain noise, leading to significant drops in\nperformance when Natural Language Processing (NLP) models are applied. Named\nEntity Recognition (NER), an essential clinical task, is particularly affected\nby such noise, often termed the ASR-NLP gap. Prior works have primarily studied\nASR's efficiency in clean recordings, leaving a research gap concerning the\nperformance in noisy environments. This paper introduces a novel dataset,\nBioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain,\nfocusing on extracting adverse drug reactions and mentions of entities from the\nBrief Test of Adult Cognition by Telephone (BTACT) exam. Our dataset offers a\ncomprehensive collection of almost 2,000 clean and noisy recordings. In\naddressing the noise challenge, we present an innovative transcript-cleaning\nmethod using GPT4, investigating both zero-shot and few-shot methodologies. Our\nstudy further delves into an error analysis, shedding light on the types of\nerrors in transcription software, corrections by GPT4, and the challenges GPT4\nfaces. This paper aims to foster improved understanding and potential solutions\nfor the ASR-NLP gap, ultimately supporting enhanced healthcare documentation\npractices.\n","authors":["Nima Ebadi","Kellen Morgan","Adrian Tan","Billy Linares","Sheri Osborn","Emma Majors","Jeremy Davis","Anthony Rios"],"pdf_url":"https://arxiv.org/pdf/2403.17363v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17361v1","updated":"2024-03-26T03:54:25Z","published":"2024-03-26T03:54:25Z","title":"Bridging Textual and Tabular Worlds for Fact Verification: A\n  Lightweight, Attention-Based Model","summary":"  FEVEROUS is a benchmark and research initiative focused on fact extraction\nand verification tasks involving unstructured text and structured tabular data.\nIn FEVEROUS, existing works often rely on extensive preprocessing and utilize\nrule-based transformations of data, leading to potential context loss or\nmisleading encodings. This paper introduces a simple yet powerful model that\nnullifies the need for modality conversion, thereby preserving the original\nevidence's context. By leveraging pre-trained models on diverse text and\ntabular datasets and by incorporating a lightweight attention-based mechanism,\nour approach efficiently exploits latent connections between different data\ntypes, thereby yielding comprehensive and reliable verdict predictions. The\nmodel's modular structure adeptly manages multi-modal information, ensuring the\nintegrity and authenticity of the original evidence are uncompromised.\nComparative analyses reveal that our approach exhibits competitive performance,\naligning itself closely with top-tier models on the FEVEROUS benchmark.\n","authors":["Shirin Dabbaghi Varnosfaderani","Canasai Kruengkrai","Ramin Yahyapour","Junichi Yamagishi"],"pdf_url":"https://arxiv.org/pdf/2403.17361v1.pdf","comment":"Accepted for a presentation at LREC-COLING 2024 - The 2024 Joint\n  International Conference on Computational Linguistics, Language Resources and\n  Evaluation"},{"id":"http://arxiv.org/abs/2403.17359v1","updated":"2024-03-26T03:51:01Z","published":"2024-03-26T03:51:01Z","title":"Chain-of-Action: Faithful and Multimodal Question Answering through\n  Large Language Models","summary":"  We present a Chain-of-Action (CoA) framework for multimodal and\nretrieval-augmented Question-Answering (QA). Compared to the literature, CoA\novercomes two major challenges of current QA applications: (i) unfaithful\nhallucination that is inconsistent with real-time or domain facts and (ii) weak\nreasoning performance over compositional information. Our key contribution is a\nnovel reasoning-retrieval mechanism that decomposes a complex question into a\nreasoning chain via systematic prompting and pre-designed actions.\nMethodologically, we propose three types of domain-adaptable `Plug-and-Play'\nactions for retrieving real-time information from heterogeneous sources. We\nalso propose a multi-reference faith score (MRFS) to verify and resolve\nconflicts in the answers. Empirically, we exploit both public benchmarks and a\nWeb3 case study to demonstrate the capability of CoA over other methods.\n","authors":["Zhenyu Pan","Haozheng Luo","Manling Li","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09887v2","updated":"2024-03-26T23:52:35Z","published":"2024-03-14T21:44:48Z","title":"Sabiá-2: A New Generation of Portuguese Large Language Models","summary":"  We introduce Sabi\\'a-2, a family of large language models trained on\nPortuguese texts. The models are evaluated on a diverse range of exams,\nincluding entry-level tests for Brazilian universities, professional\ncertification exams, and graduate-level exams for various disciplines such as\naccounting, economics, engineering, law and medicine. Our results reveal that\nour best model so far, Sabi\\'a-2 Medium, matches or surpasses GPT-4's\nperformance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64\nexams. Notably, specialization has a significant impact on a model's\nperformance without the need to increase its size, allowing us to offer\nSabi\\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.\nFinally, we identified that math and coding are key abilities that need\nimprovement.\n","authors":["Thales Sales Almeida","Hugo Abonizio","Rodrigo Nogueira","Ramon Pires"],"pdf_url":"https://arxiv.org/pdf/2403.09887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18159v1","updated":"2024-03-26T23:51:44Z","published":"2024-03-26T23:51:44Z","title":"Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal\n  Propagation Analysis for Large Language Models","summary":"  Large generative models, such as large language models (LLMs) and diffusion\nmodels have as revolutionized the fields of NLP and computer vision\nrespectively. However, their slow inference, high computation and memory\nrequirement makes it challenging to deploy them on edge devices. In this study,\nwe propose a light-weight quantization aware fine tuning technique using\nknowledge distillation (KD-QAT) to improve the performance of 4-bit weight\nquantized LLMs using commonly available datasets to realize a popular language\nuse case, on device chat applications. To improve this paradigm of finetuning,\nas main contributions, we provide insights into stability of KD-QAT by\nempirically studying the gradient propagation during training to better\nunderstand the vulnerabilities of KD-QAT based approaches to low-bit\nquantization errors. Based on our insights, we propose ov-freeze, a simple\ntechnique to stabilize the KD-QAT process. Finally, we experiment with the\npopular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that\nov-freeze results in near float-point precision performance, i.e., less than\n0.7% loss of accuracy on Commonsense Reasoning benchmarks.\n","authors":["Kartikeya Bhardwaj","Nilesh Prasad Pandey","Sweta Priyadarshi","Kyunggeun Lee","Jun Ma","Harris Teague"],"pdf_url":"https://arxiv.org/pdf/2403.18159v1.pdf","comment":"Accepted at Practical ML for Low Resource Settings Workshop at ICLR\n  2024"},{"id":"http://arxiv.org/abs/2403.18152v1","updated":"2024-03-26T23:32:52Z","published":"2024-03-26T23:32:52Z","title":"Large Language Models as Financial Data Annotators: A Study on\n  Effectiveness and Efficiency","summary":"  Collecting labeled datasets in finance is challenging due to scarcity of\ndomain experts and higher cost of employing them. While Large Language Models\n(LLMs) have demonstrated remarkable performance in data annotation tasks on\ngeneral domain datasets, their effectiveness on domain specific datasets\nremains underexplored. To address this gap, we investigate the potential of\nLLMs as efficient data annotators for extracting relations in financial\ndocuments. We compare the annotations produced by three LLMs (GPT-4, PaLM 2,\nand MPT Instruct) against expert annotators and crowdworkers. We demonstrate\nthat the current state-of-the-art LLMs can be sufficient alternatives to\nnon-expert crowdworkers. We analyze models using various prompts and parameter\nsettings and find that customizing the prompts for each relation group by\nproviding specific examples belonging to those groups is paramount.\nFurthermore, we introduce a reliability index (LLM-RelIndex) used to identify\noutputs that may require expert attention. Finally, we perform an extensive\ntime, cost and error analysis and provide recommendations for the collection\nand usage of automated annotations in domain-specific settings.\n","authors":["Toyin Aguda","Suchetha Siddagangappa","Elena Kochkina","Simerjot Kaur","Dongsheng Wang","Charese Smiley","Sameena Shah"],"pdf_url":"https://arxiv.org/pdf/2403.18152v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18148v1","updated":"2024-03-26T23:14:34Z","published":"2024-03-26T23:14:34Z","title":"Large Language Models Produce Responses Perceived to be Empathic","summary":"  Large Language Models (LLMs) have demonstrated surprising performance on many\ntasks, including writing supportive messages that display empathy. Here, we had\nthese models generate empathic messages in response to posts describing common\nlife experiences, such as workplace situations, parenting, relationships, and\nother anxiety- and anger-eliciting situations. Across two studies (N=192, 202),\nwe showed human raters a variety of responses written by several models (GPT4\nTurbo, Llama2, and Mistral), and had people rate these responses on how\nempathic they seemed to be. We found that LLM-generated responses were\nconsistently rated as more empathic than human-written responses. Linguistic\nanalyses also show that these models write in distinct, predictable ``styles\",\nin terms of their use of punctuation, emojis, and certain words. These results\nhighlight the potential of using LLMs to enhance human peer support in contexts\nwhere empathy is important.\n","authors":["Yoon Kyung Lee","Jina Suh","Hongli Zhan","Junyi Jessy Li","Desmond C. Ong"],"pdf_url":"https://arxiv.org/pdf/2403.18148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09618v2","updated":"2024-03-26T22:59:52Z","published":"2023-03-16T19:47:41Z","title":"HIVE: Harnessing Human Feedback for Instructional Visual Editing","summary":"  Incorporating human feedback has been shown to be crucial to align text\ngenerated by large language models to human preferences. We hypothesize that\nstate-of-the-art instructional image editing models, where outputs are\ngenerated based on an input image and an editing instruction, could similarly\nbenefit from human feedback, as their outputs may not adhere to the correct\ninstructions and preferences of users. In this paper, we present a novel\nframework to harness human feedback for instructional visual editing (HIVE).\nSpecifically, we collect human feedback on the edited images and learn a reward\nfunction to capture the underlying user preferences. We then introduce scalable\ndiffusion model fine-tuning methods that can incorporate human preferences\nbased on the estimated reward. Besides, to mitigate the bias brought by the\nlimitation of data, we contribute a new 1M training dataset, a 3.6K reward\ndataset for rewards learning, and a 1K evaluation dataset to boost the\nperformance of instructional image editing. We conduct extensive empirical\nexperiments quantitatively and qualitatively, showing that HIVE is favored over\nprevious state-of-the-art instructional image editing approaches by a large\nmargin.\n","authors":["Shu Zhang","Xinyi Yang","Yihao Feng","Can Qin","Chia-Chih Chen","Ning Yu","Zeyuan Chen","Huan Wang","Silvio Savarese","Stefano Ermon","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2303.09618v2.pdf","comment":"In CVPR, 2024"},{"id":"http://arxiv.org/abs/2310.19055v2","updated":"2024-03-26T22:59:36Z","published":"2023-10-29T16:02:46Z","title":"A Few-Shot Learning Focused Survey on Recent Named Entity Recognition\n  and Relation Classification Methods","summary":"  Named Entity Recognition (NER) and Relation Classification (RC) are important\nsteps in extracting information from unstructured text and formatting it into a\nmachine-readable format. We present a survey of recent deep learning models\nthat address named entity recognition and relation classification, with focus\non few-shot learning performance. Our survey is helpful for researchers in\nknowing the recent techniques in text mining and extracting structured\ninformation from raw text.\n","authors":["Sakher Khalil Alqaaidi","Elika Bozorgi","Afsaneh Shams","Krzysztof Kochut"],"pdf_url":"https://arxiv.org/pdf/2310.19055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05882v2","updated":"2024-03-26T22:54:48Z","published":"2023-06-09T13:24:27Z","title":"Good, but not always Fair: An Evaluation of Gender Bias for three\n  commercial Machine Translation Systems","summary":"  Machine Translation (MT) continues to make significant strides in quality and\nis increasingly adopted on a larger scale. Consequently, analyses have been\nredirected to more nuanced aspects, intricate phenomena, as well as potential\nrisks that may arise from the widespread use of MT tools. Along this line, this\npaper offers a meticulous assessment of three commercial MT systems - Google\nTranslate, DeepL, and Modern MT - with a specific focus on gender translation\nand bias. For three language pairs (English/Spanish, English/Italian, and\nEnglish/French), we scrutinize the behavior of such systems at several levels\nof granularity and on a variety of naturally occurring gender phenomena in\ntranslation. Our study takes stock of the current state of online MT tools, by\nrevealing significant discrepancies in the gender translation of the three\nsystems, with each system displaying varying degrees of bias despite their\noverall translation quality.\n","authors":["Silvia Alma Piazzolla","Beatrice Savoldi","Luisa Bentivogli"],"pdf_url":"https://arxiv.org/pdf/2306.05882v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18140v1","updated":"2024-03-26T22:54:12Z","published":"2024-03-26T22:54:12Z","title":"Juru: Legal Brazilian Large Language Model from Reputable Sources","summary":"  The high computational cost associated with pretraining large language models\nlimits their research. Two strategies have emerged to address this issue:\ndomain specialization and pretraining with high-quality data. To explore these\nstrategies, we specialized the Sabi\\'a-2 Small model with 1.9 billion unique\ntokens from reputable Brazilian legal sources and conducted few-shot\nevaluations on legal and general knowledge exams. Our model, Juru, demonstrates\nthe benefits of domain specialization with a reduced amount of pretraining\ndata. However, this specialization comes at the expense of degrading\nperformance in other knowledge areas within the same language. This study\ncontributes to the growing body of scientific evidence showing that pretraining\ndata selection may enhance the performance of large language models, enabling\nthe exploration of these models at a lower cost.\n","authors":["Roseval Malaquias Junior","Ramon Pires","Roseli Romero","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2403.18140v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.17937v1","updated":"2024-03-26T17:59:58Z","published":"2024-03-26T17:59:58Z","title":"Efficient Video Object Segmentation via Modulated Cross-Attention Memory","summary":"  Recently, transformer-based approaches have shown promising results for\nsemi-supervised video object segmentation. However, these approaches typically\nstruggle on long videos due to increased GPU memory demands, as they frequently\nexpand the memory bank every few frames. We propose a transformer-based\napproach, named MAVOS, that introduces an optimized and dynamic long-term\nmodulated cross-attention (MCA) memory to model temporal smoothness without\nrequiring frequent memory expansion. The proposed MCA effectively encodes both\nlocal and global features at various levels of granularity while efficiently\nmaintaining consistent speed regardless of the video length. Extensive\nexperiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017,\ndemonstrate the effectiveness of our proposed contributions leading to\nreal-time inference and markedly reduced memory demands without any degradation\nin segmentation accuracy on long videos. Compared to the best existing\ntransformer-based approach, our MAVOS increases the speed by 7.6x, while\nsignificantly reducing the GPU memory by 87% with comparable segmentation\nperformance on short and long video datasets. Notably on the LVOS dataset, our\nMAVOS achieves a J&F score of 63.3% while operating at 37 frames per second\n(FPS) on a single V100 GPU. Our code and models will be publicly available at:\nhttps://github.com/Amshaker/MAVOS.\n","authors":["Abdelrahman Shaker","Syed Talal Wasim","Martin Danelljan","Salman Khan","Ming-Hsuan Yang","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.17937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17936v1","updated":"2024-03-26T17:59:52Z","published":"2024-03-26T17:59:52Z","title":"ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture\n  Synthesis","summary":"  Gestures play a key role in human communication. Recent methods for co-speech\ngesture generation, while managing to generate beat-aligned motions, struggle\ngenerating gestures that are semantically aligned with the utterance. Compared\nto beat gestures that align naturally to the audio signal, semantically\ncoherent gestures require modeling the complex interactions between the\nlanguage and human motion, and can be controlled by focusing on certain words.\nTherefore, we present ConvoFusion, a diffusion-based approach for multi-modal\ngesture synthesis, which can not only generate gestures based on multi-modal\nspeech inputs, but can also facilitate controllability in gesture synthesis.\nOur method proposes two guidance objectives that allow the users to modulate\nthe impact of different conditioning modalities (e.g. audio vs text) as well as\nto choose certain words to be emphasized during gesturing. Our method is\nversatile in that it can be trained either for generating monologue gestures or\neven the conversational gestures. To further advance the research on\nmulti-party interactive gestures, the DnD Group Gesture dataset is released,\nwhich contains 6 hours of gesture data showing 5 people interacting with one\nanother. We compare our method with several recent works and demonstrate\neffectiveness of our method on a variety of tasks. We urge the reader to watch\nour supplementary video at our website.\n","authors":["Muhammad Hamza Mughal","Rishabh Dabral","Ikhsanul Habibie","Lucia Donatelli","Marc Habermann","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2403.17936v1.pdf","comment":"CVPR 2024. Project Page:\n  https://vcai.mpi-inf.mpg.de/projects/ConvoFusion/"},{"id":"http://arxiv.org/abs/2403.17935v1","updated":"2024-03-26T17:59:24Z","published":"2024-03-26T17:59:24Z","title":"OmniVid: A Generative Framework for Universal Video Understanding","summary":"  The core of video understanding tasks, such as recognition, captioning, and\ntracking, is to automatically detect objects or actions in a video and analyze\ntheir temporal evolution. Despite sharing a common goal, different tasks often\nrely on distinct model architectures and annotation formats. In contrast,\nnatural language processing benefits from a unified output space, i.e., text\nsequences, which simplifies the training of powerful foundational language\nmodels, such as GPT-3, with extensive training corpora. Inspired by this, we\nseek to unify the output space of video understanding tasks by using languages\nas labels and additionally introducing time and box tokens. In this way, a\nvariety of video tasks could be formulated as video-grounded token generation.\nThis enables us to address various types of video tasks, including\nclassification (such as action recognition), captioning (covering clip\ncaptioning, video question answering, and dense video captioning), and\nlocalization tasks (such as visual object tracking) within a fully shared\nencoder-decoder architecture, following a generative framework. Through\ncomprehensive experiments, we demonstrate such a simple and straightforward\nidea is quite effective and can achieve state-of-the-art or competitive results\non seven video benchmarks, providing a novel perspective for more universal\nvideo understanding. Code is available at https://github.com/wangjk666/OmniVid.\n","authors":["Junke Wang","Dongdong Chen","Chong Luo","Bo He","Lu Yuan","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.17935v1.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17934v1","updated":"2024-03-26T17:59:23Z","published":"2024-03-26T17:59:23Z","title":"AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation","summary":"  Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh\nrecovery) involves the human body, hand, and expression estimation. Most\nexisting methods have tackled this task in a two-stage manner, first detecting\nthe human body part with an off-the-shelf detection model and inferring the\ndifferent human body parts individually. Despite the impressive results\nachieved, these methods suffer from 1) loss of valuable contextual information\nvia cropping, 2) introducing distractions, and 3) lacking inter-association\namong different persons and body parts, inevitably causing performance\ndegradation, especially for crowded scenes. To address these issues, we\nintroduce a novel all-in-one-stage framework, AiOS, for multiple expressive\nhuman pose and shape recovery without an additional human detection step.\nSpecifically, our method is built upon DETR, which treats multi-person\nwhole-body mesh recovery task as a progressive set prediction problem with\nvarious sequential detection. We devise the decoder tokens and extend them to\nour task. Specifically, we first employ a human token to probe a human location\nin the image and encode global features for each instance, which provides a\ncoarse location for the later transformer block. Then, we introduce a\njoint-related token to probe the human joint in the image and encoder a\nfine-grained local feature, which collaborates with the global feature to\nregress the whole-body mesh. This straightforward but effective model\noutperforms previous state-of-the-art methods by a 9% reduction in NMVE on\nAGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a\n3% reduction in PVE on EgoBody.\n","authors":["Qingping Sun","Yanjun Wang","Ailing Zeng","Wanqi Yin","Chen Wei","Wenjia Wang","Haiyi Mei","Chi Sing Leung","Ziwei Liu","Lei Yang","Zhongang Cai"],"pdf_url":"https://arxiv.org/pdf/2403.17934v1.pdf","comment":"Homepage: https://ttxskk.github.io/AiOS/"},{"id":"http://arxiv.org/abs/2403.17933v1","updated":"2024-03-26T17:58:29Z","published":"2024-03-26T17:58:29Z","title":"SLEDGE: Synthesizing Simulation Environments for Driving Agents with\n  Generative Models","summary":"  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.\n","authors":["Kashyap Chitta","Daniel Dauner","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2403.17933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17931v1","updated":"2024-03-26T17:58:22Z","published":"2024-03-26T17:58:22Z","title":"Track Everything Everywhere Fast and Robustly","summary":"  We propose a novel test-time optimization approach for efficiently and\nrobustly tracking any pixel at any time in a video. The latest state-of-the-art\noptimization-based tracking technique, OmniMotion, requires a prohibitively\nlong optimization time, rendering it impractical for downstream applications.\nOmniMotion is sensitive to the choice of random seeds, leading to unstable\nconvergence. To improve efficiency and robustness, we introduce a novel\ninvertible deformation network, CaDeX++, which factorizes the function\nrepresentation into a local spatial-temporal feature grid and enhances the\nexpressivity of the coupling blocks with non-linear functions. While CaDeX++\nincorporates a stronger geometric bias within its architectural design, it also\ntakes advantage of the inductive bias provided by the vision foundation models.\nOur system utilizes monocular depth estimation to represent scene geometry and\nenhances the objective by incorporating DINOv2 long-term semantics to regulate\nthe optimization process. Our experiments demonstrate a substantial improvement\nin training speed (more than \\textbf{10 times} faster), robustness, and\naccuracy in tracking over the SoTA optimization-based method OmniMotion.\n","authors":["Yunzhou Song","Jiahui Lei","Ziyun Wang","Lingjie Liu","Kostas Daniilidis"],"pdf_url":"https://arxiv.org/pdf/2403.17931v1.pdf","comment":"project page: https://timsong412.github.io/FastOmniTrack/"},{"id":"http://arxiv.org/abs/2403.17929v1","updated":"2024-03-26T17:58:07Z","published":"2024-03-26T17:58:07Z","title":"Towards Explaining Hypercomplex Neural Networks","summary":"  Hypercomplex neural networks are gaining increasing interest in the deep\nlearning community. The attention directed towards hypercomplex models\noriginates from several aspects, spanning from purely theoretical and\nmathematical characteristics to the practical advantage of lightweight models\nover conventional networks, and their unique properties to capture both global\nand local relations. In particular, a branch of these architectures,\nparameterized hypercomplex neural networks (PHNNs), has also gained popularity\ndue to their versatility across a multitude of application domains.\nNonetheless, only few attempts have been made to explain or interpret their\nintricacies. In this paper, we propose inherently interpretable PHNNs and\nquaternion-like networks, thus without the need for any post-hoc method. To\nachieve this, we define a type of cosine-similarity transform within the\nparameterized hypercomplex domain. This PHB-cos transform induces weight\nalignment with relevant input features and allows to reduce the model into a\nsingle linear transform, rendering it directly interpretable. In this work, we\nstart to draw insights into how this unique branch of neural models operates.\nWe observe that hypercomplex networks exhibit a tendency to concentrate on the\nshape around the main object of interest, in addition to the shape of the\nobject itself. We provide a thorough analysis, studying single neurons of\ndifferent layers and comparing them against how real-valued networks learn. The\ncode of the paper is available at https://github.com/ispamm/HxAI.\n","authors":["Eleonora Lopez","Eleonora Grassucci","Debora Capriotti","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2403.17929v1.pdf","comment":"The paper has been accepted at IEEE WCCI 2024"},{"id":"http://arxiv.org/abs/2403.17926v1","updated":"2024-03-26T17:57:20Z","published":"2024-03-26T17:57:20Z","title":"FastCAR: Fast Classification And Regression Multi-Task Learning via Task\n  Consolidation for Modelling a Continuous Property Variable of Object Classes","summary":"  FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL)\nfor a classification and a regression task, despite task heterogeneity with\nonly subtle correlation. It addresses object classification and continuous\nproperty variable regression, a crucial use case in science and engineering.\nFastCAR involves a labeling transformation approach that can be used with a\nsingle-task regression network architecture. FastCAR outperforms traditional\nMTL model families, parametrized in the landscape of architecture and loss\nweighting schemes, when learning of both tasks are collectively considered\n(classification accuracy of 99.54%, regression mean absolute percentage error\nof 2.3%). The experiments performed used an Advanced Steel Property dataset\ncontributed by us. The dataset comprises 4536 images of 224x224 pixels,\nannotated with object classes and hardness properties that take continuous\nvalues. With the labeling transformation and single-task regression network\narchitecture, FastCAR achieves reduced latency and time efficiency.\n","authors":["Anoop Kini","Andreas Jansche","Timo Bernthaler","Gerhard Schneider"],"pdf_url":"https://arxiv.org/pdf/2403.17926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17924v1","updated":"2024-03-26T17:57:05Z","published":"2024-03-26T17:57:05Z","title":"AID: Attention Interpolation of Text-to-Image Diffusion","summary":"  Conditional diffusion models can create unseen images in various settings,\naiding image interpolation. Interpolation in latent spaces is well-studied, but\ninterpolation with specific conditions like text or poses is less understood.\nSimple approaches, such as linear interpolation in the space of conditions,\noften result in images that lack consistency, smoothness, and fidelity. To that\nend, we introduce a novel training-free technique named Attention Interpolation\nvia Diffusion (AID). Our key contributions include 1) proposing an inner/outer\ninterpolated attention layer; 2) fusing the interpolated attention with\nself-attention to boost fidelity; and 3) applying beta distribution to\nselection to increase smoothness. We also present a variant, Prompt-guided\nAttention Interpolation via Diffusion (PAID), that considers interpolation as a\ncondition-dependent generative process. This method enables the creation of new\nimages with greater consistency, smoothness, and efficiency, and offers control\nover the exact path of interpolation. Our approach demonstrates effectiveness\nfor conceptual and spatial interpolation. Code and demo are available at\nhttps://github.com/QY-H00/attention-interpolation-diffusion.\n","authors":["Qiyuan He","Jinghao Wang","Ziwei Liu","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17920v1","updated":"2024-03-26T17:55:11Z","published":"2024-03-26T17:55:11Z","title":"TC4D: Trajectory-Conditioned Text-to-4D Generation","summary":"  Recent techniques for text-to-4D generation synthesize dynamic 3D scenes\nusing supervision from pre-trained text-to-video models. However, existing\nrepresentations for motion, such as deformation models or time-dependent neural\nrepresentations, are limited in the amount of motion they can generate-they\ncannot synthesize motion extending far beyond the bounding box used for volume\nrendering. The lack of a more flexible motion model contributes to the gap in\nrealism between 4D generation methods and recent, near-photorealistic video\ngeneration models. Here, we propose TC4D: trajectory-conditioned text-to-4D\ngeneration, which factors motion into global and local components. We represent\nthe global motion of a scene's bounding box using rigid transformation along a\ntrajectory parameterized by a spline. We learn local deformations that conform\nto the global trajectory using supervision from a text-to-video model. Our\napproach enables the synthesis of scenes animated along arbitrary trajectories,\ncompositional scene generation, and significant improvements to the realism and\namount of generated motion, which we evaluate qualitatively and through a user\nstudy. Video results can be viewed on our website:\nhttps://sherwinbahmani.github.io/tc4d.\n","authors":["Sherwin Bahmani","Xian Liu","Yifan Wang","Ivan Skorokhodov","Victor Rong","Ziwei Liu","Xihui Liu","Jeong Joon Park","Sergey Tulyakov","Gordon Wetzstein","Andrea Tagliasacchi","David B. Lindell"],"pdf_url":"https://arxiv.org/pdf/2403.17920v1.pdf","comment":"Project Page: https://sherwinbahmani.github.io/tc4d"},{"id":"http://arxiv.org/abs/2403.17916v1","updated":"2024-03-26T17:53:27Z","published":"2024-03-26T17:53:27Z","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","summary":"  The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies, we demonstrate the effectiveness of our method in cooperative\nperception, tracking, and motion prediction tasks. In particular, CMP reduces\nthe average prediction error by 17.2\\% with fewer missing detections compared\nwith the no cooperation setting. Our work marks a significant step forward in\nthe cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios.\n","authors":["Zhuoyuan Wu","Yuping Wang","Hengbo Ma","Zhaowei Li","Hang Qiu","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2403.17916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17915v1","updated":"2024-03-26T17:52:23Z","published":"2024-03-26T17:52:23Z","title":"Leveraging Near-Field Lighting for Monocular Depth Estimation from\n  Endoscopy Videos","summary":"  Monocular depth estimation in endoscopy videos can enable assistive and\nrobotic surgery to obtain better coverage of the organ and detection of various\nhealth issues. Despite promising progress on mainstream, natural image depth\nestimation, techniques perform poorly on endoscopy images due to a lack of\nstrong geometric features and challenging illumination effects. In this paper,\nwe utilize the photometric cues, i.e., the light emitted from an endoscope and\nreflected by the surface, to improve monocular depth estimation. We first\ncreate two novel loss functions with supervised and self-supervised variants\nthat utilize a per-pixel shading representation. We then propose a novel depth\nrefinement network (PPSNet) that leverages the same per-pixel shading\nrepresentation. Finally, we introduce teacher-student transfer learning to\nproduce better depth maps from both synthetic data with supervision and\nclinical data with self-supervision. We achieve state-of-the-art results on the\nC3VD dataset while estimating high-quality depth maps from clinical data. Our\ncode, pre-trained models, and supplementary materials can be found on our\nproject page: https://ppsnet.github.io/\n","authors":["Akshay Paruchuri","Samuel Ehrenstein","Shuxian Wang","Inbar Fried","Stephen M. Pizer","Marc Niethammer","Roni Sengupta"],"pdf_url":"https://arxiv.org/pdf/2403.17915v1.pdf","comment":"26 pages, 7 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.17909v1","updated":"2024-03-26T17:46:25Z","published":"2024-03-26T17:46:25Z","title":"ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing\n  Change Detection","summary":"  Deep learning has shown remarkable success in remote sensing change detection\n(CD), aiming to identify semantic change regions between co-registered\nsatellite image pairs acquired at distinct time stamps. However, existing\nconvolutional neural network and transformer-based frameworks often struggle to\naccurately segment semantic change regions. Moreover, transformers-based\nmethods with standard self-attention suffer from quadratic computational\ncomplexity with respect to the image resolution, making them less practical for\nCD tasks with limited training data. To address these issues, we propose an\nefficient change detection framework, ELGC-Net, which leverages rich contextual\ninformation to precisely estimate change regions while reducing the model size.\nOur ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The\nfocus of our design is the introduction of an Efficient Local-Global Context\nAggregator module within the encoder, capturing enhanced global context and\nlocal spatial information through a novel pooled-transpose (PT) attention and\ndepthwise convolution, respectively. The PT attention employs pooling\noperations for robust feature extraction and minimizes computational cost with\ntransposed attention. Extensive experiments on three challenging CD datasets\ndemonstrate that ELGC-Net outperforms existing methods. Compared to the recent\ntransformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in\nintersection over union metric on the LEVIR-CD dataset, while significantly\nreducing trainable parameters. Our proposed ELGC-Net sets a new\nstate-of-the-art performance in remote sensing change detection benchmarks.\nFinally, we also introduce ELGC-Net-LW, a lighter variant with significantly\nreduced computational complexity, suitable for resource-constrained settings,\nwhile achieving comparable performance. Project url\nhttps://github.com/techmn/elgcnet.\n","authors":["Mubashir Noman","Mustansar Fiaz","Hisham Cholakkal","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.17909v1.pdf","comment":"accepted at IEEE TGRS"},{"id":"http://arxiv.org/abs/2403.17905v1","updated":"2024-03-26T17:45:06Z","published":"2024-03-26T17:45:06Z","title":"Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2","summary":"  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n","authors":["Chen Yiwei","Tang Chao","Aghabiglou Amir","Chu Chung San","Wiaux Yves"],"pdf_url":"https://arxiv.org/pdf/2403.17905v1.pdf","comment":"submitted to IEEE EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.17902v1","updated":"2024-03-26T17:43:15Z","published":"2024-03-26T17:43:15Z","title":"Serpent: Scalable and Efficient Image Restoration via Multi-scale\n  Structured State Space Models","summary":"  The landscape of computational building blocks of efficient image restoration\narchitectures is dominated by a combination of convolutional processing and\nvarious attention mechanisms. However, convolutional filters are inherently\nlocal and therefore struggle at modeling long-range dependencies in images. On\nthe other hand, attention excels at capturing global interactions between\narbitrary image regions, however at a quadratic cost in image dimension. In\nthis work, we propose Serpent, an architecture that leverages recent advances\nin state space models (SSMs) in its core computational block. SSMs, originally\nintroduced for sequence modeling, can maintain a global receptive field with a\nfavorable linear scaling in input size. Our preliminary results demonstrate\nthat Serpent can achieve reconstruction quality on par with state-of-the-art\ntechniques, while requiring orders of magnitude less compute (up to $150$ fold\nreduction in FLOPS) and a factor of up to $5\\times$ less GPU memory while\nmaintaining a compact model size.\n","authors":["Mohammad Shahab Sepehri","Zalan Fabian","Mahdi Soltanolkotabi"],"pdf_url":"https://arxiv.org/pdf/2403.17902v1.pdf","comment":"7 pages, 5 figures, preliminary workshop submission of a\n  comprehensive work to be released soon"},{"id":"http://arxiv.org/abs/2307.16897v2","updated":"2024-03-26T17:40:47Z","published":"2023-07-31T17:59:48Z","title":"DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields","summary":"  Advances in neural fields are enabling high-fidelity capture of the shape and\nappearance of dynamic 3D scenes. However, their capabilities lag behind those\noffered by conventional representations such as 2D videos because of\nalgorithmic challenges and the lack of large-scale multi-view real-world\ndatasets. We address the dataset limitation with DiVa-360, a real-world 360\ndynamic visual dataset that contains synchronized high-resolution and\nlong-duration multi-view video sequences of table-scale scenes captured using a\ncustomized low-cost system with 53 cameras. It contains 21 object-centric\nsequences categorized by different motion types, 25 intricate hand-object\ninteraction sequences, and 8 long-duration sequences for a total of 17.4 M\nimage frames. In addition, we provide foreground-background segmentation masks,\nsynchronized audio, and text descriptions. We benchmark the state-of-the-art\ndynamic neural field methods on DiVa-360 and provide insights about existing\nmethods and future challenges on long-duration neural field capture.\n","authors":["Cheng-You Lu","Peisen Zhou","Angela Xing","Chandradeep Pokhariya","Arnab Dey","Ishaan Shah","Rugved Mavidipalli","Dylan Hu","Andrew Comport","Kefan Chen","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2307.16897v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17898v1","updated":"2024-03-26T17:39:36Z","published":"2024-03-26T17:39:36Z","title":"Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D\n  Gaussians","summary":"  The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering\nfidelity and efficiency compared to NeRF-based neural scene representations.\nWhile demonstrating the potential for real-time rendering, 3D-GS encounters\nrendering bottlenecks in large scenes with complex details due to an excessive\nnumber of Gaussian primitives located within the viewing frustum. This\nlimitation is particularly noticeable in zoom-out views and can lead to\ninconsistent rendering speeds in scenes with varying details. Moreover, it\noften struggles to capture the corresponding level of details at different\nscales with its heuristic density control operation. Inspired by the\nLevel-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an\nLOD-structured 3D Gaussian approach supporting level-of-detail decomposition\nfor scene representation that contributes to the final rendering results. Our\nmodel dynamically selects the appropriate level from the set of\nmulti-resolution anchor points, ensuring consistent rendering performance with\nadaptive LOD adjustments while maintaining high-fidelity rendering results.\n","authors":["Kerui Ren","Lihan Jiang","Tao Lu","Mulin Yu","Linning Xu","Zhangkai Ni","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2403.17898v1.pdf","comment":"Project page: https://city-super.github.io/octree-gs/"},{"id":"http://arxiv.org/abs/2403.17893v1","updated":"2024-03-26T17:29:26Z","published":"2024-03-26T17:29:26Z","title":"A Survey on 3D Egocentric Human Pose Estimation","summary":"  Egocentric human pose estimation aims to estimate human body poses and\ndevelop body representations from a first-person camera perspective. It has\ngained vast popularity in recent years because of its wide range of\napplications in sectors like XR-technologies, human-computer interaction, and\nfitness tracking. However, to the best of our knowledge, there is no systematic\nliterature review based on the proposed solutions regarding egocentric 3D human\npose estimation. To that end, the aim of this survey paper is to provide an\nextensive overview of the current state of egocentric pose estimation research.\nIn this paper, we categorize and discuss the popular datasets and the different\npose estimation models, highlighting the strengths and weaknesses of different\nmethods by comparative analysis. This survey can be a valuable resource for\nboth researchers and practitioners in the field, offering insights into key\nconcepts and cutting-edge solutions in egocentric pose estimation, its\nwide-ranging applications, as well as the open problems with future scope.\n","authors":["Md Mushfiqur Azam","Kevin Desai"],"pdf_url":"https://arxiv.org/pdf/2403.17893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17888v1","updated":"2024-03-26T17:21:24Z","published":"2024-03-26T17:21:24Z","title":"2D Gaussian Splatting for Geometrically Accurate Radiance Fields","summary":"  3D Gaussian Splatting (3DGS) has recently revolutionized radiance field\nreconstruction, achieving high quality novel view synthesis and fast rendering\nspeed without baking. However, 3DGS fails to accurately represent surfaces due\nto the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian\nSplatting (2DGS), a novel approach to model and reconstruct geometrically\naccurate radiance fields from multi-view images. Our key idea is to collapse\nthe 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D\nGaussians, 2D Gaussians provide view-consistent geometry while modeling\nsurfaces intrinsically. To accurately recover thin surfaces and achieve stable\noptimization, we introduce a perspective-accurate 2D splatting process\nutilizing ray-splat intersection and rasterization. Additionally, we\nincorporate depth distortion and normal consistency terms to further enhance\nthe quality of the reconstructions. We demonstrate that our differentiable\nrenderer allows for noise-free and detailed geometry reconstruction while\nmaintaining competitive appearance quality, fast training speed, and real-time\nrendering. Our code will be made publicly available.\n","authors":["Binbin Huang","Zehao Yu","Anpei Chen","Andreas Geiger","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17888v1.pdf","comment":"12 pages, 12 figures"},{"id":"http://arxiv.org/abs/2403.17884v1","updated":"2024-03-26T17:16:04Z","published":"2024-03-26T17:16:04Z","title":"Sen2Fire: A Challenging Benchmark Dataset for Wildfire Detection using\n  Sentinel Data","summary":"  Utilizing satellite imagery for wildfire detection presents substantial\npotential for practical applications. To advance the development of machine\nlearning algorithms in this domain, our study introduces the \\textit{Sen2Fire}\ndataset--a challenging satellite remote sensing dataset tailored for wildfire\ndetection. This dataset is curated from Sentinel-2 multi-spectral data and\nSentinel-5P aerosol product, comprising a total of 2466 image patches. Each\npatch has a size of 512$\\times$512 pixels with 13 bands. Given the distinctive\nsensitivities of various wavebands to wildfire responses, our research focuses\non optimizing wildfire detection by evaluating different wavebands and\nemploying a combination of spectral indices, such as normalized burn ratio\n(NBR) and normalized difference vegetation index (NDVI). The results suggest\nthat, in contrast to using all bands for wildfire detection, selecting specific\nband combinations yields superior performance. Additionally, our study\nunderscores the positive impact of integrating Sentinel-5 aerosol data for\nwildfire detection. The code and dataset are available online\n(https://zenodo.org/records/10881058).\n","authors":["Yonghao Xu","Amanda Berg","Leif Haglund"],"pdf_url":"https://arxiv.org/pdf/2403.17884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02640v3","updated":"2024-03-26T17:14:14Z","published":"2024-03-05T04:08:19Z","title":"HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic\n  Intersection and Vehicle-Infrastructure Cooperative","summary":"  Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous\nDriving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one\nof the important research area. Due to the complexity of traffic conditions\nsuch as blind spots and occlusion, it greatly limits the perception\ncapabilities of single-view roadside sensing systems. To further enhance the\naccuracy of roadside perception and provide better information to the vehicle\nside, in this paper, we constructed holographic intersections with various\nlayouts to build a large-scale multi-sensor holographic vehicle-infrastructure\ncooperation dataset, called HoloVIC. Our dataset includes 3 different types of\nsensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the\ndifferent intersections. Each intersection is equipped with 6-18 sensors to\ncapture synchronous data. While autonomous vehicles pass through these\nintersections for collecting VIC data. HoloVIC contains in total on 100k+\nsynchronous frames from different sensors. Additionally, we annotated 3D\nbounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs\nof the same objects across different devices and consecutive frames in\nsequence. Based on HoloVIC, we formulated four tasks to facilitate the\ndevelopment of related research. We also provide benchmarks for these tasks.\n","authors":["Cong Ma","Lei Qiao","Chengkai Zhu","Kai Liu","Zelong Kong","Qing Li","Xueqi Zhou","Yuheng Kan","Wei Wu"],"pdf_url":"https://arxiv.org/pdf/2403.02640v3.pdf","comment":"Accept to CVPR 2024, Benchmark Website: https://holovic.net"},{"id":"http://arxiv.org/abs/2403.17883v1","updated":"2024-03-26T17:13:17Z","published":"2024-03-26T17:13:17Z","title":"Superior and Pragmatic Talking Face Generation with Teacher-Student\n  Framework","summary":"  Talking face generation technology creates talking videos from arbitrary\nappearance and motion signal, with the \"arbitrary\" offering ease of use but\nalso introducing challenges in practical applications. Existing methods work\nwell with standard inputs but suffer serious performance degradation with\nintricate real-world ones. Moreover, efficiency is also an important concern in\ndeployment. To comprehensively address these issues, we introduce SuperFace, a\nteacher-student framework that balances quality, robustness, cost and\neditability. We first propose a simple but effective teacher model capable of\nhandling inputs of varying qualities to generate high-quality results. Building\non this, we devise an efficient distillation strategy to acquire an\nidentity-specific student model that maintains quality with significantly\nreduced computational load. Our experiments validate that SuperFace offers a\nmore comprehensive solution than existing methods for the four mentioned\nobjectives, especially in reducing FLOPs by 99\\% with the student model.\nSuperFace can be driven by both video and audio and allows for localized facial\nattributes editing.\n","authors":["Chao Liang","Jianwen Jiang","Tianyun Zhong","Gaojie Lin","Zhengkun Rong","Jiaqi Yang","Yongming Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.17883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17881v1","updated":"2024-03-26T17:12:34Z","published":"2024-03-26T17:12:34Z","title":"Deepfake Generation and Detection: A Benchmark and Survey","summary":"  In addition to the advancements in deepfake generation, corresponding\ndetection technologies need to continuously evolve to regulate the potential\nmisuse of deepfakes, such as for privacy invasion and phishing attacks. This\nsurvey comprehensively reviews the latest developments in deepfake generation\nand detection, summarizing and analyzing the current state of the art in this\nrapidly evolving field. We first unify task definitions, comprehensively\nintroduce datasets and metrics, and discuss the development of generation and\ndetection technology frameworks. Then, we discuss the development of several\nrelated sub-fields and focus on researching four mainstream deepfake fields:\npopular face swap, face reenactment, talking face generation, and facial\nattribute editing, as well as foreign detection. Subsequently, we\ncomprehensively benchmark representative methods on popular datasets for each\nfield, fully evaluating the latest and influential works published in top\nconferences/journals. Finally, we analyze the challenges and future research\ndirections of the discussed fields. We closely follow the latest developments\nin https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.\n","authors":["Gan Pei","Jiangning Zhang","Menghan Hu","Guangtao Zhai","Chengjie Wang","Zhenyu Zhang","Jian Yang","Chunhua Shen","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2403.17881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17879v1","updated":"2024-03-26T17:11:51Z","published":"2024-03-26T17:11:51Z","title":"Low-Latency Neural Stereo Streaming","summary":"  The rise of new video modalities like virtual reality or autonomous driving\nhas increased the demand for efficient multi-view video compression methods,\nboth in terms of rate-distortion (R-D) performance and in terms of delay and\nruntime. While most recent stereo video compression approaches have shown\npromising performance, they compress left and right views sequentially, leading\nto poor parallelization and runtime performance. This work presents Low-Latency\nneural codec for Stereo video Streaming (LLSS), a novel parallel stereo video\ncoding method designed for fast and efficient low-latency stereo video\nstreaming. Instead of using a sequential cross-view motion compensation like\nexisting methods, LLSS introduces a bidirectional feature shifting module to\ndirectly exploit mutual information among views and encode them effectively\nwith a joint cross-view prior model for entropy coding. Thanks to this design,\nLLSS processes left and right views in parallel, minimizing latency; all while\nsubstantially improving R-D performance compared to both existing neural and\nconventional codecs.\n","authors":["Qiqi Hou","Farzad Farhadzadeh","Amir Said","Guillaume Sautiere","Hoang Le"],"pdf_url":"https://arxiv.org/pdf/2403.17879v1.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.17870v1","updated":"2024-03-26T16:57:55Z","published":"2024-03-26T16:57:55Z","title":"Boosting Diffusion Models with Moving Average Sampling in Frequency\n  Domain","summary":"  Diffusion models have recently brought a powerful revolution in image\ngeneration. Despite showing impressive generative capabilities, most of these\nmodels rely on the current sample to denoise the next one, possibly resulting\nin denoising instability. In this paper, we reinterpret the iterative denoising\nprocess as model optimization and leverage a moving average mechanism to\nensemble all the prior samples. Instead of simply applying moving average to\nthe denoised samples at different timesteps, we first map the denoised samples\nto data space and then perform moving average to avoid distribution shift\nacross timesteps. In view that diffusion models evolve the recovery from\nlow-frequency components to high-frequency details, we further decompose the\nsamples into different frequency components and execute moving average\nseparately on each component. We name the complete approach \"Moving Average\nSampling in Frequency domain (MASF)\". MASF could be seamlessly integrated into\nmainstream pre-trained diffusion models and sampling schedules. Extensive\nexperiments on both unconditional and conditional diffusion models demonstrate\nthat our MASF leads to superior performances compared to the baselines, with\nalmost negligible additional complexity cost.\n","authors":["Yurui Qian","Qi Cai","Yingwei Pan","Yehao Li","Ting Yao","Qibin Sun","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17870v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17869v1","updated":"2024-03-26T16:57:33Z","published":"2024-03-26T16:57:33Z","title":"To Supervise or Not to Supervise: Understanding and Addressing the Key\n  Challenges of 3D Transfer Learning","summary":"  Transfer learning has long been a key factor in the advancement of many\nfields including 2D image analysis. Unfortunately, its applicability in 3D data\nprocessing has been relatively limited. While several approaches for 3D\ntransfer learning have been proposed in recent literature, with contrastive\nlearning gaining particular prominence, most existing methods in this domain\nhave only been studied and evaluated in limited scenarios. Most importantly,\nthere is currently a lack of principled understanding of both when and why 3D\ntransfer learning methods are applicable. Remarkably, even the applicability of\nstandard supervised pre-training is poorly understood. In this work, we conduct\nthe first in-depth quantitative and qualitative investigation of supervised and\ncontrastive pre-training strategies and their utility in downstream 3D tasks.\nWe demonstrate that layer-wise analysis of learned features provides\nsignificant insight into the downstream utility of trained networks. Informed\nby this analysis, we propose a simple geometric regularization strategy, which\nimproves the transferability of supervised pre-training. Our work thus sheds\nlight onto both the specific challenges of 3D transfer learning, as well as\nstrategies to overcome them.\n","authors":["Souhail Hadgi","Lei Li","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2403.17869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2401.06003v2","updated":"2024-03-26T16:30:20Z","published":"2024-01-11T16:06:36Z","title":"TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering","summary":"  Point-based radiance field rendering has demonstrated impressive results for\nnovel view synthesis, offering a compelling blend of rendering quality and\ncomputational efficiency. However, also latest approaches in this domain are\nnot without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.\n2023] struggles when tasked with rendering highly detailed scenes, due to\nblurring and cloudy artifacts. On the other hand, ADOP [R\\\"uckert et al. 2022]\ncan accommodate crisper images, but the neural reconstruction network decreases\nperformance, it grapples with temporal instability and it is unable to\neffectively address large gaps in the point cloud.\n  In this paper, we present TRIPS (Trilinear Point Splatting), an approach that\ncombines ideas from both Gaussian Splatting and ADOP. The fundamental concept\nbehind our novel technique involves rasterizing points into a screen-space\nimage pyramid, with the selection of the pyramid layer determined by the\nprojected point size. This approach allows rendering arbitrarily large points\nusing a single trilinear write. A lightweight neural network is then used to\nreconstruct a hole-free image including detail beyond splat resolution.\nImportantly, our render pipeline is entirely differentiable, allowing for\nautomatic optimization of both point sizes and positions.\n  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art\nmethods in terms of rendering quality while maintaining a real-time frame rate\nof 60 frames per second on readily available hardware. This performance extends\nto challenging scenarios, such as scenes featuring intricate geometry,\nexpansive landscapes, and auto-exposed footage.\n  The project page is located at: https://lfranke.github.io/trips/\n","authors":["Linus Franke","Darius Rückert","Laura Fink","Marc Stamminger"],"pdf_url":"https://arxiv.org/pdf/2401.06003v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17839v1","updated":"2024-03-26T16:27:37Z","published":"2024-03-26T16:27:37Z","title":"ReMamber: Referring Image Segmentation with Mamba Twister","summary":"  Referring Image Segmentation (RIS) leveraging transformers has achieved great\nsuccess on the interpretation of complex visual-language tasks. However, the\nquadratic computation cost makes it resource-consuming in capturing long-range\nvisual-language dependencies. Fortunately, Mamba addresses this with efficient\nlinear complexity in processing. However, directly applying Mamba to\nmulti-modal interactions presents challenges, primarily due to inadequate\nchannel interactions for the effective fusion of multi-modal data. In this\npaper, we propose ReMamber, a novel RIS architecture that integrates the power\nof Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly\nmodels image-text interaction, and fuses textual and visual features through\nits unique channel and spatial twisting mechanism. We achieve the\nstate-of-the-art on three challenging benchmarks. Moreover, we conduct thorough\nanalyses of ReMamber and discuss other fusion designs using Mamba. These\nprovide valuable perspectives for future research.\n","authors":["Yuhuan Yang","Chaofan Ma","Jiangchao Yao","Zhun Zhong","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time- consuming. Therefore, the challenging task of\nreconstructing visually accurate HDR images from their Low Dynamic Range (LDR)\ncounterparts is gaining attention in the vision research community. A major\nchallenge in this research problem is the lack of datasets, which capture\ndiverse scene conditions (e.g., lighting, shadows, weather, locations,\nlandscapes, objects, humans, buildings) and various image features (e.g.,\ncolor, contrast, saturation, hue, luminance, brightness, radiance). To address\nthis gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset\nof photo-realistic HDR images sampled from the GTA-V video game. We perform\nthorough evaluation of the proposed dataset, which demonstrates significant\nqualitative and quantitative improvements of the state-of-the-art HDR image\nreconstruction methods. Furthermore, we demonstrate the effectiveness of the\nproposed dataset and its impact on additional computer vision tasks including\n3D human pose estimation, human body part segmentation, and holistic scene\nsegmentation. The dataset, data collection pipeline, and evaluation code are\navailable at: https://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2403.17834v1","updated":"2024-03-26T16:19:56Z","published":"2024-03-26T16:19:56Z","title":"A foundation model utilizing chest CT volumes and radiology reports for\n  supervised-level zero-shot detection of abnormalities","summary":"  A major challenge in computational research in 3D medical imaging is the lack\nof comprehensive datasets. Addressing this issue, our study introduces CT-RATE,\nthe first 3D medical imaging dataset that pairs images with textual reports.\nCT-RATE consists of 25,692 non-contrast chest CT volumes, expanded to 50,188\nthrough various reconstructions, from 21,304 unique patients, along with\ncorresponding radiology text reports. Leveraging CT-RATE, we developed CT-CLIP,\na CT-focused contrastive language-image pre-training framework. As a versatile,\nself-supervised model, CT-CLIP is designed for broad application and does not\nrequire task-specific training. Remarkably, CT-CLIP outperforms\nstate-of-the-art, fully supervised methods in multi-abnormality detection\nacross all key metrics, thus eliminating the need for manual annotation. We\nalso demonstrate its utility in case retrieval, whether using imagery or\ntextual queries, thereby advancing knowledge dissemination. The open-source\nrelease of CT-RATE and CT-CLIP marks a significant advancement in medical AI,\nenhancing 3D imaging analysis and fostering innovation in healthcare.\n","authors":["Ibrahim Ethem Hamamci","Sezgin Er","Furkan Almas","Ayse Gulnihan Simsek","Sevval Nil Esirgun","Irem Dogan","Muhammed Furkan Dasdelen","Bastian Wittmann","Enis Simsar","Mehmet Simsar","Emine Bensu Erdemir","Abdullah Alanbay","Anjany Sekuboyina","Berkan Lafci","Mehmet K. Ozdemir","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2403.17834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.13969v3","updated":"2024-03-26T16:13:26Z","published":"2021-08-31T16:51:00Z","title":"Semi-Supervised Crowd Counting from Unlabeled Data","summary":"  Automatic Crowd behavior analysis can be applied to effectively help the\ndaily transportation statistics and planning, which helps the smart city\nconstruction. As one of the most important keys, crowd counting has drawn\nincreasing attention. Recent works achieved promising performance but relied on\nthe supervised paradigm with expensive crowd annotations. To alleviate the\nannotation cost in real-world transportation scenarios, in this work we\nproposed a semi-supervised learning framework $S^{4}\\textit{Crowd}$, which can\nleverage both unlabeled/labeled data for robust crowd counting. In the\nunsupervised pathway, two \\textit{self-supervised losses} were proposed to\nsimulate the crowd variations such as scale, illumination, based on which\nsupervised information pseudo labels were generated and gradually refined. We\nalso proposed a crowd-driven recurrent unit \\textit{Gated-Crowd-Recurrent-Unit\n(GCRU)}, which can preserve discriminant crowd information by extracting\nsecond-order statistics, yielding pseudo labels with improved quality. A joint\nloss including both unsupervised/supervised information was proposed, and a\ndynamic weighting strategy was employed to balance the importance of the\nunsupervised loss and supervised loss at different training stages. We\nconducted extensive experiments on four popular crowd counting datasets in\nsemi-supervised settings. Experimental results supported the effectiveness of\neach proposed component in our $S^{4}$Crowd framework. Our method achieved\ncompetitive performance in semi-supervised learning approaches on these crowd\ncounting datasets.\n","authors":["Haoran Duan","Fan Wan","Rui Sun","Zeyu Wang","Varun Ojha","Yu Guan","Hubert P. H. Shum","Bingzhang Hu","Yang Long"],"pdf_url":"https://arxiv.org/pdf/2108.13969v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17830v1","updated":"2024-03-26T16:10:21Z","published":"2024-03-26T16:10:21Z","title":"Assessment of Multimodal Large Language Models in Alignment with Human\n  Values","summary":"  Large Language Models (LLMs) aim to serve as versatile assistants aligned\nwith human values, as defined by the principles of being helpful, honest, and\nharmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs),\ndespite their commendable performance in perception and reasoning tasks, their\nalignment with human values remains largely unexplored, given the complexity of\ndefining hhh dimensions in the visual world and the difficulty in collecting\nrelevant data that accurately mirrors real-world situations. To address this\ngap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for\nassessing alignment with human expectations. Ch3Ef dataset contains 1002\nhuman-annotated data samples, covering 12 domains and 46 tasks based on the hhh\nprinciple. We also present a unified evaluation strategy supporting assessment\nacross various scenarios and different perspectives. Based on the evaluation\nresults, we summarize over 10 key findings that deepen the understanding of\nMLLM capabilities, limitations, and the dynamic relationships between\nevaluation levels, guiding future advancements in the field.\n","authors":["Zhelun Shi","Zhipin Wang","Hongxing Fan","Zaibin Zhang","Lijun Li","Yongting Zhang","Zhenfei Yin","Lu Sheng","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2403.17830v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2311.02692"},{"id":"http://arxiv.org/abs/2403.17827v1","updated":"2024-03-26T16:06:42Z","published":"2024-03-26T16:06:42Z","title":"DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from\n  Textual Descriptions","summary":"  Generating natural hand-object interactions in 3D is challenging as the\nresulting hand and object motions are expected to be physically plausible and\nsemantically meaningful. Furthermore, generalization to unseen objects is\nhindered by the limited scale of available hand-object interaction datasets. We\npropose DiffH2O, a novel method to synthesize realistic, one or two-handed\nobject interactions from provided text prompts and geometry of the object. The\nmethod introduces three techniques that enable effective learning from limited\ndata. First, we decompose the task into a grasping stage and a text-based\ninteraction stage and use separate diffusion models for each. In the grasping\nstage, the model only generates hand motions, whereas in the interaction phase\nboth hand and object poses are synthesized. Second, we propose a compact\nrepresentation that tightly couples hand and object poses. Third, we propose\ntwo different guidance schemes to allow more control of the generated motions:\ngrasp guidance and detailed textual guidance. Grasp guidance takes a single\ntarget grasping pose and guides the diffusion model to reach this grasp at the\nend of the grasping stage, which provides control over the grasping pose. Given\na grasping motion from this stage, multiple different actions can be prompted\nin the interaction phase. For textual guidance, we contribute comprehensive\ntext descriptions to the GRAB dataset and show that they enable our method to\nhave more fine-grained control over hand-object interactions. Our quantitative\nand qualitative evaluation demonstrates that the proposed method outperforms\nbaseline methods and leads to natural hand-object motions. Moreover, we\ndemonstrate the practicality of our framework by utilizing a hand pose estimate\nfrom an off-the-shelf pose estimator for guidance, and then sampling multiple\ndifferent actions in the interaction stage.\n","authors":["Sammy Christen","Shreyas Hampali","Fadime Sener","Edoardo Remelli","Tomas Hodan","Eric Sauser","Shugao Ma","Bugra Tekin"],"pdf_url":"https://arxiv.org/pdf/2403.17827v1.pdf","comment":"Project Page: https://diffh2o.github.io/"},{"id":"http://arxiv.org/abs/2403.17823v1","updated":"2024-03-26T16:04:19Z","published":"2024-03-26T16:04:19Z","title":"Efficient Image Pre-Training with Siamese Cropped Masked Autoencoders","summary":"  Self-supervised pre-training of image encoders is omnipresent in the\nliterature, particularly following the introduction of Masked autoencoders\n(MAE). Current efforts attempt to learn object-centric representations from\nmotion in videos. In particular, SiamMAE recently introduced a Siamese network,\ntraining a shared-weight encoder from two frames of a video with a high\nasymmetric masking ratio (95%). In this work, we propose CropMAE, an\nalternative approach to the Siamese pre-training introduced by SiamMAE. Our\nmethod specifically differs by exclusively considering pairs of cropped images\nsourced from the same image but cropped differently, deviating from the\nconventional pairs of frames extracted from a video. CropMAE therefore\nalleviates the need for video datasets, while maintaining competitive\nperformances and drastically reducing pre-training time. Furthermore, we\ndemonstrate that CropMAE learns similar object-centric representations without\nexplicit motion, showing that current self-supervised learning methods do not\nlearn objects from motion, but rather thanks to the Siamese architecture.\nFinally, CropMAE achieves the highest masking ratio to date (98.5%), enabling\nthe reconstruction of images using only two visible patches. Our code is\navailable at https://github.com/alexandre-eymael/CropMAE.\n","authors":["Alexandre Eymaël","Renaud Vandeghen","Anthony Cioppa","Silvio Giancola","Bernard Ghanem","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2403.17823v1.pdf","comment":"19 pages, 6 figures, 3 tables, 1 page of supplementary material"},{"id":"http://arxiv.org/abs/2403.17822v1","updated":"2024-03-26T16:00:31Z","published":"2024-03-26T16:00:31Z","title":"DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing","summary":"  3D Gaussian splatting, a novel differentiable rendering technique, has\nachieved state-of-the-art novel view synthesis results with high rendering\nspeeds and relatively low training times. However, its performance on scenes\ncommonly seen in indoor datasets is poor due to the lack of geometric\nconstraints during optimization. We extend 3D Gaussian splatting with depth and\nnormal cues to tackle challenging indoor datasets and showcase techniques for\nefficient mesh extraction, an important downstream application. Specifically,\nwe regularize the optimization procedure with depth information, enforce local\nsmoothness of nearby Gaussians, and use the geometry of the 3D Gaussians\nsupervised by normal cues to achieve better alignment with the true scene\ngeometry. We improve depth estimation and novel view synthesis results over\nbaselines and show how this simple yet effective regularization technique can\nbe used to directly extract meshes from the Gaussian representation yielding\nmore physically accurate reconstructions on indoor scenes. Our code will be\nreleased in https://github.com/maturk/dn-splatter.\n","authors":["Matias Turkulainen","Xuqian Ren","Iaroslav Melekhov","Otto Seiskari","Esa Rahtu","Juho Kannala"],"pdf_url":"https://arxiv.org/pdf/2403.17822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15964v2","updated":"2024-03-26T15:58:26Z","published":"2023-11-27T16:07:37Z","title":"Efficient Pre-training for Localized Instruction Generation of Videos","summary":"  Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.\n","authors":["Anil Batra","Davide Moltisanti","Laura Sevilla-Lara","Marcus Rohrbach","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2311.15964v2.pdf","comment":"This version has some missing experiments and elaborative technical\n  details"},{"id":"http://arxiv.org/abs/2403.17808v1","updated":"2024-03-26T15:45:29Z","published":"2024-03-26T15:45:29Z","title":"Annotated Biomedical Video Generation using Denoising Diffusion\n  Probabilistic Models and Flow Fields","summary":"  The segmentation and tracking of living cells play a vital role within the\nbiomedical domain, particularly in cancer research, drug development, and\ndevelopmental biology. These are usually tedious and time-consuming tasks that\nare traditionally done by biomedical experts. Recently, to automatize these\nprocesses, deep learning based segmentation and tracking methods have been\nproposed. These methods require large-scale datasets and their full potential\nis constrained by the scarcity of annotated data in the biomedical imaging\ndomain. To address this limitation, we propose Biomedical Video Diffusion Model\n(BVDM), capable of generating realistic-looking synthetic microscopy videos.\nTrained only on a single real video, BVDM can generate videos of arbitrary\nlength with pixel-level annotations that can be used for training data-hungry\nmodels. It is composed of a denoising diffusion probabilistic model (DDPM)\ngenerating high-fidelity synthetic cell microscopy images and a flow prediction\nmodel (FPM) predicting the non-rigid transformation between consecutive video\nframes. During inference, initially, the DDPM imposes realistic cell textures\non synthetic cell masks which are generated based on real data statistics. The\nflow prediction model predicts the flow field between consecutive masks and\napplies that to the DDPM output from the previous time frame to create the next\none while keeping temporal consistency. BVDM outperforms state-of-the-art\nsynthetic live cell microscopy video generation models. Furthermore, we\ndemonstrate that a sufficiently large synthetic dataset enhances the\nperformance of cell segmentation and tracking models compared to using a\nlimited amount of available real data.\n","authors":["Rüveyda Yilmaz","Dennis Eschweiler","Johannes Stegmaier"],"pdf_url":"https://arxiv.org/pdf/2403.17808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17804v1","updated":"2024-03-26T15:42:01Z","published":"2024-03-26T15:42:01Z","title":"Improving Text-to-Image Consistency via Automatic Prompt Optimization","summary":"  Impressive advances in text-to-image (T2I) generative models have yielded a\nplethora of high performing models which are able to generate aesthetically\nappealing, photorealistic images. Despite the progress, these models still\nstruggle to produce images that are consistent with the input prompt,\noftentimes failing to capture object quantities, relations and attributes\nproperly. Existing solutions to improve prompt-image consistency suffer from\nthe following challenges: (1) they oftentimes require model fine-tuning, (2)\nthey only focus on nearby prompt samples, and (3) they are affected by\nunfavorable trade-offs among image quality, representation diversity, and\nprompt-image consistency. In this paper, we address these challenges and\nintroduce a T2I optimization-by-prompting framework, OPT2I, which leverages a\nlarge language model (LLM) to improve prompt-image consistency in T2I models.\nOur framework starts from a user prompt and iteratively generates revised\nprompts with the goal of maximizing a consistency score. Our extensive\nvalidation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost\nthe initial consistency score by up to 24.9% in terms of DSG score while\npreserving the FID and increasing the recall between generated and real data.\nOur work paves the way toward building more reliable and robust T2I systems by\nharnessing the power of LLMs.\n","authors":["Oscar Mañas","Pietro Astolfi","Melissa Hall","Candace Ross","Jack Urbanek","Adina Williams","Aishwarya Agrawal","Adriana Romero-Soriano","Michal Drozdzal"],"pdf_url":"https://arxiv.org/pdf/2403.17804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00454v3","updated":"2024-03-26T15:41:17Z","published":"2023-09-30T18:13:41Z","title":"SimLVSeg: Simplifying Left Ventricular Segmentation in 2D+Time\n  Echocardiograms with Self- and Weakly-Supervised Learning","summary":"  Echocardiography has become an indispensable clinical imaging modality for\ngeneral heart health assessment. From calculating biomarkers such as ejection\nfraction to the probability of a patient's heart failure, accurate segmentation\nof the heart structures allows doctors to assess the heart's condition and\ndevise treatments with greater precision and accuracy. However, achieving\naccurate and reliable left ventricle segmentation is time-consuming and\nchallenging due to different reasons. Hence, clinicians often rely on\nsegmenting the left ventricular (LV) in two specific echocardiogram frames to\nmake a diagnosis. This limited coverage in manual LV segmentation poses a\nchallenge for developing automatic LV segmentation with high temporal\nconsistency, as the resulting dataset is typically annotated sparsely. In\nresponse to this challenge, this work introduces SimLVSeg, a novel paradigm\nthat enables video-based networks for consistent LV segmentation from sparsely\nannotated echocardiogram videos. SimLVSeg consists of self-supervised\npre-training with temporal masking, followed by weakly supervised learning\ntailored for LV segmentation from sparse annotations. We demonstrate how\nSimLVSeg outperforms the state-of-the-art solutions by achieving a 93.32%\n(95%CI 93.21-93.43%) dice score on the largest 2D+time echocardiography dataset\n(EchoNet-Dynamic) while being more efficient. SimLVSeg is compatible with two\ntypes of video segmentation networks: 2D super image and 3D segmentation. To\nshow the effectiveness of our approach, we provide extensive ablation studies,\nincluding pre-training settings and various deep learning backbones. We further\nconduct an out-of-distribution test to showcase SimLVSeg's generalizability on\nunseen distribution (CAMUS dataset). The code is publicly available at\nhttps://github.com/fadamsyah/SimLVSeg.\n","authors":["Fadillah Maani","Asim Ukaye","Nada Saadi","Numan Saeed","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2310.00454v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08639v2","updated":"2024-03-26T15:40:20Z","published":"2024-03-13T15:51:23Z","title":"HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map\n  Construction","summary":"  Vectorized High-Definition (HD) map construction requires predictions of the\ncategory and point coordinates of map elements (e.g. road boundary, lane\ndivider, pedestrian crossing, etc.). State-of-the-art methods are mainly based\non point-level representation learning for regressing accurate point\ncoordinates. However, this pipeline has limitations in obtaining element-level\ninformation and handling element-level failures, e.g. erroneous element shape\nor entanglement between elements. To tackle the above issues, we propose a\nsimple yet effective HybrId framework named HIMap to sufficiently learn and\ninteract both point-level and element-level information. Concretely, we\nintroduce a hybrid representation called HIQuery to represent all map elements,\nand propose a point-element interactor to interactively extract and encode the\nhybrid information of elements, e.g. point position and element shape, into the\nHIQuery. Additionally, we present a point-element consistency constraint to\nenhance the consistency between the point-level and element-level information.\nFinally, the output point-element integrated HIQuery can be directly converted\ninto map elements' class, point coordinates, and mask. We conduct extensive\nexperiments and consistently outperform previous methods on both nuScenes and\nArgoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes\ndataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.\n","authors":["Yi Zhou","Hui Zhang","Jiaqian Yu","Yifan Yang","Sangil Jung","Seung-In Park","ByungIn Yoo"],"pdf_url":"https://arxiv.org/pdf/2403.08639v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17801v1","updated":"2024-03-26T15:40:05Z","published":"2024-03-26T15:40:05Z","title":"Towards 3D Vision with Low-Cost Single-Photon Cameras","summary":"  We present a method for reconstructing 3D shape of arbitrary Lambertian\nobjects based on measurements by miniature, energy-efficient, low-cost\nsingle-photon cameras. These cameras, operating as time resolved image sensors,\nilluminate the scene with a very fast pulse of diffuse light and record the\nshape of that pulse as it returns back from the scene at a high temporal\nresolution. We propose to model this image formation process, account for its\nnon-idealities, and adapt neural rendering to reconstruct 3D geometry from a\nset of spatially distributed sensors with known poses. We show that our\napproach can successfully recover complex 3D shapes from simulated data. We\nfurther demonstrate 3D object reconstruction from real-world captures,\nutilizing measurements from a commodity proximity sensor. Our work draws a\nconnection between image-based modeling and active range scanning and is a step\ntowards 3D vision with single-photon cameras.\n","authors":["Fangzhou Mu","Carter Sifferman","Sacha Jungerman","Yiquan Li","Mark Han","Michael Gleicher","Mohit Gupta","Yin Li"],"pdf_url":"https://arxiv.org/pdf/2403.17801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17787v1","updated":"2024-03-26T15:20:49Z","published":"2024-03-26T15:20:49Z","title":"Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models\n  Versus Fine-Tuned Vision Transformers in Image-Based Security Applications","summary":"  The success of Large Language Models (LLMs) has led to a parallel rise in the\ndevelopment of Large Multimodal Models (LMMs), such as Gemini-pro, which have\nbegun to transform a variety of applications. These sophisticated multimodal\nmodels are designed to interpret and analyze complex data, integrating both\ntextual and visual information on a scale previously unattainable, opening new\navenues for a range of applications. This paper investigates the applicability\nand effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision\nTransformer (ViT) models in addressing critical security challenges. We focus\non two distinct tasks: a visually evident task of detecting simple triggers,\nsuch as small squares in images, indicative of potential backdoors, and a\nnon-visually evident task of malware classification through visual\nrepresentations. Our results highlight a significant divergence in performance,\nwith Gemini-pro falling short in accuracy and reliability when compared to\nfine-tuned ViT models. The ViT models, on the other hand, demonstrate\nexceptional accuracy, achieving near-perfect performance on both tasks. This\nstudy not only showcases the strengths and limitations of prompt-engineered\nLMMs in cybersecurity applications but also emphasizes the unmatched efficacy\nof fine-tuned ViT models for precise and dependable tasks.\n","authors":["Fouad Trad","Ali Chehab"],"pdf_url":"https://arxiv.org/pdf/2403.17787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17782v1","updated":"2024-03-26T15:15:15Z","published":"2024-03-26T15:15:15Z","title":"GenesisTex: Adapting Image Denoising Diffusion to Texture Space","summary":"  We present GenesisTex, a novel method for synthesizing textures for 3D\ngeometries from text descriptions. GenesisTex adapts the pretrained image\ndiffusion model to texture space by texture space sampling. Specifically, we\nmaintain a latent texture map for each viewpoint, which is updated with\npredicted noise on the rendering of the corresponding viewpoint. The sampled\nlatent texture maps are then decoded into a final texture map. During the\nsampling process, we focus on both global and local consistency across multiple\nviewpoints: global consistency is achieved through the integration of style\nconsistency mechanisms within the noise prediction network, and low-level\nconsistency is achieved by dynamically aligning latent textures. Finally, we\napply reference-based inpainting and img2img on denser views for texture\nrefinement. Our approach overcomes the limitations of slow optimization in\ndistillation-based methods and instability in inpainting-based methods.\nExperiments on meshes from various sources demonstrate that our method\nsurpasses the baseline methods quantitatively and qualitatively.\n","authors":["Chenjian Gao","Boyan Jiang","Xinghui Li","Yingpeng Zhang","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17782v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.16167v2","updated":"2024-03-26T15:14:25Z","published":"2024-03-24T14:21:06Z","title":"Exploiting Semantic Reconstruction to Mitigate Hallucinations in\n  Vision-Language Models","summary":"  Hallucinations in vision-language models pose a significant challenge to\ntheir reliability, particularly in the generation of long captions. Current\nmethods fall short of accurately identifying and mitigating these\nhallucinations. To address this issue, we introduce ESREAL, a novel\nunsupervised learning framework designed to suppress the generation of\nhallucinations through accurate localization and penalization of hallucinated\ntokens. Initially, ESREAL creates a reconstructed image based on the generated\ncaption and aligns its corresponding regions with those of the original image.\nThis semantic reconstruction aids in identifying both the presence and type of\ntoken-level hallucinations within the generated caption. Subsequently, ESREAL\ncomputes token-level hallucination scores by assessing the semantic similarity\nof aligned regions based on the type of hallucination. Finally, ESREAL employs\na proximal policy optimization algorithm, where it selectively penalizes\nhallucinated tokens according to their token-level hallucination scores. Our\nframework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2\nby 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved\nsolely through signals derived from the image itself, without the need for any\nimage-text pairs.\n","authors":["Minchan Kim","Minyeong Kim","Junik Bae","Suhwan Choi","Sungkyung Kim","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2403.16167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12225v2","updated":"2024-03-26T15:06:00Z","published":"2024-02-19T15:33:09Z","title":"Pushing Auto-regressive Models for 3D Shape Generation at Capacity and\n  Scalability","summary":"  Auto-regressive models have achieved impressive results in 2D image\ngeneration by modeling joint distributions in grid space. In this paper, we\nextend auto-regressive models to 3D domains, and seek a stronger ability of 3D\nshape generation by improving auto-regressive models at capacity and\nscalability simultaneously. Firstly, we leverage an ensemble of publicly\navailable 3D datasets to facilitate the training of large-scale models. It\nconsists of a comprehensive collection of approximately 900,000 objects, with\nmultiple properties of meshes, points, voxels, rendered images, and text\ncaptions. This diverse labeled dataset, termed Objaverse-Mix, empowers our\nmodel to learn from a wide range of object variations. However, directly\napplying 3D auto-regression encounters critical challenges of high\ncomputational demands on volumetric grids and ambiguous auto-regressive order\nalong grid dimensions, resulting in inferior quality of 3D shapes. To this end,\nwe then present a novel framework Argus3D in terms of capacity. Concretely, our\napproach introduces discrete representation learning based on a latent vector\ninstead of volumetric grids, which not only reduces computational costs but\nalso preserves essential geometric details by learning the joint distributions\nin a more tractable order. The capacity of conditional generation can thus be\nrealized by simply concatenating various conditioning inputs to the latent\nvector, such as point clouds, categories, images, and texts. In addition,\nthanks to the simplicity of our model architecture, we naturally scale up our\napproach to a larger model with an impressive 3.6 billion parameters, further\nenhancing the quality of versatile 3D generation. Extensive experiments on four\ngeneration tasks demonstrate that Argus3D can synthesize diverse and faithful\nshapes across multiple categories, achieving remarkable performance.\n","authors":["Xuelin Qian","Yu Wang","Simian Luo","Yinda Zhang","Ying Tai","Zhenyu Zhang","Chengjie Wang","Xiangyang Xue","Bo Zhao","Tiejun Huang","Yunsheng Wu","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2402.12225v2.pdf","comment":"Project page: https://argus-3d.github.io/ . Datasets:\n  https://huggingface.co/datasets/BAAI/Objaverse-MIX. arXiv admin note:\n  substantial text overlap with arXiv:2303.14700"},{"id":"http://arxiv.org/abs/2403.17770v1","updated":"2024-03-26T14:59:11Z","published":"2024-03-26T14:59:11Z","title":"CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node\n  Segmentation","summary":"  Despite the significant success achieved by deep learning methods in medical\nimage segmentation, researchers still struggle in the computer-aided diagnosis\nof abdominal lymph nodes due to the complex abdominal environment, small and\nindistinguishable lesions, and limited annotated data. To address these\nproblems, we present a pipeline that integrates the conditional diffusion model\nfor lymph node generation and the nnU-Net model for lymph node segmentation to\nimprove the segmentation performance of abdominal lymph nodes through\nsynthesizing a diversity of realistic abdominal lymph node data. We propose\nLN-DDPM, a conditional denoising diffusion probabilistic model (DDPM) for lymph\nnode (LN) generation. LN-DDPM utilizes lymph node masks and anatomical\nstructure masks as model conditions. These conditions work in two conditioning\nmechanisms: global structure conditioning and local detail conditioning, to\ndistinguish between lymph nodes and their surroundings and better capture lymph\nnode characteristics. The obtained paired abdominal lymph node images and masks\nare used for the downstream segmentation task. Experimental results on the\nabdominal lymph node datasets demonstrate that LN-DDPM outperforms other\ngenerative methods in the abdominal lymph node image synthesis and better\nassists the downstream abdominal lymph node segmentation task.\n","authors":["Yongrui Yu","Hanyu Chen","Zitian Zhang","Qiong Xiao","Wenhui Lei","Linrui Dai","Yu Fu","Hui Tan","Guan Wang","Peng Gao","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17057v2","updated":"2024-03-26T14:54:04Z","published":"2023-11-28T18:59:52Z","title":"ReMoS: 3D Motion-Conditioned Reaction Synthesis for Two-Person\n  Interactions","summary":"  Current approaches for 3D human motion synthesis generate high-quality\nanimations of digital humans performing a wide variety of actions and gestures.\nHowever, a notable technological gap exists in addressing the complex dynamics\nof multi-human interactions within this paradigm. In this work, we present\nReMoS, a denoising diffusion-based model that synthesizes full-body reactive\nmotion of a person in a two-person interaction scenario. Assuming the motion of\none person is given, we employ a combined spatio-temporal cross-attention\nmechanism to synthesize the reactive body and hand motion of the second person,\nthereby completing the interactions between the two. We demonstrate ReMoS\nacross challenging two-person scenarios such as pair-dancing, Ninjutsu,\nkickboxing, and acrobatics, where one person's movements have complex and\ndiverse influences on the other. We also contribute the ReMoCap dataset for\ntwo-person interactions containing full-body and finger motions. We evaluate\nReMoS through multiple quantitative metrics, qualitative visualizations, and a\nuser study, and also indicate usability in interactive motion editing\napplications.\n","authors":["Anindita Ghosh","Rishabh Dabral","Vladislav Golyanik","Christian Theobalt","Philipp Slusallek"],"pdf_url":"https://arxiv.org/pdf/2311.17057v2.pdf","comment":"17 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2403.17765v1","updated":"2024-03-26T14:53:24Z","published":"2024-03-26T14:53:24Z","title":"MUTE-SLAM: Real-Time Neural SLAM with Multiple Tri-Plane Hash\n  Representations","summary":"  We introduce MUTE-SLAM, a real-time neural RGB-D SLAM system employing\nmultiple tri-plane hash-encodings for efficient scene representation. MUTE-SLAM\neffectively tracks camera positions and incrementally builds a scalable\nmulti-map representation for both small and large indoor environments. It\ndynamically allocates sub-maps for newly observed local regions, enabling\nconstraint-free mapping without prior scene information. Unlike traditional\ngrid-based methods, we use three orthogonal axis-aligned planes for\nhash-encoding scene properties, significantly reducing hash collisions and the\nnumber of trainable parameters. This hybrid approach not only speeds up\nconvergence but also enhances the fidelity of surface reconstruction.\nFurthermore, our optimization strategy concurrently optimizes all sub-maps\nintersecting with the current camera frustum, ensuring global consistency.\nExtensive testing on both real-world and synthetic datasets has shown that\nMUTE-SLAM delivers state-of-the-art surface reconstruction quality and\ncompetitive tracking performance across diverse indoor settings. The code will\nbe made public upon acceptance of the paper.\n","authors":["Yifan Yan","Ruomin He","Zhenghua Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15585v2","updated":"2024-03-26T14:51:57Z","published":"2024-03-22T19:19:51Z","title":"MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis","summary":"  Chest X-ray images are commonly used for predicting acute and chronic\ncardiopulmonary conditions, but efforts to integrate them with structured\nclinical data face challenges due to incomplete electronic health records\n(EHR). This paper introduces \\textbf{MedPromptX}, the first model to integrate\nmultimodal large language models (MLLMs), few-shot prompting (FP) and visual\ngrounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A\npre-trained MLLM is utilized to complement the missing EHR information,\nproviding a comprehensive understanding of patients' medical history.\nAdditionally, FP reduces the necessity for extensive training of MLLMs while\neffectively tackling the issue of hallucination. Nevertheless, the process of\ndetermining the optimal number of few-shot examples and selecting high-quality\ncandidates can be burdensome, yet it profoundly influences model performance.\nHence, we propose a new technique that dynamically refines few-shot data for\nreal-time adjustment to new patient scenarios. Moreover, VG aids in focusing\nthe model's attention on relevant regions of interest in X-ray images,\nenhancing the identification of abnormalities. We release MedPromptX-VQA, a new\nin-context visual question answering dataset encompassing interleaved image and\nEHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the\nSOTA performance of MedPromptX, achieving an 11% improvement in F1-score\ncompared to the baselines. Code and data are available at\nhttps://github.com/BioMedIA-MBZUAI/MedPromptX\n","authors":["Mai A. Shaaban","Adnan Khan","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.15585v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17761v1","updated":"2024-03-26T14:51:53Z","published":"2024-03-26T14:51:53Z","title":"Makeup Prior Models for 3D Facial Makeup Estimation and Applications","summary":"  In this work, we introduce two types of makeup prior models to extend\nexisting 3D face prior models: PCA-based and StyleGAN2-based priors. The\nPCA-based prior model is a linear model that is easy to construct and is\ncomputationally efficient. However, it retains only low-frequency information.\nConversely, the StyleGAN2-based model can represent high-frequency information\nwith relatively higher computational cost than the PCA-based model. Although\nthere is a trade-off between the two models, both are applicable to 3D facial\nmakeup estimation and related applications. By leveraging makeup prior models\nand designing a makeup consistency module, we effectively address the\nchallenges that previous methods faced in robustly estimating makeup,\nparticularly in the context of handling self-occluded faces. In experiments, we\ndemonstrate that our approach reduces computational costs by several orders of\nmagnitude, achieving speeds up to 180 times faster. In addition, by improving\nthe accuracy of the estimated makeup, we confirm that our methods are highly\nadvantageous for various 3D facial makeup applications such as 3D makeup face\nreconstruction, user-friendly makeup editing, makeup transfer, and\ninterpolation.\n","authors":["Xingchao Yang","Takafumi Taketomi","Yuki Endo","Yoshihiro Kanamori"],"pdf_url":"https://arxiv.org/pdf/2403.17761v1.pdf","comment":"CVPR2024. Project: https://yangxingchao.github.io/makeup-priors-page"},{"id":"http://arxiv.org/abs/2403.17757v1","updated":"2024-03-26T14:49:22Z","published":"2024-03-26T14:49:22Z","title":"Noise2Noise Denoising of CRISM Hyperspectral Data","summary":"  Hyperspectral data acquired by the Compact Reconnaissance Imaging\nSpectrometer for Mars (CRISM) have allowed for unparalleled mapping of the\nsurface mineralogy of Mars. Due to sensor degradation over time, a significant\nportion of the recently acquired data is considered unusable. Here a new\ndata-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to\nremove noise from CRISM images. Our model is self-supervised and does not\nrequire zero-noise target data, making it well suited for use in Planetary\nScience applications where high quality labelled data is scarce. We demonstrate\nits strong performance on synthetic-noise data and CRISM images, and its impact\non downstream classification performance, outperforming benchmark methods on\nmost metrics. This allows for detailed analysis for critical sites of interest\non the Martian surface, including proposed lander sites.\n","authors":["Robert Platt","Rossella Arcucci","Cédric John"],"pdf_url":"https://arxiv.org/pdf/2403.17757v1.pdf","comment":"5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024\n  ML4RS Workshop"},{"id":"http://arxiv.org/abs/2403.17755v1","updated":"2024-03-26T14:44:51Z","published":"2024-03-26T14:44:51Z","title":"DataCook: Crafting Anti-Adversarial Examples for Healthcare Data\n  Copyright Protection","summary":"  In the realm of healthcare, the challenges of copyright protection and\nunauthorized third-party misuse are increasingly significant. Traditional\nmethods for data copyright protection are applied prior to data distribution,\nimplying that models trained on these data become uncontrollable. This paper\nintroduces a novel approach, named DataCook, designed to safeguard the\ncopyright of healthcare data during the deployment phase. DataCook operates by\n\"cooking\" the raw data before distribution, enabling the development of models\nthat perform normally on this processed data. However, during the deployment\nphase, the original test data must be also \"cooked\" through DataCook to ensure\nnormal model performance. This process grants copyright holders control over\nauthorization during the deployment phase. The mechanism behind DataCook is by\ncrafting anti-adversarial examples (AntiAdv), which are designed to enhance\nmodel confidence, as opposed to standard adversarial examples (Adv) that aim to\nconfuse models. Similar to Adv, AntiAdv introduces imperceptible perturbations,\nensuring that the data processed by DataCook remains easily understandable. We\nconducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D\ndata and the high-resolution variants. The outcomes indicate that DataCook\neffectively meets its objectives, preventing models trained on AntiAdv from\nanalyzing unauthorized data effectively, without compromising the validity and\naccuracy of the data in legitimate scenarios. Code and data are available at\nhttps://github.com/MedMNIST/DataCook.\n","authors":["Sihan Shang","Jiancheng Yang","Zhenglong Sun","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2403.17755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06247v2","updated":"2024-03-26T14:42:21Z","published":"2024-03-10T16:11:17Z","title":"Text-Guided Variational Image Generation for Industrial Anomaly\n  Detection and Segmentation","summary":"  We propose a text-guided variational image generation method to address the\nchallenge of getting clean data for anomaly detection in industrial\nmanufacturing. Our method utilizes text information about the target object,\nlearned from extensive text library documents, to generate non-defective data\nimages resembling the input image. The proposed framework ensures that the\ngenerated non-defective images align with anticipated distributions derived\nfrom textual and image-based knowledge, ensuring stability and generality.\nExperimental results demonstrate the effectiveness of our approach, surpassing\nprevious methods even with limited non-defective data. Our approach is\nvalidated through generalization tests across four baseline models and three\ndistinct datasets. We present an additional analysis to enhance the\neffectiveness of anomaly detection models by utilizing the generated images.\n","authors":["Mingyu Lee","Jongwon Choi"],"pdf_url":"https://arxiv.org/pdf/2403.06247v2.pdf","comment":"18 pages, Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17749v1","updated":"2024-03-26T14:40:17Z","published":"2024-03-26T14:40:17Z","title":"Multi-Task Dense Prediction via Mixture of Low-Rank Experts","summary":"  Previous multi-task dense prediction methods based on the Mixture of Experts\n(MoE) have received great performance but they neglect the importance of\nexplicitly modeling the global relations among all tasks. In this paper, we\npresent a novel decoder-focused method for multi-task dense prediction, called\nMixture-of-Low-Rank-Experts (MLoRE). To model the global task relationships,\nMLoRE adds a generic convolution path to the original MoE structure, where each\ntask feature can go through this path for explicit parameter sharing.\nFurthermore, to control the parameters and computational cost brought by the\nincrease in the number of experts, we take inspiration from LoRA and propose to\nleverage the low-rank format of a vanilla convolution in the expert network.\nSince the low-rank experts have fewer parameters and can be dynamically\nparameterized into the generic convolution, the parameters and computational\ncost do not change much with the increase of experts. Benefiting from this\ndesign, we increase the number of experts and its reception field to enlarge\nthe representation capacity, facilitating multiple dense tasks learning in a\nunified network. Extensive experiments on the PASCAL-Context and NYUD-v2\nbenchmarks show that our MLoRE achieves superior performance compared to\nprevious state-of-the-art methods on all metrics. Our code is available at\nhttps://github.com/YuqiYang213/MLoRE.\n","authors":["Yuqi Yang","Peng-Tao Jiang","Qibin Hou","Hao Zhang","Jinwei Chen","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2403.17749v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.08270v2","updated":"2024-03-26T14:39:43Z","published":"2024-03-13T05:46:36Z","title":"Identity-aware Dual-constraint Network for Cloth-Changing Person\n  Re-identification","summary":"  Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify\nthe target person in more realistic surveillance scenarios, where pedestrians\nusually change their clothing. Despite great progress, limited cloth-changing\ntraining samples in existing CC-ReID datasets still prevent the model from\nadequately learning cloth-irrelevant features. In addition, due to the absence\nof explicit supervision to keep the model constantly focused on\ncloth-irrelevant areas, existing methods are still hampered by the disruption\nof clothing variations. To solve the above issues, we propose an Identity-aware\nDual-constraint Network (IDNet) for the CC-ReID task. Specifically, to help the\nmodel extract cloth-irrelevant clues, we propose a Clothes Diversity\nAugmentation (CDA), which generates more realistic cloth-changing samples by\nenriching the clothing color while preserving the texture. In addition, a\nMulti-scale Constraint Block (MCB) is designed, which extracts fine-grained\nidentity-related features and effectively transfers cloth-irrelevant knowledge.\nMoreover, a Counterfactual-guided Attention Module (CAM) is presented, which\nlearns cloth-irrelevant features from channel and space dimensions and utilizes\nthe counterfactual intervention for supervising the attention map to highlight\nidentity-related regions. Finally, a Semantic Alignment Constraint (SAC) is\ndesigned to facilitate high-level semantic feature interaction. Comprehensive\nexperiments on four CC-ReID datasets indicate that our method outperforms prior\nstate-of-the-art approaches.\n","authors":["Peini Guo","Mengyuan Liu","Hong Liu","Ruijia Fan","Guoquan Wang","Bin He"],"pdf_url":"https://arxiv.org/pdf/2403.08270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02129v4","updated":"2024-03-26T14:38:23Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.17734v1","updated":"2024-03-26T14:21:49Z","published":"2024-03-26T14:21:49Z","title":"Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation\n  scans using Linked Denoising Diffusion Probabilistic Models","summary":"  The rapid advancement of Artificial Intelligence (AI) in biomedical imaging\nand radiotherapy is hindered by the limited availability of large imaging data\nrepositories. With recent research and improvements in denoising diffusion\nprobabilistic models (DDPM), high quality synthetic medical scans are now\npossible. Despite this, there is currently no way of generating multiple\nrelated images, such as a corresponding ground truth which can be used to train\nmodels, so synthetic scans are often manually annotated before use. This\nresearch introduces a novel architecture that is able to generate multiple,\nrelated PET-CT-tumour mask pairs using paired networks and conditional\nencoders. Our approach includes innovative, time step-controlled mechanisms and\na `noise-seeding' strategy to improve DDPM sampling consistency. While our\nmodel requires a modified perceptual loss function to ensure accurate feature\nalignment we show generation of clearly aligned synthetic images and\nimprovement in segmentation accuracy with generated images.\n","authors":["Rowan Bradbury","Katherine A. Vallis","Bartlomiej W. Papiez"],"pdf_url":"https://arxiv.org/pdf/2403.17734v1.pdf","comment":"to be published in IEEE International Symposium on Biomedical Imaging\n  2024"},{"id":"http://arxiv.org/abs/2403.17727v1","updated":"2024-03-26T14:16:56Z","published":"2024-03-26T14:16:56Z","title":"FastPerson: Enhancing Video Learning through Effective Video\n  Summarization that Preserves Linguistic and Visual Contexts","summary":"  Quickly understanding lengthy lecture videos is essential for learners with\nlimited time and interest in various topics to improve their learning\nefficiency. To this end, video summarization has been actively researched to\nenable users to view only important scenes from a video. However, these studies\nfocus on either the visual or audio information of a video and extract\nimportant segments in the video. Therefore, there is a risk of missing\nimportant information when both the teacher's speech and visual information on\nthe blackboard or slides are important, such as in a lecture video. To tackle\nthis issue, we propose FastPerson, a video summarization approach that\nconsiders both the visual and auditory information in lecture videos.\nFastPerson creates summary videos by utilizing audio transcriptions along with\non-screen images and text, minimizing the risk of overlooking crucial\ninformation for learners. Further, it provides a feature that allows learners\nto switch between the summary and original videos for each chapter of the\nvideo, enabling them to adjust the pace of learning based on their interests\nand level of understanding. We conducted an evaluation with 40 participants to\nassess the effectiveness of our method and confirmed that it reduced viewing\ntime by 53\\% at the same level of comprehension as that when using traditional\nvideo playback methods.\n","authors":["Kazuki Kawamura","Jun Rekimoto"],"pdf_url":"https://arxiv.org/pdf/2403.17727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17464v2","updated":"2024-03-26T14:15:25Z","published":"2024-02-27T12:42:06Z","title":"Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing","summary":"  Generative 3D part assembly involves understanding part relationships and\npredicting their 6-DoF poses for assembling a realistic 3D shape. Prior work\noften focus on the geometry of individual parts, neglecting part-whole\nhierarchies of objects. Leveraging two key observations: 1) super-part poses\nprovide strong hints about part poses, and 2) predicting super-part poses is\neasier due to fewer superparts, we propose a part-whole-hierarchy message\npassing network for efficient 3D part assembly. We first introduce super-parts\nby grouping geometrically similar parts without any semantic labels. Then we\nemploy a part-whole hierarchical encoder, wherein a super-part encoder predicts\nlatent super-part poses based on input parts. Subsequently, we transform the\npoint cloud using the latent poses, feeding it to the part encoder for\naggregating super-part information and reasoning about part relationships to\npredict all part poses. In training, only ground-truth part poses are required.\nDuring inference, the predicted latent poses of super-parts enhance\ninterpretability. Experimental results on the PartNet dataset show that our\nmethod achieves state-of-the-art performance in part and connectivity accuracy\nand enables an interpretable hierarchical part assembly.\n","authors":["Bi'an Du","Xiang Gao","Wei Hu","Renjie Liao"],"pdf_url":"https://arxiv.org/pdf/2402.17464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17725v1","updated":"2024-03-26T14:13:44Z","published":"2024-03-26T14:13:44Z","title":"Deep Learning for Segmentation of Cracks in High-Resolution Images of\n  Steel Bridges","summary":"  Automating the current bridge visual inspection practices using drones and\nimage processing techniques is a prominent way to make these inspections more\neffective, robust, and less expensive. In this paper, we investigate the\ndevelopment of a novel deep-learning method for the detection of fatigue cracks\nin high-resolution images of steel bridges. First, we present a novel and\nchallenging dataset comprising of images of cracks in steel bridges. Secondly,\nwe integrate the ConvNext neural network with a previous state- of-the-art\nencoder-decoder network for crack segmentation. We study and report, the\neffects of the use of background patches on the network performance when\napplied to high-resolution images of cracks in steel bridges. Finally, we\nintroduce a loss function that allows the use of more background patches for\nthe training process, which yields a significant reduction in false positive\nrates.\n","authors":["Andrii Kompanets","Gautam Pai","Remco Duits","Davide Leonetti","Bert Snijder"],"pdf_url":"https://arxiv.org/pdf/2403.17725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17712v1","updated":"2024-03-26T13:58:47Z","published":"2024-03-26T13:58:47Z","title":"Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A\n  New Benchmark","summary":"  The widespread use of various chemical gases in industrial processes\nnecessitates effective measures to prevent their leakage during transportation\nand storage, given their high toxicity. Thermal infrared-based computer vision\ndetection techniques provide a straightforward approach to identify gas leakage\nareas. However, the development of high-quality algorithms has been challenging\ndue to the low texture in thermal images and the lack of open-source datasets.\nIn this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN),\nwhich employs an RGB-assisted two-stream network architecture to integrate\ntexture information from RGB images and gas area information from thermal\nimages. Additionally, to facilitate the research of invisible gas detection, we\nintroduce Gas-DB, an extensive open-source gas detection database including\nabout 1.3K well-annotated RGB-thermal images with eight variant collection\nscenes. Experimental results demonstrate that our method successfully leverages\nthe advantages of both modalities, achieving state-of-the-art (SOTA)\nperformance among RGB-thermal methods, surpassing single-stream SOTA models in\nterms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%,\nand 4.88%, respectively. The code and data will be made available soon.\n","authors":["Jue Wang","Yuxiang Lin","Qi Zhao","Dong Luo","Shuaibao Chen","Wei Chen","Xiaojiang Peng"],"pdf_url":"https://arxiv.org/pdf/2403.17712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15094v2","updated":"2024-03-26T13:57:26Z","published":"2023-05-24T12:22:23Z","title":"InNeRF360: Text-Guided 3D-Consistent Object Inpainting on 360-degree\n  Neural Radiance Fields","summary":"  We propose InNeRF360, an automatic system that accurately removes\ntext-specified objects from 360-degree Neural Radiance Fields (NeRF). The\nchallenge is to effectively remove objects while inpainting perceptually\nconsistent content for the missing regions, which is particularly demanding for\nexisting NeRF models due to their implicit volumetric representation. Moreover,\nunbounded scenes are more prone to floater artifacts in the inpainted region\nthan frontal-facing scenes, as the change of object appearance and background\nacross views is more sensitive to inaccurate segmentations and inconsistent\ninpainting. With a trained NeRF and a text description, our method efficiently\nremoves specified objects and inpaints visually consistent content without\nartifacts. We apply depth-space warping to enforce consistency across multiview\ntext-encoded segmentations, and then refine the inpainted NeRF model using\nperceptual priors and 3D diffusion-based geometric priors to ensure visual\nplausibility. Through extensive experiments in segmentation and inpainting on\n360-degree and frontal-facing NeRFs, we show that our approach is effective and\nenhances NeRF's editability. Project page: https://ivrl.github.io/InNeRF360.\n","authors":["Dongqing Wang","Tong Zhang","Alaa Abboud","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2305.15094v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17709v1","updated":"2024-03-26T13:56:34Z","published":"2024-03-26T13:56:34Z","title":"Groupwise Query Specialization and Quality-Aware Multi-Assignment for\n  Transformer-based Visual Relationship Detection","summary":"  Visual Relationship Detection (VRD) has seen significant advancements with\nTransformer-based architectures recently. However, we identify two key\nlimitations in a conventional label assignment for training Transformer-based\nVRD models, which is a process of mapping a ground-truth (GT) to a prediction.\nUnder the conventional assignment, an unspecialized query is trained since a\nquery is expected to detect every relation, which makes it difficult for a\nquery to specialize in specific relations. Furthermore, a query is also\ninsufficiently trained since a GT is assigned only to a single prediction,\ntherefore near-correct or even correct predictions are suppressed by being\nassigned no relation as a GT. To address these issues, we propose Groupwise\nQuery Specialization and Quality-Aware Multi-Assignment (SpeaQ). Groupwise\nQuery Specialization trains a specialized query by dividing queries and\nrelations into disjoint groups and directing a query in a specific query group\nsolely toward relations in the corresponding relation group. Quality-Aware\nMulti-Assignment further facilitates the training by assigning a GT to multiple\npredictions that are significantly close to a GT in terms of a subject, an\nobject, and the relation in between. Experimental results and analyses show\nthat SpeaQ effectively trains specialized queries, which better utilize the\ncapacity of a model, resulting in consistent performance gains with zero\nadditional inference cost across multiple VRD models and benchmarks. Code is\navailable at https://github.com/mlvlab/SpeaQ.\n","authors":["Jongha Kim","Jihwan Park","Jinyoung Park","Jinyoung Kim","Sehyung Kim","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17709v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2312.16014v2","updated":"2024-03-26T13:55:40Z","published":"2023-12-26T11:49:23Z","title":"Passive Non-Line-of-Sight Imaging with Light Transport Modulation","summary":"  Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in\nrecent years, due to its ability to image objects that are out of sight. The\nlight transport condition plays an important role in this task since changing\nthe conditions will lead to different imaging models. Existing learning-based\nNLOS methods usually train independent models for different light transport\nconditions, which is computationally inefficient and impairs the practicality\nof the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging\nmethod that effectively handles multiple light transport conditions with a\nsingle network. We achieve this by inferring a latent light transport\nrepresentation from the projection image and using this representation to\nmodulate the network that reconstructs the hidden image from the projection\nimage. We train a light transport encoder together with a vector quantizer to\nobtain the light transport representation. To further regulate this\nrepresentation, we jointly learn both the reconstruction network and the\nreprojection network during training. A set of light transport modulation\nblocks is used to modulate the two jointly trained networks in a multi-scale\nway. Extensive experiments on a large-scale passive NLOS dataset demonstrate\nthe superiority of the proposed method. The code is available at\nhttps://github.com/JerryOctopus/NLOS-LTM.\n","authors":["Jiarui Zhang","Ruixu Geng","Xiaolong Du","Yan Chen","Houqiang Li","Yang Hu"],"pdf_url":"https://arxiv.org/pdf/2312.16014v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17708v1","updated":"2024-03-26T13:54:52Z","published":"2024-03-26T13:54:52Z","title":"Panonut360: A Head and Eye Tracking Dataset for Panoramic Video","summary":"  With the rapid development and widespread application of VR/AR technology,\nmaximizing the quality of immersive panoramic video services that match users'\npersonal preferences and habits has become a long-standing challenge.\nUnderstanding the saliency region where users focus, based on data collected\nwith HMDs, can promote multimedia encoding, transmission, and quality\nassessment. At the same time, large-scale datasets are essential for\nresearchers and developers to explore short/long-term user behavior patterns\nand train AI models related to panoramic videos. However, existing panoramic\nvideo datasets often include low-frequency user head or eye movement data\nthrough short-term videos only, lacking sufficient data for analyzing users'\nField of View (FoV) and generating video saliency regions.\n  Driven by these practical factors, in this paper, we present a head and eye\ntracking dataset involving 50 users (25 males and 25 females) watching 15\npanoramic videos. The dataset provides details on the viewport and gaze\nattention locations of users. Besides, we present some statistics samples\nextracted from the dataset. For example, the deviation between head and eye\nmovements challenges the widely held assumption that gaze attention decreases\nfrom the center of the FoV following a Gaussian distribution. Our analysis\nreveals a consistent downward offset in gaze fixations relative to the FoV in\nexperimental settings involving multiple users and videos. That's why we name\nthe dataset Panonut, a saliency weighting shaped like a donut. Finally, we also\nprovide a script that generates saliency distributions based on given head or\neye coordinates and pre-generated saliency distribution map sets of each video\nfrom the collected eye tracking data.\n  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.\n","authors":["Yutong Xu","Junhao Du","Jiahe Wang","Yuwei Ning","Sihan Zhou Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2403.17708v1.pdf","comment":"7 pages,ACM MMSys'24 accepted"},{"id":"http://arxiv.org/abs/2403.17702v1","updated":"2024-03-26T13:40:52Z","published":"2024-03-26T13:40:52Z","title":"The Solution for the CVPR 2023 1st foundation model challenge-Track2","summary":"  In this paper, we propose a solution for cross-modal transportation\nretrieval. Due to the cross-domain problem of traffic images, we divide the\nproblem into two sub-tasks of pedestrian retrieval and vehicle retrieval\nthrough a simple strategy. In pedestrian retrieval tasks, we use IRRA as the\nbase model and specifically design an Attribute Classification to mine the\nknowledge implied by attribute labels. More importantly, We use the strategy of\nInclusion Relation Matching to make the image-text pairs with inclusion\nrelation have similar representation in the feature space. For the vehicle\nretrieval task, we use BLIP as the base model. Since aligning the color\nattributes of vehicles is challenging, we introduce attribute-based object\ndetection techniques to add color patch blocks to vehicle images for color data\naugmentation. This serves as strong prior information, helping the model\nperform the image-text alignment. At the same time, we incorporate labeled\nattributes into the image-text alignment loss to learn fine-grained alignment\nand prevent similar images and texts from being incorrectly separated. Our\napproach ranked first in the final B-board test with a score of 70.9.\n","authors":["Haonan Xu","Yurui Huang","Sishun Pan","Zhihao Guan","Yi Xu","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17701v1","updated":"2024-03-26T13:40:18Z","published":"2024-03-26T13:40:18Z","title":"Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical\n  Image Segmentation","summary":"  Image segmentation holds a vital position in the realms of diagnosis and\ntreatment within the medical domain. Traditional convolutional neural networks\n(CNNs) and Transformer models have made significant advancements in this realm,\nbut they still encounter challenges because of limited receptive field or high\ncomputing complexity. Recently, State Space Models (SSMs), particularly Mamba\nand its variants, have demonstrated notable performance in the field of vision.\nHowever, their feature extraction methods may not be sufficiently effective and\nretain some redundant structures, leaving room for parameter reduction.\nMotivated by previous spatial and channel attention methods, we propose Triplet\nMamba-UNet. The method leverages residual VSS Blocks to extract intensive\ncontextual features, while Triplet SSM is employed to fuse features across\nspatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,\nCVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,\ndemonstrating the superior segmentation performance of our proposed TM-UNet.\nAdditionally, compared to the previous VM-UNet, our model achieves a one-third\nreduction in parameters.\n","authors":["Hao Tang","Lianglun Cheng","Guoheng Huang","Zhengguang Tan","Junhao Lu","Kaihong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.17701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17695v1","updated":"2024-03-26T13:35:10Z","published":"2024-03-26T13:35:10Z","title":"PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition","summary":"  We present PlainMamba: a simple non-hierarchical state space model (SSM)\ndesigned for general visual recognition. The recent Mamba model has shown how\nSSMs can be highly competitive with other architectures on sequential data and\ninitial attempts have been made to apply it to images. In this paper, we\nfurther adapt the selective scanning process of Mamba to the visual domain,\nenhancing its ability to learn features from two-dimensional images by (i) a\ncontinuous 2D scanning process that improves spatial continuity by ensuring\nadjacency of tokens in the scanning sequence, and (ii) direction-aware updating\nwhich enables the model to discern the spatial relations of tokens by encoding\ndirectional information. Our architecture is designed to be easy to use and\neasy to scale, formed by stacking identical PlainMamba blocks, resulting in a\nmodel with constant width throughout all layers. The architecture is further\nsimplified by removing the need for special tokens. We evaluate PlainMamba on a\nvariety of visual recognition tasks including image classification, semantic\nsegmentation, object detection, and instance segmentation. Our method achieves\nperformance gains over previous non-hierarchical models and is competitive with\nhierarchical alternatives. For tasks requiring high-resolution inputs, in\nparticular, PlainMamba requires much less computing while maintaining high\nperformance. Code and models are available at\nhttps://github.com/ChenhongyiYang/PlainMamba\n","authors":["Chenhongyi Yang","Zehui Chen","Miguel Espinosa","Linus Ericsson","Zhenyu Wang","Jiaming Liu","Elliot J. Crowley"],"pdf_url":"https://arxiv.org/pdf/2403.17695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17694v1","updated":"2024-03-26T13:35:02Z","published":"2024-03-26T13:35:02Z","title":"AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation","summary":"  In this study, we propose AniPortrait, a novel framework for generating\nhigh-quality animation driven by audio and a reference portrait image. Our\nmethodology is divided into two stages. Initially, we extract 3D intermediate\nrepresentations from audio and project them into a sequence of 2D facial\nlandmarks. Subsequently, we employ a robust diffusion model, coupled with a\nmotion module, to convert the landmark sequence into photorealistic and\ntemporally consistent portrait animation. Experimental results demonstrate the\nsuperiority of AniPortrait in terms of facial naturalness, pose diversity, and\nvisual quality, thereby offering an enhanced perceptual experience. Moreover,\nour methodology exhibits considerable potential in terms of flexibility and\ncontrollability, which can be effectively applied in areas such as facial\nmotion editing or face reenactment. We release code and model weights at\nhttps://github.com/scutzzj/AniPortrait\n","authors":["Huawei Wei","Zejun Yang","Zhisheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17692v1","updated":"2024-03-26T13:33:16Z","published":"2024-03-26T13:33:16Z","title":"Manifold-Guided Lyapunov Control with Diffusion Models","summary":"  This paper presents a novel approach to generating stabilizing controllers\nfor a large class of dynamical systems using diffusion models. The core\nobjective is to develop stabilizing control functions by identifying the\nclosest asymptotically stable vector field relative to a predetermined manifold\nand adjusting the control function based on this finding. To achieve this, we\nemploy a diffusion model trained on pairs consisting of asymptotically stable\nvector fields and their corresponding Lyapunov functions. Our numerical results\ndemonstrate that this pre-trained model can achieve stabilization over\npreviously unseen systems efficiently and rapidly, showcasing the potential of\nour approach in fast zero-shot control and generalizability.\n","authors":["Amartya Mukherjee","Thanin Quartz","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17692v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2403.17691v1","updated":"2024-03-26T13:32:32Z","published":"2024-03-26T13:32:32Z","title":"Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to\n  Inform GenAI Copyright Disputes","summary":"  The advent of Generative Artificial Intelligence (GenAI) models, including\nGitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content\ncreation, enabling non-professionals to produce high-quality content across\nvarious domains. This transformative technology has led to a surge of synthetic\ncontent and sparked legal disputes over copyright infringement. To address\nthese challenges, this paper introduces a novel approach that leverages the\nlearning capacity of GenAI models for copyright legal analysis, demonstrated\nwith GPT2 and Stable Diffusion models. Copyright law distinguishes between\noriginal expressions and generic ones (Sc\\`enes \\`a faire), protecting the\nformer and permitting reproduction of the latter. However, this distinction has\nhistorically been challenging to make consistently, leading to over-protection\nof copyrighted works. GenAI offers an unprecedented opportunity to enhance this\nlegal analysis by revealing shared patterns in preexisting works. We propose a\ndata-driven approach to identify the genericity of works created by GenAI,\nemploying \"data-driven bias\" to assess the genericity of expressive\ncompositions. This approach aids in copyright scope determination by utilizing\nthe capabilities of GenAI to identify and prioritize expressive elements and\nrank them according to their frequency in the model's dataset. The potential\nimplications of measuring expressive genericity for copyright law are profound.\nSuch scoring could assist courts in determining copyright scope during\nlitigation, inform the registration practices of Copyright Offices, allowing\nregistration of only highly original synthetic works, and help copyright owners\nsignal the value of their works and facilitate fairer licensing deals. More\ngenerally, this approach offers valuable insights to policymakers grappling\nwith adapting copyright law to the challenges posed by the era of GenAI.\n","authors":["Uri Hacohen","Adi Haviv","Shahar Sarfaty","Bruria Friedman","Niva Elkin-Koren","Roi Livni","Amit H Bermano"],"pdf_url":"https://arxiv.org/pdf/2403.17691v1.pdf","comment":"Presented at ACM CSLAW 2024"},{"id":"http://arxiv.org/abs/2311.16081v2","updated":"2024-03-26T13:32:06Z","published":"2023-11-27T18:52:09Z","title":"ViT-Lens: Towards Omni-modal Representations","summary":"  Aiming to advance AI agents, large foundation models significantly improve\nreasoning and instruction execution, yet the current focus on vision and\nlanguage neglects the potential of perceiving diverse modalities in open-world\nenvironments. However, the success of data-driven vision and language models is\ncostly or even infeasible to be reproduced for rare modalities. In this paper,\nwe present ViT-Lens-2 that facilitates efficient omni-modal representation\nlearning by perceiving novel modalities with a pretrained ViT and aligning them\nto a pre-defined space. Specifically, the modality-specific lens is tuned to\nproject any-modal signals to an intermediate embedding space, which are then\nprocessed by a strong ViT with pre-trained visual knowledge. The encoded\nrepresentations are optimized toward aligning with the modal-independent space,\npre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified\nsolution for representation learning of increasing modalities with two\nappealing advantages: (i) Unlocking the great potential of pretrained ViTs to\nnovel modalities effectively with efficient data regime; (ii) Enabling emergent\ndownstream capabilities through modality alignment and shared ViT parameters.\nWe tailor ViT-Lens-2 to learn representations for 3D point cloud, depth, audio,\ntactile and EEG, and set new state-of-the-art results across various\nunderstanding tasks, such as zero-shot classification. By seamlessly\nintegrating ViT-Lens-2 into Multimodal Foundation Models, we enable\nAny-modality to Text and Image Generation in a zero-shot manner. Code and\nmodels are available at https://github.com/TencentARC/ViT-Lens.\n","authors":["Weixian Lei","Yixiao Ge","Kun Yi","Jianfeng Zhang","Difei Gao","Dylan Sun","Yuying Ge","Ying Shan","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2311.16081v2.pdf","comment":"This work is a follow-up of arXiv:2308.10185. Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.11708v3","updated":"2024-03-26T13:21:52Z","published":"2024-03-18T12:12:45Z","title":"Implicit Discriminative Knowledge Learning for Visible-Infrared Person\n  Re-Identification","summary":"  Visible-Infrared Person Re-identification (VI-ReID) is a challenging\ncross-modal pedestrian retrieval task, due to significant intra-class\nvariations and cross-modal discrepancies among different cameras. Existing\nworks mainly focus on embedding images of different modalities into a unified\nspace to mine modality-shared features. They only seek distinctive information\nwithin these shared features, while ignoring the identity-aware useful\ninformation that is implicit in the modality-specific features. To address this\nissue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL)\nnetwork to uncover and leverage the implicit discriminative information\ncontained within the modality-specific. First, we extract modality-specific and\nmodality-shared features using a novel dual-stream network. Then, the\nmodality-specific features undergo purification to reduce their modality style\ndiscrepancies while preserving identity-aware discriminative knowledge.\nSubsequently, this kind of implicit knowledge is distilled into the\nmodality-shared feature to enhance its distinctiveness. Finally, an alignment\nloss is proposed to minimize modality discrepancy on enhanced modality-shared\nfeatures. Extensive experiments on multiple public datasets demonstrate the\nsuperiority of IDKL network over the state-of-the-art methods. Code is\navailable at https://github.com/1KK077/IDKL.\n","authors":["Kaijie Ren","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11708v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2311.17094v2","updated":"2024-03-26T13:21:43Z","published":"2023-11-28T06:17:49Z","title":"In Search of a Data Transformation That Accelerates Neural Field\n  Training","summary":"  Neural field is an emerging paradigm in data representation that trains a\nneural network to approximate the given signal. A key obstacle that prevents\nits widespread adoption is the encoding speed-generating neural fields requires\nan overfitting of a neural network, which can take a significant number of SGD\nsteps to reach the desired fidelity level. In this paper, we delve into the\nimpacts of data transformations on the speed of neural field training,\nspecifically focusing on how permuting pixel locations affect the convergence\nspeed of SGD. Counterintuitively, we find that randomly permuting the pixel\nlocations can considerably accelerate the training. To explain this phenomenon,\nwe examine the neural field training through the lens of PSNR curves, loss\nlandscapes, and error patterns. Our analyses suggest that the random pixel\npermutations remove the easy-to-fit patterns, which facilitate easy\noptimization in the early stage but hinder capturing fine details of the\nsignal.\n","authors":["Junwon Seo","Sangyoon Lee","Kwang In Kim","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2311.17094v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2312.02512v2","updated":"2024-03-26T13:21:28Z","published":"2023-12-05T05:36:44Z","title":"AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation\n  with Unified Audio-Visual Speech Representation","summary":"  This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech\nTranslation (AV2AV) framework, where the input and output of the system are\nmultimodal (i.e., audio and visual speech). With the proposed AV2AV, two key\nadvantages can be brought: 1) We can perform real-like conversations with\nindividuals worldwide in a virtual meeting by utilizing our own primary\nlanguages. In contrast to Speech-to-Speech Translation (A2A), which solely\ntranslates between audio modalities, the proposed AV2AV directly translates\nbetween audio-visual speech. This capability enhances the dialogue experience\nby presenting synchronized lip movements along with the translated speech. 2)\nWe can improve the robustness of the spoken language translation system. By\nemploying the complementary information of audio-visual speech, the system can\neffectively translate spoken language even in the presence of acoustic noise,\nshowcasing robust performance. To mitigate the problem of the absence of a\nparallel AV2AV translation dataset, we propose to train our spoken language\ntranslation system with the audio-only dataset of A2A. This is done by learning\nunified audio-visual speech representations through self-supervised learning in\nadvance to train the translation system. Moreover, we propose an AV-Renderer\nthat can generate raw audio and video in parallel. It is designed with\nzero-shot speaker modeling, thus the speaker in source audio-visual speech can\nbe maintained at the target translated audio-visual speech. The effectiveness\nof AV2AV is evaluated with extensive experiments in a many-to-many language\ntranslation setting. Demo page is available on\nhttps://choijeongsoo.github.io/av2av.\n","authors":["Jeongsoo Choi","Se Jin Park","Minsu Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2312.02512v2.pdf","comment":"CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av"},{"id":"http://arxiv.org/abs/2304.10417v3","updated":"2024-03-26T13:16:02Z","published":"2023-04-20T16:01:55Z","title":"SINC: Spatial Composition of 3D Human Motions for Simultaneous Action\n  Generation","summary":"  Our goal is to synthesize 3D human motions given textual inputs describing\nsimultaneous actions, for example 'waving hand' while 'walking' at the same\ntime. We refer to generating such simultaneous movements as performing 'spatial\ncompositions'. In contrast to temporal compositions that seek to transition\nfrom one action to another, spatial compositing requires understanding which\nbody parts are involved in which action, to be able to move them\nsimultaneously. Motivated by the observation that the correspondence between\nactions and body parts is encoded in powerful language models, we extract this\nknowledge by prompting GPT-3 with text such as \"what are the body parts\ninvolved in the action <action name>?\", while also providing the parts list and\nfew-shot examples. Given this action-part mapping, we combine body parts from\ntwo motions together and establish the first automated method to spatially\ncompose two actions. However, training data with compositional actions is\nalways limited by the combinatorics. Hence, we further create synthetic data\nwith this approach, and use it to train a new state-of-the-art text-to-motion\ngeneration model, called SINC (\"SImultaneous actioN Compositions for 3D human\nmotions\"). In our experiments, that training with such GPT-guided synthetic\ndata improves spatial composition generation over baselines. Our code is\npublicly available at https://sinc.is.tue.mpg.de/.\n","authors":["Nikos Athanasiou","Mathis Petrovich","Michael J. Black","Gül Varol"],"pdf_url":"https://arxiv.org/pdf/2304.10417v3.pdf","comment":"Teaser Fixed"},{"id":"http://arxiv.org/abs/2403.14135v2","updated":"2024-03-26T13:15:12Z","published":"2024-03-21T05:10:26Z","title":"Powerful Lossy Compression for Noisy Images","summary":"  Image compression and denoising represent fundamental challenges in image\nprocessing with many real-world applications. To address practical demands,\ncurrent solutions can be categorized into two main strategies: 1) sequential\nmethod; and 2) joint method. However, sequential methods have the disadvantage\nof error accumulation as there is information loss between multiple individual\nmodels. Recently, the academic community began to make some attempts to tackle\nthis problem through end-to-end joint methods. Most of them ignore that\ndifferent regions of noisy images have different characteristics. To solve\nthese problems, in this paper, our proposed signal-to-noise ratio~(SNR) aware\njoint solution exploits local and non-local features for image compression and\ndenoising simultaneously. We design an end-to-end trainable network, which\nincludes the main encoder branch, the guidance branch, and the signal-to-noise\nratio~(SNR) aware branch. We conducted extensive experiments on both synthetic\nand real-world datasets, demonstrating that our joint solution outperforms\nexisting state-of-the-art methods.\n","authors":["Shilv Cai","Xiaoguo Liang","Shuning Cao","Luxin Yan","Sheng Zhong","Liqun Chen","Xu Zou"],"pdf_url":"https://arxiv.org/pdf/2403.14135v2.pdf","comment":"Accepted by ICME 2024"},{"id":"http://arxiv.org/abs/2308.10185v2","updated":"2024-03-26T13:11:07Z","published":"2023-08-20T07:26:51Z","title":"ViT-Lens: Initiating Omni-Modal Exploration through 3D Insights","summary":"  Though the success of CLIP-based training recipes in vision-language models,\ntheir scalability to more modalities (e.g., 3D, audio, etc.) is limited to\nlarge-scale data, which is expensive or even inapplicable for rare modalities.\nIn this paper, we present ViT-Lens that facilitates efficient omni-modal\nrepresentation learning by perceiving novel modalities with a pretrained ViT\nand aligning to a pre-defined space. Specifically, the modality-specific lens\nis tuned to project multimodal signals to the shared embedding space, which are\nthen processed by a strong ViT that carries pre-trained image knowledge. The\nencoded multimodal representations are optimized toward aligning with the\nmodal-independent space, pre-defined by off-the-shelf foundation models. A\nwell-trained lens with a ViT backbone has the potential to serve as one of\nthese foundation models, supervising the learning of subsequent modalities.\nViT-Lens provides a unified solution for representation learning of increasing\nmodalities with two appealing benefits: (i) Exploiting the pretrained ViT\nacross tasks and domains effectively with efficient data regime; (ii) Emergent\ndownstream capabilities of novel modalities are demonstrated due to the\nmodality alignment space. We evaluate ViT-Lens in the context of 3D as an\ninitial verification. In zero-shot 3D classification, ViT-Lens achieves\nsubstantial improvements over previous state-of-the-art, showing 52.0% accuracy\non Objaverse-LVIS, 87.4% on ModelNet40, and 60.6% on ScanObjectNN. Furthermore,\nwe enable zero-shot 3D question-answering by simply integrating the trained 3D\nlens into the InstructBLIP model without any adaptation. We will release the\nresults of ViT-Lens on more modalities in the near future.\n","authors":["Weixian Lei","Yixiao Ge","Jianfeng Zhang","Dylan Sun","Kun Yi","Ying Shan","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2308.10185v2.pdf","comment":"19 pages, 4 figures and 9 tables"},{"id":"http://arxiv.org/abs/2403.17678v1","updated":"2024-03-26T13:05:49Z","published":"2024-03-26T13:05:49Z","title":"Hierarchical Light Transformer Ensembles for Multimodal Trajectory\n  Forecasting","summary":"  Accurate trajectory forecasting is crucial for the performance of various\nsystems, such as advanced driver-assistance systems and self-driving vehicles.\nThese forecasts allow to anticipate events leading to collisions and,\ntherefore, to mitigate them. Deep Neural Networks have excelled in motion\nforecasting, but issues like overconfidence and uncertainty quantification\npersist. Deep Ensembles address these concerns, yet applying them to multimodal\ndistributions remains challenging. In this paper, we propose a novel approach\nnamed Hierarchical Light Transformer Ensembles (HLT-Ens), aimed at efficiently\ntraining an ensemble of Transformer architectures using a novel hierarchical\nloss function. HLT-Ens leverages grouped fully connected layers, inspired by\ngrouped convolution techniques, to capture multimodal distributions,\neffectively. Through extensive experimentation, we demonstrate that HLT-Ens\nachieves state-of-the-art performance levels, offering a promising avenue for\nimproving trajectory forecasting techniques.\n","authors":["Adrien Lafage","Mathieu Barbier","Gianni Franchi","David Filliat"],"pdf_url":"https://arxiv.org/pdf/2403.17678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17672v1","updated":"2024-03-26T13:02:38Z","published":"2024-03-26T13:02:38Z","title":"Predicting Perceived Gloss: Do Weak Labels Suffice?","summary":"  Estimating perceptual attributes of materials directly from images is a\nchallenging task due to their complex, not fully-understood interactions with\nexternal factors, such as geometry and lighting. Supervised deep learning\nmodels have recently been shown to outperform traditional approaches, but rely\non large datasets of human-annotated images for accurate perception\npredictions. Obtaining reliable annotations is a costly endeavor, aggravated by\nthe limited ability of these models to generalise to different aspects of\nappearance. In this work, we show how a much smaller set of human annotations\n(\"strong labels\") can be effectively augmented with automatically derived \"weak\nlabels\" in the context of learning a low-dimensional image-computable gloss\nmetric. We evaluate three alternative weak labels for predicting human gloss\nperception from limited annotated data. Incorporating weak labels enhances our\ngloss prediction beyond the current state of the art. Moreover, it enables a\nsubstantial reduction in human annotation costs without sacrificing accuracy,\nwhether working with rendered images or real photographs.\n","authors":["Julia Guerrero-Viu","J. Daniel Subias","Ana Serrano","Katherine R. Storrs","Roland W. Fleming","Belen Masia","Diego Gutierrez"],"pdf_url":"https://arxiv.org/pdf/2403.17672v1.pdf","comment":"Computer Graphics Forum (Eurographics 2024)"},{"id":"http://arxiv.org/abs/2310.01819v3","updated":"2024-03-26T12:59:39Z","published":"2023-10-03T06:16:38Z","title":"TP2O: Creative Text Pair-to-Object Generation using Balance\n  Swap-Sampling","summary":"  Generating creative combinatorial objects from two seemingly unrelated object\ntexts is a challenging task in text-to-image synthesis, often hindered by a\nfocus on emulating existing data distributions. In this paper, we develop a\nstraightforward yet highly effective method, called \\textbf{balance\nswap-sampling}. First, we propose a swapping mechanism that generates a novel\ncombinatorial object image set by randomly exchanging intrinsic elements of two\ntext embeddings through a cutting-edge diffusion model. Second, we introduce a\nbalance swapping region to efficiently sample a small subset from the newly\ngenerated image set by balancing CLIP distances between the new images and\ntheir original generations, increasing the likelihood of accepting the\nhigh-quality combinations. Last, we employ a segmentation method to compare\nCLIP distances among the segmented components, ultimately selecting the most\npromising object from the sampled subset. Extensive experiments demonstrate\nthat our approach outperforms recent SOTA T2I methods. Surprisingly, our\nresults even rival those of human artists, such as frog-broccoli.\n","authors":["Jun Li","Zedong Zhang","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2310.01819v3.pdf","comment":"Project page: https://tp2o.github.io/anon/"},{"id":"http://arxiv.org/abs/2312.00869v2","updated":"2024-03-26T12:56:55Z","published":"2023-12-01T19:00:17Z","title":"Segment and Caption Anything","summary":"  We propose a method to efficiently equip the Segment Anything Model (SAM)\nwith the ability to generate regional captions. SAM presents strong\ngeneralizability to segment anything while is short for semantic understanding.\nBy introducing a lightweight query-based feature mixer, we align the\nregion-specific features with the embedding space of language models for later\ncaption generation. As the number of trainable parameters is small (typically\nin the order of tens of millions), it costs less computation, less memory\nusage, and less communication bandwidth, resulting in both fast and scalable\ntraining. To address the scarcity problem of regional caption data, we propose\nto first pre-train our model on objection detection and segmentation tasks. We\ncall this step weak supervision pretraining since the pre-training data only\ncontains category names instead of full-sentence descriptions. The weak\nsupervision pretraining allows us to leverage many publicly available object\ndetection and segmentation datasets. We conduct extensive experiments to\ndemonstrate the superiority of our method and validate each design choice. This\nwork serves as a stepping stone towards scaling up regional captioning data and\nsheds light on exploring efficient ways to augment SAM with regional semantics.\nThe project page, along with the associated code, can be accessed via\nhttps://xk-huang.github.io/segment-caption-anything/.\n","authors":["Xiaoke Huang","Jianfeng Wang","Yansong Tang","Zheng Zhang","Han Hu","Jiwen Lu","Lijuan Wang","Zicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2312.00869v2.pdf","comment":"The project page, along with the associated code, can be accessed via\n  https://xk-huang.github.io/segment-caption-anything/; Update author\n  information; Accepted by CVPR 24"},{"id":"http://arxiv.org/abs/2403.17664v1","updated":"2024-03-26T12:53:10Z","published":"2024-03-26T12:53:10Z","title":"DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with\n  Space-sensitive Customization and Semantic Preservation","summary":"  Facial Appearance Editing (FAE) aims to modify physical attributes, such as\npose, expression and lighting, of human facial images while preserving\nattributes like identity and background, showing great importance in\nphotograph. In spite of the great progress in this area, current researches\ngenerally meet three challenges: low generation fidelity, poor attribute\npreservation, and inefficient inference. To overcome above challenges, this\npaper presents DiffFAE, a one-stage and highly-efficient diffusion-based\nframework tailored for high-fidelity FAE. For high-fidelity query attributes\ntransfer, we adopt Space-sensitive Physical Customization (SPC), which ensures\nthe fidelity and generalization ability by utilizing rendering texture derived\nfrom 3D Morphable Model (3DMM). In order to preserve source attributes, we\nintroduce the Region-responsive Semantic Composition (RSC). This module is\nguided to learn decoupled source-regarding features, thereby better preserving\nthe identity and alleviating artifacts from non-facial attributes such as hair,\nclothes, and background. We further introduce a consistency regularization for\nour pipeline to enhance editing controllability by leveraging prior knowledge\nin the attention matrices of diffusion model. Extensive experiments demonstrate\nthe superiority of DiffFAE over existing methods, achieving state-of-the-art\nperformance in facial appearance editing.\n","authors":["Qilin Wang","Jiangning Zhang","Chengming Xu","Weijian Cao","Ying Tai","Yue Han","Yanhao Ge","Hong Gu","Chengjie Wang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2403.17664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14149v4","updated":"2024-03-26T12:47:12Z","published":"2023-12-21T18:59:06Z","title":"TagAlign: Improving Vision-Language Alignment with Multi-Tag\n  Classification","summary":"  The crux of learning vision-language models is to extract semantically\naligned information from visual and linguistic data. Existing attempts usually\nface the problem of coarse alignment, e.g., the vision encoder struggles in\nlocalizing an attribute-specified object. In this work, we propose an\nembarrassingly simple approach to better align image and text features with no\nneed of additional data formats other than image-text pairs. Concretely, given\nan image and its paired text, we manage to parse objects (e.g., cat) and\nattributes (e.g., black) from the description, which are highly likely to exist\nin the image. It is noteworthy that the parsing pipeline is fully automatic and\nthus enjoys good scalability. With these parsed semantics as supervision\nsignals, we can complement the commonly used image-text contrastive loss with\nthe multi-tag classification loss. Extensive experimental results on a broad\nsuite of semantic segmentation datasets substantiate the average 5.2\\%\nimprovement of our framework over existing alternatives. Furthermore, the\nvisualization results indicate that attribute supervision makes vision-language\nmodels accurately localize attribute-specified objects. Project page can be\nfound at https://qinying-liu.github.io/Tag-Align.\n","authors":["Qinying Liu","Wei Wu","Kecheng Zheng","Zhan Tong","Jiawei Liu","Yu Liu","Wei Chen","Zilei Wang","Yujun Shen"],"pdf_url":"https://arxiv.org/pdf/2312.14149v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03246v5","updated":"2024-03-26T12:35:03Z","published":"2024-02-05T18:03:53Z","title":"SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM","summary":"  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian\nSplatting. It incorporates appearance, geometry, and semantic features through\nmulti-channel optimization, addressing the oversmoothing limitations of neural\nimplicit SLAM systems in high-quality rendering, scene understanding, and\nobject-level geometry. We introduce a unique semantic feature loss that\neffectively compensates for the shortcomings of traditional depth and color\nlosses in object optimization. Through a semantic-guided keyframe selection\nstrategy, we prevent erroneous reconstructions caused by cumulative errors.\nExtensive experiments demonstrate that SGS-SLAM delivers state-of-the-art\nperformance in camera pose estimation, map reconstruction, precise semantic\nsegmentation, and object-level geometric accuracy, while ensuring real-time\nrendering capabilities.\n","authors":["Mingrui Li","Shuhong Liu","Heng Zhou","Guohao Zhu","Na Cheng","Tianchen Deng","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2402.03246v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17651v1","updated":"2024-03-26T12:31:58Z","published":"2024-03-26T12:31:58Z","title":"Exploring Dynamic Transformer for Efficient Object Tracking","summary":"  The speed-precision trade-off is a critical problem for visual object\ntracking which usually requires low latency and deployment on constrained\nresources. Existing solutions for efficient tracking mainly focus on adopting\nlight-weight backbones or modules, which nevertheless come at the cost of a\nsacrifice in precision. In this paper, inspired by dynamic network routing, we\npropose DyTrack, a dynamic transformer framework for efficient tracking.\nReal-world tracking scenarios exhibit diverse levels of complexity. We argue\nthat a simple network is sufficient for easy frames in video sequences, while\nmore computation could be assigned to difficult ones. DyTrack automatically\nlearns to configure proper reasoning routes for various inputs, gaining better\nutilization of the available computational budget. Thus, it can achieve higher\nperformance with the same running speed. We formulate instance-specific\ntracking as a sequential decision problem and attach terminating branches to\nintermediate layers of the entire model. Especially, to fully utilize the\ncomputations, we introduce the feature recycling mechanism to reuse the outputs\nof predecessors. Furthermore, a target-aware self-distillation strategy is\ndesigned to enhance the discriminating capabilities of early predictions by\neffectively mimicking the representation pattern of the deep model. Extensive\nexperiments on multiple benchmarks demonstrate that DyTrack achieves promising\nspeed-precision trade-offs with only a single model. For instance, DyTrack\nobtains 64.9% AUC on LaSOT with a speed of 256 fps.\n","authors":["Jiawen Zhu","Xin Chen","Haiwen Diao","Shuai Li","Jun-Yan He","Chenyang Li","Bin Luo","Dong Wang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2403.17651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02109v2","updated":"2024-03-26T12:28:02Z","published":"2023-12-04T18:39:00Z","title":"ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder\n  and Explicit Adaptation","summary":"  This work introduces ArtAdapter, a transformative text-to-image (T2I) style\ntransfer framework that transcends traditional limitations of color,\nbrushstrokes, and object shape, capturing high-level style elements such as\ncomposition and distinctive artistic expression. The integration of a\nmulti-level style encoder with our proposed explicit adaptation mechanism\nenables ArtAdapter to achieve unprecedented fidelity in style transfer,\nensuring close alignment with textual descriptions. Additionally, the\nincorporation of an Auxiliary Content Adapter (ACA) effectively separates\ncontent from style, alleviating the borrowing of content from style references.\nMoreover, our novel fast finetuning approach could further enhance zero-shot\nstyle representation while mitigating the risk of overfitting. Comprehensive\nevaluations confirm that ArtAdapter surpasses current state-of-the-art methods.\n","authors":["Dar-Yen Chen","Hamish Tennent","Ching-Wen Hsu"],"pdf_url":"https://arxiv.org/pdf/2312.02109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17639v1","updated":"2024-03-26T12:21:47Z","published":"2024-03-26T12:21:47Z","title":"High-Resolution Image Translation Model Based on Grayscale Redefinition","summary":"  Image-to-image translation is a technique that focuses on transferring images\nfrom one domain to another while maintaining the essential content\nrepresentations. In recent years, image-to-image translation has gained\nsignificant attention and achieved remarkable advancements due to its diverse\napplications in computer vision and image processing tasks. In this work, we\npropose an innovative method for image translation between different domains.\nFor high-resolution image translation tasks, we use a grayscale adjustment\nmethod to achieve pixel-level translation. For other tasks, we utilize the\nPix2PixHD model with a coarse-to-fine generator, multi-scale discriminator, and\nimproved loss to enhance the image translation performance. On the other hand,\nto tackle the issue of sparse training data, we adopt model weight\ninitialization from other task to optimize the performance of the current task.\n","authors":["Xixian Wu","Dian Chao","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2403.17639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17638v1","updated":"2024-03-26T12:17:46Z","published":"2024-03-26T12:17:46Z","title":"Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with\n  Relative Geometric Consistency","summary":"  We propose a voxel-based optimization framework, ReVoRF, for few-shot\nradiance fields that strategically address the unreliability in pseudo novel\nview synthesis. Our method pivots on the insight that relative depth\nrelationships within neighboring regions are more reliable than the absolute\ncolor values in disoccluded areas. Consequently, we devise a bilateral\ngeometric consistency loss that carefully navigates the trade-off between color\nfidelity and geometric accuracy in the context of depth consistency for\nuncertain regions. Moreover, we present a reliability-guided learning strategy\nto discern and utilize the variable quality across synthesized views,\ncomplemented by a reliability-aware voxel smoothing algorithm that smoothens\nthe transition between reliable and unreliable data patches. Our approach\nallows for a more nuanced use of all available data, promoting enhanced\nlearning from regions previously considered unsuitable for high-quality\nreconstruction. Extensive experiments across diverse datasets reveal that our\napproach attains significant gains in efficiency and accuracy, delivering\nrendering speeds of 3 FPS, 7 mins to train a $360^\\circ$ scene, and a 5\\%\nimprovement in PSNR over existing few-shot methods. Code is available at\nhttps://github.com/HKCLynn/ReVoRF.\n","authors":["Yingjie Xu","Bangzhen Liu","Hao Tang","Bailin Deng","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2403.17638v1.pdf","comment":"CVPR 2024 final version"},{"id":"http://arxiv.org/abs/2403.15010v2","updated":"2024-03-26T12:16:14Z","published":"2024-03-22T07:47:13Z","title":"Clean-image Backdoor Attacks","summary":"  To gather a significant quantity of annotated training data for\nhigh-performance image classification models, numerous companies opt to enlist\nthird-party providers to label their unlabeled data. This practice is widely\nregarded as secure, even in cases where some annotated errors occur, as the\nimpact of these minor inaccuracies on the final performance of the models is\nnegligible and existing backdoor attacks require attacker's ability to poison\nthe training images. Nevertheless, in this paper, we propose clean-image\nbackdoor attacks which uncover that backdoors can still be injected via a\nfraction of incorrect labels without modifying the training images.\nSpecifically, in our attacks, the attacker first seeks a trigger feature to\ndivide the training images into two parts: those with the feature and those\nwithout it. Subsequently, the attacker falsifies the labels of the former part\nto a backdoor class. The backdoor will be finally implanted into the target\nmodel after it is trained on the poisoned data. During the inference phase, the\nattacker can activate the backdoor in two ways: slightly modifying the input\nimage to obtain the trigger feature, or taking an image that naturally has the\ntrigger feature as input. We conduct extensive experiments to demonstrate the\neffectiveness and practicality of our attacks. According to the experimental\nresults, we conclude that our attacks seriously jeopardize the fairness and\nrobustness of image classification models, and it is necessary to be vigilant\nabout the incorrect labels in outsourced labeling.\n","authors":["Dazhong Rong","Guoyao Yu","Shuheng Shen","Xinyi Fu","Peng Qian","Jianhai Chen","Qinming He","Xing Fu","Weiqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06683v2","updated":"2024-03-26T12:10:13Z","published":"2024-03-11T12:57:51Z","title":"Transferring Relative Monocular Depth to Surgical Vision with Temporal\n  Consistency","summary":"  Relative monocular depth, inferring depth up to shift and scale from a single\nimage, is an active research topic. Recent deep learning models, trained on\nlarge and varied meta-datasets, now provide excellent performance in the domain\nof natural images. However, few datasets exist which provide ground truth depth\nfor endoscopic images, making training such models from scratch unfeasible.\nThis work investigates the transfer of these models into the surgical domain,\nand presents an effective and simple way to improve on standard supervision\nthrough the use of temporal consistency self-supervision. We show temporal\nconsistency significantly improves supervised training alone when transferring\nto the low-data regime of endoscopy, and outperforms the prevalent\nself-supervision technique for this task. In addition we show our method\ndrastically outperforms the state-of-the-art method from within the domain of\nendoscopy. We also release our code, model and ensembled meta-dataset,\nMeta-MED, establishing a strong benchmark for future work.\n","authors":["Charlie Budd","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2403.06683v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17633v1","updated":"2024-03-26T12:08:14Z","published":"2024-03-26T12:08:14Z","title":"UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object\n  Detection with Sparse LiDAR and Large Domain Gaps","summary":"  In this study, we address a gap in existing unsupervised domain adaptation\napproaches on LiDAR-based 3D object detection, which have predominantly\nconcentrated on adapting between established, high-density autonomous driving\ndatasets. We focus on sparser point clouds, capturing scenarios from different\nperspectives: not just from vehicles on the road but also from mobile robots on\nsidewalks, which encounter significantly different environmental conditions and\nsensor configurations. We introduce Unsupervised Adversarial Domain Adaptation\nfor 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source\nmodels or teacher-student architectures. Instead, it uses an adversarial\napproach to directly learn domain-invariant features. We demonstrate its\nefficacy in various adaptation scenarios, showing significant improvements in\nboth self-driving car and mobile robot domains. Our code is open-source and\nwill be available soon.\n","authors":["Maciej K Wozniak","Mattias Hansson","Marko Thiel","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.17633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17631v1","updated":"2024-03-26T12:08:04Z","published":"2024-03-26T12:08:04Z","title":"AniArtAvatar: Animatable 3D Art Avatar from a Single Image","summary":"  We present a novel approach for generating animatable 3D-aware art avatars\nfrom a single image, with controllable facial expressions, head poses, and\nshoulder movements. Unlike previous reenactment methods, our approach utilizes\na view-conditioned 2D diffusion model to synthesize multi-view images from a\nsingle art portrait with a neutral expression. With the generated colors and\nnormals, we synthesize a static avatar using an SDF-based neural surface. For\navatar animation, we extract control points, transfer the motion with these\npoints, and deform the implicit canonical space. Firstly, we render the front\nimage of the avatar, extract the 2D landmarks, and project them to the 3D space\nusing a trained SDF network. We extract 3D driving landmarks using 3DMM and\ntransfer the motion to the avatar landmarks. To animate the avatar pose, we\nmanually set the body height and bound the head and torso of an avatar with two\ncages. The head and torso can be animated by transforming the two cages. Our\napproach is a one-shot pipeline that can be applied to various styles.\nExperiments demonstrate that our method can generate high-quality 3D art\navatars with desired control over different motions.\n","authors":["Shaoxu Li"],"pdf_url":"https://arxiv.org/pdf/2403.17631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01598v3","updated":"2024-03-26T11:54:40Z","published":"2023-06-02T15:09:19Z","title":"Towards Source-free Domain Adaptive Semantic Segmentation via\n  Importance-aware and Prototype-contrast Learning","summary":"  Domain adaptive semantic segmentation enables robust pixel-wise understanding\nin real-world driving scenes. Source-free domain adaptation, as a more\npractical technique, addresses the concerns of data privacy and storage\nlimitations in typical unsupervised domain adaptation methods, making it\nespecially relevant in the context of intelligent vehicles. It utilizes a\nwell-trained source model and unlabeled target data to achieve adaptation in\nthe target domain. However, in the absence of source data and target labels,\ncurrent solutions cannot sufficiently reduce the impact of domain shift and\nfully leverage the information from the target data. In this paper, we propose\nan end-to-end source-free domain adaptation semantic segmentation method via\nImportance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPC\nframework effectively extracts domain-invariant knowledge from the well-trained\nsource model and learns domain-specific knowledge from the unlabeled target\ndomain. Specifically, considering the problem of domain shift in the prediction\nof the target domain by the source model, we put forward an importance-aware\nmechanism for the biased target prediction probability distribution to extract\ndomain-invariant knowledge from the source model. We further introduce a\nprototype-contrast strategy, which includes a prototype-symmetric cross-entropy\nloss and a prototype-enhanced cross-entropy loss, to learn target intra-domain\nknowledge without relying on labels. A comprehensive variety of experiments on\ntwo domain adaptive semantic segmentation benchmarks demonstrates that the\nproposed end-to-end IAPC solution outperforms existing state-of-the-art\nmethods. The source code is publicly available at\nhttps://github.com/yihong-97/Source-free-IAPC.\n","authors":["Yihong Cao","Hui Zhang","Xiao Lu","Zheng Xiao","Kailun Yang","Yaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2306.01598v3.pdf","comment":"Accepted to IEEE Transactions on Intelligent Vehicles (T-IV). The\n  source code is publicly available at\n  https://github.com/yihong-97/Source-free-IAPC"},{"id":"http://arxiv.org/abs/2310.17569v2","updated":"2024-03-26T11:52:23Z","published":"2023-10-26T16:58:01Z","title":"SD4Match: Learning to Prompt Stable Diffusion Model for Semantic\n  Matching","summary":"  In this paper, we address the challenge of matching semantically similar\nkeypoints across image pairs. Existing research indicates that the intermediate\noutput of the UNet within the Stable Diffusion (SD) can serve as robust image\nfeature maps for such a matching task. We demonstrate that by employing a basic\nprompt tuning technique, the inherent potential of Stable Diffusion can be\nharnessed, resulting in a significant enhancement in accuracy over previous\napproaches. We further introduce a novel conditional prompting module that\nconditions the prompt on the local details of the input image pairs, leading to\na further improvement in performance. We designate our approach as SD4Match,\nshort for Stable Diffusion for Semantic Matching. Comprehensive evaluations of\nSD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets\nnew benchmarks in accuracy across all these datasets. Particularly, SD4Match\noutperforms the previous state-of-the-art by a margin of 12 percentage points\non the challenging SPair-71k dataset.\n","authors":["Xinghui Li","Jingyi Lu","Kai Han","Victor Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2310.17569v2.pdf","comment":"Accepted to CVPR 2024. Project website:\n  https://sd4match.active.vision/"},{"id":"http://arxiv.org/abs/2403.17615v1","updated":"2024-03-26T11:48:37Z","published":"2024-03-26T11:48:37Z","title":"Grad-CAMO: Learning Interpretable Single-Cell Morphological Profiles\n  from 3D Cell Painting Images","summary":"  Despite their black-box nature, deep learning models are extensively used in\nimage-based drug discovery to extract feature vectors from single cells in\nmicroscopy images. To better understand how these networks perform\nrepresentation learning, we employ visual explainability techniques (e.g.,\nGrad-CAM). Our analyses reveal several mechanisms by which supervised models\ncheat, exploiting biologically irrelevant pixels when extracting morphological\nfeatures from images, such as noise in the background. This raises doubts\nregarding the fidelity of learned single-cell representations and their\nrelevance when investigating downstream biological questions. To address this\nmisalignment between researcher expectations and machine behavior, we introduce\nGrad-CAMO, a novel single-cell interpretability score for supervised feature\nextractors. Grad-CAMO measures the proportion of a model's attention that is\nconcentrated on the cell of interest versus the background. This metric can be\nassessed per-cell or averaged across a validation set, offering a tool to audit\nindividual features vectors or guide the improved design of deep learning\narchitectures. Importantly, Grad-CAMO seamlessly integrates into existing\nworkflows, requiring no dataset or model modifications, and is compatible with\nboth 2D and 3D Cell Painting data. Additional results are available at\nhttps://github.com/eigenvivek/Grad-CAMO.\n","authors":["Vivek Gopalakrishnan","Jingzhe Ma","Zhiyong Xie"],"pdf_url":"https://arxiv.org/pdf/2403.17615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17610v1","updated":"2024-03-26T11:43:05Z","published":"2024-03-26T11:43:05Z","title":"MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors","summary":"  Foot contact is an important cue not only for human motion capture but also\nfor motion understanding and physically plausible motion generation. However,\nmost of the foot-contact annotations in existing datasets are estimated by\npurely visual matching and distance thresholding, which results in low accuracy\nand coarse granularity. Even though existing multimodal datasets\nsynergistically capture plantar pressure (foot contact) and visual signals,\nthey are specifically designed for small-range and slow motion such as Taiji\nQuan and Yoga. Therefore, there is still a lack of a vision-pressure multimodal\ndataset with large-range and fast human motion, as well as accurate and dense\nfoot-contact annotation. To fill this gap, we propose a Multimodal MoCap\nDataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate\nand dense plantar pressure signals synchronized with RGBD observations, which\nis especially useful for both plausible shape estimation, robust pose fitting\nwithout foot drifting, and accurate global translation tracking. To validate\nthe dataset, we propose an RGBD-P SMPL fitting method and also a\nmonocular-video-based baseline framework, VP-MoCap, for human motion capture.\nExperiments demonstrate that our RGBD-P SMPL Fitting results significantly\noutperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA\nmethods in foot-contact and global translation estimation accuracy. We believe\nthe configuration of the dataset and the baseline frameworks will stimulate the\nresearch in this direction and also provide a good reference for MoCap\napplications in various domains. Project page:\nhttps://haolyuan.github.io/MMVP-Dataset/.\n","authors":["He Zhang","Shenghao Ren","Haolei Yuan","Jianhui Zhao","Fan Li","Shuangpeng Sun","Zhenghao Liang","Tao Yu","Qiu Shen","Xun Cao"],"pdf_url":"https://arxiv.org/pdf/2403.17610v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.17608v1","updated":"2024-03-26T11:39:00Z","published":"2024-03-26T11:39:00Z","title":"Fake or JPEG? Revealing Common Biases in Generated Image Detection\n  Datasets","summary":"  The widespread adoption of generative image models has highlighted the urgent\nneed to detect artificial content, which is a crucial step in combating\nwidespread manipulation and misinformation. Consequently, numerous detectors\nand associated datasets have emerged. However, many of these datasets\ninadvertently introduce undesirable biases, thereby impacting the effectiveness\nand evaluation of detectors. In this paper, we emphasize that many datasets for\nAI-generated image detection contain biases related to JPEG compression and\nimage size. Using the GenImage dataset, we demonstrate that detectors indeed\nlearn from these undesired factors. Furthermore, we show that removing the\nnamed biases substantially increases robustness to JPEG compression and\nsignificantly alters the cross-generator performance of evaluated detectors.\nSpecifically, it leads to more than 11 percentage points increase in\ncross-generator performance for ResNet50 and Swin-T detectors on the GenImage\ndataset, achieving state-of-the-art results.\n  We provide the dataset and source codes of this paper on the anonymous\nwebsite: https://www.unbiased-genimage.org\n","authors":["Patrick Grommelt","Louis Weiss","Franz-Josef Pfreundt","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2403.17608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04701v3","updated":"2024-03-26T11:26:17Z","published":"2024-03-07T17:48:48Z","title":"ObjectCompose: Evaluating Resilience of Vision-Based Models on\n  Object-to-Background Compositional Changes","summary":"  Given the large-scale multi-modal training of recent vision-based models and\ntheir generalization capabilities, understanding the extent of their robustness\nis critical for their real-world deployment. In this work, we evaluate the\nresilience of current vision-based models against diverse object-to-background\ncontext variations. The majority of robustness evaluation methods have\nintroduced synthetic datasets to induce changes to object characteristics\n(viewpoints, scale, color) or utilized image transformation techniques\n(adversarial changes, common corruptions) on real images to simulate shifts in\ndistributions. Recent works have explored leveraging large language models and\ndiffusion models to generate changes in the background. However, these methods\neither lack in offering control over the changes to be made or distort the\nobject semantics, making them unsuitable for the task. Our method, on the other\nhand, can induce diverse object-to-background changes while preserving the\noriginal semantics and appearance of the object. To achieve this goal, we\nharness the generative capabilities of text-to-image, image-to-text, and\nimage-to-segment models to automatically generate a broad spectrum of\nobject-to-background changes. We induce both natural and adversarial background\nchanges by either modifying the textual prompts or optimizing the latents and\ntextual embedding of text-to-image models. We produce various versions of\nstandard vision datasets (ImageNet, COCO), incorporating either diverse and\nrealistic backgrounds into the images or introducing color, texture, and\nadversarial changes in the background. We conduct extensive experiment to\nanalyze the robustness of vision-based models against object-to-background\ncontext variations across diverse tasks. Code\nhttps://github.com/Muhammad-Huzaifaa/ObjectCompose.git\n","authors":["Hashmat Shadab Malik","Muhammad Huzaifa","Muzammal Naseer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2403.04701v3.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.17901v1","updated":"2024-03-26T17:43:08Z","published":"2024-03-26T17:43:08Z","title":"Search and Society: Reimagining Information Access for Radical Futures","summary":"  Information retrieval (IR) technologies and research are undergoing\ntransformative changes. It is our perspective that the community should accept\nthis opportunity to re-center our research agendas on societal needs while\ndismantling the artificial separation between the work on fairness,\naccountability, transparency, and ethics in IR and the rest of IR research.\nInstead of adopting a reactionary strategy of trying to mitigate potential\nsocial harms from emerging technologies, the community should aim to\nproactively set the research agenda for the kinds of systems we should build\ninspired by diverse explicitly stated sociotechnical imaginaries. The\nsociotechnical imaginaries that underpin the design and development of\ninformation access technologies needs to be explicitly articulated, and we need\nto develop theories of change in context of these diverse perspectives. Our\nguiding future imaginaries must be informed by other academic fields, such as\ndemocratic theory and critical theory, and should be co-developed with social\nscience scholars, legal scholars, civil rights and social justice activists,\nand artists, among others. In this perspective paper, we motivate why the\ncommunity must consider this radical shift in how we do research and what we\nwork on, and sketch a path forward towards this transformation.\n","authors":["Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2403.17901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17876v1","updated":"2024-03-26T17:06:56Z","published":"2024-03-26T17:06:56Z","title":"MIND Your Language: A Multilingual Dataset for Cross-lingual News\n  Recommendation","summary":"  Digital news platforms use news recommenders as the main instrument to cater\nto the individual information needs of readers. Despite an increasingly\nlanguage-diverse online community, in which many Internet users consume news in\nmultiple languages, the majority of news recommendation focuses on major,\nresource-rich languages, and English in particular. Moreover, nearly all news\nrecommendation efforts assume monolingual news consumption, whereas more and\nmore users tend to consume information in at least two languages. Accordingly,\nthe existing body of work on news recommendation suffers from a lack of\npublicly available multilingual benchmarks that would catalyze development of\nnews recommenders effective in multilingual settings and for low-resource\nlanguages. Aiming to fill this gap, we introduce xMIND, an open, multilingual\nnews recommendation dataset derived from the English MIND dataset using machine\ntranslation, covering a set of 14 linguistically and geographically diverse\nlanguages, with digital footprints of varying sizes. Using xMIND, we\nsystematically benchmark several state-of-the-art content-based neural news\nrecommenders (NNRs) in both zero-shot (ZS-XLT) and few-shot (FS-XLT)\ncross-lingual transfer scenarios, considering both monolingual and bilingual\nnews consumption patterns. Our findings reveal that (i) current NNRs, even when\nbased on a multilingual language model, suffer from substantial performance\nlosses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT\ntraining has limited benefits, particularly when combined with a bilingual news\nconsumption. Our findings thus warrant a broader research effort in\nmultilingual and cross-lingual news recommendation. The xMIND dataset is\navailable at https://github.com/andreeaiana/xMIND.\n","authors":["Andreea Iana","Goran Glavaš","Heiko Paulheim"],"pdf_url":"https://arxiv.org/pdf/2403.17876v1.pdf","comment":"Accepted at the 47th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval (SIGIR 2024)"},{"id":"http://arxiv.org/abs/2403.17848v1","updated":"2024-03-26T16:37:54Z","published":"2024-03-26T16:37:54Z","title":"ArabicaQA: A Comprehensive Dataset for Arabic Question Answering","summary":"  In this paper, we address the significant gap in Arabic natural language\nprocessing (NLP) resources by introducing ArabicaQA, the first large-scale\ndataset for machine reading comprehension and open-domain question answering in\nArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701\nunanswerable questions created by crowdworkers to look similar to answerable\nones, along with additional labels of open-domain questions marks a crucial\nadvancement in Arabic NLP resources. We also present AraDPR, the first dense\npassage retrieval model trained on the Arabic Wikipedia corpus, specifically\ndesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,\nour study includes extensive benchmarking of large language models (LLMs) for\nArabic question answering, critically evaluating their performance in the\nArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking\nof LLMs in Arabic question answering offer significant advancements in the\nfield of Arabic NLP. The dataset and code are publicly accessible for further\nresearch https://github.com/DataScienceUIBK/ArabicaQA.\n","authors":["Abdelrahman Abdallah","Mahmoud Kasem","Mahmoud Abdalla","Mohamed Mahmoud","Mohamed Elkasaby","Yasser Elbendary","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.17848v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2306.13186v3","updated":"2024-03-26T16:29:17Z","published":"2023-06-22T20:03:09Z","title":"A Decade of Scholarly Research on Open Knowledge Graphs","summary":"  The proliferation of open knowledge graphs has led to a surge in scholarly\nresearch on the topic over the past decade. This paper presents a bibliometric\nanalysis of the scholarly literature on open knowledge graphs published between\n2013 and 2023. The study aims to identify the trends, patterns, and impact of\nresearch in this field, as well as the key topics and research questions that\nhave emerged. The work uses bibliometric techniques to analyze a sample of 4445\nscholarly articles retrieved from Scopus. The findings reveal an\never-increasing number of publications on open knowledge graphs published every\nyear, particularly in developed countries (+50 per year). These outputs are\npublished in highly-referred scholarly journals and conferences. The study\nidentifies three main research themes: (1) knowledge graph construction and\nenrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into\nNLP systems. Within these themes, the study identifies specific tasks that have\nreceived considerable attention, including entity linking, knowledge graph\nembedding, and graph neural networks.\n","authors":["Houcemeddine Turki","Abraham Toluwase Owodunni","Mohamed Ali Hadj Taieb","René Fabrice Bile","Mohamed Ben Aouicha"],"pdf_url":"https://arxiv.org/pdf/2306.13186v3.pdf","comment":"Camera-ready edition for LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17780v1","updated":"2024-03-26T15:13:16Z","published":"2024-03-26T15:13:16Z","title":"CaseLink: Inductive Graph Learning for Legal Case Retrieval","summary":"  In case law, the precedents are the relevant cases that are used to support\nthe decisions made by the judges and the opinions of lawyers towards a given\ncase. This relevance is referred to as the case-to-case reference relation. To\nefficiently find relevant cases from a large case pool, retrieval tools are\nwidely used by legal practitioners. Existing legal case retrieval models mainly\nwork by comparing the text representations of individual cases. Although they\nobtain a decent retrieval accuracy, the intrinsic case connectivity\nrelationships among cases have not been well exploited for case encoding,\ntherefore limiting the further improvement of retrieval performance. In a case\npool, there are three types of case connectivity relationships: the case\nreference relationship, the case semantic relationship, and the case legal\ncharge relationship. Due to the inductive manner in the task of legal case\nretrieval, using case reference as input is not applicable for testing. Thus,\nin this paper, a CaseLink model based on inductive graph learning is proposed\nto utilise the intrinsic case connectivity for legal case retrieval, a novel\nGlobal Case Graph is incorporated to represent both the case semantic\nrelationship and the case legal charge relationship. A novel contrastive\nobjective with a regularisation on the degree of case nodes is proposed to\nleverage the information carried by the case reference relationship to optimise\nthe model. Extensive experiments have been conducted on two benchmark datasets,\nwhich demonstrate the state-of-the-art performance of CaseLink. The code has\nbeen released on https://github.com/yanran-tang/CaseLink.\n","authors":["Yanran Tang","Ruihong Qiu","Hongzhi Yin","Xue Li","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2403.17780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17759v1","updated":"2024-03-26T14:51:03Z","published":"2024-03-26T14:51:03Z","title":"TWOLAR: a TWO-step LLM-Augmented distillation method for passage\n  Reranking","summary":"  In this paper, we present TWOLAR: a two-stage pipeline for passage reranking\nbased on the distillation of knowledge from Large Language Models (LLM). TWOLAR\nintroduces a new scoring strategy and a distillation process consisting in the\ncreation of a novel and diverse training dataset. The dataset consists of 20K\nqueries, each associated with a set of documents retrieved via four distinct\nretrieval methods to ensure diversity, and then reranked by exploiting the\nzero-shot reranking capabilities of an LLM. Our ablation studies demonstrate\nthe contribution of each new component we introduced. Our experimental results\nshow that TWOLAR significantly enhances the document reranking ability of the\nunderlying model, matching and in some cases even outperforming\nstate-of-the-art models with three orders of magnitude more parameters on the\nTREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate\nfuture work we release our data set, finetuned models, and code.\n","authors":["Davide Baldelli","Junfeng Jiang","Akiko Aizawa","Paolo Torroni"],"pdf_url":"https://arxiv.org/pdf/2403.17759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17740v1","updated":"2024-03-26T14:29:34Z","published":"2024-03-26T14:29:34Z","title":"All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating\n  Prediction","summary":"  Cold-start rating prediction is a fundamental problem in recommender systems\nthat has been extensively studied. Many methods have been proposed that exploit\nexplicit relations among existing data, such as collaborative filtering, social\nrecommendations and heterogeneous information network, to alleviate the data\ninsufficiency issue for cold-start users and items. However, the explicit\nrelations constructed based on data between different roles may be unreliable\nand irrelevant, which limits the performance ceiling of the specific\nrecommendation task. Motivated by this, in this paper, we propose a flexible\nframework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not\nsolely rely on the pre-defined interaction pattern or the manually constructed\nheterogeneous information network. Instead, we devise a Heterogeneous\nInteraction Module (HIM) to jointly model the heterogeneous interactions and\ndirectly infer the important interactions via the observed data. In the\nexperiments, we evaluate our model under three cold-start settings on three\nreal-world datasets. The experimental results show that HIRE outperforms other\nbaselines by a large margin. Furthermore, we visualize the inferred\ninteractions of HIRE to confirm the contribution of our model.\n","authors":["Shuheng Fang","Kangfei Zhao","Yu Rong","Zhixun Li","Jeffrey Xu Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17740v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2309.17078v2","updated":"2024-03-26T14:27:59Z","published":"2023-09-29T09:14:53Z","title":"Unsupervised Large Language Model Alignment for Information Retrieval\n  via Contrastive Feedback","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious research domains, including the field of Information Retrieval (IR).\nHowever, the responses generated by off-the-shelf LLMs tend to be generic,\ni.e., cannot capture the distinctiveness of each document with similar content.\nThis limits the performance of LLMs in IR because finding and distinguishing\nrelevant documents from substantial similar documents is a typical problem in\nmany IR tasks. To address this issue, we propose an unsupervised alignment\nmethod, namely Reinforcement Learning from Contrastive Feedback (RLCF),\nempowering LLMs to generate both high-quality and context-specific responses.\nOur approach constructs unsupervised contrastive feedback signals based on\nsimilar document groups, and adopts a reward function, named group-wise\nreciprocal rank, to optimize LLMs within a standard Proximal Policy\nOptimization. We conduct extensive experiments to evaluate the effectiveness of\nRLCF on LLMs built with different languages and parameter sizes on multiple\ndownstream IR applications. RLCF significantly outperforms existing alignment\nmethods, and RLCF-optimized LLMs demonstrate considerable improvement in\ngenerating responses with distinctiveness.\n","authors":["Qian Dong","Yiding Liu","Qingyao Ai","Zhijing Wu","Haitao Li","Yiqun Liu","Shuaiqiang Wang","Dawei Yin","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2309.17078v2.pdf","comment":"Accepted by SIGIR24"},{"id":"http://arxiv.org/abs/2403.17729v1","updated":"2024-03-26T14:18:43Z","published":"2024-03-26T14:18:43Z","title":"EulerFormer: Sequential User Behavior Modeling with Complex Vector\n  Attention","summary":"  To capture user preference, transformer models have been widely applied to\nmodel sequential user behavior data. The core of transformer architecture lies\nin the self-attention mechanism, which computes the pairwise attention scores\nin a sequence. Due to the permutation-equivariant nature, positional encoding\nis used to enhance the attention between token representations. In this\nsetting, the pairwise attention scores can be derived by both semantic\ndifference and positional difference. However, prior studies often model the\ntwo kinds of difference measurements in different ways, which potentially\nlimits the expressive capacity of sequence modeling. To address this issue,\nthis paper proposes a novel transformer variant with complex vector attention,\nnamed EulerFormer, which provides a unified theoretical framework to formulate\nboth semantic difference and positional difference. The EulerFormer involves\ntwo key technical improvements. First, it employs a new transformation function\nfor efficiently transforming the sequence tokens into polar-form complex\nvectors using Euler's formula, enabling the unified modeling of both semantic\nand positional information in a complex rotation form.Secondly, it develops a\ndifferential rotation mechanism, where the semantic rotation angles can be\ncontrolled by an adaptation function, enabling the adaptive integration of the\nsemantic and positional information according to the semantic\ncontexts.Furthermore, a phase contrastive learning task is proposed to improve\nthe anisotropy of contextual representations in EulerFormer. Our theoretical\nframework possesses a high degree of completeness and generality. It is more\nrobust to semantic variations and possesses moresuperior theoretical properties\nin principle. Extensive experiments conducted on four public datasets\ndemonstrate the effectiveness and efficiency of our approach.\n","authors":["Zhen Tian","Wayne Xin Zhao","Changwang Zhang","Xin Zhao","Zhongrui Ma","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.17729v1.pdf","comment":"Accepted for publication in SIGIR'24"},{"id":"http://arxiv.org/abs/2403.17688v1","updated":"2024-03-26T13:31:33Z","published":"2024-03-26T13:31:33Z","title":"Large Language Models Enhanced Collaborative Filtering","summary":"  Recent advancements in Large Language Models (LLMs) have attracted\nconsiderable interest among researchers to leverage these models to enhance\nRecommender Systems (RSs). Existing work predominantly utilizes LLMs to\ngenerate knowledge-rich texts or utilizes LLM-derived embeddings as features to\nimprove RSs. Al- though the extensive world knowledge embedded in LLMs\ngenerally benefits RSs, the application can only take limited number of users\nand items as inputs, without adequately exploiting collaborative filtering\ninformation. Considering its crucial role in RSs, one key challenge in\nenhancing RSs with LLMs lies in providing better collaborative filtering\ninformation through LLMs. In this paper, drawing inspiration from the\nin-context learning and chain of thought reasoning in LLMs, we propose the\nLarge Language Models enhanced Collaborative Filtering (LLM-CF) framework,\nwhich distils the world knowledge and reasoning capabilities of LLMs into\ncollaborative filtering. We also explored a concise and efficient\ninstruction-tuning method, which improves the recommendation capabilities of\nLLMs while preserving their general functionalities (e.g., not decreasing on\nthe LLM benchmark). Comprehensive experiments on three real-world datasets\ndemonstrate that LLM-CF significantly enhances several backbone recommendation\nmodels and consistently outperforms competitive baselines, showcasing its\neffectiveness in distilling the world knowledge and reasoning capabilities of\nLLM into collaborative filtering.\n","authors":["Zhongxiang Sun","Zihua Si","Xiaoxue Zang","Kai Zheng","Yang Song","Xiao Zhang","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2403.17688v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.16915v2","updated":"2024-03-26T13:11:44Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17643v1","updated":"2024-03-26T12:23:34Z","published":"2024-03-26T12:23:34Z","title":"S+t-SNE - Bringing dimensionality reduction to data streams","summary":"  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle\ninfinite data streams. The core idea behind S+t-SNE is to update the t-SNE\nembedding incrementally as new data arrives, ensuring scalability and\nadaptability to handle streaming scenarios. By selecting the most important\npoints at each step, the algorithm ensures scalability while keeping\ninformative visualisations. Employing a blind method for drift management\nadjusts the embedding space, facilitating continuous visualisation of evolving\ndata dynamics. Our experimental evaluations demonstrate the effectiveness and\nefficiency of S+t-SNE. The results highlight its ability to capture patterns in\na streaming scenario. We hope our approach offers researchers and practitioners\na real-time tool for understanding and interpreting high-dimensional data.\n","authors":["Pedro C. Vieira","João P. Montrezol","João T. Vieira","João Gama"],"pdf_url":"https://arxiv.org/pdf/2403.17643v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. We will soon add a link to the final version of\n  this contribution that underwent peer-review and post-acceptance improvements\n  and was presented at IDA2024 (https://ida2024.org/)"},{"id":"http://arxiv.org/abs/2403.17634v1","updated":"2024-03-26T12:08:58Z","published":"2024-03-26T12:08:58Z","title":"Retentive Decision Transformer with Adaptive Masking for Reinforcement\n  Learning based Recommendation Systems","summary":"  Reinforcement Learning-based Recommender Systems (RLRS) have shown promise\nacross a spectrum of applications, from e-commerce platforms to streaming\nservices. Yet, they grapple with challenges, notably in crafting reward\nfunctions and harnessing large pre-existing datasets within the RL framework.\nRecent advancements in offline RLRS provide a solution for how to address these\ntwo challenges. However, existing methods mainly rely on the transformer\narchitecture, which, as sequence lengths increase, can introduce challenges\nassociated with computational resources and training costs. Additionally, the\nprevalent methods employ fixed-length input trajectories, restricting their\ncapacity to capture evolving user preferences. In this study, we introduce a\nnew offline RLRS method to deal with the above problems. We reinterpret the\nRLRS challenge by modeling sequential decision-making as an inference task,\nleveraging adaptive masking configurations. This adaptive approach selectively\nmasks input tokens, transforming the recommendation task into an inference\nchallenge based on varying token subsets, thereby enhancing the agent's ability\nto infer across diverse trajectory lengths. Furthermore, we incorporate a\nmulti-scale segmented retention mechanism that facilitates efficient modeling\nof long sequences, significantly enhancing computational efficiency. Our\nexperimental analysis, conducted on both online simulator and offline datasets,\nclearly demonstrates the advantages of our proposed method.\n","authors":["Siyu Wang","Xiaocong Chen","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17603v1","updated":"2024-03-26T11:28:31Z","published":"2024-03-26T11:28:31Z","title":"END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential\n  Recommendation","summary":"  In recommendation systems, users frequently engage in multiple types of\nbehaviors, such as clicking, adding to a cart, and purchasing. However, with\ndiversified behavior data, user behavior sequences will become very long in the\nshort term, which brings challenges to the efficiency of the sequence\nrecommendation model. Meanwhile, some behavior data will also bring inevitable\nnoise to the modeling of user interests. To address the aforementioned issues,\nfirstly, we develop the Efficient Behavior Sequence Miner (EBM) that\nefficiently captures intricate patterns in user behavior while maintaining low\ntime complexity and parameter count. Secondly, we design hard and soft\ndenoising modules for different noise types and fully explore the relationship\nbetween behaviors and noise. Finally, we introduce a contrastive loss function\nalong with a guided training strategy to compare the valid information in the\ndata with the noisy signal, and seamlessly integrate the two denoising\nprocesses to achieve a high degree of decoupling of the noisy signal.\nSufficient experiments on real-world datasets demonstrate the effectiveness and\nefficiency of our approach in dealing with multi-behavior sequential\nrecommendation.\n","authors":["Yongqiang Han","Hao Wang","Kefan Wang","Likang Wu","Zhi Li","Wei Guo","Yong Liu","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.17603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10909v2","updated":"2024-03-26T09:26:32Z","published":"2023-01-26T02:38:59Z","title":"Optimizing Feature Set for Click-Through Rate Prediction","summary":"  Click-through prediction (CTR) models transform features into latent vectors\nand enumerate possible feature interactions to improve performance based on the\ninput feature set. Therefore, when selecting an optimal feature set, we should\nconsider the influence of both feature and its interaction. However, most\nprevious works focus on either feature field selection or only select feature\ninteraction based on the fixed feature set to produce the feature set. The\nformer restricts search space to the feature field, which is too coarse to\ndetermine subtle features. They also do not filter useless feature\ninteractions, leading to higher computation costs and degraded model\nperformance. The latter identifies useful feature interaction from all\navailable features, resulting in many redundant features in the feature set. In\nthis paper, we propose a novel method named OptFS to address these problems. To\nunify the selection of feature and its interaction, we decompose the selection\nof each feature interaction into the selection of two correlated features. Such\na decomposition makes the model end-to-end trainable given various feature\ninteraction operations. By adopting feature-level search space, we set a\nlearnable gate to determine whether each feature should be within the feature\nset. Because of the large-scale search space, we develop a\nlearning-by-continuation training scheme to learn such gates. Hence, OptFS\ngenerates the feature set only containing features which improve the final\nprediction results. Experimentally, we evaluate OptFS on three public datasets,\ndemonstrating OptFS can optimize feature sets which enhance the model\nperformance and further reduce both the storage and computational cost.\n","authors":["Fuyuan Lyu","Xing Tang","Dugang Liu","Liang Chen","Xiuqiang He","Xue Liu"],"pdf_url":"https://arxiv.org/pdf/2301.10909v2.pdf","comment":"Accepted by WWW 2023 Research Tracks"},{"id":"http://arxiv.org/abs/2311.08744v2","updated":"2024-03-26T08:14:22Z","published":"2023-11-15T07:25:14Z","title":"Graph Signal Diffusion Model for Collaborative Filtering","summary":"  Collaborative filtering is a critical technique in recommender systems. Among\nvarious methods, an increasingly popular paradigm is to reconstruct user-item\ninteractions based on the historical observations. This can be viewed as a\nconditional generative task, where recently developed diffusion model\ndemonstrates great potential. However, existing studies on diffusion models\nlack effective solutions for modeling implicit feedback data. Particularly, the\nisotropic nature of the standard diffusion process fails to account for the\nheterogeneous dependencies among items, leading to a misalignment with the\ngraphical structure of the interaction space. Meanwhile, random noise\ndestroying personalized information in interaction vectors, causing difficulty\nin reverse reconstruction. In this paper, we make novel adaptions of diffusion\nmodel and propose Graph Signal Diffusion Model for Collaborative Filtering\n(named GiffCF). To better represent the high-dimensional and sparse\ndistribution of implicit feedback, we define a generalized form of denoising\ndiffusion using heat equation on the item-item similarity graph. Our forward\nprocess smooths interaction signals with an advanced family of graph filters.\nHence, instead of losing information, it involves item-item similarities as\nbeneficial prior knowledge for recommendation. To reconstruct high-quality\ninteractions, our reverse process iteratively refines and sharpens preference\nsignals in a deterministic manner, where the update direction is conditioned on\nthe user history and computed from a carefully designed two-stage denoiser.\nFinally, through extensive experiments, we show that GiffCF effectively\nleverages the advantages of both diffusion model and graph signal processing,\nand achieves state-of-the-art performance on three benchmark datasets.\n","authors":["Yunqin Zhu","Chao Wang","Qi Zhang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2311.08744v2.pdf","comment":"11 pages, 8 figures, Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.17473v1","updated":"2024-03-26T08:07:33Z","published":"2024-03-26T08:07:33Z","title":"Document Set Expansion with Positive-Unlabelled Learning Using\n  Intractable Density Estimation","summary":"  The Document Set Expansion (DSE) task involves identifying relevant documents\nfrom large collections based on a limited set of example documents. Previous\nresearch has highlighted Positive and Unlabeled (PU) learning as a promising\napproach for this task. However, most PU methods rely on the unrealistic\nassumption of knowing the class prior for positive samples in the collection.\nTo address this limitation, this paper introduces a novel PU learning framework\nthat utilizes intractable density estimation models. Experiments conducted on\nPubMed and Covid datasets in a transductive setting showcase the effectiveness\nof the proposed method for DSE. Code is available from\nhttps://github.com/Beautifuldog01/Document-set-expansion-puDE.\n","authors":["Haiyang Zhang","Qiuyi Chen","Yuanjie Zou","Yushan Pan","Jia Wang","Mark Stevenson"],"pdf_url":"https://arxiv.org/pdf/2403.17473v1.pdf","comment":"Accepted at LREC-COLING 2024. arXiv admin note: text overlap with\n  arXiv:2401.11145"},{"id":"http://arxiv.org/abs/2403.17442v1","updated":"2024-03-26T07:19:26Z","published":"2024-03-26T07:19:26Z","title":"Touch the Core: Exploring Task Dependence Among Hybrid Targets for\n  Recommendation","summary":"  As user behaviors become complicated on business platforms, online\nrecommendations focus more on how to touch the core conversions, which are\nhighly related to the interests of platforms. These core conversions are\nusually continuous targets, such as \\textit{watch time}, \\textit{revenue}, and\nso on, whose predictions can be enhanced by previous discrete conversion\nactions. Therefore, multi-task learning (MTL) can be adopted as the paradigm to\nlearn these hybrid targets. However, existing works mainly emphasize\ninvestigating the sequential dependence among discrete conversion actions,\nwhich neglects the complexity of dependence between discrete conversions and\nthe final continuous conversion. Moreover, simultaneously optimizing hybrid\ntasks with stronger task dependence will suffer from volatile issues where the\ncore regression task might have a larger influence on other tasks. In this\npaper, we study the MTL problem with hybrid targets for the first time and\npropose the model named Hybrid Targets Learning Network (HTLNet) to explore\ntask dependence and enhance optimization. Specifically, we introduce label\nembedding for each task to explicitly transfer the label information among\nthese tasks, which can effectively explore logical task dependence. We also\nfurther design the gradient adjustment regime between the final regression task\nand other classification tasks to enhance the optimization. Extensive\nexperiments on two offline public datasets and one real-world industrial\ndataset are conducted to validate the effectiveness of HTLNet. Moreover, online\nA/B tests on the financial recommender system also show our model has superior\nimprovement.\n","authors":["Xing Tang","Yang Qiao","Fuyuan Lyu","Dugang Liu","Xiuqiang He"],"pdf_url":"https://arxiv.org/pdf/2403.17442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.01262v2","updated":"2024-03-26T06:54:43Z","published":"2022-07-04T08:54:43Z","title":"Understanding Performance of Long-Document Ranking Models through\n  Comprehensive Evaluation and Leaderboarding","summary":"  We evaluated 20+ Transformer models for ranking of long documents (including\nrecent LongP models trained with FlashAttention) and compared them with simple\nFirstP baselines (applying the same model to input truncated to the first 512\ntokens). We used MS MARCO Documents v1 as a primary training set and evaluated\nmodels in the zero-shot scenario as well as after fine-tuning on other\ncollections.\n  In our initial experiments with standard collections we found that\nlong-document models underperformed FirstP or outperformed it by at most 5% on\naverage in terms of MRR or NDCG. We then conjectured that this was not due to\nmodels inability to process long context but rather due to a positional bias of\nrelevant passages, which tended to be among the first 512 document tokens. We\nfound evidence that this bias was, indeed, present in at least two test sets,\nwhich motivated us to create a new collection MS MARCO FarRelevant where the\nrelevant passages were not present among the first 512 tokens.\n  Unlike standard collections where we observed both little benefit from\nincorporating longer contexts and limited variability in model performance\n(within a few %), experiments on MS MARCO FarRelevant uncovered dramatic\ndifferences among models. FirstP models performed roughly at the\nrandom-baseline level in both zero-shot and fine-tuning scenarios. Simple\naggregation models (e.g., MaxP) had good zero-shot accuracy but benefited\nlittle from fine-tuning. Most other models had poor zero-shot performance\n(sometimes at a random baseline level) but outstripped MaxP by as much 13-28\\%\nafter finetuning. Thus, positional bias not only diminishes benefits of\nprocessing longer document contexts but also leads to model overfitting to this\nbias and performing poorly in a zero-shot setting when a distribution of\nrelevant passages changes substantially.\n  We make our software and MS MARCO FarRelevant available.\n","authors":["Leonid Boytsov","David Akinpelu","Tianyi Lin","Fangwei Gao","Yutian Zhao","Jeffrey Huang","Eric Nyberg"],"pdf_url":"https://arxiv.org/pdf/2207.01262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17425v1","updated":"2024-03-26T06:42:23Z","published":"2024-03-26T06:42:23Z","title":"Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion\n  Rate Prediction with a Single Model","summary":"  In real-world advertising systems, conversions have different types in nature\nand ads can be shown in different display scenarios, both of which highly\nimpact the actual conversion rate (CVR). This results in the multi-type and\nmulti-scenario CVR prediction problem. A desired model for this problem should\nsatisfy the following requirements: 1) Accuracy: the model should achieve\nfine-grained accuracy with respect to any conversion type in any display\nscenario. 2) Scalability: the model parameter size should be affordable. 3)\nConvenience: the model should not require a large amount of effort in data\npartitioning, subset processing and separate storage. Existing approaches\ncannot simultaneously satisfy these requirements. For example, building a\nseparate model for each (conversion type, display scenario) pair is neither\nscalable nor convenient. Building a unified model trained on all the data with\nconversion type and display scenario included as two features is not accurate\nenough. In this paper, we propose the Masked Multi-domain Network (MMN) to\nsolve this problem. To achieve the accuracy requirement, we model\ndomain-specific parameters and propose a dynamically weighted loss to account\nfor the loss scale imbalance issue within each mini-batch. To achieve the\nscalability requirement, we propose a parameter sharing and composition\nstrategy to reduce model parameters from a product space to a sum space. To\nachieve the convenience requirement, we propose an auto-masking strategy which\ncan take mixed data from all the domains as input. It avoids the overhead\ncaused by data partitioning, individual processing and separate storage. Both\noffline and online experimental results validate the superiority of MMN for\nmulti-type and multi-scenario CVR prediction. MMN is now the serving model for\nreal-time CVR prediction in UC Toutiao.\n","authors":["Wentao Ouyang","Xiuwu Zhang","Chaofeng Guo","Shukui Ren","Yupei Sui","Kun Zhang","Jinmei Luo","Yunfeng Chen","Dongbo Xu","Xiangzheng Liu","Yanlong Du"],"pdf_url":"https://arxiv.org/pdf/2403.17425v1.pdf","comment":"CIKM 2023 (larger figures)"},{"id":"http://arxiv.org/abs/2403.17421v1","updated":"2024-03-26T06:34:23Z","published":"2024-03-26T06:34:23Z","title":"MA4DIV: Multi-Agent Reinforcement Learning for Search Result\n  Diversification","summary":"  The objective of search result diversification (SRD) is to ensure that\nselected documents cover as many different subtopics as possible. Existing\nmethods primarily utilize a paradigm of \"greedy selection\", i.e., selecting one\ndocument with the highest diversity score at a time. These approaches tend to\nbe inefficient and are easily trapped in a suboptimal state. In addition, some\nother methods aim to approximately optimize the diversity metric, such as\n$\\alpha$-NDCG, but the results still remain suboptimal. To address these\nchallenges, we introduce Multi-Agent reinforcement learning (MARL) for search\nresult DIVersity, which called MA4DIV. In this approach, each document is an\nagent and the search result diversification is modeled as a cooperative task\namong multiple agents. This approach allows for directly optimizing the\ndiversity metrics, such as $\\alpha$-NDCG, while achieving high training\nefficiency. We conducted preliminary experiments on public TREC datasets to\ndemonstrate the effectiveness and potential of MA4DIV. Considering the limited\nnumber of queries in public TREC datasets, we construct a large-scale dataset\nfrom industry sources and show that MA4DIV achieves substantial improvements in\nboth effectiveness and efficiency than existing baselines on a industrial scale\ndataset.\n","authors":["Yiqun Chen","Jiaxin Mao","Yi Zhang","Dehong MA","Long Xia","Jun Fan","Daiting Shi","Zhicong Cheng","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2403.17421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17416v1","updated":"2024-03-26T06:14:19Z","published":"2024-03-26T06:14:19Z","title":"AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering\n  for Recommendations","summary":"  Collaborative filtering methods based on graph neural networks (GNNs) have\nwitnessed significant success in recommender systems (RS), capitalizing on\ntheir ability to capture collaborative signals within intricate user-item\nrelationships via message-passing mechanisms. However, these GNN-based RS\ninadvertently introduce excess linear correlation between user and item\nembeddings, contradicting the goal of providing personalized recommendations.\nWhile existing research predominantly ascribes this flaw to the over-smoothing\nproblem, this paper underscores the critical, often overlooked role of the\nover-correlation issue in diminishing the effectiveness of GNN representations\nand subsequent recommendation performance. Up to now, the over-correlation\nissue remains unexplored in RS. Meanwhile, how to mitigate the impact of\nover-correlation while preserving collaborative filtering signals is a\nsignificant challenge. To this end, this paper aims to address the\naforementioned gap by undertaking a comprehensive study of the over-correlation\nissue in graph collaborative filtering models. Firstly, we present empirical\nevidence to demonstrate the widespread prevalence of over-correlation in these\nmodels. Subsequently, we dive into a theoretical analysis which establishes a\npivotal connection between the over-correlation and over-smoothing issues.\nLeveraging these insights, we introduce the Adaptive Feature De-correlation\nGraph Collaborative Filtering (AFDGCF) framework, which dynamically applies\ncorrelation penalties to the feature dimensions of the representation matrix,\neffectively alleviating both over-correlation and over-smoothing issues. The\nefficacy of the proposed framework is corroborated through extensive\nexperiments conducted with four representative graph collaborative filtering\nmodels across four publicly available datasets.\n","authors":["Wei Wu","Chao Wang","Dazhong Shen","Chuan Qin","Liyi Chen","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2403.17416v1.pdf","comment":"Accepted by SIGIR2024"},{"id":"http://arxiv.org/abs/2403.17374v1","updated":"2024-03-26T04:30:40Z","published":"2024-03-26T04:30:40Z","title":"Multi-Domain Recommendation to Attract Users via Domain Preference\n  Modeling","summary":"  Recently, web platforms have been operating various service domains\nsimultaneously. Targeting a platform that operates multiple service domains, we\nintroduce a new task, Multi-Domain Recommendation to Attract Users (MDRAU),\nwhich recommends items from multiple ``unseen'' domains with which each user\nhas not interacted yet, by using knowledge from the user's ``seen'' domains. In\nthis paper, we point out two challenges of MDRAU task. First, there are\nnumerous possible combinations of mappings from seen to unseen domains because\nusers have usually interacted with a different subset of service domains.\nSecond, a user might have different preferences for each of the target unseen\ndomains, which requires that recommendations reflect the user's preferences on\ndomains as well as items. To tackle these challenges, we propose DRIP framework\nthat models users' preferences at two levels (i.e., domain and item) and learns\nvarious seen-unseen domain mappings in a unified way with masked domain\nmodeling. Our extensive experiments demonstrate the effectiveness of DRIP in\nMDRAU task and its ability to capture users' domain-level preferences.\n","authors":["Hyuunjun Ju","SeongKu Kang","Dongha Lee","Junyoung Hwang","Sanghwan Jang","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2403.17374v1.pdf","comment":"Accepted to AAAI'24"},{"id":"http://arxiv.org/abs/2403.17372v1","updated":"2024-03-26T04:16:57Z","published":"2024-03-26T04:16:57Z","title":"An Empirical Study of Training ID-Agnostic Multi-modal Sequential\n  Recommenders","summary":"  Sequential Recommendation (SR) aims to predict future user-item interactions\nbased on historical interactions. While many SR approaches concentrate on user\nIDs and item IDs, the human perception of the world through multi-modal\nsignals, like text and images, has inspired researchers to delve into\nconstructing SR from multi-modal information without using IDs. However, the\ncomplexity of multi-modal learning manifests in diverse feature extractors,\nfusion methods, and pre-trained models. Consequently, designing a simple and\nuniversal \\textbf{M}ulti-\\textbf{M}odal \\textbf{S}equential\n\\textbf{R}ecommendation (\\textbf{MMSR}) framework remains a formidable\nchallenge. We systematically summarize the existing multi-modal related SR\nmethods and distill the essence into four core components: visual encoder, text\nencoder, multimodal fusion module, and sequential architecture. Along these\ndimensions, we dissect the model designs, and answer the following\nsub-questions: First, we explore how to construct MMSR from scratch, ensuring\nits performance either on par with or exceeds existing SR methods without\ncomplex techniques. Second, we examine if MMSR can benefit from existing\nmulti-modal pre-training paradigms. Third, we assess MMSR's capability in\ntackling common challenges like cold start and domain transferring. Our\nexperiment results across four real-world recommendation scenarios demonstrate\nthe great potential ID-agnostic multi-modal sequential recommendation. Our\nframework can be found at: https://github.com/MMSR23/MMSR.\n","authors":["Youhua Li","Hanwen Du","Yongxin Ni","Yuanqi He","Junchen Fu","Xiangyan Liu","Qi Guo"],"pdf_url":"https://arxiv.org/pdf/2403.17372v1.pdf","comment":"An Empirical Study of Training ID-Agnostic Multi-modal Sequential\n  Recommenders"},{"id":"http://arxiv.org/abs/2403.09963v2","updated":"2024-03-26T04:08:47Z","published":"2024-03-15T02:04:35Z","title":"Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias\n  in Factual Knowledge Extraction","summary":"  Recent research shows that pre-trained language models (PLMs) suffer from\n\"prompt bias\" in factual knowledge extraction, i.e., prompts tend to introduce\nbiases toward specific labels. Prompt bias presents a significant challenge in\nassessing the factual knowledge within PLMs. Therefore, this paper aims to\nimprove the reliability of existing benchmarks by thoroughly investigating and\nmitigating prompt bias. We show that: 1) all prompts in the experiments exhibit\nnon-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt\ndisplaying significantly higher levels of bias; 2) prompt bias can amplify\nbenchmark accuracy unreasonably by overfitting the test datasets, especially on\nimbalanced datasets like LAMA. Based on these findings, we propose a\nrepresentation-based approach to mitigate the prompt bias during inference\ntime. Specifically, we first estimate the biased representation using\nprompt-only querying, and then remove it from the model's internal\nrepresentations to generate the debiased representations, which are used to\nproduce the final debiased outputs. Experiments across various prompts, PLMs,\nand benchmarks show that our approach can not only correct the overfitted\nperformance caused by prompt bias, but also significantly improve the prompt\nretrieval capability (up to 10% absolute performance gain). These results\nindicate that our approach effectively alleviates prompt bias in knowledge\nevaluation, thereby enhancing the reliability of benchmark assessments.\nHopefully, our plug-and-play approach can be a golden standard to strengthen\nPLMs toward reliable knowledge bases. Code and data are released in\nhttps://github.com/FelliYang/PromptBias.\n","authors":["Ziyang Xu","Keqin Peng","Liang Ding","Dacheng Tao","Xiliang Lu"],"pdf_url":"https://arxiv.org/pdf/2403.09963v2.pdf","comment":"Accepted by COLING 2024"},{"id":"http://arxiv.org/abs/2112.06460v5","updated":"2024-03-26T03:44:29Z","published":"2021-12-13T07:33:28Z","title":"Improving Sequential Recommendations via Bidirectional Temporal Data\n  Augmentation with Pre-training","summary":"  Sequential recommendation systems are integral to discerning temporal user\npreferences. Yet, the task of learning from abbreviated user interaction\nsequences poses a notable challenge. Data augmentation has been identified as a\npotent strategy to enhance the informational richness of these sequences.\nTraditional augmentation techniques, such as item randomization, may disrupt\nthe inherent temporal dynamics. Although recent advancements in reverse\nchronological pseudo-item generation have shown promise, they can introduce\ntemporal discrepancies when assessed in a natural chronological context. In\nresponse, we introduce a sophisticated approach, Bidirectional temporal data\nAugmentation with pre-training (BARec). Our approach leverages bidirectional\ntemporal augmentation and knowledge-enhanced fine-tuning to synthesize\nauthentic pseudo-prior items that \\emph{retain user preferences and capture\ndeeper item semantic correlations}, thus boosting the model's expressive power.\nOur comprehensive experimental analysis confirms the superiority of BARec\nacross both short and elongated sequence contexts. Moreover, theoretical\nexamination and visual representation of item embeddings offer further insight\ninto the model's logical processes and interpretability. The source code for\nour study is available at\n\\textcolor{blue}{\\href{https://github.com/juyongjiang/BARec}{https://github.com/juyongjiang/BARec}}.\n","authors":["Juyong Jiang","Peiyan Zhang","Yingtao Luo","Chaozhuo Li","Jaeboum Kim","Kai Zhang","Senzhang Wang","Sunghun Kim"],"pdf_url":"https://arxiv.org/pdf/2112.06460v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17286v1","updated":"2024-03-26T00:29:52Z","published":"2024-03-26T00:29:52Z","title":"Cognitively Biased Users Interacting with Algorithmically Biased Results\n  in Whole-Session Search on Controversial Topics","summary":"  When interacting with information retrieval (IR) systems, users, affected by\nconfirmation biases, tend to select search results that confirm their existing\nbeliefs on socially significant contentious issues. To understand the judgments\nand attitude changes of users searching online, our study examined how\ncognitively biased users interact with algorithmically biased search engine\nresult pages (SERPs). We designed three-query search sessions on debated topics\nunder various bias conditions. We recruited 1,321 crowdsourcing participants\nand explored their attitude changes, search interactions, and the effects of\nconfirmation bias. Three key findings emerged: 1) most attitude changes occur\nin the initial query of a search session; 2) confirmation bias and result\npresentation on SERPs affect search behaviors in the current query and\nperceived familiarity with clicked results in subsequent queries. The bias\nposition also affect attitude changes of users with lower perceived openness to\nconflicting opinions; 3) Interactions in the first query and and dwell time\nthroughout the session are associated with users' attitude changes in different\nforms. Our study goes beyond traditional simulation-based evaluation settings\nand simulated rational users, sheds light on the mixed effects of human biases\nand algorithmic biases in controversial information retrieval tasks, and can\ninform the design of bias-aware user models, human-centered bias mitigation\ntechniques, and socially responsible intelligent IR systems.\n","authors":["Ben Wang","Jiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18025v1","updated":"2024-03-26T18:23:16Z","published":"2024-03-26T18:23:16Z","title":"Improving Pre-trained Language Model Sensitivity via Mask Specific\n  losses: A case study on Biomedical NER","summary":"  Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.\n","authors":["Micheal Abaho","Danushka Bollegala","Gary Leeming","Dan Joyce","Iain E Buchan"],"pdf_url":"https://arxiv.org/pdf/2403.18025v1.pdf","comment":"Paper alrerady accepted for publishing by the NAACL 2024 conference\n  (main conference paper)"},{"id":"http://arxiv.org/abs/2403.17688v1","updated":"2024-03-26T13:31:33Z","published":"2024-03-26T13:31:33Z","title":"Large Language Models Enhanced Collaborative Filtering","summary":"  Recent advancements in Large Language Models (LLMs) have attracted\nconsiderable interest among researchers to leverage these models to enhance\nRecommender Systems (RSs). Existing work predominantly utilizes LLMs to\ngenerate knowledge-rich texts or utilizes LLM-derived embeddings as features to\nimprove RSs. Although the extensive world knowledge embedded in LLMs generally\nbenefits RSs, the application can only take limited number of users and items\nas inputs, without adequately exploiting collaborative filtering information.\nConsidering its crucial role in RSs, one key challenge in enhancing RSs with\nLLMs lies in providing better collaborative filtering information through LLMs.\nIn this paper, drawing inspiration from the in-context learning and chain of\nthought reasoning in LLMs, we propose the Large Language Models enhanced\nCollaborative Filtering (LLM-CF) framework, which distils the world knowledge\nand reasoning capabilities of LLMs into collaborative filtering. We also\nexplored a concise and efficient instruction-tuning method, which improves the\nrecommendation capabilities of LLMs while preserving their general\nfunctionalities (e.g., not decreasing on the LLM benchmark). Comprehensive\nexperiments on three real-world datasets demonstrate that LLM-CF significantly\nenhances several backbone recommendation models and consistently outperforms\ncompetitive baselines, showcasing its effectiveness in distilling the world\nknowledge and reasoning capabilities of LLM into collaborative filtering.\n","authors":["Zhongxiang Sun","Zihua Si","Xiaoxue Zang","Kai Zheng","Yang Song","Xiao Zhang","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2403.17688v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.17643v1","updated":"2024-03-26T12:23:34Z","published":"2024-03-26T12:23:34Z","title":"S+t-SNE -- Bringing dimensionality reduction to data streams","summary":"  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle\ninfinite data streams. The core idea behind S+t-SNE is to update the t-SNE\nembedding incrementally as new data arrives, ensuring scalability and\nadaptability to handle streaming scenarios. By selecting the most important\npoints at each step, the algorithm ensures scalability while keeping\ninformative visualisations. Employing a blind method for drift management\nadjusts the embedding space, facilitating continuous visualisation of evolving\ndata dynamics. Our experimental evaluations demonstrate the effectiveness and\nefficiency of S+t-SNE. The results highlight its ability to capture patterns in\na streaming scenario. We hope our approach offers researchers and practitioners\na real-time tool for understanding and interpreting high-dimensional data.\n","authors":["Pedro C. Vieira","João P. Montrezol","João T. Vieira","João Gama"],"pdf_url":"https://arxiv.org/pdf/2403.17643v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. We will soon add a link to the final version of\n  this contribution that underwent peer-review and post-acceptance improvements\n  and was presented at IDA2024 (https://ida2024.org/)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2402.13452v2","updated":"2024-03-26T17:59:14Z","published":"2024-02-21T01:11:28Z","title":"LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based\n  on Twitter Data","summary":"  Prior research on Twitter (now X) data has provided positive evidence of its\nutility in developing supplementary health surveillance systems. In this study,\nwe present a new framework to surveil public health, focusing on mental health\n(MH) outcomes. We hypothesize that locally posted tweets are indicative of\nlocal MH outcomes and collect tweets posted from 765 neighborhoods (census\nblock groups) in the USA. We pair these tweets from each neighborhood with the\ncorresponding MH outcome reported by the Center for Disease Control (CDC) to\ncreate a benchmark dataset, LocalTweets. With LocalTweets, we present the first\npopulation-level evaluation task for Twitter-based MH surveillance systems. We\nthen develop an efficient and effective method, LocalHealth, for predicting MH\noutcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the\nhighest F1-score and accuracy of 0.7429 and 79.78\\%, respectively, a 59\\%\nimprovement in F1-score over the GPT3.5 in zero-shot setting. We also utilize\nLocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,\nachieving an F1-score of 0.7291. Our work suggests that Twitter data can be\neffectively leveraged to simulate neighborhood-level MH outcomes.\n","authors":["Vijeta Deshpande","Minhwa Lee","Zonghai Yao","Zihao Zhang","Jason Brian Gibbons","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.13452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08763v3","updated":"2024-03-26T17:58:48Z","published":"2024-03-13T17:58:57Z","title":"Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models","summary":"  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.\n","authors":["Adam Ibrahim","Benjamin Thérien","Kshitij Gupta","Mats L. Richter","Quentin Anthony","Timothée Lesort","Eugene Belilovsky","Irina Rish"],"pdf_url":"https://arxiv.org/pdf/2403.08763v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17933v1","updated":"2024-03-26T17:58:29Z","published":"2024-03-26T17:58:29Z","title":"SLEDGE: Synthesizing Simulation Environments for Driving Agents with\n  Generative Models","summary":"  SLEDGE is the first generative simulator for vehicle motion planning trained\non real-world driving logs. Its core component is a learned model that is able\nto generate agent bounding boxes and lane graphs. The model's outputs serve as\nan initial state for traffic simulation. The unique properties of the entities\nto be generated for SLEDGE, such as their connectivity and variable count per\nscene, render the naive application of most modern generative models to this\ntask non-trivial. Therefore, together with a systematic study of existing lane\ngraph representations, we introduce a novel raster-to-vector autoencoder\n(RVAE). It encodes agents and the lane graph into distinct channels in a\nrasterized latent map. This facilitates both lane-conditioned agent generation\nand combined generation of lanes and agents with a Diffusion Transformer. Using\ngenerated entities in SLEDGE enables greater control over the simulation, e.g.\nupsampling turns or increasing traffic density. Further, SLEDGE can support\n500m long routes, a capability not found in existing data-driven simulators\nlike nuPlan. It presents new challenges for planning algorithms, evidenced by\nfailure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,\nwhen tested on hard routes and dense traffic generated by our model. Compared\nto nuPlan, SLEDGE requires 500$\\times$ less storage to set up (<4GB), making it\na more accessible option and helping with democratizing future research in this\nfield.\n","authors":["Kashyap Chitta","Daniel Dauner","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2403.17933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17921v1","updated":"2024-03-26T17:55:58Z","published":"2024-03-26T17:55:58Z","title":"The Need for Speed: Pruning Transformers with One Recipe","summary":"  We introduce the $\\textbf{O}$ne-shot $\\textbf{P}$runing $\\textbf{T}$echnique\nfor $\\textbf{I}$nterchangeable $\\textbf{N}$etworks ($\\textbf{OPTIN}$) framework\nas a tool to increase the efficiency of pre-trained transformer architectures\n$\\textit{without requiring re-training}$. Recent works have explored improving\ntransformer efficiency, however often incur computationally expensive\nre-training procedures or depend on architecture-specific characteristics, thus\nimpeding practical wide-scale adoption. To address these shortcomings, the\nOPTIN framework leverages intermediate feature distillation, capturing the\nlong-range dependencies of model parameters (coined $\\textit{trajectory}$), to\nproduce state-of-the-art results on natural language, image classification,\ntransfer learning, and semantic segmentation tasks $\\textit{without\nre-training}$. Given a FLOP constraint, the OPTIN framework will compress the\nnetwork while maintaining competitive accuracy performance and improved\nthroughput. Particularly, we show a $\\leq 2$% accuracy degradation from NLP\nbaselines and a $0.5$% improvement from state-of-the-art methods on image\nclassification at competitive FLOPs reductions. We further demonstrate the\ngeneralization of tasks and architecture with comparative performance using\nMask2Former for semantic segmentation and cnn-style networks. OPTIN presents\none of the first one-shot efficient frameworks for compressing transformer\narchitectures that generalizes well across different class domains, in\nparticular: natural language and image-related tasks, without\n$\\textit{re-training}$.\n","authors":["Samir Khaki","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2403.17921v1.pdf","comment":"Accepted in the International Conference on Learning Representations\n  (ICLR) 2024"},{"id":"http://arxiv.org/abs/2403.17919v1","updated":"2024-03-26T17:55:02Z","published":"2024-03-26T17:55:02Z","title":"LISA: Layerwise Importance Sampling for Memory-Efficient Large Language\n  Model Fine-Tuning","summary":"  The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.\n","authors":["Rui Pan","Xiang Liu","Shizhe Diao","Renjie Pi","Jipeng Zhang","Chi Han","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17916v1","updated":"2024-03-26T17:53:27Z","published":"2024-03-26T17:53:27Z","title":"CMP: Cooperative Motion Prediction with Multi-Agent Communication","summary":"  The confluence of the advancement of Autonomous Vehicles (AVs) and the\nmaturity of Vehicle-to-Everything (V2X) communication has enabled the\ncapability of cooperative connected and automated vehicles (CAVs). Building on\ntop of cooperative perception, this paper explores the feasibility and\neffectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR\nsignals as input to enhance tracking and prediction capabilities. Unlike\nprevious work that focuses separately on either cooperative perception or\nmotion prediction, our framework, to the best of our knowledge, is the first to\naddress the unified problem where CAVs share information in both perception and\nprediction modules. Incorporated into our design is the unique capability to\ntolerate realistic V2X bandwidth limitations and transmission delays, while\ndealing with bulky perception representations. We also propose a prediction\naggregation module, which unifies the predictions obtained by different CAVs\nand generates the final prediction. Through extensive experiments and ablation\nstudies, we demonstrate the effectiveness of our method in cooperative\nperception, tracking, and motion prediction tasks. In particular, CMP reduces\nthe average prediction error by 17.2\\% with fewer missing detections compared\nwith the no cooperation setting. Our work marks a significant step forward in\nthe cooperative capabilities of CAVs, showcasing enhanced performance in\ncomplex scenarios.\n","authors":["Zhuoyuan Wu","Yuping Wang","Hengbo Ma","Zhaowei Li","Hang Qiu","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2403.17916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17905v1","updated":"2024-03-26T17:45:06Z","published":"2024-03-26T17:45:06Z","title":"Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2","summary":"  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n","authors":["Chen Yiwei","Tang Chao","Aghabiglou Amir","Chu Chung San","Wiaux Yves"],"pdf_url":"https://arxiv.org/pdf/2403.17905v1.pdf","comment":"submitted to IEEE EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2211.01364v3","updated":"2024-03-26T17:45:01Z","published":"2022-11-02T17:59:09Z","title":"An optimal control perspective on diffusion-based generative modeling","summary":"  We establish a connection between stochastic optimal control and generative\nmodels based on stochastic differential equations (SDEs), such as recently\ndeveloped diffusion probabilistic models. In particular, we derive a\nHamilton-Jacobi-Bellman equation that governs the evolution of the\nlog-densities of the underlying SDE marginals. This perspective allows to\ntransfer methods from optimal control theory to generative modeling. First, we\nshow that the evidence lower bound is a direct consequence of the well-known\nverification theorem from control theory. Further, we can formulate\ndiffusion-based generative modeling as a minimization of the Kullback-Leibler\ndivergence between suitable measures in path space. Finally, we develop a novel\ndiffusion-based method for sampling from unnormalized densities -- a problem\nfrequently occurring in statistics and computational sciences. We demonstrate\nthat our time-reversed diffusion sampler (DIS) can outperform other\ndiffusion-based sampling approaches on multiple numerical examples.\n","authors":["Julius Berner","Lorenz Richter","Karen Ullrich"],"pdf_url":"https://arxiv.org/pdf/2211.01364v3.pdf","comment":"Accepted for oral presentation at NeurIPS 2022 Workshop on\n  Score-Based Methods"},{"id":"http://arxiv.org/abs/2401.15059v2","updated":"2024-03-26T17:44:45Z","published":"2024-01-26T18:42:01Z","title":"Fully Independent Communication in Multi-Agent Reinforcement Learning","summary":"  Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research\nwithin the field of multi-agent systems. Several recent works have focused\nspecifically on the study of communication approaches in MARL. While multiple\ncommunication methods have been proposed, these might still be too complex and\nnot easily transferable to more practical contexts. One of the reasons for that\nis due to the use of the famous parameter sharing trick. In this paper, we\ninvestigate how independent learners in MARL that do not share parameters can\ncommunicate. We demonstrate that this setting might incur into some problems,\nto which we propose a new learning scheme as a solution. Our results show that,\ndespite the challenges, independent agents can still learn communication\nstrategies following our method. Additionally, we use this method to\ninvestigate how communication in MARL is affected by different network\ncapacities, both for sharing and not sharing parameters. We observe that\ncommunication may not always be needed and that the chosen agent network sizes\nneed to be considered when used together with communication in order to achieve\nefficient learning.\n","authors":["Rafael Pina","Varuna De Silva","Corentin Artaud","Xiaolan Liu"],"pdf_url":"https://arxiv.org/pdf/2401.15059v2.pdf","comment":"Extended version of the paper appearing on AAMAS 2024 with the same\n  title. 11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.17902v1","updated":"2024-03-26T17:43:15Z","published":"2024-03-26T17:43:15Z","title":"Serpent: Scalable and Efficient Image Restoration via Multi-scale\n  Structured State Space Models","summary":"  The landscape of computational building blocks of efficient image restoration\narchitectures is dominated by a combination of convolutional processing and\nvarious attention mechanisms. However, convolutional filters are inherently\nlocal and therefore struggle at modeling long-range dependencies in images. On\nthe other hand, attention excels at capturing global interactions between\narbitrary image regions, however at a quadratic cost in image dimension. In\nthis work, we propose Serpent, an architecture that leverages recent advances\nin state space models (SSMs) in its core computational block. SSMs, originally\nintroduced for sequence modeling, can maintain a global receptive field with a\nfavorable linear scaling in input size. Our preliminary results demonstrate\nthat Serpent can achieve reconstruction quality on par with state-of-the-art\ntechniques, while requiring orders of magnitude less compute (up to $150$ fold\nreduction in FLOPS) and a factor of up to $5\\times$ less GPU memory while\nmaintaining a compact model size.\n","authors":["Mohammad Shahab Sepehri","Zalan Fabian","Mahdi Soltanolkotabi"],"pdf_url":"https://arxiv.org/pdf/2403.17902v1.pdf","comment":"7 pages, 5 figures, preliminary workshop submission of a\n  comprehensive work to be released soon"},{"id":"http://arxiv.org/abs/2310.18841v2","updated":"2024-03-26T17:39:30Z","published":"2023-10-28T22:57:56Z","title":"A randomized algorithm for nonconvex minimization with inexact\n  evaluations and complexity guarantees","summary":"  We consider minimization of a smooth nonconvex function with inexact oracle\naccess to gradient and Hessian (without assuming access to the function value)\nto achieve approximate second-order optimality. A novel feature of our method\nis that if an approximate direction of negative curvature is chosen as the\nstep, we choose its sense to be positive or negative with equal probability. We\nallow gradients to be inexact in a relative sense and relax the coupling\nbetween inexactness thresholds for the first- and second-order optimality\nconditions. Our convergence analysis includes both an expectation bound based\non martingale analysis and a high-probability bound based on concentration\ninequalities. We apply our algorithm to empirical risk minimization problems\nand obtain improved gradient sample complexity over existing works.\n","authors":["Shuyao Li","Stephen J. Wright"],"pdf_url":"https://arxiv.org/pdf/2310.18841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09428v2","updated":"2024-03-26T17:38:38Z","published":"2024-03-14T14:19:48Z","title":"Borrowing Treasures from Neighbors: In-Context Learning for Multimodal\n  Learning with Missing Modalities and Data Scarcity","summary":"  Multimodal machine learning with missing modalities is an increasingly\nrelevant challenge arising in various applications such as healthcare. This\npaper extends the current research into missing modalities to the low-data\nregime, i.e., a downstream task has both missing modalities and limited sample\nsize issues. This problem setting is particularly challenging and also\npractical as it is often expensive to get full-modality data and sufficient\nannotated training samples. We propose to use retrieval-augmented in-context\nlearning to address these two crucial issues by unleashing the potential of a\ntransformer's in-context learning ability. Diverging from existing methods,\nwhich primarily belong to the parametric paradigm and often require sufficient\ntraining samples, our work exploits the value of the available full-modality\ndata, offering a novel perspective on resolving the challenge. The proposed\ndata-dependent framework exhibits a higher degree of sample efficiency and is\nempirically demonstrated to enhance the classification model's performance on\nboth full- and missing-modality data in the low-data regime across various\nmultimodal learning tasks. When only 1% of the training data are available, our\nproposed method demonstrates an average improvement of 6.1% over a recent\nstrong baseline across various datasets and missing states. Notably, our method\nalso reduces the performance gap between full-modality and missing-modality\ndata compared with the baseline.\n","authors":["Zhuo Zhi","Ziquan Liu","Moe Elbadawi","Adam Daneshmend","Mine Orlu","Abdul Basit","Andreas Demosthenous","Miguel Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2403.09428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02156v4","updated":"2024-03-26T17:36:54Z","published":"2023-10-03T15:43:59Z","title":"Probabilistically Rewired Message-Passing Neural Networks","summary":"  Message-passing graph neural networks (MPNNs) emerged as powerful tools for\nprocessing graph-structured input. However, they operate on a fixed input graph\nstructure, ignoring potential noise and missing information. Furthermore, their\nlocal aggregation mechanism can lead to problems such as over-squashing and\nlimited expressive power in capturing relevant graph structures. Existing\nsolutions to these challenges have primarily relied on heuristic methods, often\ndisregarding the underlying data distribution. Hence, devising principled\napproaches for learning to infer graph structures relevant to the given\nprediction task remains an open challenge. In this work, leveraging recent\nprogress in exact and differentiable $k$-subset sampling, we devise\nprobabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges\nwhile omitting less beneficial ones. For the first time, our theoretical\nanalysis explores how PR-MPNNs enhance expressive power, and we identify\nprecise conditions under which they outperform purely randomized approaches.\nEmpirically, we demonstrate that our approach effectively mitigates issues like\nover-squashing and under-reaching. In addition, on established real-world\ndatasets, our method exhibits competitive or superior predictive performance\ncompared to traditional MPNN models and recent graph transformer architectures.\n","authors":["Chendi Qian","Andrei Manolache","Kareem Ahmed","Zhe Zeng","Guy Van den Broeck","Mathias Niepert","Christopher Morris"],"pdf_url":"https://arxiv.org/pdf/2310.02156v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2401.07809v2","updated":"2024-03-26T17:29:07Z","published":"2024-01-15T16:30:12Z","title":"Optimal Data Splitting in Distributed Optimization for Machine Learning","summary":"  The distributed optimization problem has become increasingly relevant\nrecently. It has a lot of advantages such as processing a large amount of data\nin less time compared to non-distributed methods. However, most distributed\napproaches suffer from a significant bottleneck - the cost of communications.\nTherefore, a large amount of research has recently been directed at solving\nthis problem. One such approach uses local data similarity. In particular,\nthere exists an algorithm provably optimally exploiting the similarity\nproperty. But this result, as well as results from other works solve the\ncommunication bottleneck by focusing only on the fact that communication is\nsignificantly more expensive than local computing and does not take into\naccount the various capacities of network devices and the different\nrelationship between communication time and local computing expenses. We\nconsider this setup and the objective of this study is to achieve an optimal\nratio of distributed data between the server and local machines for any costs\nof communications and local computations. The running times of the network are\ncompared between uniform and optimal distributions. The superior theoretical\nperformance of our solutions is experimentally validated.\n","authors":["Daniil Medyakov","Gleb Molodtsov","Aleksandr Beznosikov","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2401.07809v2.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.17891v1","updated":"2024-03-26T17:22:29Z","published":"2024-03-26T17:22:29Z","title":"Image-based Novel Fault Detection with Deep Learning Classifiers using\n  Hierarchical Labels","summary":"  One important characteristic of modern fault classification systems is the\nability to flag the system when faced with previously unseen fault types. This\nwork considers the unknown fault detection capabilities of deep neural\nnetwork-based fault classifiers. Specifically, we propose a methodology on how,\nwhen available, labels regarding the fault taxonomy can be used to increase\nunknown fault detection performance without sacrificing model performance. To\nachieve this, we propose to utilize soft label techniques to improve the\nstate-of-the-art deep novel fault detection techniques during the training\nprocess and novel hierarchically consistent detection statistics for online\nnovel fault detection. Finally, we demonstrated increased detection performance\non novel fault detection in inspection images from the hot steel rolling\nprocess, with results well replicated across multiple scenarios and baseline\ndetection methods.\n","authors":["Nurettin Sergin","Jiayu Huang","Tzyy-Shuh Chang","Hao Yan"],"pdf_url":"https://arxiv.org/pdf/2403.17891v1.pdf","comment":"Accepted in IISE Transaction"},{"id":"http://arxiv.org/abs/2403.17889v1","updated":"2024-03-26T17:21:54Z","published":"2024-03-26T17:21:54Z","title":"Large scale paired antibody language models","summary":"  Antibodies are proteins produced by the immune system that can identify and\nneutralise a wide variety of antigens with high specificity and affinity, and\nconstitute the most successful class of biotherapeutics. With the advent of\nnext-generation sequencing, billions of antibody sequences have been collected\nin recent years, though their application in the design of better therapeutics\nhas been constrained by the sheer volume and complexity of the data. To address\nthis challenge, we present IgBert and IgT5, the best performing\nantibody-specific language models developed to date which can consistently\nhandle both paired and unpaired variable region sequences as input. These\nmodels are trained comprehensively using the more than two billion unpaired\nsequences and two million paired sequences of light and heavy chains present in\nthe Observed Antibody Space dataset. We show that our models outperform\nexisting antibody and protein language models on a diverse range of design and\nregression tasks relevant to antibody engineering. This advancement marks a\nsignificant leap forward in leveraging machine learning, large scale data sets\nand high-performance computing for enhancing antibody design for therapeutic\ndevelopment.\n","authors":["Henry Kenlay","Frédéric A. Dreyer","Aleksandr Kovaltsuk","Dom Miketa","Douglas Pires","Charlotte M. Deane"],"pdf_url":"https://arxiv.org/pdf/2403.17889v1.pdf","comment":"14 pages, 2 figures, 6 tables, model weights available at\n  https://zenodo.org/doi/10.5281/zenodo.10876908"},{"id":"http://arxiv.org/abs/2403.17887v1","updated":"2024-03-26T17:20:04Z","published":"2024-03-26T17:20:04Z","title":"The Unreasonable Ineffectiveness of the Deeper Layers","summary":"  We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.\n","authors":["Andrey Gromov","Kushal Tirumala","Hassan Shapourian","Paolo Glorioso","Daniel A. Roberts"],"pdf_url":"https://arxiv.org/pdf/2403.17887v1.pdf","comment":"12 + 10 pages, 5 + 4 figures"},{"id":"http://arxiv.org/abs/2403.17886v1","updated":"2024-03-26T17:19:23Z","published":"2024-03-26T17:19:23Z","title":"Compressed Multi-task embeddings for Data-Efficient Downstream training\n  and inference in Earth Observation","summary":"  As repositories of large scale data in earth observation (EO) have grown, so\nhave transfer and storage costs for model training and inference, expending\nsignificant resources. We introduce Neural Embedding Compression (NEC), based\non the transfer of compressed embeddings to data consumers instead of raw data.\nWe adapt foundation models (FM) through learned neural compression to generate\nmulti-task embeddings while navigating the tradeoff between compression rate\nand embedding utility. We update only a small fraction of the FM parameters\n(10%) for a short training period (1% of the iterations of pre-training). We\nevaluate NEC on two EO tasks: scene classification and semantic segmentation.\nCompared with applying traditional compression to the raw data, NEC achieves\nsimilar accuracy with a 75% to 90% reduction in data. Even at 99.7%\ncompression, performance drops by only 5% on the scene classification task.\nOverall, NEC is a data-efficient yet performant approach for multi-task EO\nmodelling.\n","authors":["Carlos Gomes","Thomas Brunschwiler"],"pdf_url":"https://arxiv.org/pdf/2403.17886v1.pdf","comment":"Published at IGARSS 2024"},{"id":"http://arxiv.org/abs/2403.04202v3","updated":"2024-03-26T17:18:33Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents. A promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents. However, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., caring about maximizing some\noutcome over time) or norm-based (i.e., focusing on conforming to a specific\nnorm here and now). The extent to which agents' co-development may be impacted\nby such moral heterogeneity in populations is not well understood. In this\npaper, we present a study of the learning dynamics of morally heterogeneous\npopulations interacting in a social dilemma setting. Using a Prisoner's Dilemma\nenvironment with a partner selection mechanism, we investigate the extent to\nwhich the prevalence of diverse moral agents in populations affects individual\nagents' learning behaviors and emergent population-level outcomes. We observe\nseveral types of non-trivial interactions between pro-social and anti-social\nagents, and find that certain classes of moral agents are able to steer selfish\nagents towards more cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17878v1","updated":"2024-03-26T17:10:15Z","published":"2024-03-26T17:10:15Z","title":"Empowering Data Mesh with Federated Learning","summary":"  The evolution of data architecture has seen the rise of data lakes, aiming to\nsolve the bottlenecks of data management and promote intelligent\ndecision-making. However, this centralized architecture is limited by the\nproliferation of data sources and the growing demand for timely analysis and\nprocessing. A new data paradigm, Data Mesh, is proposed to overcome these\nchallenges. Data Mesh treats domains as a first-class concern by distributing\nthe data ownership from the central team to each data domain, while keeping the\nfederated governance to monitor domains and their data products. Many\nmulti-million dollar organizations like Paypal, Netflix, and Zalando have\nalready transformed their data analysis pipelines based on this new\narchitecture. In this decentralized architecture where data is locally\npreserved by each domain team, traditional centralized machine learning is\nincapable of conducting effective analysis across multiple domains, especially\nfor security-sensitive organizations. To this end, we introduce a pioneering\napproach that incorporates Federated Learning into Data Mesh. To the best of\nour knowledge, this is the first open-source applied work that represents a\ncritical advancement toward the integration of federated learning methods into\nthe Data Mesh paradigm, underscoring the promising prospects for\nprivacy-preserving and decentralized data analysis strategies within Data Mesh\narchitecture.\n","authors":["Haoyuan Li","Salman Toor"],"pdf_url":"https://arxiv.org/pdf/2403.17878v1.pdf","comment":"In Proceedings of ACM Knowledge Discovery and Data Mining, Barcelona,\n  Spain, 25th - 29th August, 2024 (Conference acronym KDD), 9 pages"},{"id":"http://arxiv.org/abs/2402.04866v2","updated":"2024-03-26T16:57:46Z","published":"2024-02-01T21:16:40Z","title":"Room Transfer Function Reconstruction Using Complex-valued Neural\n  Networks and Irregularly Distributed Microphones","summary":"  Reconstructing the room transfer functions needed to calculate the complex\nsound field in a room has several impor- tant real-world applications. However,\nan unpractical number of microphones is often required. Recently, in addition\nto classical signal processing methods, deep learning techniques have been\napplied to reconstruct the room transfer function starting from a very limited\nset of measurements at scattered points in the room. In this paper, we employ\ncomplex-valued neural networks to estimate room transfer functions in the\nfrequency range of the first room resonances, using a few irregularly\ndistributed microphones. To the best of our knowledge, this is the first time\nthat complex-valued neural networks are used to estimate room transfer\nfunctions. To analyze the benefits of applying complex- valued optimization to\nthe considered task, we compare the proposed technique with a state-of-the-art\nkernel-based signal processing approach for sound field reconstruction, showing\nthat the proposed technique exhibits relevant advantages in terms of phase\naccuracy and overall quality of the reconstructed sound field. For informative\npurposes, we also compare the model with a similarly-structured data-driven\napproach that, however, applies a real-valued neural network to reconstruct\nonly the magnitude of the sound field.\n","authors":["Francesca Ronchini","Luca Comanducci","Mirco Pezzoli","Fabio Antonacci","Augusto Sarti"],"pdf_url":"https://arxiv.org/pdf/2402.04866v2.pdf","comment":"Submitted to EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.17868v1","updated":"2024-03-26T16:57:01Z","published":"2024-03-26T16:57:01Z","title":"Sample complexity of quantum hypothesis testing","summary":"  Quantum hypothesis testing has been traditionally studied from the\ninformation-theoretic perspective, wherein one is interested in the optimal\ndecay rate of error probabilities as a function of the number of samples of an\nunknown state. In this paper, we study the sample complexity of quantum\nhypothesis testing, wherein the goal is to determine the minimum number of\nsamples needed to reach a desired error probability. By making use of the\nwealth of knowledge that already exists in the literature on quantum hypothesis\ntesting, we characterize the sample complexity of binary quantum hypothesis\ntesting in the symmetric and asymmetric settings, and we provide bounds on the\nsample complexity of multiple quantum hypothesis testing. In more detail, we\nprove that the sample complexity of symmetric binary quantum hypothesis testing\ndepends logarithmically on the inverse error probability and inversely on the\nnegative logarithm of the fidelity. As a counterpart of the quantum Stein's\nlemma, we also find that the sample complexity of asymmetric binary quantum\nhypothesis testing depends logarithmically on the inverse type~II error\nprobability and inversely on the quantum relative entropy. Finally, we provide\nlower and upper bounds on the sample complexity of multiple quantum hypothesis\ntesting, with it remaining an intriguing open question to improve these bounds.\n","authors":["Hao-Chung Cheng","Nilanjana Datta","Nana Liu","Theshani Nuradha","Robert Salzmann","Mark M. Wilde"],"pdf_url":"https://arxiv.org/pdf/2403.17868v1.pdf","comment":"38 pages, 1 figure, preliminary version; see independent and\n  concurrent work of Pensia, Jog, Loh at arXiv:2403.16981"},{"id":"http://arxiv.org/abs/2401.07788v2","updated":"2024-03-26T16:49:44Z","published":"2024-01-15T15:54:54Z","title":"Activations and Gradients Compression for Model-Parallel Training","summary":"  Large neural networks require enormous computational clusters of machines.\nModel-parallel training, when the model architecture is partitioned\nsequentially between workers, is a popular approach for training modern models.\nInformation compression can be applied to decrease workers communication time,\nas it is often a bottleneck in such systems. This work explores how\nsimultaneous compression of activations and gradients in model-parallel\ndistributed training setup affects convergence. We analyze compression methods\nsuch as quantization and TopK compression, and also experiment with error\ncompensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error\nfeedback approach. We conduct experiments on image classification and language\nmodel fine-tuning tasks. Our findings demonstrate that gradients require milder\ncompression rates than activations. We observe that $K=10\\%$ is the lowest TopK\ncompression level, which does not harm model convergence severely. Experiments\nalso show that models trained with TopK perform well only when compression is\nalso applied during inference. We find that error feedback techniques do not\nimprove model-parallel training compared to plain compression, but allow model\ninference without compression with almost no quality drop. Finally, when\napplied with the AQ-SGD approach, TopK stronger than with $ K=30\\%$ worsens\nmodel performance significantly.\n","authors":["Mikhail Rudakov","Aleksandr Beznosikov","Yaroslav Kholodov","Alexander Gasnikov"],"pdf_url":"https://arxiv.org/pdf/2401.07788v2.pdf","comment":"17 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2210.06459v2","updated":"2024-03-26T16:49:11Z","published":"2022-10-12T17:56:04Z","title":"Differentially private multivariate medians","summary":"  Statistical tools which satisfy rigorous privacy guarantees are necessary for\nmodern data analysis. It is well-known that robustness against contamination is\nlinked to differential privacy. Despite this fact, using multivariate medians\nfor differentially private and robust multivariate location estimation has not\nbeen systematically studied. We develop novel finite-sample performance\nguarantees for differentially private multivariate depth-based medians, which\nare essentially sharp. Our results cover commonly used depth functions, such as\nthe halfspace (or Tukey) depth, spatial depth, and the integrated dual depth.\nWe show that under Cauchy marginals, the cost of heavy-tailed location\nestimation outweighs the cost of privacy. We demonstrate our results\nnumerically using a Gaussian contamination model in dimensions up to d = 100,\nand compare them to a state-of-the-art private mean estimation algorithm. As a\nby-product of our investigation, we prove concentration inequalities for the\noutput of the exponential mechanism about the maximizer of the population\nobjective function. This bound applies to objective functions that satisfy a\nmild regularity condition.\n","authors":["Kelly Ramsay","Aukosh Jagannath","Shoja'eddin Chenouri"],"pdf_url":"https://arxiv.org/pdf/2210.06459v2.pdf","comment":"42 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2401.06795v2","updated":"2024-03-26T16:44:34Z","published":"2024-01-08T18:42:55Z","title":"AI and Generative AI for Research Discovery and Summarization","summary":"  AI and generative AI tools, including chatbots like ChatGPT that rely on\nlarge language models (LLMs), have burst onto the scene this year, creating\nincredible opportunities to increase work productivity and improve our lives.\nStatisticians and data scientists have begun experiencing the benefits from the\navailability of these tools in numerous ways, such as the generation of\nprogramming code from text prompts to analyze data or fit statistical models.\nOne area that these tools can make a substantial impact is in research\ndiscovery and summarization. Standalone tools and plugins to chatbots are being\ndeveloped that allow researchers to more quickly find relevant literature than\npre-2023 search tools. Furthermore, generative AI tools have improved to the\npoint where they can summarize and extract the key points from research\narticles in succinct language. Finally, chatbots based on highly parameterized\nLLMs can be used to simulate abductive reasoning, which provides researchers\nthe ability to make connections among related technical topics, which can also\nbe used for research discovery. We review the developments in AI and generative\nAI for research discovery and summarization, and propose directions where these\ntypes of tools are likely to head in the future that may be of interest to\nstatistician and data scientists.\n","authors":["Mark Glickman","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.06795v2.pdf","comment":"29 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.17853v1","updated":"2024-03-26T16:42:30Z","published":"2024-03-26T16:42:30Z","title":"Using Domain Knowledge to Guide Dialog Structure Induction via Neural\n  Probabilistic Soft Logic","summary":"  Dialog Structure Induction (DSI) is the task of inferring the latent dialog\nstructure (i.e., a set of dialog states and their temporal transitions) of a\ngiven goal-oriented dialog. It is a critical component for modern dialog system\ndesign and discourse analysis. Existing DSI approaches are often purely\ndata-driven, deploy models that infer latent states without access to domain\nknowledge, underperform when the training corpus is limited/noisy, or have\ndifficulty when test dialogs exhibit distributional shifts from the training\ndomain. This work explores a neural-symbolic approach as a potential solution\nto these problems. We introduce Neural Probabilistic Soft Logic Dialogue\nStructure Induction (NEUPSL DSI), a principled approach that injects symbolic\nknowledge into the latent space of a generative neural model. We conduct a\nthorough empirical investigation on the effect of NEUPSL DSI learning on hidden\nrepresentation quality, few-shot learning, and out-of-domain generalization\nperformance. Over three dialog structure induction datasets and across\nunsupervised and semi-supervised settings for standard and cross-domain\ngeneralization, the injection of symbolic knowledge using NEUPSL DSI provides a\nconsistent boost in performance over the canonical baselines.\n","authors":["Connor Pryor","Quan Yuan","Jeremiah Liu","Mehran Kazemi","Deepak Ramachandran","Tania Bedrax-Weiss","Lise Getoor"],"pdf_url":"https://arxiv.org/pdf/2403.17853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17852v1","updated":"2024-03-26T16:40:08Z","published":"2024-03-26T16:40:08Z","title":"Counterfactual Fairness through Transforming Data Orthogonal to Bias","summary":"  Machine learning models have shown exceptional prowess in solving complex\nissues across various domains. Nonetheless, these models can sometimes exhibit\nbiased decision-making, leading to disparities in treatment across different\ngroups. Despite the extensive research on fairness, the nuanced effects of\nmultivariate and continuous sensitive variables on decision-making outcomes\nremain insufficiently studied. We introduce a novel data pre-processing\nalgorithm, Orthogonal to Bias (OB), designed to remove the influence of a group\nof continuous sensitive variables, thereby facilitating counterfactual fairness\nin machine learning applications. Our approach is grounded in the assumption of\na jointly normal distribution within a structural causal model (SCM), proving\nthat counterfactual fairness can be achieved by ensuring the data is\nuncorrelated with sensitive variables. The OB algorithm is model-agnostic,\ncatering to a wide array of machine learning models and tasks, and includes a\nsparse variant to enhance numerical stability through regularization. Through\nempirical evaluation on simulated and real-world datasets - including the adult\nincome and the COMPAS recidivism datasets - our methodology demonstrates its\ncapacity to enable fairer outcomes without compromising accuracy.\n","authors":["Shuyi Chen","Shixiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.17852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17847v1","updated":"2024-03-26T16:36:50Z","published":"2024-03-26T16:36:50Z","title":"Climate Downscaling: A Deep-Learning Based Super-resolution Model of\n  Precipitation Data with Attention Block and Skip Connections","summary":"  Human activities accelerate consumption of fossil fuels and produce\ngreenhouse gases, resulting in urgent issues today: global warming and the\nclimate change. These indirectly cause severe natural disasters, plenty of\nlives suffering and huge losses of agricultural properties. To mitigate impacts\non our lands, scientists are developing renewable, reusable, and clean energies\nand climatologists are trying to predict the extremes. Meanwhile, governments\nare publicizing resource-saving policies for a more eco-friendly society and\narousing environment awareness. One of the most influencing factors is the\nprecipitation, bringing condensed water vapor onto lands. Water resources are\nthe most significant but basic needs in society, not only supporting our\nlivings, but also economics. In Taiwan, although the average annual\nprecipitation is up to 2,500 millimeter (mm), the water allocation for each\nperson is lower than the global average due to drastically geographical\nelevation changes and uneven distribution through the year. Thus, it is crucial\nto track and predict the rainfall to make the most use of it and to prevent the\nfloods. However, climate models have limited resolution and require intensive\ncomputational power for local-scale use. Therefore, we proposed a deep\nconvolutional neural network with skip connections, attention blocks, and\nauxiliary data concatenation, in order to downscale the low-resolution\nprecipitation data into high-resolution one. Eventually, we compare with other\nclimate downscaling methods and show better performance in metrics of Mean\nAbsolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,\nstructural similarity index (SSIM), and forecast indicators.\n","authors":["Chia-Hao Chiang","Zheng-Han Huang","Liwen Liu","Hsin-Chien Liang","Yi-Chi Wang","Wan-Ling Tseng","Chao Wang","Che-Ta Chen","Ko-Chih Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17846v1","updated":"2024-03-26T16:36:43Z","published":"2024-03-26T16:36:43Z","title":"Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot\n  Navigation","summary":"  Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features. While these maps allow for the prediction\nof point-wise saliency maps when queried for a certain language concept,\nlarge-scale environments and abstract queries beyond the object level still\npose a considerable hurdle, ultimately limiting language-grounded robotic\nnavigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D\nscene graph mapping approach for language-grounded robot navigation. Leveraging\nopen-vocabulary vision foundation models, we first obtain state-of-the-art\nopen-vocabulary segment-level maps in 3D and subsequently construct a 3D scene\ngraph hierarchy consisting of floor, room, and object concepts, each enriched\nwith open-vocabulary features. Our approach is able to represent multi-story\nbuildings and allows robotic traversal of those using a cross-floor Voronoi\ngraph. HOV-SG is evaluated on three distinct datasets and surpasses previous\nbaselines in open-vocabulary semantic accuracy on the object, room, and floor\nlevel while producing a 75% reduction in representation size compared to dense\nopen-vocabulary maps. In order to prove the efficacy and generalization\ncapabilities of HOV-SG, we showcase successful long-horizon\nlanguage-conditioned robot navigation within real-world multi-storage\nenvironments. We provide code and trial video data at http://hovsg.github.io/.\n","authors":["Abdelrhman Werby","Chenguang Huang","Martin Büchner","Abhinav Valada","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2403.17846v1.pdf","comment":"Code and video are available at http://hovsg.github.io/"},{"id":"http://arxiv.org/abs/2312.17336v2","updated":"2024-03-26T16:35:15Z","published":"2023-12-28T19:28:23Z","title":"PINN surrogate of Li-ion battery models for parameter inference. Part\n  II: Regularization and application of the pseudo-2D model","summary":"  Bayesian parameter inference is useful to improve Li-ion battery diagnostics\nand can help formulate battery aging models. However, it is computationally\nintensive and cannot be easily repeated for multiple cycles, multiple operating\nconditions, or multiple replicate cells. To reduce the computational cost of\nBayesian calibration, numerical solvers for physics-based models can be\nreplaced with faster surrogates. A physics-informed neural network (PINN) is\ndeveloped as a surrogate for the pseudo-2D (P2D) battery model calibration. For\nthe P2D surrogate, additional training regularization was needed as compared to\nthe PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and\nP2D surrogate models are exercised for parameter inference and compared to data\nobtained from a direct numerical solution of the governing equations. A\nparameter inference study highlights the ability to use these PINNs to\ncalibrate scaling parameters for the cathode Li diffusion and the anode\nexchange current density. By realizing computational speed-ups of 2250x for the\nP2D model, as compared to using standard integrating methods, the PINN\nsurrogates enable rapid state-of-health diagnostics. In the low-data\navailability scenario, the testing error was estimated to 2mV for the SPM\nsurrogate and 10mV for the P2D surrogate which could be mitigated with\nadditional data.\n","authors":["Malik Hassanaly","Peter J. Weddle","Ryan N. King","Subhayan De","Alireza Doostan","Corey R. Randall","Eric J. Dufek","Andrew M. Colclasure","Kandler Smith"],"pdf_url":"https://arxiv.org/pdf/2312.17336v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17845v1","updated":"2024-03-26T16:34:05Z","published":"2024-03-26T16:34:05Z","title":"TractOracle: towards an anatomically-informed reward function for\n  RL-based tractography","summary":"  Reinforcement learning (RL)-based tractography is a competitive alternative\nto machine learning and classical tractography algorithms due to its high\nanatomical accuracy obtained without the need for any annotated data. However,\nthe reward functions so far used to train RL agents do not encapsulate\nanatomical knowledge which causes agents to generate spurious false positives\ntracts. In this paper, we propose a new RL tractography system, TractOracle,\nwhich relies on a reward network trained for streamline classification. This\nnetwork is used both as a reward function during training as well as a mean for\nstopping the tracking process early and thus reduce the number of false\npositive streamlines. This makes our system a unique method that evaluates and\nreconstructs WM streamlines at the same time. We report an improvement of true\npositive ratios by almost 20\\% and a reduction of 3x of false positive ratios\non one dataset and an increase between 2x and 7x in the number true positive\nstreamlines on another dataset.\n","authors":["Antoine Théberge","Maxime Descoteaux","Pierre-Marc Jodoin"],"pdf_url":"https://arxiv.org/pdf/2403.17845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17844v1","updated":"2024-03-26T16:33:12Z","published":"2024-03-26T16:33:12Z","title":"Mechanistic Design and Scaling of Hybrid Architectures","summary":"  The development of deep learning architectures is a resource-demanding\nprocess, due to a vast design space, long prototyping times, and high compute\ncosts associated with at-scale model training and evaluation. We set out to\nsimplify this process by grounding it in an end-to-end mechanistic architecture\ndesign (MAD) pipeline, encompassing small-scale capability unit tests\npredictive of scaling laws. Through a suite of synthetic token manipulation\ntasks such as compression and recall, designed to probe capabilities, we\nidentify and test new hybrid architectures constructed from a variety of\ncomputational primitives. We experimentally validate the resulting\narchitectures via an extensive compute-optimal and a new state-optimal scaling\nlaw analysis, training over 500 language models between 70M to 7B parameters.\nSurprisingly, we find MAD synthetics to correlate with compute-optimal\nperplexity, enabling accurate evaluation of new architectures via isolated\nproxy tasks. The new architectures found via MAD, based on simple ideas such as\nhybridization and sparsity, outperform state-of-the-art Transformer,\nconvolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in\nscaling, both at compute-optimal budgets and in overtrained regimes. Overall,\nthese results provide evidence that performance on curated synthetic tasks can\nbe predictive of scaling laws, and that an optimal architecture should leverage\nspecialized layers via a hybrid topology.\n","authors":["Michael Poli","Armin W Thomas","Eric Nguyen","Pragaash Ponnusamy","Björn Deiseroth","Kristian Kersting","Taiji Suzuki","Brian Hie","Stefano Ermon","Christopher Ré","Ce Zhang","Stefano Massaroli"],"pdf_url":"https://arxiv.org/pdf/2403.17844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time- consuming. Therefore, the challenging task of\nreconstructing visually accurate HDR images from their Low Dynamic Range (LDR)\ncounterparts is gaining attention in the vision research community. A major\nchallenge in this research problem is the lack of datasets, which capture\ndiverse scene conditions (e.g., lighting, shadows, weather, locations,\nlandscapes, objects, humans, buildings) and various image features (e.g.,\ncolor, contrast, saturation, hue, luminance, brightness, radiance). To address\nthis gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset\nof photo-realistic HDR images sampled from the GTA-V video game. We perform\nthorough evaluation of the proposed dataset, which demonstrates significant\nqualitative and quantitative improvements of the state-of-the-art HDR image\nreconstruction methods. Furthermore, we demonstrate the effectiveness of the\nproposed dataset and its impact on additional computer vision tasks including\n3D human pose estimation, human body part segmentation, and holistic scene\nsegmentation. The dataset, data collection pipeline, and evaluation code are\navailable at: https://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2305.03123v2","updated":"2024-03-26T16:22:54Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for AI policy act, if designed by the governments.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v2.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2312.17329v2","updated":"2024-03-26T16:22:36Z","published":"2023-12-28T19:09:56Z","title":"PINN surrogate of Li-ion battery models for parameter inference. Part I:\n  Implementation and multi-fidelity hierarchies for the single-particle model","summary":"  To plan and optimize energy storage demands that account for Li-ion battery\naging dynamics, techniques need to be developed to diagnose battery internal\nstates accurately and rapidly. This study seeks to reduce the computational\nresources needed to determine a battery's internal states by replacing\nphysics-based Li-ion battery models -- such as the single-particle model (SPM)\nand the pseudo-2D (P2D) model -- with a physics-informed neural network (PINN)\nsurrogate. The surrogate model makes high-throughput techniques, such as\nBayesian calibration, tractable to determine battery internal parameters from\nvoltage responses. This manuscript is the first of a two-part series that\nintroduces PINN surrogates of Li-ion battery models for parameter inference\n(i.e., state-of-health diagnostics). In this first part, a method is presented\nfor constructing a PINN surrogate of the SPM. A multi-fidelity hierarchical\ntraining, where several neural nets are trained with multiple physics-loss\nfidelities is shown to significantly improve the surrogate accuracy when only\ntraining on the governing equation residuals. The implementation is made\navailable in a companion repository (https://github.com/NREL/pinnstripes). The\ntechniques used to develop a PINN surrogate of the SPM are extended in Part II\nfor the PINN surrogate for the P2D battery model, and explore the Bayesian\ncalibration capabilities of both surrogates.\n","authors":["Malik Hassanaly","Peter J. Weddle","Ryan N. King","Subhayan De","Alireza Doostan","Corey R. Randall","Eric J. Dufek","Andrew M. Colclasure","Kandler Smith"],"pdf_url":"https://arxiv.org/pdf/2312.17329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14427v2","updated":"2024-03-26T16:15:40Z","published":"2023-11-24T12:00:50Z","title":"Disentangling the Spectral Properties of the Hodge Laplacian: Not All\n  Small Eigenvalues Are Equal","summary":"  The rich spectral information of the graph Laplacian has been instrumental in\ngraph theory, machine learning, and graph signal processing for applications\nsuch as graph classification, clustering, or eigenmode analysis. Recently, the\nHodge Laplacian has come into focus as a generalisation of the ordinary\nLaplacian for higher-order graph models such as simplicial and cellular\ncomplexes. Akin to the traditional analysis of graph Laplacians, many authors\nanalyse the smallest eigenvalues of the Hodge Laplacian, which are connected to\nimportant topological properties such as homology. However, small eigenvalues\nof the Hodge Laplacian can carry different information depending on whether\nthey are related to curl or gradient eigenmodes, and thus may not be\ncomparable. We therefore introduce the notion of persistent eigenvector\nsimilarity and provide a method to track individual harmonic, curl, and\ngradient eigenvectors/-values through the so-called persistence filtration,\nleveraging the full information contained in the Hodge-Laplacian spectrum\nacross all possible scales of a point cloud. Finally, we use our insights (a)\nto introduce a novel form of Hodge spectral clustering and (b) to classify\nedges and higher-order simplices based on their relationship to the smallest\nharmonic, curl, and gradient eigenvectors.\n","authors":["Vincent P. Grande","Michael T. Schaub"],"pdf_url":"https://arxiv.org/pdf/2311.14427v2.pdf","comment":"5 pages, 4 figures, comments welcome"},{"id":"http://arxiv.org/abs/2403.17833v1","updated":"2024-03-26T16:14:43Z","published":"2024-03-26T16:14:43Z","title":"GPFL: A Gradient Projection-Based Client Selection Framework for\n  Efficient Federated Learning","summary":"  Federated learning client selection is crucial for determining participant\nclients while balancing model accuracy and communication efficiency. Existing\nmethods have limitations in handling data heterogeneity, computational burdens,\nand independent client treatment. To address these challenges, we propose GPFL,\nwhich measures client value by comparing local and global descent directions.\nWe also employ an Exploit-Explore mechanism to enhance performance.\nExperimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL\noutperforms baselines in Non-IID scenarios, achieving over 9\\% improvement in\nFEMINST test accuracy. Moreover, GPFL exhibits shorter computation times\nthrough pre-selection and parameter reuse in federated learning.\n","authors":["Shijie Na","Yuzhi Liang","Siu-Ming Yiu"],"pdf_url":"https://arxiv.org/pdf/2403.17833v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.17831v1","updated":"2024-03-26T16:13:55Z","published":"2024-03-26T16:13:55Z","title":"Learning the Optimal Power Flow: Environment Design Matters","summary":"  To solve the optimal power flow (OPF) problem, reinforcement learning (RL)\nemerges as a promising new approach. However, the RL-OPF literature is strongly\ndivided regarding the exact formulation of the OPF problem as an RL\nenvironment. In this work, we collect and implement diverse environment design\ndecisions from the literature regarding training data, observation space,\nepisode definition, and reward function choice. In an experimental analysis, we\nshow the significant impact of these environment design options on RL-OPF\ntraining performance. Further, we derive some first recommendations regarding\nthe choice of these design decisions. The created environment framework is\nfully open-source and can serve as a benchmark for future research in the\nRL-OPF field.\n","authors":["Thomas Wolgast","Astrid Nieße"],"pdf_url":"https://arxiv.org/pdf/2403.17831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.13969v3","updated":"2024-03-26T16:13:26Z","published":"2021-08-31T16:51:00Z","title":"Semi-Supervised Crowd Counting from Unlabeled Data","summary":"  Automatic Crowd behavior analysis can be applied to effectively help the\ndaily transportation statistics and planning, which helps the smart city\nconstruction. As one of the most important keys, crowd counting has drawn\nincreasing attention. Recent works achieved promising performance but relied on\nthe supervised paradigm with expensive crowd annotations. To alleviate the\nannotation cost in real-world transportation scenarios, in this work we\nproposed a semi-supervised learning framework $S^{4}\\textit{Crowd}$, which can\nleverage both unlabeled/labeled data for robust crowd counting. In the\nunsupervised pathway, two \\textit{self-supervised losses} were proposed to\nsimulate the crowd variations such as scale, illumination, based on which\nsupervised information pseudo labels were generated and gradually refined. We\nalso proposed a crowd-driven recurrent unit \\textit{Gated-Crowd-Recurrent-Unit\n(GCRU)}, which can preserve discriminant crowd information by extracting\nsecond-order statistics, yielding pseudo labels with improved quality. A joint\nloss including both unsupervised/supervised information was proposed, and a\ndynamic weighting strategy was employed to balance the importance of the\nunsupervised loss and supervised loss at different training stages. We\nconducted extensive experiments on four popular crowd counting datasets in\nsemi-supervised settings. Experimental results supported the effectiveness of\neach proposed component in our $S^{4}$Crowd framework. Our method achieved\ncompetitive performance in semi-supervised learning approaches on these crowd\ncounting datasets.\n","authors":["Haoran Duan","Fan Wan","Rui Sun","Zeyu Wang","Varun Ojha","Yu Guan","Hubert P. H. Shum","Bingzhang Hu","Yang Long"],"pdf_url":"https://arxiv.org/pdf/2108.13969v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17827v1","updated":"2024-03-26T16:06:42Z","published":"2024-03-26T16:06:42Z","title":"DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from\n  Textual Descriptions","summary":"  Generating natural hand-object interactions in 3D is challenging as the\nresulting hand and object motions are expected to be physically plausible and\nsemantically meaningful. Furthermore, generalization to unseen objects is\nhindered by the limited scale of available hand-object interaction datasets. We\npropose DiffH2O, a novel method to synthesize realistic, one or two-handed\nobject interactions from provided text prompts and geometry of the object. The\nmethod introduces three techniques that enable effective learning from limited\ndata. First, we decompose the task into a grasping stage and a text-based\ninteraction stage and use separate diffusion models for each. In the grasping\nstage, the model only generates hand motions, whereas in the interaction phase\nboth hand and object poses are synthesized. Second, we propose a compact\nrepresentation that tightly couples hand and object poses. Third, we propose\ntwo different guidance schemes to allow more control of the generated motions:\ngrasp guidance and detailed textual guidance. Grasp guidance takes a single\ntarget grasping pose and guides the diffusion model to reach this grasp at the\nend of the grasping stage, which provides control over the grasping pose. Given\na grasping motion from this stage, multiple different actions can be prompted\nin the interaction phase. For textual guidance, we contribute comprehensive\ntext descriptions to the GRAB dataset and show that they enable our method to\nhave more fine-grained control over hand-object interactions. Our quantitative\nand qualitative evaluation demonstrates that the proposed method outperforms\nbaseline methods and leads to natural hand-object motions. Moreover, we\ndemonstrate the practicality of our framework by utilizing a hand pose estimate\nfrom an off-the-shelf pose estimator for guidance, and then sampling multiple\ndifferent actions in the interaction stage.\n","authors":["Sammy Christen","Shreyas Hampali","Fadime Sener","Edoardo Remelli","Tomas Hodan","Eric Sauser","Shugao Ma","Bugra Tekin"],"pdf_url":"https://arxiv.org/pdf/2403.17827v1.pdf","comment":"Project Page: https://diffh2o.github.io/"},{"id":"http://arxiv.org/abs/2311.15964v2","updated":"2024-03-26T15:58:26Z","published":"2023-11-27T16:07:37Z","title":"Efficient Pre-training for Localized Instruction Generation of Videos","summary":"  Procedural videos show step-by-step demonstrations of tasks like recipe\npreparation. Understanding such videos is challenging, involving the precise\nlocalization of steps and the generation of textual instructions. Manually\nannotating steps and writing instructions is costly, which limits the size of\ncurrent datasets and hinders effective learning. Leveraging large but noisy\nvideo-transcript datasets for pre-training can boost performance, but demands\nsignificant computational resources. Furthermore, transcripts contain\nirrelevant content and exhibit style variation compared to instructions written\nby human annotators. To mitigate both issues, we propose a technique,\nSieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters\nirrelevant transcripts and (ii) Swap enhances the quality of the text\ninstruction by automatically replacing the transcripts with human-written\ninstructions from a text-only recipe dataset. The curated dataset, three orders\nof magnitude smaller than current web-scale datasets, enables efficient\ntraining of large-scale models with competitive performance. We complement our\nSieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step\nlocalization and instruction generation for procedural videos. When this model\nis pre-trained on our curated dataset, it achieves state-of-the-art performance\nin zero-shot and finetuning settings on YouCook2 and Tasty, while using a\nfraction of the computational resources.\n","authors":["Anil Batra","Davide Moltisanti","Laura Sevilla-Lara","Marcus Rohrbach","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2311.15964v2.pdf","comment":"This version has some missing experiments and elaborative technical\n  details"},{"id":"http://arxiv.org/abs/2403.17811v1","updated":"2024-03-26T15:50:37Z","published":"2024-03-26T15:50:37Z","title":"Are Compressed Language Models Less Subgroup Robust?","summary":"  To reduce the inference cost of large language models, model compression is\nincreasingly used to create smaller scalable models. However, little is known\nabout their robustness to minority subgroups defined by the labels and\nattributes of a dataset. In this paper, we investigate the effects of 18\ndifferent compression methods and settings on the subgroup robustness of BERT\nlanguage models. We show that worst-group performance does not depend on model\nsize alone, but also on the compression method used. Additionally, we find that\nmodel compression does not always worsen the performance on minority subgroups.\nAltogether, our analysis serves to further research into the subgroup\nrobustness of model compression.\n","authors":["Leonidas Gee","Andrea Zugarini","Novi Quadrianto"],"pdf_url":"https://arxiv.org/pdf/2403.17811v1.pdf","comment":"The 2023 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2023)"},{"id":"http://arxiv.org/abs/2403.17808v1","updated":"2024-03-26T15:45:29Z","published":"2024-03-26T15:45:29Z","title":"Annotated Biomedical Video Generation using Denoising Diffusion\n  Probabilistic Models and Flow Fields","summary":"  The segmentation and tracking of living cells play a vital role within the\nbiomedical domain, particularly in cancer research, drug development, and\ndevelopmental biology. These are usually tedious and time-consuming tasks that\nare traditionally done by biomedical experts. Recently, to automatize these\nprocesses, deep learning based segmentation and tracking methods have been\nproposed. These methods require large-scale datasets and their full potential\nis constrained by the scarcity of annotated data in the biomedical imaging\ndomain. To address this limitation, we propose Biomedical Video Diffusion Model\n(BVDM), capable of generating realistic-looking synthetic microscopy videos.\nTrained only on a single real video, BVDM can generate videos of arbitrary\nlength with pixel-level annotations that can be used for training data-hungry\nmodels. It is composed of a denoising diffusion probabilistic model (DDPM)\ngenerating high-fidelity synthetic cell microscopy images and a flow prediction\nmodel (FPM) predicting the non-rigid transformation between consecutive video\nframes. During inference, initially, the DDPM imposes realistic cell textures\non synthetic cell masks which are generated based on real data statistics. The\nflow prediction model predicts the flow field between consecutive masks and\napplies that to the DDPM output from the previous time frame to create the next\none while keeping temporal consistency. BVDM outperforms state-of-the-art\nsynthetic live cell microscopy video generation models. Furthermore, we\ndemonstrate that a sufficiently large synthetic dataset enhances the\nperformance of cell segmentation and tracking models compared to using a\nlimited amount of available real data.\n","authors":["Rüveyda Yilmaz","Dennis Eschweiler","Johannes Stegmaier"],"pdf_url":"https://arxiv.org/pdf/2403.17808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17806v1","updated":"2024-03-26T15:44:58Z","published":"2024-03-26T15:44:58Z","title":"Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding\n  Model Mechanisms","summary":"  Many recent language model (LM) interpretability studies have adopted the\ncircuits framework, which aims to find the minimal computational subgraph, or\ncircuit, that explains LM behavior on a given task. Most studies determine\nwhich edges belong in a LM's circuit by performing causal interventions on each\nedge independently, but this scales poorly with model size. Edge attribution\npatching (EAP), gradient-based approximation to interventions, has emerged as a\nscalable but imperfect solution to this problem. In this paper, we introduce a\nnew method - EAP with integrated gradients (EAP-IG) - that aims to better\nmaintain a core property of circuits: faithfulness. A circuit is faithful if\nall model edges outside the circuit can be ablated without changing the model's\nperformance on the task; faithfulness is what justifies studying circuits,\nrather than the full model. Our experiments demonstrate that circuits found\nusing EAP are less faithful than those found using EAP-IG, even though both\nhave high node overlap with circuits found previously using causal\ninterventions. We conclude more generally that when using circuits to compare\nthe mechanisms models use to solve tasks, faithfulness, not overlap, is what\nshould be measured.\n","authors":["Michael Hanna","Sandro Pezzelle","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2403.17806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17805v1","updated":"2024-03-26T15:42:04Z","published":"2024-03-26T15:42:04Z","title":"Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving","summary":"  The automated generation of diverse and complex training scenarios has been\nan important ingredient in many complex learning tasks. Especially in\nreal-world application domains, such as autonomous driving, auto-curriculum\ngeneration is considered vital for obtaining robust and general policies.\nHowever, crafting traffic scenarios with multiple, heterogeneous agents is\ntypically considered as a tedious and time-consuming task, especially in more\ncomplex simulation environments. In our work, we introduce MATS-Gym, a\nMulti-Agent Traffic Scenario framework to train agents in CARLA, a\nhigh-fidelity driving simulator. MATS-Gym is a multi-agent training framework\nfor autonomous driving that uses partial scenario specifications to generate\ntraffic scenarios with variable numbers of agents. This paper unifies various\nexisting approaches to traffic scenario description into a single training\nframework and demonstrates how it can be integrated with techniques from\nunsupervised environment design to automate the generation of adaptive\nauto-curricula. The code is available at\nhttps://github.com/AutonomousDrivingExaminer/mats-gym.\n","authors":["Axel Brunnbauer","Luigi Berducci","Peter Priller","Dejan Nickovic","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2403.17805v1.pdf","comment":"7 Pages, Under Review"},{"id":"http://arxiv.org/abs/2302.03788v4","updated":"2024-03-26T15:41:28Z","published":"2023-02-07T22:56:58Z","title":"Toward a Theory of Causation for Interpreting Neural Code Models","summary":"  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly\nprogressing from research prototypes to commercial developer tools. As such,\nunderstanding the capabilities and limitations of such models is becoming\ncritical. However, the abilities of these models are typically measured using\nautomated metrics that often only reveal a portion of their real-world\nperformance. While, in general, the performance of NCMs appears promising,\ncurrently much is unknown about how such models arrive at decisions. To this\nend, this paper introduces $do_{code}$, a post hoc interpretability method\nspecific to NCMs that is capable of explaining model predictions. $do_{code}$\nis based upon causal inference to enable programming language-oriented\nexplanations. While the theoretical underpinnings of $do_{code}$ are extensible\nto exploring different model properties, we provide a concrete instantiation\nthat aims to mitigate the impact of spurious correlations by grounding\nexplanations of model behavior in properties of programming languages. To\ndemonstrate the practical benefit of $do_{code}$, we illustrate the insights\nthat our framework can provide by performing a case study on two popular deep\nlearning architectures and ten NCMs. The results of this case study illustrate\nthat our studied NCMs are sensitive to changes in code syntax. All our NCMs,\nexcept for the BERT-like model, statistically learn to predict tokens related\nto blocks of code (\\eg brackets, parenthesis, semicolon) with less confounding\nbias as compared to other programming language constructs. These insights\ndemonstrate the potential of $do_{code}$ as a useful method to detect and\nfacilitate the elimination of confounding bias in NCMs.\n","authors":["David N. Palacio","Alejandro Velasco","Nathan Cooper","Alvaro Rodriguez","Kevin Moran","Denys Poshyvanyk"],"pdf_url":"https://arxiv.org/pdf/2302.03788v4.pdf","comment":"Accepted to appear in IEEE Transactions on Software Engineering"},{"id":"http://arxiv.org/abs/2306.15909v4","updated":"2024-03-26T15:13:20Z","published":"2023-06-28T04:16:16Z","title":"RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$","summary":"  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as\npromising approaches for learning data-efficient RL algorithms tailored to a\ngiven task distribution. However, they show poor asymptotic performance and\nstruggle with out-of-distribution tasks because they rely on sequence models,\nsuch as recurrent neural networks or transformers, to process experiences\nrather than summarize them using general-purpose RL components such as value\nfunctions. In contrast, traditional RL algorithms are data-inefficient as they\ndo not use domain knowledge, but they do converge to an optimal policy in the\nlimit. We propose RL$^3$, a principled hybrid approach that incorporates\naction-values, learned per task through traditional RL, in the inputs to\nmeta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,\ncompared to RL$^2$, while maintaining data-efficiency in the short term, and\ngeneralizes better to out-of-distribution tasks. Experiments are conducted on\nboth custom and benchmark discrete domains from the meta-RL literature that\nexhibit a range of short-term, long-term, and complex dependencies.\n","authors":["Abhinav Bhatia","Samer B. Nashed","Shlomo Zilberstein"],"pdf_url":"https://arxiv.org/pdf/2306.15909v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12243v4","updated":"2024-03-26T15:12:19Z","published":"2023-08-23T16:42:27Z","title":"Multi-Objective Optimization for Sparse Deep Multi-Task Learning","summary":"  Different conflicting optimization criteria arise naturally in various Deep\nLearning scenarios. These can address different main tasks (i.e., in the\nsetting of Multi-Task Learning), but also main and secondary tasks such as loss\nminimization versus sparsity. The usual approach is a simple weighting of the\ncriteria, which formally only works in the convex setting. In this paper, we\npresent a Multi-Objective Optimization algorithm using a modified Weighted\nChebyshev scalarization for training Deep Neural Networks (DNNs) with respect\nto several tasks. By employing this scalarization technique, the algorithm can\nidentify all optimal solutions of the original problem while reducing its\ncomplexity to a sequence of single-objective problems. The simplified problems\nare then solved using an Augmented Lagrangian method, enabling the use of\npopular optimization techniques such as Adam and Stochastic Gradient Descent,\nwhile efficaciously handling constraints. Our work aims to address the\n(economical and also ecological) sustainability issue of DNN models, with a\nparticular focus on Deep Multi-Task models, which are typically designed with a\nvery large number of weights to perform equally well on multiple tasks. Through\nexperiments conducted on two Machine Learning datasets, we demonstrate the\npossibility of adaptively sparsifying the model during training without\nsignificantly impacting its performance, if we are willing to apply\ntask-specific adaptations to the network weights. Code is available at\nhttps://github.com/salomonhotegni/MDMTN\n","authors":["S. S. Hotegni","M. Berkemeier","S. Peitz"],"pdf_url":"https://arxiv.org/pdf/2308.12243v4.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.17775v1","updated":"2024-03-26T15:07:58Z","published":"2024-03-26T15:07:58Z","title":"Secure Aggregation is Not Private Against Membership Inference Attacks","summary":"  Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in\nfederated learning, affording the server access only to the aggregate of model\nupdates while safeguarding the confidentiality of individual updates. Despite\nwidespread claims regarding SecAgg's privacy-preserving capabilities, a formal\nanalysis of its privacy is lacking, making such presumptions unjustified. In\nthis paper, we delve into the privacy implications of SecAgg by treating it as\na local differential privacy (LDP) mechanism for each local update. We design a\nsimple attack wherein an adversarial server seeks to discern which update\nvector a client submitted, out of two possible ones, in a single training round\nof federated learning under SecAgg. By conducting privacy auditing, we assess\nthe success probability of this attack and quantify the LDP guarantees provided\nby SecAgg. Our numerical results unveil that, contrary to prevailing claims,\nSecAgg offers weak privacy against membership inference attacks even in a\nsingle training round. Indeed, it is difficult to hide a local update by adding\nother independent local updates when the updates are of high dimension. Our\nfindings underscore the imperative for additional privacy-enhancing mechanisms,\nsuch as noise injection, in federated learning.\n","authors":["Khac-Hoang Ngo","Johan Östman","Giuseppe Durisi","Alexandre Graell i Amat"],"pdf_url":"https://arxiv.org/pdf/2403.17775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17768v1","updated":"2024-03-26T14:54:48Z","published":"2024-03-26T14:54:48Z","title":"SciNews: From Scholarly Complexities to Public Narratives -- A Dataset\n  for Scientific News Report Generation","summary":"  Scientific news reports serve as a bridge, adeptly translating complex\nresearch articles into reports that resonate with the broader public. The\nautomated generation of such narratives enhances the accessibility of scholarly\ninsights. In this paper, we present a new corpus to facilitate this paradigm\ndevelopment. Our corpus comprises a parallel compilation of academic\npublications and their corresponding scientific news reports across nine\ndisciplines. To demonstrate the utility and reliability of our dataset, we\nconduct an extensive analysis, highlighting the divergences in readability and\nbrevity between scientific news narratives and academic manuscripts. We\nbenchmark our dataset employing state-of-the-art text generation models. The\nevaluation process involves both automatic and human evaluation, which lays the\ngroundwork for future explorations into the automated generation of scientific\nnews reports. The dataset and code related to this work are available at\nhttps://dongqi.me/projects/SciNews.\n","authors":["Dongqi Pu","Yifan Wang","Jia Loy","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2403.17768v1.pdf","comment":"LREC-COLING 2024 Main Conference Paper"},{"id":"http://arxiv.org/abs/2403.17767v1","updated":"2024-03-26T14:54:35Z","published":"2024-03-26T14:54:35Z","title":"Asymptotic Bayes risk of semi-supervised learning with uncertain\n  labeling","summary":"  This article considers a semi-supervised classification setting on a Gaussian\nmixture model, where the data is not labeled strictly as usual, but instead\nwith uncertain labels. Our main aim is to compute the Bayes risk for this\nmodel. We compare the behavior of the Bayes risk and the best known algorithm\nfor this model. This comparison eventually gives new insights over the\nalgorithm.\n","authors":["Victor Leger","Romain Couillet"],"pdf_url":"https://arxiv.org/pdf/2403.17767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17757v1","updated":"2024-03-26T14:49:22Z","published":"2024-03-26T14:49:22Z","title":"Noise2Noise Denoising of CRISM Hyperspectral Data","summary":"  Hyperspectral data acquired by the Compact Reconnaissance Imaging\nSpectrometer for Mars (CRISM) have allowed for unparalleled mapping of the\nsurface mineralogy of Mars. Due to sensor degradation over time, a significant\nportion of the recently acquired data is considered unusable. Here a new\ndata-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to\nremove noise from CRISM images. Our model is self-supervised and does not\nrequire zero-noise target data, making it well suited for use in Planetary\nScience applications where high quality labelled data is scarce. We demonstrate\nits strong performance on synthetic-noise data and CRISM images, and its impact\non downstream classification performance, outperforming benchmark methods on\nmost metrics. This allows for detailed analysis for critical sites of interest\non the Martian surface, including proposed lander sites.\n","authors":["Robert Platt","Rossella Arcucci","Cédric John"],"pdf_url":"https://arxiv.org/pdf/2403.17757v1.pdf","comment":"5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024\n  ML4RS Workshop"},{"id":"http://arxiv.org/abs/2403.11996v2","updated":"2024-03-26T14:46:04Z","published":"2024-03-18T17:30:27Z","title":"Accelerating Scientific Discovery with Generative Knowledge Extraction,\n  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning","summary":"  Leveraging generative Artificial Intelligence (AI), we have transformed a\ndataset comprising 1,000 scientific papers into an ontological knowledge graph.\nThrough an in-depth structural analysis, we have calculated node degrees,\nidentified communities and connectivities, and evaluated clustering\ncoefficients and betweenness centrality of pivotal nodes, uncovering\nfascinating knowledge architectures. The graph has an inherently scale-free\nnature, is highly connected, and can be used for graph reasoning by taking\nadvantage of transitive and isomorphic properties that reveal unprecedented\ninterdisciplinary relationships that can be used to answer queries, identify\ngaps in knowledge, propose never-before-seen material designs, and predict\nmaterial behaviors. We compute deep node embeddings for combinatorial node\nsimilarity ranking for use in a path sampling strategy links dissimilar\nconcepts that have previously not been related. One comparison revealed\nstructural parallels between biological materials and Beethoven's 9th Symphony,\nhighlighting shared patterns of complexity through isomorphic mapping. In\nanother example, the algorithm proposed a hierarchical mycelium-based composite\nbased on integrating path sampling with principles extracted from Kandinsky's\n'Composition VII' painting. The resulting material integrates an innovative set\nof concepts that include a balance of chaos/order, adjustable porosity,\nmechanical strength, and complex patterned chemical functionalization. We\nuncover other isomorphisms across science, technology and art, revealing a\nnuanced ontology of immanence that reveal a context-dependent heterarchical\ninterplay of constituents. Graph-based generative AI achieves a far higher\ndegree of novelty, explorative capacity, and technical detail, than\nconventional approaches and establishes a widely useful framework for\ninnovation by revealing hidden connections.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2403.11996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17753v1","updated":"2024-03-26T14:43:57Z","published":"2024-03-26T14:43:57Z","title":"CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream\n  Enhanced Rectified Transformer Model","summary":"  Accurate, and effective traffic forecasting is vital for smart traffic\nsystems, crucial in urban traffic planning and management. Current\nSpatio-Temporal Transformer models, despite their prediction capabilities,\nstruggle with balancing computational efficiency and accuracy, favoring global\nover local information, and handling spatial and temporal data separately,\nlimiting insight into complex interactions. We introduce the Criss-Crossed\nDual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes\nthree innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA),\nEnhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified\nTemporal Self-attention (ReTSA). These modules aim to lower computational needs\nvia sparse attention, focus on local information for better traffic dynamics\nunderstanding, and merge spatial and temporal insights through a unique\nlearning method. Extensive tests on six real-world datasets highlight\nCCDSReFormer's superior performance. An ablation study also confirms the\nsignificant impact of each component on the model's predictive accuracy,\nshowcasing our model's ability to forecast traffic flow effectively.\n","authors":["Zhiqi Shao","Michael G. H. Bell","Ze Wang","D. Glenn Geers","Xusheng Yao","Junbin Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17753v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2310.02129v4","updated":"2024-03-26T14:38:23Z","published":"2023-10-03T15:10:46Z","title":"Unveiling the Pitfalls of Knowledge Editing for Large Language Models","summary":"  As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.\n","authors":["Zhoubo Li","Ningyu Zhang","Yunzhi Yao","Mengru Wang","Xi Chen","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02129v4.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.17745v1","updated":"2024-03-26T14:36:22Z","published":"2024-03-26T14:36:22Z","title":"Leave No Patient Behind: Enhancing Medication Recommendation for Rare\n  Disease Patients","summary":"  Medication recommendation systems have gained significant attention in\nhealthcare as a means of providing tailored and effective drug combinations\nbased on patients' clinical information. However, existing approaches often\nsuffer from fairness issues, as recommendations tend to be more accurate for\npatients with common diseases compared to those with rare conditions. In this\npaper, we propose a novel model called Robust and Accurate REcommendations for\nMedication (RAREMed), which leverages the pretrain-finetune learning paradigm\nto enhance accuracy for rare diseases. RAREMed employs a transformer encoder\nwith a unified input sequence approach to capture complex relationships among\ndisease and procedure codes. Additionally, it introduces two self-supervised\npre-training tasks, namely Sequence Matching Prediction (SMP) and Self\nReconstruction (SR), to learn specialized medication needs and interrelations\namong clinical codes. Experimental results on two real-world datasets\ndemonstrate that RAREMed provides accurate drug sets for both rare and common\ndisease patients, thereby mitigating unfairness in medication recommendation\nsystems.\n","authors":["Zihao Zhao","Yi Jing","Fuli Feng","Jiancan Wu","Chongming Gao","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2403.17745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17729v1","updated":"2024-03-26T14:18:43Z","published":"2024-03-26T14:18:43Z","title":"EulerFormer: Sequential User Behavior Modeling with Complex Vector\n  Attention","summary":"  To capture user preference, transformer models have been widely applied to\nmodel sequential user behavior data. The core of transformer architecture lies\nin the self-attention mechanism, which computes the pairwise attention scores\nin a sequence. Due to the permutation-equivariant nature, positional encoding\nis used to enhance the attention between token representations. In this\nsetting, the pairwise attention scores can be derived by both semantic\ndifference and positional difference. However, prior studies often model the\ntwo kinds of difference measurements in different ways, which potentially\nlimits the expressive capacity of sequence modeling. To address this issue,\nthis paper proposes a novel transformer variant with complex vector attention,\nnamed EulerFormer, which provides a unified theoretical framework to formulate\nboth semantic difference and positional difference. The EulerFormer involves\ntwo key technical improvements. First, it employs a new transformation function\nfor efficiently transforming the sequence tokens into polar-form complex\nvectors using Euler's formula, enabling the unified modeling of both semantic\nand positional information in a complex rotation form.Secondly, it develops a\ndifferential rotation mechanism, where the semantic rotation angles can be\ncontrolled by an adaptation function, enabling the adaptive integration of the\nsemantic and positional information according to the semantic\ncontexts.Furthermore, a phase contrastive learning task is proposed to improve\nthe anisotropy of contextual representations in EulerFormer. Our theoretical\nframework possesses a high degree of completeness and generality. It is more\nrobust to semantic variations and possesses moresuperior theoretical properties\nin principle. Extensive experiments conducted on four public datasets\ndemonstrate the effectiveness and efficiency of our approach.\n","authors":["Zhen Tian","Wayne Xin Zhao","Changwang Zhang","Xin Zhao","Zhongrui Ma","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2403.17729v1.pdf","comment":"Accepted for publication in SIGIR'24"},{"id":"http://arxiv.org/abs/2403.17728v1","updated":"2024-03-26T14:17:01Z","published":"2024-03-26T14:17:01Z","title":"Masked Autoencoders are PDE Learners","summary":"  Neural solvers for partial differential equations (PDEs) have great\npotential, yet their practicality is currently limited by their\ngeneralizability. PDEs evolve over broad scales and exhibit diverse behaviors;\npredicting these phenomena will require learning representations across a wide\nvariety of inputs, which may encompass different coefficients, geometries, or\nequations. As a step towards generalizable PDE modeling, we adapt masked\npretraining for PDEs. Through self-supervised learning across PDEs, masked\nautoencoders can learn useful latent representations for downstream tasks. In\nparticular, masked pretraining can improve coefficient regression and\ntimestepping performance of neural solvers on unseen equations. We hope that\nmasked pretraining can emerge as a unifying method across large, unlabeled, and\nheterogeneous datasets to learn latent physics at scale.\n","authors":["Anthony Zhou","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2403.17728v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.05337v2","updated":"2024-03-26T14:09:56Z","published":"2023-12-08T19:52:48Z","title":"Artificial Neural Nets and the Representation of Human Concepts","summary":"  What do artificial neural networks (ANNs) learn? The machine learning (ML)\ncommunity shares the narrative that ANNs must develop abstract human concepts\nto perform complex tasks. Some go even further and believe that these concepts\nare stored in individual units of the network. Based on current research, I\nsystematically investigate the assumptions underlying this narrative. I\nconclude that ANNs are indeed capable of performing complex prediction tasks,\nand that they may learn human and non-human concepts to do so. However,\nevidence indicates that ANNs do not represent these concepts in individual\nunits.\n","authors":["Timo Freiesleben"],"pdf_url":"https://arxiv.org/pdf/2312.05337v2.pdf","comment":"For: Philosophy of Science for Machine Learning: Core Issues and New\n  Perspectives, edited by Juan Duran and Giorgia Pozzi"},{"id":"http://arxiv.org/abs/2310.02969v2","updated":"2024-03-26T14:00:59Z","published":"2023-10-04T17:06:30Z","title":"Dual Conic Proxies for AC Optimal Power Flow","summary":"  In recent years, there has been significant interest in the development of\nmachine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF).\nAlthough significant progress has been achieved in predicting high-quality\nprimal solutions, no existing learning-based approach can provide valid dual\nbounds for AC-OPF. This paper addresses this gap by training optimization\nproxies for a convex relaxation of AC-OPF. Namely, the paper considers a\nsecond-order cone (SOC) relaxation of AC-OPF, and proposes \\revision{a novel\narchitecture} that embeds a fast, differentiable (dual) feasibility recovery,\nthus providing valid dual bounds. The paper combines this new architecture with\na self-supervised learning scheme, which alleviates the need for costly\ntraining data generation. Extensive numerical experiments on medium- and\nlarge-scale power grids demonstrate the efficiency and scalability of the\nproposed methodology.\n","authors":["Guancheng Qiu","Mathieu Tanneau","Pascal Van Hentenryck"],"pdf_url":"https://arxiv.org/pdf/2310.02969v2.pdf","comment":"accepted to PSCC 2024"},{"id":"http://arxiv.org/abs/2310.02869v2","updated":"2024-03-26T13:43:55Z","published":"2023-10-04T15:03:56Z","title":"Harmonic Control Lyapunov Barrier Functions for Constrained Optimal\n  Control with Reach-Avoid Specifications","summary":"  This paper introduces harmonic control Lyapunov barrier functions (harmonic\nCLBF) that aid in constrained control problems such as reach-avoid problems.\nHarmonic CLBFs exploit the maximum principle that harmonic functions satisfy to\nencode the properties of control Lyapunov barrier functions (CLBFs). As a\nresult, they can be initiated at the start of an experiment rather than trained\nbased on sample trajectories. The control inputs are selected to maximize the\ninner product of the system dynamics with the steepest descent direction of the\nharmonic CLBF. Numerical results are presented with four different systems\nunder different reach-avoid environments. Harmonic CLBFs show a significantly\nlow risk of entering unsafe regions and a high probability of entering the goal\nregion.\n","authors":["Amartya Mukherjee","Ruikun Zhou","Haocheng Chang","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2310.02869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17701v1","updated":"2024-03-26T13:40:18Z","published":"2024-03-26T13:40:18Z","title":"Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical\n  Image Segmentation","summary":"  Image segmentation holds a vital position in the realms of diagnosis and\ntreatment within the medical domain. Traditional convolutional neural networks\n(CNNs) and Transformer models have made significant advancements in this realm,\nbut they still encounter challenges because of limited receptive field or high\ncomputing complexity. Recently, State Space Models (SSMs), particularly Mamba\nand its variants, have demonstrated notable performance in the field of vision.\nHowever, their feature extraction methods may not be sufficiently effective and\nretain some redundant structures, leaving room for parameter reduction.\nMotivated by previous spatial and channel attention methods, we propose Triplet\nMamba-UNet. The method leverages residual VSS Blocks to extract intensive\ncontextual features, while Triplet SSM is employed to fuse features across\nspatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,\nCVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,\ndemonstrating the superior segmentation performance of our proposed TM-UNet.\nAdditionally, compared to the previous VM-UNet, our model achieves a one-third\nreduction in parameters.\n","authors":["Hao Tang","Lianglun Cheng","Guoheng Huang","Zhengguang Tan","Junhao Lu","Kaihong Wu"],"pdf_url":"https://arxiv.org/pdf/2403.17701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17698v1","updated":"2024-03-26T13:38:06Z","published":"2024-03-26T13:38:06Z","title":"MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding\n  Length Extrapolation","summary":"  When the predicted sequence length exceeds the length seen during training,\nthe transformer's inference accuracy diminishes. Existing relative position\nencoding methods, such as those based on the ALiBi technique, address the\nlength extrapolation challenge exclusively through the implementation of a\nsingle kernel function, which introduces a constant bias to every post-softmax\nattention scores according to their distance. These approaches do not\ninvestigate or employ multiple kernel functions to address the extrapolation\nchallenge. Drawing on the ALiBi approach, this study proposes a novel relative\npositional encoding method, called MEP, which employs a weighted average to\ncombine distinct kernel functions(such as the exponential kernel and the\nGaussian kernel) to generate a bias that is applied to post-softmax attention\nscores. Initially, the framework utilizes various kernel functions to construct\nmultiple kernel functions. Each kernel function adheres to a consistent mean\nweight coefficient, harnessing the synergistic advantages of different kernels\nto formulate an innovative bias function. Subsequently, specific slopes are\ntailored for each kernel function, applying penalties at varying rates, to\nenhance the model's extrapolation capabilities. Finally, this bias is\nseamlessly incorporated as a penalty to the post-softmax scores. We present two\ndistinct versions of our method: a parameter-free variant that requires no new\nlearnable parameters, which enhances length extrapolation capabilities without\ncompromising training efficiency, and a parameterized variant capable of\nintegrating state-of-the-art techniques. Empirical evaluations across diverse\ndatasets have demonstrated that both variants of our method achieve\nstate-of-the-art performance, outperforming traditional parameter-free and\nparameterized approaches.\n","authors":["Weiguo Gao"],"pdf_url":"https://arxiv.org/pdf/2403.17698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17695v1","updated":"2024-03-26T13:35:10Z","published":"2024-03-26T13:35:10Z","title":"PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition","summary":"  We present PlainMamba: a simple non-hierarchical state space model (SSM)\ndesigned for general visual recognition. The recent Mamba model has shown how\nSSMs can be highly competitive with other architectures on sequential data and\ninitial attempts have been made to apply it to images. In this paper, we\nfurther adapt the selective scanning process of Mamba to the visual domain,\nenhancing its ability to learn features from two-dimensional images by (i) a\ncontinuous 2D scanning process that improves spatial continuity by ensuring\nadjacency of tokens in the scanning sequence, and (ii) direction-aware updating\nwhich enables the model to discern the spatial relations of tokens by encoding\ndirectional information. Our architecture is designed to be easy to use and\neasy to scale, formed by stacking identical PlainMamba blocks, resulting in a\nmodel with constant width throughout all layers. The architecture is further\nsimplified by removing the need for special tokens. We evaluate PlainMamba on a\nvariety of visual recognition tasks including image classification, semantic\nsegmentation, object detection, and instance segmentation. Our method achieves\nperformance gains over previous non-hierarchical models and is competitive with\nhierarchical alternatives. For tasks requiring high-resolution inputs, in\nparticular, PlainMamba requires much less computing while maintaining high\nperformance. Code and models are available at\nhttps://github.com/ChenhongyiYang/PlainMamba\n","authors":["Chenhongyi Yang","Zehui Chen","Miguel Espinosa","Linus Ericsson","Zhenyu Wang","Jiaming Liu","Elliot J. Crowley"],"pdf_url":"https://arxiv.org/pdf/2403.17695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17692v1","updated":"2024-03-26T13:33:16Z","published":"2024-03-26T13:33:16Z","title":"Manifold-Guided Lyapunov Control with Diffusion Models","summary":"  This paper presents a novel approach to generating stabilizing controllers\nfor a large class of dynamical systems using diffusion models. The core\nobjective is to develop stabilizing control functions by identifying the\nclosest asymptotically stable vector field relative to a predetermined manifold\nand adjusting the control function based on this finding. To achieve this, we\nemploy a diffusion model trained on pairs consisting of asymptotically stable\nvector fields and their corresponding Lyapunov functions. Our numerical results\ndemonstrate that this pre-trained model can achieve stabilization over\npreviously unseen systems efficiently and rapidly, showcasing the potential of\nour approach in fast zero-shot control and generalizability.\n","authors":["Amartya Mukherjee","Thanin Quartz","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17692v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2301.12778v2","updated":"2024-03-26T13:29:16Z","published":"2023-01-30T10:48:10Z","title":"Investigating Feature and Model Importance in Android Malware Detection:\n  An Implemented Survey and Experimental Comparison of ML-Based Methods","summary":"  The popularity of Android means it is a common target for malware. Over the\nyears, various studies have found that machine learning models can effectively\ndiscriminate malware from benign applications. However, as the operating system\nevolves, so does malware, bringing into question the findings of these previous\nstudies, many of which report very high accuracies using small, outdated, and\noften imbalanced datasets. In this paper, we reimplement 18 representative past\nworks and reevaluate them using a balanced, relevant, and up-to-date dataset\ncomprising 124,000 applications. We also carry out new experiments designed to\nfill holes in existing knowledge, and use our findings to identify the most\neffective features and models to use for Android malware detection within a\ncontemporary environment. We show that high detection accuracies (up to 96.8%)\ncan be achieved using features extracted through static analysis alone,\nyielding a modest benefit (1%) from using far more expensive dynamic analysis.\nAPI calls and opcodes are the most productive static and TCP network traffic\nprovide the most predictive dynamic features. Random forests are generally the\nmost effective model, outperforming more complex deep learning approaches.\nWhilst directly combining static and dynamic features is generally ineffective,\nensembling models separately leads to performances comparable to the best\nmodels but using less brittle features.\n","authors":["Ali Muzaffar","Hani Ragab Hassen","Hind Zantout","Michael A Lones"],"pdf_url":"https://arxiv.org/pdf/2301.12778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17094v2","updated":"2024-03-26T13:21:43Z","published":"2023-11-28T06:17:49Z","title":"In Search of a Data Transformation That Accelerates Neural Field\n  Training","summary":"  Neural field is an emerging paradigm in data representation that trains a\nneural network to approximate the given signal. A key obstacle that prevents\nits widespread adoption is the encoding speed-generating neural fields requires\nan overfitting of a neural network, which can take a significant number of SGD\nsteps to reach the desired fidelity level. In this paper, we delve into the\nimpacts of data transformations on the speed of neural field training,\nspecifically focusing on how permuting pixel locations affect the convergence\nspeed of SGD. Counterintuitively, we find that randomly permuting the pixel\nlocations can considerably accelerate the training. To explain this phenomenon,\nwe examine the neural field training through the lens of PSNR curves, loss\nlandscapes, and error patterns. Our analyses suggest that the random pixel\npermutations remove the easy-to-fit patterns, which facilitate easy\noptimization in the early stage but hinder capturing fine details of the\nsignal.\n","authors":["Junwon Seo","Sangyoon Lee","Kwang In Kim","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2311.17094v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.16915v2","updated":"2024-03-26T13:11:44Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.17673v1","updated":"2024-03-26T13:02:43Z","published":"2024-03-26T13:02:43Z","title":"How Private is DP-SGD?","summary":"  We demonstrate a substantial gap between the privacy guarantees of the\nAdaptive Batch Linear Queries (ABLQ) mechanism under different types of batch\nsampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of\nDifferentially Private Stochastic Gradient Descent (DP-SGD) follows by\ninterpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is\nmore commonly used in practical implementations, it is neither analytically nor\nnumerically amenable to easy privacy analysis. On the other hand, Poisson\nsubsampling based DP-SGD is challenging to scalably implement, but has a\nwell-understood privacy analysis, with multiple open-source numerically tight\nprivacy accountants available. This has led to a common practice of using\nshuffling based DP-SGD in practice, but using the privacy analysis for the\ncorresponding Poisson subsampling version. Our result shows that there can be a\nsubstantial gap between the privacy analysis when using the two types of batch\nsampling, and thus advises caution in reporting privacy parameters for DP-SGD.\n","authors":["Lynn Chua","Badih Ghazi","Pritish Kamath","Ravi Kumar","Pasin Manurangsi","Amer Sinha","Chiyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01885v2","updated":"2024-03-26T12:59:44Z","published":"2023-11-03T12:54:05Z","title":"Domain Randomization via Entropy Maximization","summary":"  Varying dynamics parameters in simulation is a popular Domain Randomization\n(DR) approach for overcoming the reality gap in Reinforcement Learning (RL).\nNevertheless, DR heavily hinges on the choice of the sampling distribution of\nthe dynamics parameters, since high variability is crucial to regularize the\nagent's behavior but notoriously leads to overly conservative policies when\nrandomizing excessively. In this paper, we propose a novel approach to address\nsim-to-real transfer, which automatically shapes dynamics distributions during\ntraining in simulation without requiring real-world data. We introduce DOmain\nRAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization\nproblem that directly maximizes the entropy of the training distribution while\nretaining generalization capabilities. In achieving this, DORAEMON gradually\nincreases the diversity of sampled dynamics parameters as long as the\nprobability of success of the current policy is sufficiently high. We\nempirically validate the consistent benefits of DORAEMON in obtaining highly\nadaptive and generalizable policies, i.e. solving the task at hand across the\nwidest range of dynamics parameters, as opposed to representative baselines\nfrom the DR literature. Notably, we also demonstrate the Sim2Real applicability\nof DORAEMON through its successful zero-shot transfer in a robotic manipulation\nsetup under unknown real-world parameters.\n","authors":["Gabriele Tiboni","Pascal Klink","Jan Peters","Tatiana Tommasi","Carlo D'Eramo","Georgia Chalvatzaki"],"pdf_url":"https://arxiv.org/pdf/2311.01885v2.pdf","comment":"Published as a conference paper at ICLR 2024. Project website at\n  https://gabrieletiboni.github.io/doraemon/"},{"id":"http://arxiv.org/abs/2403.17660v1","updated":"2024-03-26T12:47:04Z","published":"2024-03-26T12:47:04Z","title":"CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1\n  Perturbations","summary":"  Optimal Power Flow (OPF) refers to a wide range of related optimization\nproblems with the goal of operating power systems efficiently and securely. In\nthe simplest setting, OPF determines how much power to generate in order to\nminimize costs while meeting demand for power and satisfying physical and\noperational constraints. In even the simplest case, power grid operators use\napproximations of the AC-OPF problem because solving the exact problem is\nprohibitively slow with state-of-the-art solvers. These approximations\nsacrifice accuracy and operational feasibility in favor of speed. This\ntrade-off leads to costly \"uplift payments\" and increased carbon emissions,\nespecially for large power grids. In the present work, we train a deep learning\nsystem (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF\ncost) without compromising speed (running in as little as 33--65 ms).\nImportantly, CANOS scales to realistic grid sizes with promising empirical\nresults on grids containing as many as 10,000 buses. Finally, because CANOS is\na Graph Neural Network, it is robust to changes in topology. We show that CANOS\nis accurate across N-1 topological perturbations of a base grid typically used\nin security-constrained analysis. This paves the way for more efficient\noptimization of more complex OPF problems which alter grid connectivity such as\nunit commitment, topology optimization and security-constrained OPF.\n","authors":["Luis Piloto","Sofia Liguori","Sephora Madjiheurem","Miha Zgubic","Sean Lovett","Hamish Tomlinson","Sophie Elster","Chris Apps","Sims Witherspoon"],"pdf_url":"https://arxiv.org/pdf/2403.17660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17656v1","updated":"2024-03-26T12:39:02Z","published":"2024-03-26T12:39:02Z","title":"SGHormer: An Energy-Saving Graph Transformer Driven by Spikes","summary":"  Graph Transformers (GTs) with powerful representation learning ability make a\nhuge success in wide range of graph tasks. However, the costs behind\noutstanding performances of GTs are higher energy consumption and computational\noverhead. The complex structure and quadratic complexity during attention\ncalculation in vanilla transformer seriously hinder its scalability on the\nlarge-scale graph data. Though existing methods have made strides in\nsimplifying combinations among blocks or attention-learning paradigm to improve\nGTs' efficiency, a series of energy-saving solutions originated from\nbiologically plausible structures are rarely taken into consideration when\nconstructing GT framework. To this end, we propose a new spiking-based graph\ntransformer (SGHormer). It turns full-precision embeddings into sparse and\nbinarized spikes to reduce memory and computational costs. The spiking graph\nself-attention and spiking rectify blocks in SGHormer explicitly capture global\nstructure information and recover the expressive power of spiking embeddings,\nrespectively. In experiments, SGHormer achieves comparable performances to\nother full-precision GTs with extremely low computational energy consumption.\nThe results show that SGHomer makes a remarkable progress in the field of\nlow-energy GTs.\n","authors":["Huizhe Zhang","Jintang Li","Liang Chen","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.17656v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.13374v2","updated":"2024-03-26T12:33:16Z","published":"2024-03-20T08:15:08Z","title":"Byzantine-resilient Federated Learning With Adaptivity to Data\n  Heterogeneity","summary":"  This paper deals with federated learning (FL) in the presence of malicious\nByzantine attacks and data heterogeneity. A novel Robust Average Gradient\nAlgorithm (RAGA) is proposed, which leverages the geometric median for\naggregation and can freely select the round number for local updating.\nDifferent from most existing resilient approaches, which perform convergence\nanalysis based on strongly-convex loss function or homogeneously distributed\ndataset, we conduct convergence analysis for not only strongly-convex but also\nnon-convex loss function over heterogeneous dataset. According to our\ntheoretical analysis, as long as the fraction of dataset from malicious users\nis less than half, RAGA can achieve convergence at rate\n$\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and\n$\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for\nstrongly-convex loss function. Moreover, stationary point or global optimal\nsolution is proved to obtainable as data heterogeneity vanishes. Experimental\nresults corroborate the robustness of RAGA to Byzantine attacks and verifies\nthe advantage of RAGA over baselines on convergence performance under various\nintensity of Byzantine attacks, for heterogeneous dataset.\n","authors":["Shiyuan Zuo","Xingrun Yan","Rongfei Fan","Han Hu","Hangguan Shan","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2403.13374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17646v1","updated":"2024-03-26T12:28:04Z","published":"2024-03-26T12:28:04Z","title":"Uncertainty-aware Distributional Offline Reinforcement Learning","summary":"  Offline reinforcement learning (RL) presents distinct challenges as it relies\nsolely on observational data. A central concern in this context is ensuring the\nsafety of the learned policy by quantifying uncertainties associated with\nvarious actions and environmental stochasticity. Traditional approaches\nprimarily emphasize mitigating epistemic uncertainty by learning risk-averse\npolicies, often overlooking environmental stochasticity. In this study, we\npropose an uncertainty-aware distributional offline RL method to simultaneously\naddress both epistemic uncertainty and environmental stochasticity. We propose\na model-free offline RL algorithm capable of learning risk-averse policies and\ncharacterizing the entire distribution of discounted cumulative rewards, as\nopposed to merely maximizing the expected value of accumulated discounted\nreturns. Our method is rigorously evaluated through comprehensive experiments\nin both risk-sensitive and risk-neutral benchmarks, demonstrating its superior\nperformance.\n","authors":["Xiaocong Chen","Siyu Wang","Tong Yu","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17637v1","updated":"2024-03-26T12:12:44Z","published":"2024-03-26T12:12:44Z","title":"PeersimGym: An Environment for Solving the Task Offloading Problem with\n  Reinforcement Learning","summary":"  Task offloading, crucial for balancing computational loads across devices in\nnetworks such as the Internet of Things, poses significant optimization\nchallenges, including minimizing latency and energy usage under strict\ncommunication and storage constraints. While traditional optimization falls\nshort in scalability; and heuristic approaches lack in achieving optimal\noutcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the\nlearning of optimal offloading strategies through iterative interactions.\nHowever, the efficacy of RL hinges on access to rich datasets and\ncustom-tailored, realistic training environments. To address this, we introduce\nPeersimGym, an open-source, customizable simulation environment tailored for\ndeveloping and optimizing task offloading strategies within computational\nnetworks. PeersimGym supports a wide range of network topologies and\ncomputational constraints and integrates a \\textit{PettingZoo}-based interface\nfor RL agent deployment in both solo and multi-agent setups. Furthermore, we\ndemonstrate the utility of the environment through experiments with Deep\nReinforcement Learning agents, showcasing the potential of RL-based approaches\nto significantly enhance offloading strategies in distributed computing\nsettings. PeersimGym thus bridges the gap between theoretical RL models and\ntheir practical applications, paving the way for advancements in efficient task\noffloading methodologies.\n","authors":["Frederico Metelo","Stevo Racković","Pedro Ákos","Cláudia Soares"],"pdf_url":"https://arxiv.org/pdf/2403.17637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17634v1","updated":"2024-03-26T12:08:58Z","published":"2024-03-26T12:08:58Z","title":"Retentive Decision Transformer with Adaptive Masking for Reinforcement\n  Learning based Recommendation Systems","summary":"  Reinforcement Learning-based Recommender Systems (RLRS) have shown promise\nacross a spectrum of applications, from e-commerce platforms to streaming\nservices. Yet, they grapple with challenges, notably in crafting reward\nfunctions and harnessing large pre-existing datasets within the RL framework.\nRecent advancements in offline RLRS provide a solution for how to address these\ntwo challenges. However, existing methods mainly rely on the transformer\narchitecture, which, as sequence lengths increase, can introduce challenges\nassociated with computational resources and training costs. Additionally, the\nprevalent methods employ fixed-length input trajectories, restricting their\ncapacity to capture evolving user preferences. In this study, we introduce a\nnew offline RLRS method to deal with the above problems. We reinterpret the\nRLRS challenge by modeling sequential decision-making as an inference task,\nleveraging adaptive masking configurations. This adaptive approach selectively\nmasks input tokens, transforming the recommendation task into an inference\nchallenge based on varying token subsets, thereby enhancing the agent's ability\nto infer across diverse trajectory lengths. Furthermore, we incorporate a\nmulti-scale segmented retention mechanism that facilitates efficient modeling\nof long sequences, significantly enhancing computational efficiency. Our\nexperimental analysis, conducted on both online simulator and offline datasets,\nclearly demonstrates the advantages of our proposed method.\n","authors":["Siyu Wang","Xiaocong Chen","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2403.17634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17632v1","updated":"2024-03-26T12:08:05Z","published":"2024-03-26T12:08:05Z","title":"Data-driven Energy Consumption Modelling for Electric Micromobility\n  using an Open Dataset","summary":"  The escalating challenges of traffic congestion and environmental degradation\nunderscore the critical importance of embracing E-Mobility solutions in urban\nspaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes,\nplay a pivotal role in this transition, offering sustainable alternatives for\nurban commuters. However, the energy consumption patterns for these tools are a\ncritical aspect that impacts their effectiveness in real-world scenarios and is\nessential for trip planning and boosting user confidence in using these. To\nthis effect, recent studies have utilised physical models customised for\nspecific mobility tools and conditions, but these models struggle with\ngeneralization and effectiveness in real-world scenarios due to a notable\nabsence of open datasets for thorough model evaluation and verification. To\nfill this gap, our work presents an open dataset, collected in Dublin, Ireland,\nspecifically designed for energy modelling research related to E-Scooters and\nE-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption\nmodelling based on the dataset using a set of representative machine learning\nalgorithms and compare their performance against the contemporary mathematical\nmodels as a baseline. Our results demonstrate a notable advantage for\ndata-driven models in comparison to the corresponding mathematical models for\nestimating energy consumption. Specifically, data-driven models outperform\nphysical models in accuracy by up to 83.83% for E-Bikes and 82.16% for\nE-Scooters based on an in-depth analysis of the dataset under certain\nassumptions.\n","authors":["Yue Ding","Sen Yan","Maqsood Hussain Shah","Hongyuan Fang","Ji Li","Mingming Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17632v1.pdf","comment":"7 pages, 5 figures, 4 tables. This manuscript has been accepted by\n  the IEEE ITEC 2024"},{"id":"http://arxiv.org/abs/2311.07939v2","updated":"2024-03-26T11:54:27Z","published":"2023-11-14T06:33:41Z","title":"Discretized Distributed Optimization over Dynamic Digraphs","summary":"  We consider a discrete-time model of continuous-time distributed optimization\nover dynamic directed-graphs (digraphs) with applications to distributed\nlearning. Our optimization algorithm works over general strongly connected\ndynamic networks under switching topologies, e.g., in mobile multi-agent\nsystems and volatile networks due to link failures. Compared to many existing\nlines of work, there is no need for bi-stochastic weight designs on the links.\nThe existing literature mostly needs the link weights to be stochastic using\nspecific weight-design algorithms needed both at the initialization and at all\ntimes when the topology of the network changes. This paper eliminates the need\nfor such algorithms and paves the way for distributed optimization over\ntime-varying digraphs. We derive the bound on the gradient-tracking step-size\nand discrete time-step for convergence and prove dynamic stability using\narguments from consensus algorithms, matrix perturbation theory, and Lyapunov\ntheory. This work, particularly, is an improvement over existing\nstochastic-weight undirected networks in case of link removal or packet drops.\nThis is because the existing literature may need to rerun time-consuming and\ncomputationally complex algorithms for stochastic design, while the proposed\nstrategy works as long as the underlying network is weight-symmetric and\nbalanced. The proposed optimization framework finds applications to distributed\nclassification and learning.\n","authors":["Mohammadreza Doostmohammadian","Wei Jiang","Muwahida Liaquat","Alireza Aghasi","Houman Zarrabi"],"pdf_url":"https://arxiv.org/pdf/2311.07939v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15694v5","updated":"2024-03-26T11:52:59Z","published":"2023-10-24T10:05:32Z","title":"COPR: Continual Learning Human Preference through Optimal Policy\n  Regularization","summary":"  The technique of Reinforcement Learning from Human Feedback (RLHF) is a\ncommonly employed method to improve pre-trained Language Models (LM), enhancing\ntheir ability to conform to human preferences. Nevertheless, the current\nRLHF-based LMs necessitate full retraining each time novel queries or feedback\nare introduced, which becomes a challenging task because human preferences can\nvary between different domains or tasks. Retraining LMs poses practical\ndifficulties in many real-world situations due to the significant time and\ncomputational resources required, along with concerns related to data privacy.\nTo address this limitation, we propose a new method called Continual Optimal\nPolicy Regularization (COPR), in which we compute the distribution of optimal\npolicy bypassing the partition function and then regularize the current policy\nbased on the historically optimal distribution to mitigate Catastrophic\nForgetting (CF). COPR involves a single learning phase and doesn't necessitate\ncomplex reinforcement learning. Importantly, it shares the capability with RLHF\nto learn from unlabeled data by maintaining a scoring module, similar to reward\nmodel, making it flexible for continually learning without human feedback. Our\nexperimental results show that COPR outperforms strong Continuous Learning (CL)\nbaselines when it comes to consistently aligning with human preferences on\nincremental tasks and domains.\n","authors":["Han Zhang","Lin Gui","Yuanzhao Zhai","Hui Wang","Yu Lei","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2310.15694v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17569v2","updated":"2024-03-26T11:52:23Z","published":"2023-10-26T16:58:01Z","title":"SD4Match: Learning to Prompt Stable Diffusion Model for Semantic\n  Matching","summary":"  In this paper, we address the challenge of matching semantically similar\nkeypoints across image pairs. Existing research indicates that the intermediate\noutput of the UNet within the Stable Diffusion (SD) can serve as robust image\nfeature maps for such a matching task. We demonstrate that by employing a basic\nprompt tuning technique, the inherent potential of Stable Diffusion can be\nharnessed, resulting in a significant enhancement in accuracy over previous\napproaches. We further introduce a novel conditional prompting module that\nconditions the prompt on the local details of the input image pairs, leading to\na further improvement in performance. We designate our approach as SD4Match,\nshort for Stable Diffusion for Semantic Matching. Comprehensive evaluations of\nSD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets\nnew benchmarks in accuracy across all these datasets. Particularly, SD4Match\noutperforms the previous state-of-the-art by a margin of 12 percentage points\non the challenging SPair-71k dataset.\n","authors":["Xinghui Li","Jingyi Lu","Kai Han","Victor Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2310.17569v2.pdf","comment":"Accepted to CVPR 2024. Project website:\n  https://sd4match.active.vision/"},{"id":"http://arxiv.org/abs/2403.17608v1","updated":"2024-03-26T11:39:00Z","published":"2024-03-26T11:39:00Z","title":"Fake or JPEG? Revealing Common Biases in Generated Image Detection\n  Datasets","summary":"  The widespread adoption of generative image models has highlighted the urgent\nneed to detect artificial content, which is a crucial step in combating\nwidespread manipulation and misinformation. Consequently, numerous detectors\nand associated datasets have emerged. However, many of these datasets\ninadvertently introduce undesirable biases, thereby impacting the effectiveness\nand evaluation of detectors. In this paper, we emphasize that many datasets for\nAI-generated image detection contain biases related to JPEG compression and\nimage size. Using the GenImage dataset, we demonstrate that detectors indeed\nlearn from these undesired factors. Furthermore, we show that removing the\nnamed biases substantially increases robustness to JPEG compression and\nsignificantly alters the cross-generator performance of evaluated detectors.\nSpecifically, it leads to more than 11 percentage points increase in\ncross-generator performance for ResNet50 and Swin-T detectors on the GenImage\ndataset, achieving state-of-the-art results.\n  We provide the dataset and source codes of this paper on the anonymous\nwebsite: https://www.unbiased-genimage.org\n","authors":["Patrick Grommelt","Louis Weiss","Franz-Josef Pfreundt","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2403.17608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03197v4","updated":"2024-03-26T11:37:38Z","published":"2023-11-06T15:39:05Z","title":"Stable Linear Subspace Identification: A Machine Learning Approach","summary":"  Machine Learning (ML) and linear System Identification (SI) have been\nhistorically developed independently. In this paper, we leverage\nwell-established ML tools - especially the automatic differentiation framework\n- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space\nSI methods using backpropagation. SIMBa relies on a novel\nLinear-Matrix-Inequality-based free parametrization of Schur matrices to ensure\nthe stability of the identified model.\n  We show how SIMBa generally outperforms traditional linear state-space SI\nmethods, and sometimes significantly, although at the price of a higher\ncomputational burden. This performance gap is particularly remarkable compared\nto other SI methods with stability guarantees, where the gain is frequently\nabove 25% in our investigations, hinting at SIMBa's ability to simultaneously\nachieve state-of-the-art fitting performance and enforce stability.\nInterestingly, these observations hold for a wide variety of input-output\nsystems and on both simulated and real-world data, showcasing the flexibility\nof the proposed approach. We postulate that this new SI paradigm presents a\ngreat extension potential to identify structured nonlinear models from data,\nand we hence open-source SIMBa on https://github.com/Cemempamoi/simba.\n","authors":["Loris Di Natale","Muhammad Zakwan","Bratislav Svetozarevic","Philipp Heer","Giancarlo Ferrari-Trecate","Colin N. Jones"],"pdf_url":"https://arxiv.org/pdf/2311.03197v4.pdf","comment":"Accepted at ECC 2024"},{"id":"http://arxiv.org/abs/2403.16451v2","updated":"2024-03-26T11:35:08Z","published":"2024-03-25T06:30:54Z","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","summary":"  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n","authors":["Xiang-Li Lu","Hwai-Jung Hsu","Che-Wei Chou","H. T. Kung","Chen-Hsin Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16451v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16212v2","updated":"2024-03-26T11:32:36Z","published":"2023-08-30T15:09:22Z","title":"RetroBridge: Modeling Retrosynthesis with Markov Bridges","summary":"  Retrosynthesis planning is a fundamental challenge in chemistry which aims at\ndesigning reaction pathways from commercially available starting materials to a\ntarget molecule. Each step in multi-step retrosynthesis planning requires\naccurate prediction of possible precursor molecules given the target molecule\nand confidence estimates to guide heuristic search algorithms. We model\nsingle-step retrosynthesis planning as a distribution learning problem in a\ndiscrete state space. First, we introduce the Markov Bridge Model, a generative\nframework aimed to approximate the dependency between two intractable discrete\ndistributions accessible via a finite sample of coupled data points. Our\nframework is based on the concept of a Markov bridge, a Markov process pinned\nat its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does\nnot need a tractable noise distribution as a sampling proxy and directly\noperates on the input product molecules as samples from the intractable prior\ndistribution. We then address the retrosynthesis planning problem with our\nnovel framework and introduce RetroBridge, a template-free retrosynthesis\nmodeling approach that achieves state-of-the-art results on standard evaluation\nbenchmarks.\n","authors":["Ilia Igashov","Arne Schneuing","Marwin Segler","Michael Bronstein","Bruno Correia"],"pdf_url":"https://arxiv.org/pdf/2308.16212v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00093v2","updated":"2024-03-26T11:20:02Z","published":"2024-01-31T12:41:27Z","title":"ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation","summary":"  System Verilog Assertion (SVA) formulation- a critical yet complex task is a\nprerequisite in the Formal Property Verification (FPV) process. Traditionally,\nSVA formulation involves expert-driven interpretation of specifications, which\nis timeconsuming and prone to human error. However, LLM-informed automatic\nassertion generation is gaining interest. We designeda novel framework called\nChIRAAG, based on OpenAI GPT4, to generate SVA assertions from natural language\nspecifications. ChIRAAG constitutes the systematic breakdown of design\nspecifications into a standardized format, further generating assertions from\nformatted specifications using LLM. Furthermore, we developed testbenches to\nverify/validate the LLM-generated assertions. Automatic feedback of log files\nfrom the simulation tool to the LLM ensures that the framework can generate\ncorrec SVAs automatically. Only 33% of LLM-generated raw assertions had errors.\nOur results on OpenTitan designs shows that LLMs can streamline and assist\nengineers in the assertion generation process, reshaping verification\nworkflows.\n","authors":["Bhabesh Mali","Karthik Maddala","Sweeya Reddy","Vatsal Gupta","Chandan Karfa","Ramesh Karri"],"pdf_url":"https://arxiv.org/pdf/2402.00093v2.pdf","comment":"6 pages, 5 figures and 2 table"},{"id":"http://arxiv.org/abs/2403.17601v1","updated":"2024-03-26T11:13:35Z","published":"2024-03-26T11:13:35Z","title":"LASIL: Learner-Aware Supervised Imitation Learning For Long-term\n  Microscopic Traffic Simulation","summary":"  Microscopic traffic simulation plays a crucial role in transportation\nengineering by providing insights into individual vehicle behavior and overall\ntraffic flow. However, creating a realistic simulator that accurately\nreplicates human driving behaviors in various traffic conditions presents\nsignificant challenges. Traditional simulators relying on heuristic models\noften fail to deliver accurate simulations due to the complexity of real-world\ntraffic environments. Due to the covariate shift issue, existing imitation\nlearning-based simulators often fail to generate stable long-term simulations.\nIn this paper, we propose a novel approach called learner-aware supervised\nimitation learning to address the covariate shift problem in multi-agent\nimitation learning. By leveraging a variational autoencoder simultaneously\nmodeling the expert and learner state distribution, our approach augments\nexpert states such that the augmented state is aware of learner state\ndistribution. Our method, applied to urban traffic simulation, demonstrates\nsignificant improvements over existing state-of-the-art baselines in both\nshort-term microscopic and long-term macroscopic realism when evaluated on the\nreal-world dataset pNEUMA.\n","authors":["Ke Guo","Zhenwei Miao","Wei Jing","Weiwei Liu","Weizi Li","Dayang Hao","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2403.17601v1.pdf","comment":"accepted by cvpr 2024. arXiv admin note: text overlap with\n  arXiv:2306.06401"},{"id":"http://arxiv.org/abs/2403.15905v2","updated":"2024-03-26T11:11:49Z","published":"2024-03-23T18:19:02Z","title":"Towards Low-Energy Adaptive Personalization for Resource-Constrained\n  Devices","summary":"  The personalization of machine learning (ML) models to address data drift is\na significant challenge in the context of Internet of Things (IoT)\napplications. Presently, most approaches focus on fine-tuning either the full\nbase model or its last few layers to adapt to new data, while often neglecting\nenergy costs. However, various types of data drift exist, and fine-tuning the\nfull base model or the last few layers may not result in optimal performance in\ncertain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy\nadaptive personalization framework designed for resource-constrained devices.\nWe categorize data drift and personalization into three types: input-level,\nfeature-level, and output-level. For each type, we fine-tune different blocks\nof the model to achieve optimal performance with reduced energy costs.\nSpecifically, input-, feature-, and output-level correspond to fine-tuning the\nfront, middle, and rear blocks of the model. We evaluate TBFT on a ResNet\nmodel, three datasets, three different training sizes, and a Raspberry Pi.\nCompared with the $Block Avg$, where each block is fine-tuned individually and\ntheir performance improvements are averaged, TBFT exhibits an improvement in\nmodel accuracy by an average of 15.30% whilst saving 41.57% energy consumption\non average compared with full fine-tuning.\n","authors":["Yushan Huang","Josh Millar","Yuxuan Long","Yuchen Zhao","Hamed Hadaddi"],"pdf_url":"https://arxiv.org/pdf/2403.15905v2.pdf","comment":"Accepetd to The 4th Workshop on Machine Learning and Systems\n  (EuroMLSys '24)"},{"id":"http://arxiv.org/abs/2306.00038v3","updated":"2024-03-26T11:07:30Z","published":"2023-05-31T09:51:45Z","title":"FedCSD: A Federated Learning Based Approach for Code-Smell Detection","summary":"  This paper proposes a Federated Learning Code Smell Detection (FedCSD)\napproach that allows organizations to collaboratively train federated ML models\nwhile preserving their data privacy. These assertions have been supported by\nthree experiments that have significantly leveraged three manually validated\ndatasets aimed at detecting and examining different code smell scenarios. In\nexperiment 1, which was concerned with a centralized training experiment,\ndataset two achieved the lowest accuracy (92.30%) with fewer smells, while\ndatasets one and three achieved the highest accuracy with a slight difference\n(98.90% and 99.5%, respectively). This was followed by experiment 2, which was\nconcerned with cross-evaluation, where each ML model was trained using one\ndataset, which was then evaluated over the other two datasets. Results from\nthis experiment show a significant drop in the model's accuracy (lowest\naccuracy: 63.80\\%) where fewer smells exist in the training dataset, which has\na noticeable reflection (technical debt) on the model's performance. Finally,\nthe last and third experiments evaluate our approach by splitting the dataset\ninto 10 companies. The ML model was trained on the company's site, then all\nmodel-updated weights were transferred to the server. Ultimately, an accuracy\nof 98.34% was achieved by the global model that has been trained using 10\ncompanies for 100 training rounds. The results reveal a slight difference in\nthe global model's accuracy compared to the highest accuracy of the centralized\nmodel, which can be ignored in favour of the global model's comprehensive\nknowledge, lower training cost, preservation of data privacy, and avoidance of\nthe technical debt problem.\n","authors":["Sadi Alawadi","Khalid Alkharabsheh","Fahed Alkhabbas","Victor Kebande","Feras M. Awaysheh","Fabio Palomba","Mohammed Awad"],"pdf_url":"https://arxiv.org/pdf/2306.00038v3.pdf","comment":"17 pages, 7 figures, Journal paper"},{"id":"http://arxiv.org/abs/2403.14438v2","updated":"2024-03-26T11:02:32Z","published":"2024-03-21T14:44:03Z","title":"A Multimodal Approach to Device-Directed Speech Detection with Large\n  Language Models","summary":"  Interactions with virtual assistants typically start with a predefined\ntrigger phrase followed by the user command. To make interactions with the\nassistant more intuitive, we explore whether it is feasible to drop the\nrequirement that users must begin each command with a trigger phrase. We\nexplore this task in three ways: First, we train classifiers using only\nacoustic information obtained from the audio waveform. Second, we take the\ndecoder outputs of an automatic speech recognition (ASR) system, such as 1-best\nhypotheses, as input features to a large language model (LLM). Finally, we\nexplore a multimodal system that combines acoustic and lexical features, as\nwell as ASR decoder signals in an LLM. Using multimodal information yields\nrelative equal-error-rate improvements over text-only and audio-only models of\nup to 39% and 61%. Increasing the size of the LLM and training with low-rank\nadaption leads to further relative EER reductions of up to 18% on our dataset.\n","authors":["Dominik Wagner","Alexander Churchill","Siddharth Sigtia","Panayiotis Georgiou","Matt Mirsamadi","Aarshee Mishra","Erik Marchi"],"pdf_url":"https://arxiv.org/pdf/2403.14438v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.03632"},{"id":"http://arxiv.org/abs/2403.17592v1","updated":"2024-03-26T11:01:53Z","published":"2024-03-26T11:01:53Z","title":"On the Benefits of Over-parameterization for Out-of-Distribution\n  Generalization","summary":"  In recent years, machine learning models have achieved success based on the\nindependently and identically distributed assumption. However, this assumption\ncan be easily violated in real-world applications, leading to the\nOut-of-Distribution (OOD) problem. Understanding how modern over-parameterized\nDNNs behave under non-trivial natural distributional shifts is essential, as\ncurrent theoretical understanding is insufficient. Existing theoretical works\noften provide meaningless results for over-parameterized models in OOD\nscenarios or even contradict empirical findings. To this end, we are\ninvestigating the performance of the over-parameterized model in terms of OOD\ngeneralization under the general benign overfitting conditions. Our analysis\nfocuses on a random feature model and examines non-trivial natural\ndistributional shifts, where the benign overfitting estimators demonstrate a\nconstant excess OOD loss, despite achieving zero excess in-distribution (ID)\nloss. We demonstrate that in this scenario, further increasing the model's\nparameterization can significantly reduce the OOD loss. Intuitively, the\nvariance term of ID loss remains low due to orthogonality of long-tail\nfeatures, meaning overfitting noise during training generally doesn't raise\ntesting loss. However, in OOD cases, distributional shift increases the\nvariance term. Thankfully, the inherent shift is unrelated to individual x,\nmaintaining the orthogonality of long-tail features. Expanding the hidden\ndimension can additionally improve this orthogonality by mapping the features\ninto higher-dimensional spaces, thereby reducing the variance term. We further\nshow that model ensembles also improve OOD loss, akin to increasing model\ncapacity. These insights explain the empirical phenomenon of enhanced OOD\ngeneralization through model ensembles, supported by consistent simulations\nwith theoretical results.\n","authors":["Yifan Hao","Yong Lin","Difan Zou","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17588v1","updated":"2024-03-26T10:54:07Z","published":"2024-03-26T10:54:07Z","title":"Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest\n  models","summary":"  Random Forest (RF) is well-known as an efficient ensemble learning method in\nterms of predictive performance. It is also considered a Black Box because of\nits hundreds of deep decision trees. This lack of interpretability can be a\nreal drawback for acceptance of RF models in several real-world applications,\nespecially those affecting one's lives, such as in healthcare, security, and\nlaw. In this work, we present Forest-ORE, a method that makes RF interpretable\nvia an optimized rule ensemble (ORE) for local and global interpretation.\nUnlike other rule-based approaches aiming at interpreting the RF model, this\nmethod simultaneously considers several parameters that influence the choice of\nan interpretable rule ensemble. Existing methods often prioritize predictive\nperformance over interpretability coverage and do not provide information about\nexisting overlaps or interactions between rules. Forest-ORE uses a\nmixed-integer optimization program to build an ORE that considers the trade-off\nbetween predictive performance, interpretability coverage, and model size (size\nof the rule ensemble, rule lengths, and rule overlaps). In addition to\nproviding an ORE competitive in predictive performance with RF, this method\nenriches the ORE through other rules that afford complementary information. It\nalso enables monitoring of the rule selection process and delivers various\nmetrics that can be used to generate a graphical representation of the final\nmodel. This framework is illustrated through an example, and its robustness is\nassessed through 36 benchmark datasets. A comparative analysis of well-known\nmethods shows that Forest-ORE provides an excellent trade-off between\npredictive performance, interpretability coverage, and model size.\n","authors":["Haddouchi Maissae","Berrado Abdelaziz"],"pdf_url":"https://arxiv.org/pdf/2403.17588v1.pdf","comment":"48 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.17589v1","updated":"2024-03-26T10:54:07Z","published":"2024-03-26T10:54:07Z","title":"Dual Memory Networks: A Versatile Adaptation Approach for\n  Vision-Language Models","summary":"  With the emergence of pre-trained vision-language models like CLIP, how to\nadapt them to various downstream classification tasks has garnered significant\nattention in recent research. The adaptation strategies can be typically\ncategorized into three paradigms: zero-shot adaptation, few-shot adaptation,\nand the recently-proposed training-free few-shot adaptation. Most existing\napproaches are tailored for a specific setting and can only cater to one or two\nof these paradigms. In this paper, we introduce a versatile adaptation approach\nthat can effectively work under all three settings. Specifically, we propose\nthe dual memory networks that comprise dynamic and static memory components.\nThe static memory caches training data knowledge, enabling training-free\nfew-shot adaptation, while the dynamic memory preserves historical test\nfeatures online during the testing process, allowing for the exploration of\nadditional data insights beyond the training set. This novel capability\nenhances model performance in the few-shot setting and enables model usability\nin the absence of training data. The two memory networks employ the same\nflexible memory interactive strategy, which can operate in a training-free mode\nand can be further enhanced by incorporating learnable projection layers. Our\napproach is tested across 11 datasets under the three task settings.\nRemarkably, in the zero-shot scenario, it outperforms existing methods by over\n3\\% and even shows superior results against methods utilizing external training\ndata. Additionally, our method exhibits robust performance against natural\ndistribution shifts. Codes are available at \\url{https://github.com/YBZh/DMN}.\n","authors":["Yabin Zhang","Wenjie Zhu","Hui Tang","Zhiyuan Ma","Kaiyang Zhou","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17589v1.pdf","comment":"CVPR2024; Codes are available at \\url{https://github.com/YBZh/DMN}"},{"id":"http://arxiv.org/abs/2403.17582v1","updated":"2024-03-26T10:45:11Z","published":"2024-03-26T10:45:11Z","title":"Towards a Zero-Data, Controllable, Adaptive Dialog System","summary":"  Conversational Tree Search (V\\\"ath et al., 2023) is a recent approach to\ncontrollable dialog systems, where domain experts shape the behavior of a\nReinforcement Learning agent through a dialog tree. The agent learns to\nefficiently navigate this tree, while adapting to information needs, e.g.,\ndomain familiarity, of different users. However, the need for additional\ntraining data hinders deployment in new domains. To address this, we explore\napproaches to generate this data directly from dialog trees. We improve the\noriginal approach, and show that agents trained on synthetic data can achieve\ncomparable dialog success to models trained on human data, both when using a\ncommercial Large Language Model for generation, or when using a smaller\nopen-source model, running on a single GPU. We further demonstrate the\nscalability of our approach by collecting and testing on two new datasets:\nONBOARD, a new domain helping foreign residents moving to a new city, and the\nmedical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and\nhead symptoms. Finally, we perform human testing, where no statistically\nsignificant differences were found in either objective or subjective measures\nbetween models trained on human and generated data.\n","authors":["Dirk Väth","Lindsey Vanderlyn","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.07773v2","updated":"2024-03-26T10:34:14Z","published":"2022-04-16T10:27:23Z","title":"FedCau: A Proactive Stop Policy for Communication and Computation\n  Efficient Federated Learning","summary":"  This paper investigates efficient distributed training of a Federated\nLearning~(FL) model over a wireless network of wireless devices. The\ncommunication iterations of the distributed training algorithm may be\nsubstantially deteriorated or even blocked by the effects of the devices'\nbackground traffic, packet losses, congestion, or latency. We abstract the\ncommunication-computation impacts as an `iteration cost' and propose a\ncost-aware causal FL algorithm~(FedCau) to tackle this problem. We propose an\niteration-termination method that trade-offs the training performance and\nnetworking costs. We apply our approach when clients use the slotted-ALOHA, the\ncarrier-sense multiple access with collision avoidance~(CSMA/CA), and the\northogonal frequency-division multiple access~(OFDMA) protocols. We show that,\ngiven a total cost budget, the training performance degrades as either the\nbackground communication traffic or the dimension of the training problem\nincreases. Our results demonstrate the importance of proactively designing\noptimal cost-efficient stopping criteria to avoid unnecessary\ncommunication-computation costs to achieve only a marginal FL training\nimprovement. We validate our method by training and testing FL over the MNIST\ndataset. Finally, we apply our approach to existing communication efficient FL\nmethods from the literature, achieving further efficiency. We conclude that\ncost-efficient stopping criteria are essential for the success of practical FL\nover wireless networks.\n","authors":["Afsaneh Mahmoudi","Hossein S. Ghadikolaei","José Mairton Barros Da Silva Júnior","Carlo Fischione"],"pdf_url":"https://arxiv.org/pdf/2204.07773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17572v1","updated":"2024-03-26T10:25:21Z","published":"2024-03-26T10:25:21Z","title":"Enhancing Privacy in Federated Learning through Local Training","summary":"  In this paper we propose the federated private local training algorithm\n(Fed-PLT) for federated learning, to overcome the challenges of (i) expensive\ncommunications and (ii) privacy preservation. We address (i) by allowing for\nboth partial participation and local training, which significantly reduce the\nnumber of communication rounds between the central coordinator and computing\nagents. The algorithm matches the state of the art in the sense that the use of\nlocal training demonstrably does not impact accuracy. Additionally, agents have\nthe flexibility to choose from various local training solvers, such as\n(stochastic) gradient descent and accelerated gradient descent. Further, we\ninvestigate how employing local training can enhance privacy, addressing point\n(ii). In particular, we derive differential privacy bounds and highlight their\ndependence on the number of local training epochs. We assess the effectiveness\nof the proposed algorithm by comparing it to alternative techniques,\nconsidering both theoretical analysis and numerical results from a\nclassification task.\n","authors":["Nicola Bastianello","Changxin Liu","Karl H. Johansson"],"pdf_url":"https://arxiv.org/pdf/2403.17572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03611v2","updated":"2024-03-26T10:13:11Z","published":"2023-12-06T16:55:53Z","title":"DreamComposer: Controllable 3D Object Generation via Multi-View\n  Conditions","summary":"  Utilizing pre-trained 2D large-scale generative models, recent works are\ncapable of generating high-quality novel views from a single in-the-wild image.\nHowever, due to the lack of information from multiple views, these works\nencounter difficulties in generating controllable novel views. In this paper,\nwe present DreamComposer, a flexible and scalable framework that can enhance\nexisting view-aware diffusion models by injecting multi-view conditions.\nSpecifically, DreamComposer first uses a view-aware 3D lifting module to obtain\n3D representations of an object from multiple views. Then, it renders the\nlatent features of the target view from 3D representations with the multi-view\nfeature fusion module. Finally the target view features extracted from\nmulti-view inputs are injected into a pre-trained diffusion model. Experiments\nshow that DreamComposer is compatible with state-of-the-art diffusion models\nfor zero-shot novel view synthesis, further enhancing them to generate\nhigh-fidelity novel view images with multi-view conditions, ready for\ncontrollable 3D object reconstruction and various other applications.\n","authors":["Yunhan Yang","Yukun Huang","Xiaoyang Wu","Yuan-Chen Guo","Song-Hai Zhang","Hengshuang Zhao","Tong He","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2312.03611v2.pdf","comment":"Project Page: https://yhyang-myron.github.io/DreamComposer/"},{"id":"http://arxiv.org/abs/2403.17561v1","updated":"2024-03-26T10:10:53Z","published":"2024-03-26T10:10:53Z","title":"A Survey on Deep Learning and State-of-the-arts Applications","summary":"  Deep learning, a branch of artificial intelligence, is a computational model\nthat uses multiple layers of interconnected units (neurons) to learn intricate\npatterns and representations directly from raw input data. Empowered by this\nlearning capability, it has become a powerful tool for solving complex problems\nand is the core driver of many groundbreaking technologies and innovations.\nBuilding a deep learning model is a challenging task due to the algorithm`s\ncomplexity and the dynamic nature of real-world problems. Several studies have\nreviewed deep learning concepts and applications. However, the studies mostly\nfocused on the types of deep learning models and convolutional neural network\narchitectures, offering limited coverage of the state-of-the-art of deep\nlearning models and their applications in solving complex problems across\ndifferent domains. Therefore, motivated by the limitations, this study aims to\ncomprehensively review the state-of-the-art deep learning models in computer\nvision, natural language processing, time series analysis and pervasive\ncomputing. We highlight the key features of the models and their effectiveness\nin solving the problems within each domain. Furthermore, this study presents\nthe fundamentals of deep learning, various deep learning model types and\nprominent convolutional neural network architectures. Finally, challenges and\nfuture directions in deep learning research are discussed to offer a broader\nperspective for future researchers.\n","authors":["Mohd Halim Mohd Noor","Ayokunle Olalekan Ige"],"pdf_url":"https://arxiv.org/pdf/2403.17561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17550v1","updated":"2024-03-26T09:58:06Z","published":"2024-03-26T09:58:06Z","title":"DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping","summary":"  Recently, significant progress has been achieved in sensing real large-scale\noutdoor 3D environments, particularly by using modern acquisition equipment\nsuch as LiDAR sensors. Unfortunately, they are fundamentally limited in their\nability to produce dense, complete 3D scenes. To address this issue, recent\nlearning-based methods integrate neural implicit representations and\noptimizable feature grids to approximate surfaces of 3D scenes. However,\nnaively fitting samples along raw LiDAR rays leads to noisy 3D mapping results\ndue to the nature of sparse, conflicting LiDAR measurements. Instead, in this\nwork we depart from fitting LiDAR data exactly, instead letting the network\noptimize a non-metric monotonic implicit field defined in 3D space. To fit our\nfield, we design a learning system integrating a monotonicity loss that enables\noptimizing neural monotonic fields and leverages recent progress in large-scale\n3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as\ncaptured by multiple quantitative and perceptual measures and visual results\nobtained for Mai City, Newer College, and KITTI benchmarks. The code of our\napproach will be made publicly available.\n","authors":["Kutay Yılmaz","Matthias Nießner","Anastasiia Kornilova","Alexey Artemov"],"pdf_url":"https://arxiv.org/pdf/2403.17550v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2307.01050v6","updated":"2024-03-26T09:48:49Z","published":"2023-07-03T14:28:36Z","title":"Transport meets Variational Inference: Controlled Monte Carlo Diffusions","summary":"  Connecting optimal transport and variational inference, we present a\nprincipled and systematic framework for sampling and generative modelling\ncentred around divergences on path space. Our work culminates in the\ndevelopment of the \\emph{Controlled Monte Carlo Diffusion} sampler (CMCD) for\nBayesian computation, a score-based annealing technique that crucially adapts\nboth forward and backward dynamics in a diffusion model. On the way, we clarify\nthe relationship between the EM-algorithm and iterative proportional fitting\n(IPF) for Schr{\\\"o}dinger bridges, deriving as well a regularised objective\nthat bypasses the iterative bottleneck of standard IPF-updates. Finally, we\nshow that CMCD has a strong foundation in the Jarzinsky and Crooks identities\nfrom statistical physics, and that it convincingly outperforms competing\napproaches across a wide array of experiments.\n","authors":["Francisco Vargas","Shreyas Padhy","Denis Blessing","Nikolas Nüsken"],"pdf_url":"https://arxiv.org/pdf/2307.01050v6.pdf","comment":"Workshop on New Frontiers in Learning, Control, and Dynamical Systems\n  at the International Conference on Machine Learning (ICML), Honolulu, Hawaii,\n  USA, 2023"},{"id":"http://arxiv.org/abs/2403.17542v1","updated":"2024-03-26T09:44:57Z","published":"2024-03-26T09:44:57Z","title":"VDSC: Enhancing Exploration Timing with Value Discrepancy and State\n  Counts","summary":"  Despite the considerable attention given to the questions of \\textit{how\nmuch} and \\textit{how to} explore in deep reinforcement learning, the\ninvestigation into \\textit{when} to explore remains relatively less researched.\nWhile more sophisticated exploration strategies can excel in specific, often\nsparse reward environments, existing simpler approaches, such as\n$\\epsilon$-greedy, persist in outperforming them across a broader spectrum of\ndomains. The appeal of these simpler strategies lies in their ease of\nimplementation and generality across a wide range of domains. The downside is\nthat these methods are essentially a blind switching mechanism, which\ncompletely disregards the agent's internal state. In this paper, we propose to\nleverage the agent's internal state to decide \\textit{when} to explore,\naddressing the shortcomings of blind switching mechanisms. We present Value\nDiscrepancy and State Counts through homeostasis (VDSC), a novel approach for\nefficient exploration timing. Experimental results on the Atari suite\ndemonstrate the superiority of our strategy over traditional methods such as\n$\\epsilon$-greedy and Boltzmann, as well as more sophisticated techniques like\nNoisy Nets.\n","authors":["Marius Captari","Remo Sasso","Matthia Sabatelli"],"pdf_url":"https://arxiv.org/pdf/2403.17542v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.17870v1","updated":"2024-03-26T16:57:55Z","published":"2024-03-26T16:57:55Z","title":"Boosting Diffusion Models with Moving Average Sampling in Frequency\n  Domain","summary":"  Diffusion models have recently brought a powerful revolution in image\ngeneration. Despite showing impressive generative capabilities, most of these\nmodels rely on the current sample to denoise the next one, possibly resulting\nin denoising instability. In this paper, we reinterpret the iterative denoising\nprocess as model optimization and leverage a moving average mechanism to\nensemble all the prior samples. Instead of simply applying moving average to\nthe denoised samples at different timesteps, we first map the denoised samples\nto data space and then perform moving average to avoid distribution shift\nacross timesteps. In view that diffusion models evolve the recovery from\nlow-frequency components to high-frequency details, we further decompose the\nsamples into different frequency components and execute moving average\nseparately on each component. We name the complete approach \"Moving Average\nSampling in Frequency domain (MASF)\". MASF could be seamlessly integrated into\nmainstream pre-trained diffusion models and sampling schedules. Extensive\nexperiments on both unconditional and conditional diffusion models demonstrate\nthat our MASF leads to superior performances compared to the baselines, with\nalmost negligible additional complexity cost.\n","authors":["Yurui Qian","Qi Cai","Yingwei Pan","Yehao Li","Ting Yao","Qibin Sun","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2403.17870v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time- consuming. Therefore, the challenging task of\nreconstructing visually accurate HDR images from their Low Dynamic Range (LDR)\ncounterparts is gaining attention in the vision research community. A major\nchallenge in this research problem is the lack of datasets, which capture\ndiverse scene conditions (e.g., lighting, shadows, weather, locations,\nlandscapes, objects, humans, buildings) and various image features (e.g.,\ncolor, contrast, saturation, hue, luminance, brightness, radiance). To address\nthis gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset\nof photo-realistic HDR images sampled from the GTA-V video game. We perform\nthorough evaluation of the proposed dataset, which demonstrates significant\nqualitative and quantitative improvements of the state-of-the-art HDR image\nreconstruction methods. Furthermore, we demonstrate the effectiveness of the\nproposed dataset and its impact on additional computer vision tasks including\n3D human pose estimation, human body part segmentation, and holistic scene\nsegmentation. The dataset, data collection pipeline, and evaluation code are\navailable at: https://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2403.17727v1","updated":"2024-03-26T14:16:56Z","published":"2024-03-26T14:16:56Z","title":"FastPerson: Enhancing Video Learning through Effective Video\n  Summarization that Preserves Linguistic and Visual Contexts","summary":"  Quickly understanding lengthy lecture videos is essential for learners with\nlimited time and interest in various topics to improve their learning\nefficiency. To this end, video summarization has been actively researched to\nenable users to view only important scenes from a video. However, these studies\nfocus on either the visual or audio information of a video and extract\nimportant segments in the video. Therefore, there is a risk of missing\nimportant information when both the teacher's speech and visual information on\nthe blackboard or slides are important, such as in a lecture video. To tackle\nthis issue, we propose FastPerson, a video summarization approach that\nconsiders both the visual and auditory information in lecture videos.\nFastPerson creates summary videos by utilizing audio transcriptions along with\non-screen images and text, minimizing the risk of overlooking crucial\ninformation for learners. Further, it provides a feature that allows learners\nto switch between the summary and original videos for each chapter of the\nvideo, enabling them to adjust the pace of learning based on their interests\nand level of understanding. We conducted an evaluation with 40 participants to\nassess the effectiveness of our method and confirmed that it reduced viewing\ntime by 53\\% at the same level of comprehension as that when using traditional\nvideo playback methods.\n","authors":["Kazuki Kawamura","Jun Rekimoto"],"pdf_url":"https://arxiv.org/pdf/2403.17727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17708v1","updated":"2024-03-26T13:54:52Z","published":"2024-03-26T13:54:52Z","title":"Panonut360: A Head and Eye Tracking Dataset for Panoramic Video","summary":"  With the rapid development and widespread application of VR/AR technology,\nmaximizing the quality of immersive panoramic video services that match users'\npersonal preferences and habits has become a long-standing challenge.\nUnderstanding the saliency region where users focus, based on data collected\nwith HMDs, can promote multimedia encoding, transmission, and quality\nassessment. At the same time, large-scale datasets are essential for\nresearchers and developers to explore short/long-term user behavior patterns\nand train AI models related to panoramic videos. However, existing panoramic\nvideo datasets often include low-frequency user head or eye movement data\nthrough short-term videos only, lacking sufficient data for analyzing users'\nField of View (FoV) and generating video saliency regions.\n  Driven by these practical factors, in this paper, we present a head and eye\ntracking dataset involving 50 users (25 males and 25 females) watching 15\npanoramic videos. The dataset provides details on the viewport and gaze\nattention locations of users. Besides, we present some statistics samples\nextracted from the dataset. For example, the deviation between head and eye\nmovements challenges the widely held assumption that gaze attention decreases\nfrom the center of the FoV following a Gaussian distribution. Our analysis\nreveals a consistent downward offset in gaze fixations relative to the FoV in\nexperimental settings involving multiple users and videos. That's why we name\nthe dataset Panonut, a saliency weighting shaped like a donut. Finally, we also\nprovide a script that generates saliency distributions based on given head or\neye coordinates and pre-generated saliency distribution map sets of each video\nfrom the collected eye tracking data.\n  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.\n","authors":["Yutong Xu","Junhao Du","Jiahe Wang","Yuwei Ning","Sihan Zhou Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2403.17708v1.pdf","comment":"7 pages,ACM MMSys'24 accepted"},{"id":"http://arxiv.org/abs/2312.02512v2","updated":"2024-03-26T13:21:28Z","published":"2023-12-05T05:36:44Z","title":"AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation\n  with Unified Audio-Visual Speech Representation","summary":"  This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech\nTranslation (AV2AV) framework, where the input and output of the system are\nmultimodal (i.e., audio and visual speech). With the proposed AV2AV, two key\nadvantages can be brought: 1) We can perform real-like conversations with\nindividuals worldwide in a virtual meeting by utilizing our own primary\nlanguages. In contrast to Speech-to-Speech Translation (A2A), which solely\ntranslates between audio modalities, the proposed AV2AV directly translates\nbetween audio-visual speech. This capability enhances the dialogue experience\nby presenting synchronized lip movements along with the translated speech. 2)\nWe can improve the robustness of the spoken language translation system. By\nemploying the complementary information of audio-visual speech, the system can\neffectively translate spoken language even in the presence of acoustic noise,\nshowcasing robust performance. To mitigate the problem of the absence of a\nparallel AV2AV translation dataset, we propose to train our spoken language\ntranslation system with the audio-only dataset of A2A. This is done by learning\nunified audio-visual speech representations through self-supervised learning in\nadvance to train the translation system. Moreover, we propose an AV-Renderer\nthat can generate raw audio and video in parallel. It is designed with\nzero-shot speaker modeling, thus the speaker in source audio-visual speech can\nbe maintained at the target translated audio-visual speech. The effectiveness\nof AV2AV is evaluated with extensive experiments in a many-to-many language\ntranslation setting. Demo page is available on\nhttps://choijeongsoo.github.io/av2av.\n","authors":["Jeongsoo Choi","Se Jin Park","Minsu Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2312.02512v2.pdf","comment":"CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av"},{"id":"http://arxiv.org/abs/2403.17589v1","updated":"2024-03-26T10:54:07Z","published":"2024-03-26T10:54:07Z","title":"Dual Memory Networks: A Versatile Adaptation Approach for\n  Vision-Language Models","summary":"  With the emergence of pre-trained vision-language models like CLIP, how to\nadapt them to various downstream classification tasks has garnered significant\nattention in recent research. The adaptation strategies can be typically\ncategorized into three paradigms: zero-shot adaptation, few-shot adaptation,\nand the recently-proposed training-free few-shot adaptation. Most existing\napproaches are tailored for a specific setting and can only cater to one or two\nof these paradigms. In this paper, we introduce a versatile adaptation approach\nthat can effectively work under all three settings. Specifically, we propose\nthe dual memory networks that comprise dynamic and static memory components.\nThe static memory caches training data knowledge, enabling training-free\nfew-shot adaptation, while the dynamic memory preserves historical test\nfeatures online during the testing process, allowing for the exploration of\nadditional data insights beyond the training set. This novel capability\nenhances model performance in the few-shot setting and enables model usability\nin the absence of training data. The two memory networks employ the same\nflexible memory interactive strategy, which can operate in a training-free mode\nand can be further enhanced by incorporating learnable projection layers. Our\napproach is tested across 11 datasets under the three task settings.\nRemarkably, in the zero-shot scenario, it outperforms existing methods by over\n3\\% and even shows superior results against methods utilizing external training\ndata. Additionally, our method exhibits robust performance against natural\ndistribution shifts. Codes are available at \\url{https://github.com/YBZh/DMN}.\n","authors":["Yabin Zhang","Wenjie Zhu","Hui Tang","Zhiyuan Ma","Kaiyang Zhou","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.17589v1.pdf","comment":"CVPR2024; Codes are available at \\url{https://github.com/YBZh/DMN}"},{"id":"http://arxiv.org/abs/2403.17420v1","updated":"2024-03-26T06:27:50Z","published":"2024-03-26T06:27:50Z","title":"Learning to Visually Localize Sound Sources from Mixtures without Prior\n  Source Knowledge","summary":"  The goal of the multi-sound source localization task is to localize sound\nsources from the mixture individually. While recent multi-sound source\nlocalization methods have shown improved performance, they face challenges due\nto their reliance on prior information about the number of objects to be\nseparated. In this paper, to overcome this limitation, we present a novel\nmulti-sound source localization method that can perform localization without\nprior knowledge of the number of sound sources. To achieve this goal, we\npropose an iterative object identification (IOI) module, which can recognize\nsound-making objects in an iterative manner. After finding the regions of\nsound-making objects, we devise object similarity-aware clustering (OSC) loss\nto guide the IOI module to effectively combine regions of the same object but\nalso distinguish between different objects and backgrounds. It enables our\nmethod to perform accurate localization of sound-making objects without any\nprior knowledge. Extensive experimental results on the MUSIC and VGGSound\nbenchmarks show the significant performance improvements of the proposed method\nover the existing methods for both single and multi-source. Our code is\navailable at: https://github.com/VisualAIKHU/NoPrior_MultiSSL\n","authors":["Dongjin Kim","Sung Jin Um","Sangmin Lee","Jung Uk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.17420v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2402.19330v2","updated":"2024-03-26T04:15:53Z","published":"2024-02-29T16:33:12Z","title":"A Novel Approach to Industrial Defect Generation through Blended Latent\n  Diffusion Model with Online Adaptation","summary":"  Effectively addressing the challenge of industrial Anomaly Detection (AD)\nnecessitates an ample supply of defective samples, a constraint often hindered\nby their scarcity in industrial contexts. This paper introduces a novel\nalgorithm designed to augment defective samples, thereby enhancing AD\nperformance. The proposed method tailors the blended latent diffusion model for\ndefect sample generation, employing a diffusion model to generate defective\nsamples in the latent space. A feature editing process, controlled by a\n``trimap\" mask and text prompts, refines the generated samples. The image\ngeneration inference process is structured into three stages: a free diffusion\nstage, an editing diffusion stage, and an online decoder adaptation stage. This\nsophisticated inference strategy yields high-quality synthetic defective\nsamples with diverse pattern variations, leading to significantly improved AD\naccuracies based on the augmented training set. Specifically, on the widely\nrecognized MVTec AD dataset, the proposed method elevates the state-of-the-art\n(SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD\nmetrics AP, IAP, and IAP90, respectively. The implementation code of this work\ncan be found at the GitHub repository\nhttps://github.com/GrandpaXun242/AdaBLDM.git\n","authors":["Hanxi Li","Zhengxun Zhang","Hao Chen","Lin Wu","Bo Li","Deyin Liu","Mingwen Wang"],"pdf_url":"https://arxiv.org/pdf/2402.19330v2.pdf","comment":"13 pages,7 figures"},{"id":"http://arxiv.org/abs/2403.18063v1","updated":"2024-03-26T19:29:21Z","published":"2024-03-26T19:29:21Z","title":"Spectral Convolutional Transformer: Harmonizing Real vs. Complex\n  Multi-View Spectral Operators for Vision Transformer","summary":"  Transformers used in vision have been investigated through diverse\narchitectures - ViT, PVT, and Swin. These have worked to improve the attention\nmechanism and make it more efficient. Differently, the need for including local\ninformation was felt, leading to incorporating convolutions in transformers\nsuch as CPVT and CvT. Global information is captured using a complex Fourier\nbasis to achieve global token mixing through various methods, such as AFNO,\nGFNet, and Spectformer. We advocate combining three diverse views of data -\nlocal, global, and long-range dependence. We also investigate the simplest\nglobal representation using only the real domain spectral representation -\nobtained through the Hartley transform. We use a convolutional operator in the\ninitial layers to capture local information. Through these two contributions,\nwe are able to optimize and obtain a spectral convolution transformer (SCT)\nthat provides improved performance over the state-of-the-art methods while\nreducing the number of parameters. Through extensive experiments, we show that\nSCT-C-small gives state-of-the-art performance on the ImageNet dataset and\nreaches 84.5\\% top-1 accuracy, while SCT-C-Large reaches 85.9\\% and SCT-C-Huge\nreaches 86.4\\%. We evaluate SCT on transfer learning on datasets such as\nCIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on\ndownstream tasks i.e. instance segmentation on the MSCOCO dataset. The project\npage is available on this webpage.\\url{https://github.com/badripatro/sct}\n","authors":["Badri N. Patro","Vinay P. Namboodiri","Vijay S. Agneeswaran"],"pdf_url":"https://arxiv.org/pdf/2403.18063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17837v1","updated":"2024-03-26T16:24:42Z","published":"2024-03-26T16:24:42Z","title":"GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction","summary":"  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range\nof applications. However, capturing HDR content from real-world scenes is\nexpensive and time-consuming. Therefore, the challenging task of reconstructing\nvisually accurate HDR images from their Low Dynamic Range (LDR) counterparts is\ngaining attention in the vision research community. A major challenge in this\nresearch problem is the lack of datasets, which capture diverse scene\nconditions (e.g., lighting, shadows, weather, locations, landscapes, objects,\nhumans, buildings) and various image features (e.g., color, contrast,\nsaturation, hue, luminance, brightness, radiance). To address this gap, in this\npaper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic\nHDR images sampled from the GTA-V video game. We perform thorough evaluation of\nthe proposed dataset, which demonstrates significant qualitative and\nquantitative improvements of the state-of-the-art HDR image reconstruction\nmethods. Furthermore, we demonstrate the effectiveness of the proposed dataset\nand its impact on additional computer vision tasks including 3D human pose\nestimation, human body part segmentation, and holistic scene segmentation. The\ndataset, data collection pipeline, and evaluation code are available at:\nhttps://github.com/HrishavBakulBarua/GTA-HDR.\n","authors":["Hrishav Bakul Barua","Kalin Stefanov","KokSheik Wong","Abhinav Dhall","Ganesh Krishnasamy"],"pdf_url":"https://arxiv.org/pdf/2403.17837v1.pdf","comment":"Submitted to IEEE"}]},"2024-03-27T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2403.18814v1","updated":"2024-03-27T17:59:04Z","published":"2024-03-27T17:59:04Z","title":"Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models","summary":"  In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.\n","authors":["Yanwei Li","Yuechen Zhang","Chengyao Wang","Zhisheng Zhong","Yixin Chen","Ruihang Chu","Shaoteng Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2403.18814v1.pdf","comment":"Code and models are available at\n  https://github.com/dvlab-research/MiniGemini"},{"id":"http://arxiv.org/abs/2403.18804v1","updated":"2024-03-27T17:50:00Z","published":"2024-03-27T17:50:00Z","title":"Is Modularity Transferable? A Case Study through the Lens of Knowledge\n  Distillation","summary":"  The rise of Modular Deep Learning showcases its potential in various Natural\nLanguage Processing applications. Parameter-efficient fine-tuning (PEFT)\nmodularity has been shown to work for various use cases, from domain adaptation\nto multilingual setups. However, all this work covers the case where the\nmodular components are trained and deployed within one single Pre-trained\nLanguage Model (PLM). This model-specific setup is a substantial limitation on\nthe very modularity that modular architectures are trying to achieve. We ask\nwhether current modular approaches are transferable between models and whether\nwe can transfer the modules from more robust and larger PLMs to smaller ones.\nIn this work, we aim to fill this gap via a lens of Knowledge Distillation,\ncommonly used for model compression, and present an extremely straightforward\napproach to transferring pre-trained, task-specific PEFT modules between\nsame-family PLMs. Moreover, we propose a method that allows the transfer of\nmodules between incompatible PLMs without any change in the inference\ncomplexity. The experiments on Named Entity Recognition, Natural Language\nInference, and Paraphrase Identification tasks over multiple languages and PEFT\nmethods showcase the initial potential of transferable modularity.\n","authors":["Mateusz Klimaszewski","Piotr Andruszkiewicz","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2403.18804v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18803v1","updated":"2024-03-27T17:49:31Z","published":"2024-03-27T17:49:31Z","title":"Projective Methods for Mitigating Gender Bias in Pre-trained Language\n  Models","summary":"  Mitigation of gender bias in NLP has a long history tied to debiasing static\nword embeddings. More recently, attention has shifted to debiasing pre-trained\nlanguage models. We study to what extent the simplest projective debiasing\nmethods, developed for word embeddings, can help when applied to BERT's\ninternal representations. Projective methods are fast to implement, use a small\nnumber of saved parameters, and make no updates to the existing model\nparameters. We evaluate the efficacy of the methods in reducing both intrinsic\nbias, as measured by BERT's next sentence prediction task, and in mitigating\nobserved bias in a downstream setting when fine-tuned. To this end, we also\nprovide a critical analysis of a popular gender-bias assessment test for\nquantifying intrinsic bias, resulting in an enhanced test set and new bias\nmeasures. We find that projective methods can be effective at both intrinsic\nbias and downstream bias mitigation, but that the two outcomes are not\nnecessarily correlated. This finding serves as a warning that intrinsic bias\ntest sets, based either on language modeling tasks or next sentence prediction,\nshould not be the only benchmark in developing a debiased language model.\n","authors":["Hillary Dawkins","Isar Nejadgholi","Daniel Gillis","Judi McCuaig"],"pdf_url":"https://arxiv.org/pdf/2403.18803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18802v1","updated":"2024-03-27T17:48:55Z","published":"2024-03-27T17:48:55Z","title":"Long-form factuality in large language models","summary":"  Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.\n","authors":["Jerry Wei","Chengrun Yang","Xinying Song","Yifeng Lu","Nathan Hu","Dustin Tran","Daiyi Peng","Ruibo Liu","Da Huang","Cosmo Du","Quoc V. Le"],"pdf_url":"https://arxiv.org/pdf/2403.18802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17574v2","updated":"2024-03-27T17:34:57Z","published":"2024-02-27T15:09:20Z","title":"Agent-Pro: Learning to Evolve via Policy-Level Reflection and\n  Optimization","summary":"  Large Language Models exhibit robust problem-solving capabilities for diverse\ntasks. However, most LLM-based agents are designed as specific task solvers\nwith sophisticated prompt engineering, rather than agents capable of learning\nand evolving through interactions. These task solvers necessitate manually\ncrafted prompts to inform task rules and regulate LLM behaviors, inherently\nincapacitating to address complex dynamic scenarios e.g., large interactive\ngames. In light of this, we propose Agent-Pro: an LLM-based Agent with\nPolicy-level Reflection and Optimization that can learn a wealth of expertise\nfrom interactive experiences and progressively elevate its behavioral policy.\nSpecifically, it involves a dynamic belief generation and reflection process\nfor policy evolution. Rather than action-level reflection, Agent-Pro\niteratively reflects on past trajectories and beliefs, fine-tuning its\nirrational beliefs for a better policy. Moreover, a depth-first search is\nemployed for policy optimization, ensuring continual enhancement in policy\npayoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,\noutperforming vanilla LLM and specialized models. Our results show Agent-Pro\ncan learn and evolve in complex and dynamic scenes, which also benefits\nnumerous LLM-based applications.\n","authors":["Wenqi Zhang","Ke Tang","Hai Wu","Mengna Wang","Yongliang Shen","Guiyang Hou","Zeqi Tan","Peng Li","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2402.17574v2.pdf","comment":"LLM-based Agent"},{"id":"http://arxiv.org/abs/2403.18783v1","updated":"2024-03-27T17:31:39Z","published":"2024-03-27T17:31:39Z","title":"Towards a World-English Language Model for On-Device Virtual Assistants","summary":"  Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are\ngenerally language-, region-, and in some cases, device-dependent, which\nincreases the effort to scale and maintain them. Combining NNLMs for one or\nmore of the categories is one way to improve scalability. In this work, we\ncombine regional variants of English to build a ``World English'' NNLM for\non-device VAs. In particular, we investigate the application of adapter\nbottlenecks to model dialect-specific characteristics in our existing\nproduction NNLMs {and enhance the multi-dialect baselines}. We find that\nadapter modules are more effective in modeling dialects than specializing\nentire sub-networks. Based on this insight and leveraging the design of our\nproduction models, we introduce a new architecture for World English NNLM that\nmeets the accuracy, latency, and memory constraints of our single-dialect\nmodels.\n","authors":["Rricha Jalota","Lyan Verwimp","Markus Nussbaum-Thom","Amr Mousa","Arturo Argueta","Youssef Oualil"],"pdf_url":"https://arxiv.org/pdf/2403.18783v1.pdf","comment":"Accepted in ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02009v2","updated":"2024-03-27T17:24:47Z","published":"2024-01-04T00:32:33Z","title":"Self-Contrast: Better Reflection Through Inconsistent Solving\n  Perspectives","summary":"  The reflection capacity of Large Language Model (LLM) has garnered extensive\nattention. A post-hoc prompting strategy, e.g., reflexion and self-refine,\nrefines LLM's response based on self-evaluated or external feedback. However,\nrecent research indicates without external feedback, LLM's intrinsic reflection\nis unstable. Our investigation unveils that the key bottleneck is the quality\nof the self-evaluated feedback. We find LLMs often exhibit overconfidence or\nhigh randomness when self-evaluate, offering stubborn or inconsistent feedback,\nwhich causes poor reflection. To remedy this, we advocate Self-Contrast: It\nadaptively explores diverse solving perspectives tailored to the request,\ncontrasts the differences, and summarizes these discrepancies into a checklist\nwhich could be used to re-examine and eliminate discrepancies. Our method\nendows LLM with diverse perspectives to alleviate stubborn biases. Moreover,\ntheir discrepancies indicate potential errors or inherent uncertainties that\nLLM often overlooks. Reflecting upon these can catalyze more accurate and\nstable reflection. Experiments conducted on a series of reasoning and\ntranslation tasks with different LLMs serve to underscore the effectiveness and\ngenerality of our strategy.\n","authors":["Wenqi Zhang","Yongliang Shen","Linjuan Wu","Qiuying Peng","Jun Wang","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2401.02009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18771v1","updated":"2024-03-27T17:20:39Z","published":"2024-03-27T17:20:39Z","title":"CheckEval: Robust Evaluation Framework using Large Language Model via\n  Checklist","summary":"  We introduce CheckEval, a novel evaluation framework using Large Language\nModels, addressing the challenges of ambiguity and inconsistency in current\nevaluation methods. CheckEval addresses these challenges by dividing evaluation\ncriteria into detailed sub-aspects and constructing a checklist of Boolean\nquestions for each, simplifying the evaluation. This approach not only renders\nthe process more interpretable but also significantly enhances the robustness\nand reliability of results by focusing on specific evaluation dimensions.\nValidated through a focused case study using the SummEval benchmark, CheckEval\nindicates a strong correlation with human judgments. Furthermore, it\ndemonstrates a highly consistent Inter-Annotator Agreement. These findings\nhighlight the effectiveness of CheckEval for objective, flexible, and precise\nevaluations. By offering a customizable and interactive framework, CheckEval\nsets a new standard for the use of LLMs in evaluation, responding to the\nevolving needs of the field and establishing a clear method for future\nLLM-based evaluation.\n","authors":["Yukyung Lee","Joonghoon Kim","Jaehee Kim","Hyowon Cho","Pilsung Kang"],"pdf_url":"https://arxiv.org/pdf/2403.18771v1.pdf","comment":"HEAL at CHI 2024"},{"id":"http://arxiv.org/abs/2403.18769v1","updated":"2024-03-27T17:13:38Z","published":"2024-03-27T17:13:38Z","title":"Improved Neural Protoform Reconstruction via Reflex Prediction","summary":"  Protolanguage reconstruction is central to historical linguistics. The\ncomparative method, one of the most influential theoretical and methodological\nframeworks in the history of the language sciences, allows linguists to infer\nprotoforms (reconstructed ancestral words) from their reflexes (related modern\nwords) based on the assumption of regular sound change. Not surprisingly,\nnumerous computational linguists have attempted to operationalize comparative\nreconstruction through various computational models, the most successful of\nwhich have been supervised encoder-decoder models, which treat the problem of\npredicting protoforms given sets of reflexes as a sequence-to-sequence problem.\nWe argue that this framework ignores one of the most important aspects of the\ncomparative method: not only should protoforms be inferable from cognate sets\n(sets of related reflexes) but the reflexes should also be inferable from the\nprotoforms. Leveraging another line of research -- reflex prediction -- we\npropose a system in which candidate protoforms from a reconstruction model are\nreranked by a reflex prediction model. We show that this more complete\nimplementation of the comparative method allows us to surpass state-of-the-art\nprotoform reconstruction methods on three of four Chinese and Romance datasets.\n","authors":["Liang Lu","Jingzhi Wang","David R. Mortensen"],"pdf_url":"https://arxiv.org/pdf/2403.18769v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18746v1","updated":"2024-03-27T16:45:02Z","published":"2024-03-27T16:45:02Z","title":"CYCLE: Learning to Self-Refine the Code Generation","summary":"  Pre-trained code language models have achieved promising performance in code\ngeneration and improved the programming efficiency of human developers.\nHowever, their self-refinement capability is typically overlooked by the\nexisting evaluations of code LMs, which focus only on the accuracy of the\none-time prediction. For the cases when code LMs fail to implement the correct\nprogram, developers actually find it hard to debug and fix the faulty\nprediction since it is not written by the developers themselves. Unfortunately,\nour study reveals that code LMs cannot efficiently self-refine their faulty\ngenerations as well.\n  In this paper, we propose CYCLE framework, learning to self-refine the faulty\ngeneration according to the available feedback, such as the execution results\nreported by the test suites. We evaluate CYCLE on three popular code generation\nbenchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE\nsuccessfully maintains, sometimes improves, the quality of one-time code\ngeneration, while significantly improving the self-refinement capability of\ncode LMs. We implement four variants of CYCLE with varied numbers of parameters\nacross 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently\nboosts the code generation performance, by up to 63.5%, across benchmarks and\nvaried model sizes. We also notice that CYCLE outperforms code LMs that have\n3$\\times$ more parameters in self-refinement.\n","authors":["Yangruibo Ding","Marcus J. Min","Gail Kaiser","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2403.18746v1.pdf","comment":"Camera-ready for OOPSLA'24"},{"id":"http://arxiv.org/abs/2403.03100v2","updated":"2024-03-27T16:14:34Z","published":"2024-03-05T16:35:25Z","title":"NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models","summary":"  While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility, and achieves on-par quality with human recordings.\nFurthermore, we achieve better performance by scaling to 1B parameters and 200K\nhours of training data.\n","authors":["Zeqian Ju","Yuancheng Wang","Kai Shen","Xu Tan","Detai Xin","Dongchao Yang","Yanqing Liu","Yichong Leng","Kaitao Song","Siliang Tang","Zhizheng Wu","Tao Qin","Xiang-Yang Li","Wei Ye","Shikun Zhang","Jiang Bian","Lei He","Jinyu Li","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.03100v2.pdf","comment":"Achieving human-level quality and naturalness on multi-speaker\n  datasets (e.g., LibriSpeech) in a zero-shot way"},{"id":"http://arxiv.org/abs/2403.18715v1","updated":"2024-03-27T16:04:47Z","published":"2024-03-27T16:04:47Z","title":"Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding","summary":"  Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.\n","authors":["Xintong Wang","Jingheng Pan","Liang Ding","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2403.18715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03123v3","updated":"2024-03-27T16:03:32Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v3.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.18697v1","updated":"2024-03-27T15:46:25Z","published":"2024-03-27T15:46:25Z","title":"The Invalsi Benchmark: measuring Language Models Mathematical and\n  Language understanding in Italian","summary":"  While Italian is by all metrics a high resource language, currently, there\nare isn't a Language Model pre-trained exclusively in this language. This\nresults in a lower number of available benchmarks to evaluate the performance\nof language models in Italian.\n  This work presents two new benchmarks to evaluate the models performance on\nmathematical understanding and language understanding in Italian. These\nbenchmarks are based on real tests that are undertaken by students of age\nbetween 11 and 18 within the Italian school system and have therefore been\nvalidated by several experts in didactics and pedagogy.\n  To validate this dataset we evaluate the performance of 9 language models\nthat are the best performing when writing in Italian, including our own\nfine-tuned models. We show that this is a challenging benchmark where current\nlanguage models are bound by 60\\% accuracy.\n  We believe that the release of this dataset paves the way for improving\nfuture models mathematical and language understanding in Italian.\n","authors":["Andrea Esuli","Giovanni Puccetti"],"pdf_url":"https://arxiv.org/pdf/2403.18697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18684v1","updated":"2024-03-27T15:27:36Z","published":"2024-03-27T15:27:36Z","title":"Scaling Laws For Dense Retrieval","summary":"  Scaling up neural models has yielded significant advancements in a wide array\nof tasks, particularly in language generation. Previous studies have found that\nthe performance of neural models frequently adheres to predictable scaling\nlaws, correlated with factors such as training set size and model size. This\ninsight is invaluable, especially as large-scale experiments grow increasingly\nresource-intensive. Yet, such scaling law has not been fully explored in dense\nretrieval due to the discrete nature of retrieval metrics and complex\nrelationships between training data and model sizes in retrieval tasks. In this\nstudy, we investigate whether the performance of dense retrieval models follows\nthe scaling law as other neural models. We propose to use contrastive\nlog-likelihood as the evaluation metric and conduct extensive experiments with\ndense retrieval models implemented with different numbers of parameters and\ntrained with different amounts of annotated data. Results indicate that, under\nour settings, the performance of dense retrieval models follows a precise\npower-law scaling related to the model size and the number of annotations.\nAdditionally, we examine scaling with prevalent data augmentation methods to\nassess the impact of annotation quality, and apply the scaling law to find the\nbest resource allocation strategy under a budget constraint. We believe that\nthese insights will significantly contribute to understanding the scaling\neffect of dense retrieval models and offer meaningful guidance for future\nresearch endeavors.\n","authors":["Yan Fang","Jingtao Zhan","Qingyao Ai","Jiaxin Mao","Weihang Su","Jia Chen","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18684v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.11128v2","updated":"2024-03-27T15:22:53Z","published":"2024-03-17T07:34:12Z","title":"Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants'\n  API Invocation Capabilities","summary":"  With the rise of Large Language Models (LLMs), AI assistants' ability to\nutilize tools, especially through API calls, has advanced notably. This\nprogress has necessitated more accurate evaluation methods. Many existing\nstudies adopt static evaluation, where they assess AI assistants' API call\nbased on pre-defined dialogue histories. However, such evaluation method can be\nmisleading, as an AI assistant might fail in generating API calls from\npreceding human interaction in real cases. Instead of the resource-intensive\nmethod of direct human-machine interactions, we propose Automated Dynamic\nEvaluation (AutoDE) to assess an assistant's API call capability without human\ninvolvement. In our framework, we endeavor to closely mirror genuine human\nconversation patterns in human-machine interactions, using a LLM-based user\nagent, equipped with a user script to ensure human alignment. Experimental\nresults highlight that AutoDE uncovers errors overlooked by static evaluations,\naligning more closely with human assessment. Testing four AI assistants using\nour crafted benchmark, our method further mirrored human evaluation compared to\nconventional static evaluations.\n","authors":["Honglin Mu","Yang Xu","Yunlong Feng","Xiaofeng Han","Yitong Li","Yutai Hou","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2403.11128v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18680v1","updated":"2024-03-27T15:22:16Z","published":"2024-03-27T15:22:16Z","title":"NL-ITI: Optimizing Probing and Intervention for Improvement of ITI\n  Method","summary":"  Large Language Models (LLM) are prone to returning false information. It\nconstitutes one of major challenges in the AI field. In our work, we explore\nparadigm introduced by Inference-Time-Intervention (ITI). In first stage, it\nidentifies attention heads, which contain the highest amount of desired type of\nknowledge (e.g., truthful). Afterwards, during inference, LLM activations are\nshifted for chosen subset of attention heads. We further improved the ITI\nframework by introducing a nonlinear probing and multi-token intervention -\nNon-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice\nbenchmarks, including TruthfulQA, on which we report around 14% MC1 metric\nimprovement with respect to the baseline ITI results. NL-ITI achieves also\nencouraging results on other testsets - on Business Ethics subdomain of MMLU,\naround 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI\nperforms better while being less invasive in the behavior of LLM at the same\ntime (as measured by Kullback-Leibler divergence).\n","authors":["Jakub Hoscilowicz","Adam Wiacek","Jan Chojnacki","Adam Cieslak","Leszek Michon","Vitalii Urbanevych","Artur Janicki"],"pdf_url":"https://arxiv.org/pdf/2403.18680v1.pdf","comment":"Code is available at https://github.com/Samsung/NL-ITI"},{"id":"http://arxiv.org/abs/2403.17143v2","updated":"2024-03-27T15:15:16Z","published":"2024-03-25T19:40:26Z","title":"Guided Distant Supervision for Multilingual Relation Extraction Data:\n  Adapting to a New Language","summary":"  Relation extraction is essential for extracting and understanding\nbiographical information in the context of digital humanities and related\nsubjects. There is a growing interest in the community to build datasets\ncapable of training machine learning models to extract relationships. However,\nannotating such datasets can be expensive and time-consuming, in addition to\nbeing limited to English. This paper applies guided distant supervision to\ncreate a large biographical relationship extraction dataset for German. Our\ndataset, composed of more than 80,000 instances for nine relationship types, is\nthe largest biographical German relationship extraction dataset. We also create\na manually annotated dataset with 2000 instances to evaluate the models and\nrelease it together with the dataset compiled using guided distant supervision.\nWe train several state-of-the-art machine learning models on the automatically\ncreated dataset and release them as well. Furthermore, we experiment with\nmultilingual and cross-lingual experiments that could benefit many low-resource\nlanguages.\n","authors":["Alistair Plum","Tharindu Ranasinghe","Christoph Purschke"],"pdf_url":"https://arxiv.org/pdf/2403.17143v2.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.18671v1","updated":"2024-03-27T15:15:14Z","published":"2024-03-27T15:15:14Z","title":"Fact Checking Beyond Training Set","summary":"  Evaluating the veracity of everyday claims is time consuming and in some\ncases requires domain expertise. We empirically demonstrate that the commonly\nused fact checking pipeline, known as the retriever-reader, suffers from\nperformance deterioration when it is trained on the labeled data from one\ndomain and used in another domain. Afterwards, we delve into each component of\nthe pipeline and propose novel algorithms to address this problem. We propose\nan adversarial algorithm to make the retriever component robust against\ndistribution shift. Our core idea is to initially train a bi-encoder on the\nlabeled source data, and then, to adversarially train two separate document and\nclaim encoders using unlabeled target data. We then focus on the reader\ncomponent and propose to train it such that it is insensitive towards the order\nof claims and evidence documents. Our empirical evaluations support the\nhypothesis that such a reader shows a higher robustness against distribution\nshift. To our knowledge, there is no publicly available multi-topic fact\nchecking dataset. Thus, we propose a simple automatic method to re-purpose two\nwell-known fact checking datasets. We then construct eight fact checking\nscenarios from these datasets, and compare our model to a set of strong\nbaseline models, including recent domain adaptation models that use GPT4 for\ngenerating synthetic data.\n","authors":["Payam Karisani","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18671v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18667v1","updated":"2024-03-27T15:11:00Z","published":"2024-03-27T15:11:00Z","title":"Improving Content Recommendation: Knowledge Graph-Based Semantic\n  Contrastive Learning for Diversity and Cold-Start Users","summary":"  Addressing the challenges related to data sparsity, cold-start problems, and\ndiversity in recommendation systems is both crucial and demanding. Many current\nsolutions leverage knowledge graphs to tackle these issues by combining both\nitem-based and user-item collaborative signals. A common trend in these\napproaches focuses on improving ranking performance at the cost of escalating\nmodel complexity, reducing diversity, and complicating the task. It is\nessential to provide recommendations that are both personalized and diverse,\nrather than solely relying on achieving high rank-based performance, such as\nClick-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task\nlearning approach, training on user-item and item-item interactions. We apply\nitem-based contrastive learning on descriptive text, sampling positive and\nnegative pairs based on item metadata. Our approach allows the model to better\nunderstand the relationships between entities within the knowledge graph by\nutilizing semantic information from text. It leads to more accurate, relevant,\nand diverse user recommendations and a benefit that extends even to cold-start\nusers who have few interactions with items. We perform extensive experiments on\ntwo widely used datasets to validate the effectiveness of our approach. Our\nfindings demonstrate that jointly training user-item interactions and\nitem-based signals using synopsis text is highly effective. Furthermore, our\nresults provide evidence that item-based contrastive learning enhances the\nquality of entity embeddings, as indicated by metrics such as uniformity and\nalignment.\n","authors":["Yejin Kim","Scott Rome","Kevin Foley","Mayur Nankani","Rimon Melamed","Javier Morales","Abhay Yadav","Maria Peifer","Sardar Hamidian","H. Howie Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18667v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2309.13320v2","updated":"2024-03-27T14:57:29Z","published":"2023-09-23T09:35:55Z","title":"GlotScript: A Resource and Tool for Low Resource Writing System\n  Identification","summary":"  We present GlotScript, an open resource and tool for low resource writing\nsystem identification. GlotScript-R is a resource that provides the attested\nwriting systems for more than 7,000 languages. It is compiled by aggregating\ninformation from existing writing system resources. GlotScript-T is a writing\nsystem identification tool that covers all 161 Unicode 15.0 scripts. For an\ninput text, it returns its script distribution where scripts are identified by\nISO 15924 codes. We also present two use cases for GlotScript. First, we\ndemonstrate that GlotScript can help cleaning multilingual corpora such as mC4\nand OSCAR. Second, we analyze the tokenization of a number of language models\nsuch as GPT-4 using GlotScript and provide insights on the coverage of low\nresource scripts and languages by each language model. We hope that GlotScript\nwill become a useful resource for work on low resource languages in the NLP\ncommunity. GlotScript-R and GlotScript-T are available at\nhttps://github.com/cisnlp/GlotScript.\n","authors":["Amir Hossein Kargaran","François Yvon","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2309.13320v2.pdf","comment":"LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18647v1","updated":"2024-03-27T14:54:27Z","published":"2024-03-27T14:54:27Z","title":"SDSAT: Accelerating LLM Inference through Speculative Decoding with\n  Semantic Adaptive Tokens","summary":"  We propose an acceleration scheme for large language models (LLMs) through\nSpeculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary\nobjective of this design is to enhance the LLM model's ability to generate\ndraft tokens more accurately without compromising the model's accuracy. The\ncore strategies involve: 1) Fine-tune the model by incorporating semantic\nadaptive tokens that possess flexible decoding capabilities without changing\nits structure, allowing them to generate high-quality draft tokens. 2) By\nemploying a training method that does not affect the standard tokens, the model\ncan acquire parallel decoding abilities atop its original framework with\nminimal training overhead. 3) We have designed the \"two-step-draft-then-verify\"\ngeneration strategies using both greedy search and nucleus sampling.\nExperiments conducted on the CodeLlama-13B and 7B models have yielded speed\nincreases of over 3.5X and 3.0X, respectively. Please refer to\nhttps://github.com/hasuoshenyun/SDSAT.\n","authors":["Chengbo Liu","Yong Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.18647v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.04507v2","updated":"2024-03-27T14:50:56Z","published":"2024-03-07T14:07:00Z","title":"NLPre: a revised approach towards language-centric benchmarking of\n  Natural Language Preprocessing systems","summary":"  With the advancements of transformer-based architectures, we observe the rise\nof natural language preprocessing (NLPre) tools capable of solving preliminary\nNLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or\nmorphological analysis) without any external linguistic guidance. It is arduous\nto compare novel solutions to well-entrenched preprocessing toolkits, relying\non rule-based morphological analysers or dictionaries. Aware of the\nshortcomings of existing NLPre evaluation approaches, we investigate a novel\nmethod of reliable and fair evaluation and performance reporting. Inspired by\nthe GLUE benchmark, the proposed language-centric benchmarking system enables\ncomprehensive ongoing evaluation of multiple NLPre tools, while credibly\ntracking their performance. The prototype application is configured for Polish\nand integrated with the thoroughly assembled NLPre-PL benchmark. Based on this\nbenchmark, we conduct an extensive evaluation of a variety of Polish NLPre\nsystems. To facilitate the construction of benchmarking environments for other\nlanguages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full\ncustomization of the publicly released source code of the benchmarking system.\nThe links to all the resources (deployed platforms, source code, trained\nmodels, datasets etc.) can be found on the project website:\nhttps://sites.google.com/view/nlpre-benchmark.\n","authors":["Martyna Wiącek","Piotr Rybak","Łukasz Pszenny","Alina Wróblewska"],"pdf_url":"https://arxiv.org/pdf/2403.04507v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18624v1","updated":"2024-03-27T14:34:29Z","published":"2024-03-27T14:34:29Z","title":"Vulnerability Detection with Code Language Models: How Far Are We?","summary":"  In the context of the rising interest in code language models (code LMs) and\nvulnerability detection, we study the effectiveness of code LMs for detecting\nvulnerabilities. Our analysis reveals significant shortcomings in existing\nvulnerability datasets, including poor data quality, low label accuracy, and\nhigh duplication rates, leading to unreliable model performance in realistic\nvulnerability detection scenarios. Additionally, the evaluation methods used\nwith these datasets are not representative of real-world vulnerability\ndetection.\n  To address these challenges, we introduce PrimeVul, a new dataset for\ntraining and evaluating code LMs for vulnerability detection. PrimeVul\nincorporates a novel set of data labeling techniques that achieve comparable\nlabel accuracy to human-verified benchmarks while significantly expanding the\ndataset. It also implements a rigorous data de-duplication and chronological\ndata splitting strategy to mitigate data leakage issues, alongside introducing\nmore realistic evaluation metrics and settings. This comprehensive approach\naims to provide a more accurate assessment of code LMs' performance in\nreal-world conditions.\n  Evaluating code LMs on PrimeVul reveals that existing benchmarks\nsignificantly overestimate the performance of these models. For instance, a\nstate-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on\nPrimeVul. Attempts to improve performance through advanced training techniques\nand larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin\nto random guessing in the most stringent settings. These findings underscore\nthe considerable gap between current capabilities and the practical\nrequirements for deploying code LMs in security roles, highlighting the need\nfor more innovative research in this domain.\n","authors":["Yangruibo Ding","Yanjun Fu","Omniyyah Ibrahim","Chawin Sitawarin","Xinyun Chen","Basel Alomair","David Wagner","Baishakhi Ray","Yizheng Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13284v2","updated":"2024-03-27T14:30:44Z","published":"2024-02-19T09:07:59Z","title":"Structure Guided Large Language Model for SQL Generation","summary":"  Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.\n","authors":["Qinggang Zhang","Junnan Dong","Hao Chen","Wentao Li","Feiran Huang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2402.13284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18609v1","updated":"2024-03-27T14:26:41Z","published":"2024-03-27T14:26:41Z","title":"A survey on learning models of spiking neural membrane systems and\n  spiking neural networks","summary":"  Spiking neural networks (SNN) are a biologically inspired model of neural\nnetworks with certain brain-like properties. In the past few decades, this\nmodel has received increasing attention in computer science community, owing\nalso to the successful phenomenon of deep learning. In SNN, communication\nbetween neurons takes place through the spikes and spike trains. This\ndifferentiates these models from the ``standard'' artificial neural networks\n(ANN) where the frequency of spikes is replaced by real-valued signals. Spiking\nneural P systems (SNPS) can be considered a branch of SNN based more on the\nprinciples of formal automata, with many variants developed within the\nframework of the membrane computing theory. In this paper, we first briefly\ncompare structure and function, advantages and drawbacks of SNN and SNPS. A key\npart of the article is a survey of recent results and applications of machine\nlearning and deep learning models of both SNN and SNPS formalisms.\n","authors":["Prithwineel Paul","Petr Sosik","Lucie Ciencialova"],"pdf_url":"https://arxiv.org/pdf/2403.18609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12997v3","updated":"2024-03-27T13:59:57Z","published":"2024-02-20T13:25:16Z","title":"Towards Trustworthy Reranking: A Simple yet Effective Abstention\n  Mechanism","summary":"  Neural Information Retrieval (NIR) has significantly improved upon\nheuristic-based IR systems. Yet, failures remain frequent, the models used\noften being unable to retrieve documents relevant to the user's query. We\naddress this challenge by proposing a lightweight abstention mechanism tailored\nfor real-world constraints, with particular emphasis placed on the reranking\nphase. We introduce a protocol for evaluating abstention strategies in a\nblack-box scenario, demonstrating their efficacy, and propose a simple yet\neffective data-driven mechanism. We provide open-source code for experiment\nreplication and abstention implementation, fostering wider adoption and\napplication in diverse contexts.\n","authors":["Hippolyte Gisserot-Boukhlef","Manuel Faysse","Emmanuel Malherbe","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2402.12997v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09283v3","updated":"2024-03-27T13:55:14Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2308.12531v2","updated":"2024-03-27T13:46:37Z","published":"2023-08-24T03:40:54Z","title":"CARE: Co-Attention Network for Joint Entity and Relation Extraction","summary":"  Joint entity and relation extraction is the fundamental task of information\nextraction, consisting of two subtasks: named entity recognition and relation\nextraction. However, most existing joint extraction methods suffer from issues\nof feature confusion or inadequate interaction between the two subtasks.\nAddressing these challenges, in this work, we propose a Co-Attention network\nfor joint entity and Relation Extraction (CARE). Our approach includes adopting\na parallel encoding strategy to learn separate representations for each\nsubtask, aiming to avoid feature overlap or confusion. At the core of our\napproach is the co-attention module that captures two-way interaction between\nthe two subtasks, allowing the model to leverage entity information for\nrelation prediction and vice versa, thus promoting mutual enhancement. Through\nextensive experiments on three benchmark datasets for joint entity and relation\nextraction (NYT, WebNLG, and SciERC), we demonstrate that our proposed model\noutperforms existing baseline models. Our code will be available at\nhttps://github.com/kwj0x7f/CARE.\n","authors":["Wenjun Kong","Yamei Xia"],"pdf_url":"https://arxiv.org/pdf/2308.12531v2.pdf","comment":"Accepted by LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2401.06712v2","updated":"2024-03-27T13:38:35Z","published":"2024-01-12T17:26:51Z","title":"Few-Shot Detection of Machine-Generated Text using Style Representations","summary":"  The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.\n","authors":["Rafael Rivera Soto","Kailin Koch","Aleem Khan","Barry Chen","Marcus Bishop","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2401.06712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18555v1","updated":"2024-03-27T13:34:59Z","published":"2024-03-27T13:34:59Z","title":"Debiasing Sentence Embedders through Contrastive Word Pairs","summary":"  Over the last years, various sentence embedders have been an integral part in\nthe success of current machine learning approaches to Natural Language\nProcessing (NLP). Unfortunately, multiple sources have shown that the bias,\ninherent in the datasets upon which these embedding methods are trained, is\nlearned by them. A variety of different approaches to remove biases in\nembeddings exists in the literature. Most of these approaches are applicable to\nword embeddings and in fewer cases to sentence embeddings. It is problematic\nthat most debiasing approaches are directly transferred from word embeddings,\ntherefore these approaches fail to take into account the nonlinear nature of\nsentence embedders and the embeddings they produce. It has been shown in\nliterature that bias information is still present if sentence embeddings are\ndebiased using such methods. In this contribution, we explore an approach to\nremove linear and nonlinear bias information for NLP solutions, without\nimpacting downstream performance. We compare our approach to common debiasing\nmethods on classical bias metrics and on bias metrics which take nonlinear\ninformation into account.\n","authors":["Philip Kenneweg","Sarah Schröder","Alexander Schulz","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08268v3","updated":"2024-03-27T13:29:31Z","published":"2023-11-14T16:02:16Z","title":"A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can\n  Fool Large Language Models Easily","summary":"  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to\nprovide useful and safe responses. However, adversarial prompts known as\n'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially\nharmful content. Exploring jailbreak prompts can help to better reveal the\nweaknesses of LLMs and further steer us to secure them. Unfortunately, existing\njailbreak methods either suffer from intricate manual design or require\noptimization on other white-box models, which compromises either generalization\nor efficiency. In this paper, we generalize jailbreak prompt attacks into two\naspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we\npropose ReNeLLM, an automatic framework that leverages LLMs themselves to\ngenerate effective jailbreak prompts. Extensive experiments demonstrate that\nReNeLLM significantly improves the attack success rate while greatly reducing\nthe time cost compared to existing baselines. Our study also reveals the\ninadequacy of current defense methods in safeguarding LLMs. Finally, we analyze\nthe failure of LLMs defense from the perspective of prompt execution priority,\nand propose corresponding defense strategies. We hope that our research can\ncatalyze both the academic community and LLMs developers towards the provision\nof safer and more regulated LLMs. The code is available at\nhttps://github.com/NJUNLP/ReNeLLM.\n","authors":["Peng Ding","Jun Kuang","Dan Ma","Xuezhi Cao","Yunsen Xian","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2311.08268v3.pdf","comment":"Acccepted by NAACL 2024, 18 pages, 7 figures, 13 tables"},{"id":"http://arxiv.org/abs/2403.18542v1","updated":"2024-03-27T13:22:38Z","published":"2024-03-27T13:22:38Z","title":"Attention-aware semantic relevance predicting Chinese sentence reading","summary":"  In recent years, several influential computational models and metrics have\nbeen proposed to predict how humans comprehend and process sentence. One\nparticularly promising approach is contextual semantic similarity. Inspired by\nthe attention algorithm in Transformer and human memory mechanisms, this study\nproposes an ``attention-aware'' approach for computing contextual semantic\nrelevance. This new approach takes into account the different contributions of\ncontextual parts and the expectation effect, allowing it to incorporate\ncontextual information fully. The attention-aware approach also facilitates the\nsimulation of existing reading models and evaluate them. The resulting\n``attention-aware'' metrics of semantic relevance can more accurately predict\nfixation durations in Chinese reading tasks recorded in an eye-tracking corpus\nthan those calculated by existing approaches. The study's findings further\nprovide strong support for the presence of semantic preview benefits in Chinese\nnaturalistic reading. Furthermore, the attention-aware metrics of semantic\nrelevance, being memory-based, possess high interpretability from both\nlinguistic and cognitive standpoints, making them a valuable computational tool\nfor modeling eye-movements in reading and further gaining insight into the\nprocess of language comprehension. Our approach underscores the potential of\nthese metrics to advance our comprehension of how humans understand and process\nlanguage, ultimately leading to a better understanding of language\ncomprehension and processing.\n","authors":["Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2403.18542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18537v1","updated":"2024-03-27T13:12:57Z","published":"2024-03-27T13:12:57Z","title":"A Path Towards Legal Autonomy: An interoperable and explainable approach\n  to extracting, transforming, loading and computing legal information using\n  large language models, expert systems and Bayesian networks","summary":"  Legal autonomy - the lawful activity of artificial intelligence agents - can\nbe achieved in one of two ways. It can be achieved either by imposing\nconstraints on AI actors such as developers, deployers and users, and on AI\nresources such as data, or by imposing constraints on the range and scope of\nthe impact that AI agents can have on the environment. The latter approach\ninvolves encoding extant rules concerning AI driven devices into the software\nof AI agents controlling those devices (e.g., encoding rules about limitations\non zones of operations into the agent software of an autonomous drone device).\nThis is a challenge since the effectivity of such an approach requires a method\nof extracting, loading, transforming and computing legal information that would\nbe both explainable and legally interoperable, and that would enable AI agents\nto reason about the law. In this paper, we sketch a proof of principle for such\na method using large language models (LLMs), expert legal systems known as\nlegal decision paths, and Bayesian networks. We then show how the proposed\nmethod could be applied to extant regulation in matters of autonomous cars,\nsuch as the California Vehicle Code.\n","authors":["Axel Constant","Hannes Westermann","Bryan Wilson","Alex Kiefer","Ines Hipolito","Sylvain Pronovost","Steven Swanson","Mahault Albarracin","Maxwell J. D. Ramstead"],"pdf_url":"https://arxiv.org/pdf/2403.18537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2403.18504v1","updated":"2024-03-27T12:33:42Z","published":"2024-03-27T12:33:42Z","title":"AcTED: Automatic Acquisition of Typical Event Duration for\n  Semi-supervised Temporal Commonsense QA","summary":"  We propose a voting-driven semi-supervised approach to automatically acquire\nthe typical duration of an event and use it as pseudo-labeled data. The human\nevaluation demonstrates that our pseudo labels exhibit surprisingly high\naccuracy and balanced coverage. In the temporal commonsense QA task,\nexperimental results show that using only pseudo examples of 400 events, we\nachieve performance comparable to the existing BERT-based weakly supervised\napproaches that require a significant amount of training examples. When\ncompared to the RoBERTa baselines, our best approach establishes\nstate-of-the-art performance with a 7% improvement in Exact Match.\n","authors":["Felix Virgo","Fei Cheng","Lis Kanashiro Pereira","Masayuki Asahara","Ichiro Kobayashi","Sadao Kurohashi"],"pdf_url":"https://arxiv.org/pdf/2403.18504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16516v2","updated":"2024-03-27T12:32:31Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v2.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2403.16432v2","updated":"2024-03-27T11:37:58Z","published":"2024-03-25T05:27:35Z","title":"$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on\n  Prompt-based Language Models","summary":"  Prompt-based learning is a new language model training paradigm that adapts\nthe Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes\nthe performance benchmarks across various natural language processing (NLP)\ntasks. Instead of using a fixed prompt template to fine-tune the model, some\nresearch demonstrates the effectiveness of searching for the prompt via\noptimization. Such prompt optimization process of prompt-based learning on PLMs\nalso gives insight into generating adversarial prompts to mislead the model,\nraising concerns about the adversarial vulnerability of this paradigm. Recent\nstudies have shown that universal adversarial triggers (UATs) can be generated\nto alter not only the predictions of the target PLMs but also the prediction of\ncorresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based\nlearning paradigm. However, UATs found in previous works are often unreadable\ntokens or characters and can be easily distinguished from natural texts with\nadaptive defenses. In this work, we consider the naturalness of the UATs and\ndevelop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs\nby a gradient-based beam search algorithm that not only effectively attacks the\ntarget PLMs and PFMs but also maintains the naturalness among the trigger\ntokens. Extensive results demonstrate the effectiveness of\n$\\textit{LinkPrompt}$, as well as the transferability of UATs generated by\n$\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and\nAPI-accessed LLM GPT-3.5-turbo.\n","authors":["Yue Xu","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16432v2.pdf","comment":"Accepted to the main conference of NAACL2024"},{"id":"http://arxiv.org/abs/2311.07838v3","updated":"2024-03-27T11:36:46Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v3.pdf","comment":"Accepted by NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2304.03544v2","updated":"2024-03-27T10:53:42Z","published":"2023-04-07T08:49:43Z","title":"InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual\n  Topic Modeling","summary":"  Cross-lingual topic models have been prevalent for cross-lingual text\nanalysis by revealing aligned latent topics. However, most existing methods\nsuffer from producing repetitive topics that hinder further analysis and\nperformance decline caused by low-coverage dictionaries. In this paper, we\npropose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).\nInstead of the direct alignment in previous work, we propose a topic alignment\nwith mutual information method. This works as a regularization to properly\nalign topics and prevent degenerate topic representations of words, which\nmitigates the repetitive topic issue. To address the low-coverage dictionary\nissue, we further propose a cross-lingual vocabulary linking method that finds\nmore linked cross-lingual words for topic alignment beyond the translations of\na given dictionary. Extensive experiments on English, Chinese, and Japanese\ndatasets demonstrate that our method outperforms state-of-the-art baselines,\nproducing more coherent, diverse, and well-aligned topics and showing better\ntransferability for cross-lingual classification tasks.\n","authors":["Xiaobao Wu","Xinshuai Dong","Thong Nguyen","Chaoqun Liu","Liangming Pan","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2304.03544v2.pdf","comment":"Accepted to AAAI2023 conference. Code is available at\n  https://github.com/BobXWu/InfoCTM"},{"id":"http://arxiv.org/abs/2309.13322v2","updated":"2024-03-27T10:50:24Z","published":"2023-09-23T09:51:37Z","title":"From Text to Source: Results in Detecting Large Language Model-Generated\n  Content","summary":"  The widespread use of Large Language Models (LLMs), celebrated for their\nability to generate human-like text, has raised concerns about misinformation\nand ethical implications. Addressing these concerns necessitates the\ndevelopment of robust methods to detect and attribute text generated by LLMs.\nThis paper investigates \"Cross-Model Detection,\" by evaluating whether a\nclassifier trained to distinguish between source LLM-generated and\nhuman-written text can also detect text from a target LLM without further\ntraining. The study comprehensively explores various LLM sizes and families,\nand assesses the impact of conversational fine-tuning techniques, quantization,\nand watermarking on classifier generalization. The research also explores Model\nAttribution, encompassing source model identification, model family, and model\nsize classification, in addition to quantization and watermarking detection.\nOur results reveal several key findings: a clear inverse relationship between\nclassifier effectiveness and model size, with larger LLMs being more\nchallenging to detect, especially when the classifier is trained on data from\nsmaller models. Training on data from similarly sized LLMs can improve\ndetection performance from larger models but may lead to decreased performance\nwhen dealing with smaller models. Additionally, model attribution experiments\nshow promising results in identifying source models and model families,\nhighlighting detectable signatures in LLM-generated text, with particularly\nremarkable outcomes in watermarking detection, while no detectable signatures\nof quantization were observed. Overall, our study contributes valuable insights\ninto the interplay of model size, family, and training data in LLM detection\nand attribution.\n","authors":["Wissam Antoun","Benoît Sagot","Djamé Seddah"],"pdf_url":"https://arxiv.org/pdf/2309.13322v2.pdf","comment":"Accepted to COLING-LREC 2024"},{"id":"http://arxiv.org/abs/2403.18435v1","updated":"2024-03-27T10:40:14Z","published":"2024-03-27T10:40:14Z","title":"DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via\n  Structural Word Alignment","summary":"  Recent research demonstrates the effectiveness of using pre-trained language\nmodels for legal case retrieval. Most of the existing works focus on improving\nthe representation ability for the contextualized embedding of the [CLS] token\nand calculate relevance using textual semantic similarity. However, in the\nlegal domain, textual semantic similarity does not always imply that the cases\nare relevant enough. Instead, relevance in legal cases primarily depends on the\nsimilarity of key facts that impact the final judgment. Without proper\ntreatments, the discriminative ability of learned representations could be\nlimited since legal cases are lengthy and contain numerous non-key facts. To\nthis end, we introduce DELTA, a discriminative model designed for legal case\nretrieval. The basic idea involves pinpointing key facts in legal cases and\npulling the contextualized embedding of the [CLS] token closer to the key facts\nwhile pushing away from the non-key facts, which can warm up the case embedding\nspace in an unsupervised manner. To be specific, this study brings the word\nalignment mechanism to the contextual masked auto-encoder. First, we leverage\nshallow decoders to create information bottlenecks, aiming to enhance the\nrepresentation ability. Second, we employ the deep decoder to enable\ntranslation between different structures, with the goal of pinpointing key\nfacts to enhance discriminative ability. Comprehensive experiments conducted on\npublicly available legal benchmarks show that our approach can outperform\nexisting state-of-the-art methods in legal case retrieval. It provides a new\nperspective on the in-depth understanding and processing of legal case\ndocuments.\n","authors":["Haitao Li","Qingyao Ai","Xinyan Han","Jia Chen","Qian Dong","Yiqun Liu","Chong Chen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18435v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.18430v1","updated":"2024-03-27T10:36:17Z","published":"2024-03-27T10:36:17Z","title":"Exploring language relations through syntactic distances and geographic\n  proximity","summary":"  Languages are grouped into families that share common linguistic traits.\nWhile this approach has been successful in understanding genetic relations\nbetween diverse languages, more analyses are needed to accurately quantify\ntheir relatedness, especially in less studied linguistic levels such as syntax.\nHere, we explore linguistic distances using series of parts of speech (POS)\nextracted from the Universal Dependencies dataset. Within an\ninformation-theoretic framework, we show that employing POS trigrams maximizes\nthe possibility of capturing syntactic variations while being at the same time\ncompatible with the amount of available data. Linguistic connections are then\nestablished by assessing pairwise distances based on the POS distributions.\nIntriguingly, our analysis reveals definite clusters that correspond to well\nknown language families and groups, with exceptions explained by distinct\nmorphological typologies. Furthermore, we obtain a significant correlation\nbetween language similarity and geographic distance, which underscores the\ninfluence of spatial proximity on language kinships.\n","authors":["Juan De Gregorio","Raúl Toral","David Sánchez"],"pdf_url":"https://arxiv.org/pdf/2403.18430v1.pdf","comment":"36 pages"},{"id":"http://arxiv.org/abs/2403.18426v1","updated":"2024-03-27T10:27:28Z","published":"2024-03-27T10:27:28Z","title":"TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions","summary":"  Nowadays, individuals tend to engage in dialogues with Large Language Models,\nseeking answers to their questions. In times when such answers are readily\naccessible to anyone, the stimulation and preservation of human's cognitive\nabilities, as well as the assurance of maintaining good reasoning skills by\nhumans becomes crucial. This study addresses such needs by proposing hints\n(instead of final answers or before giving answers) as a viable solution. We\nintroduce a framework for the automatic hint generation for factoid questions,\nemploying it to construct TriviaHG, a novel large-scale dataset featuring\n160,230 hints corresponding to 16,645 questions from the TriviaQA dataset.\nAdditionally, we present an automatic evaluation method that measures the\nConvergence and Familiarity quality attributes of hints. To evaluate the\nTriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals\nto annotate 2,791 hints and tasked 6 humans with answering questions using the\nprovided hints. The effectiveness of hints varied, with success rates of 96%,\n78%, and 36% for questions with easy, medium, and hard answers, respectively.\nMoreover, the proposed automatic evaluation methods showed a robust correlation\nwith annotators' results. Conclusively, the findings highlight three key\ninsights: the facilitative role of hints in resolving unknown questions, the\ndependence of hint quality on answer difficulty, and the feasibility of\nemploying automatic evaluation methods for hint assessment.\n","authors":["Jamshid Mozafari","Anubhav Jangra","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2403.18426v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.18423v1","updated":"2024-03-27T10:24:25Z","published":"2024-03-27T10:24:25Z","title":"SemRoDe: Macro Adversarial Training to Learn Representations That are\n  Robust to Word-Level Attacks","summary":"  Language models (LMs) are indispensable tools for natural language processing\ntasks, but their vulnerability to adversarial attacks remains a concern. While\ncurrent research has explored adversarial training techniques, their\nimprovements to defend against word-level attacks have been limited. In this\nwork, we propose a novel approach called Semantic Robust Defence (SemRoDe), a\nMacro Adversarial Training strategy to enhance the robustness of LMs. Drawing\ninspiration from recent studies in the image domain, we investigate and later\nconfirm that in a discrete data setting such as language, adversarial samples\ngenerated via word substitutions do indeed belong to an adversarial domain\nexhibiting a high Wasserstein distance from the base domain. Our method learns\na robust representation that bridges these two domains. We hypothesize that if\nsamples were not projected into an adversarial domain, but instead to a domain\nwith minimal shift, it would improve attack robustness. We align the domains by\nincorporating a new distance-based objective. With this, our model is able to\nlearn more generalized representations by aligning the model's high-level\noutput features and therefore better handling unseen adversarial samples. This\nmethod can be generalized across word embeddings, even when they share minimal\noverlap at both vocabulary and word-substitution levels. To evaluate the\neffectiveness of our approach, we conduct experiments on BERT and RoBERTa\nmodels on three datasets. The results demonstrate promising state-of-the-art\nrobustness.\n","authors":["Brian Formento","Wenjie Feng","Chuan Sheng Foo","Luu Anh Tuan","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18423v1.pdf","comment":"Published in NAACL 2024 (Main Track)"},{"id":"http://arxiv.org/abs/2402.01739v2","updated":"2024-03-27T10:21:24Z","published":"2024-01-29T12:05:02Z","title":"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models","summary":"  To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.\n","authors":["Fuzhao Xue","Zian Zheng","Yao Fu","Jinjie Ni","Zangwei Zheng","Wangchunshu Zhou","Yang You"],"pdf_url":"https://arxiv.org/pdf/2402.01739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18421v1","updated":"2024-03-27T10:18:21Z","published":"2024-03-27T10:18:21Z","title":"BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text","summary":"  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance\non a wide variety of biomedical NLP tasks. However, these models have hundreds\nof billions of parameters, are computationally expensive to run, require users\nto send their input data over the internet, and are trained on unknown data\nsources. Can smaller, more targeted models compete? To address this question,\nwe build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive\nmodel trained exclusively on PubMed abstracts and full articles. When\nfine-tuned, BioMedLM can produce strong multiple-choice biomedical\nquestion-answering results competitive with much larger models, such as\nachieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical\nGenetics exam. BioMedLM can also be fine-tuned to produce useful answers to\npatient questions on medical topics. This demonstrates that smaller models can\npotentially serve as transparent, privacy-preserving, economical and\nenvironmentally friendly foundations for particular NLP applications, such as\nin biomedicine. The model is available on the Hugging Face Hub:\nhttps://huggingface.co/stanford-crfm/BioMedLM.\n","authors":["Elliot Bolton","Abhinav Venigalla","Michihiro Yasunaga","David Hall","Betty Xiong","Tony Lee","Roxana Daneshjou","Jonathan Frankle","Percy Liang","Michael Carbin","Christopher D. Manning"],"pdf_url":"https://arxiv.org/pdf/2403.18421v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2403.17647v2","updated":"2024-03-27T10:07:59Z","published":"2024-03-26T12:29:18Z","title":"Intrinsic Subgraph Generation for Interpretable Graph based Visual\n  Question Answering","summary":"  The large success of deep learning based methods in Visual Question Answering\n(VQA) has concurrently increased the demand for explainable methods. Most\nmethods in Explainable Artificial Intelligence (XAI) focus on generating\npost-hoc explanations rather than taking an intrinsic approach, the latter\ncharacterizing an interpretable model. In this work, we introduce an\ninterpretable approach for graph-based VQA and demonstrate competitive\nperformance on the GQA dataset. This approach bridges the gap between\ninterpretability and performance. Our model is designed to intrinsically\nproduce a subgraph during the question-answering process as its explanation,\nproviding insight into the decision making. To evaluate the quality of these\ngenerated subgraphs, we compare them against established post-hoc\nexplainability methods for graph neural networks, and perform a human\nevaluation. Moreover, we present quantitative metrics that correlate with the\nevaluations of human assessors, acting as automatic metrics for the generated\nexplanatory subgraphs. Our implementation is available at\nhttps://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.\n","authors":["Pascal Tilli","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2403.17647v2.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.18381v1","updated":"2024-03-27T09:19:13Z","published":"2024-03-27T09:19:13Z","title":"Improving Attributed Text Generation of Large Language Models via\n  Preference Learning","summary":"  Large language models have been widely adopted in natural language\nprocessing, yet they face the challenge of generating unreliable content.\nRecent works aim to reduce misinformation and hallucinations by resorting to\nattribution as a means to provide evidence (i.e., citations). However, current\nattribution methods usually focus on the retrieval stage and automatic\nevaluation that neglect mirroring the citation mechanisms in human scholarly\nwriting to bolster credibility. In this paper, we address these challenges by\nmodelling the attribution task as preference learning and introducing an\nAutomatic Preference Optimization (APO) framework. First, we create a curated\ncollection for post-training with 6,330 examples by collecting and filtering\nfrom existing datasets. Second, considering the high cost of labelling\npreference data, we further propose an automatic method to synthesize\nattribution preference data resulting in 95,263 pairs. Moreover, inspired by\nthe human citation process, we further propose a progressive preference\noptimization method by leveraging fine-grained information. Extensive\nexperiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate\nthat APO achieves state-of-the-art citation F1 with higher answer quality.\n","authors":["Dongfang Li","Zetian Sun","Baotian Hu","Zhenyu Liu","Xinshuo Hu","Xuebo Liu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18381v1.pdf","comment":"23 pages, 15 tables, 2 figures"},{"id":"http://arxiv.org/abs/2312.10997v5","updated":"2024-03-27T09:16:57Z","published":"2023-12-18T07:47:33Z","title":"Retrieval-Augmented Generation for Large Language Models: A Survey","summary":"  Large Language Models (LLMs) showcase impressive capabilities but encounter\nchallenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the generation,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval, the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces up-to-date evaluation framework and\nbenchmark. At the end, this article delineates the challenges currently faced\nand points out prospective avenues for research and development.\n","authors":["Yunfan Gao","Yun Xiong","Xinyu Gao","Kangxiang Jia","Jinliu Pan","Yuxi Bi","Yi Dai","Jiawei Sun","Meng Wang","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10997v5.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2403.18365v1","updated":"2024-03-27T08:57:21Z","published":"2024-03-27T08:57:21Z","title":"BLADE: Enhancing Black-box Large Language Models with Small\n  Domain-Specific Models","summary":"  Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable\nof addressing a diverse range of tasks. However, general LLMs, which are\ndeveloped on open-domain data, may lack the domain-specific knowledge essential\nfor tasks in vertical domains, such as legal, medical, etc. To address this\nissue, previous approaches either conduct continuous pre-training with\ndomain-specific data or employ retrieval augmentation to support general LLMs.\nUnfortunately, these strategies are either cost-intensive or unreliable in\npractical applications. To this end, we present a novel framework named BLADE,\nwhich enhances Black-box LArge language models with small Domain-spEcific\nmodels. BLADE consists of a black-box LLM and a small domain-specific LM. The\nsmall LM preserves domain-specific knowledge and offers specialized insights,\nwhile the general LLM contributes robust language comprehension and reasoning\ncapabilities. Specifically, our method involves three steps: 1) pre-training\nthe small LM with domain-specific data, 2) fine-tuning this model using\nknowledge instruction data, and 3) joint Bayesian optimization of the general\nLLM and the small LM. Extensive experiments conducted on public legal and\nmedical benchmarks reveal that BLADE significantly outperforms existing\napproaches. This shows the potential of BLADE as an effective and\ncost-efficient solution in adapting general LLMs for vertical domains.\n","authors":["Haitao Li","Qingyao Ai","Jia Chen","Qian Dong","Zhijing Wu","Yiqun Liu","Chong Chen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18365v1.pdf","comment":"11pages"},{"id":"http://arxiv.org/abs/2307.16071v2","updated":"2024-03-27T08:56:01Z","published":"2023-07-29T20:42:50Z","title":"ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus","summary":"  We introduce \\`{I}r\\`{o}y\\`{i}nSpeech, a new corpus influenced by the desire\nto increase the amount of high quality, contemporary Yor\\`{u}b\\'{a} speech\ndata, which can be used for both Text-to-Speech (TTS) and Automatic Speech\nRecognition (ASR) tasks. We curated about 23000 text sentences from news and\ncreative writing domains with the open license CC-BY-4.0. To encourage a\nparticipatory approach to data creation, we provide 5000 curated sentences to\nthe Mozilla Common Voice platform to crowd-source the recording and validation\nof Yor\\`{u}b\\'{a} speech data. In total, we created about 42 hours of speech\ndata recorded by 80 volunteers in-house, and 6 hours of validated recordings on\nMozilla Common Voice platform. Our TTS evaluation suggests that a\nhigh-fidelity, general domain, single-speaker Yor\\`{u}b\\'{a} voice is possible\nwith as little as 5 hours of speech. Similarly, for ASR we obtained a baseline\nword error rate (WER) of 23.8.\n","authors":["Tolulope Ogunremi","Kola Tubosun","Anuoluwapo Aremu","Iroro Orife","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2307.16071v2.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.15837v2","updated":"2024-03-27T08:54:06Z","published":"2024-03-23T13:24:31Z","title":"Centered Masking for Language-Image Pre-Training","summary":"  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,\nstraightforward, and effective technique for masking image patches during\npre-training of a vision-language model. GLIP builds on Fast Language-Image\nPre-Training (FLIP), which randomly masks image patches while training a CLIP\nmodel. GLIP replaces random masking with centered masking, that uses a Gaussian\ndistribution and is inspired by the importance of image patches at the center\nof the image. GLIP retains the same computational savings as FLIP, while\nimproving performance across a range of downstream datasets and tasks, as\ndemonstrated by our experimental results. We show the benefits of GLIP to be\neasy to obtain, requiring no delicate tuning of the Gaussian, and also\napplicable to data sets containing images without an obvious center focus.\n","authors":["Mingliang Liang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2403.15837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02151v2","updated":"2024-03-27T08:43:28Z","published":"2023-05-03T14:33:23Z","title":"Identifying the Correlation Between Language Distance and Cross-Lingual\n  Transfer in a Multilingual Representation Space","summary":"  Prior research has investigated the impact of various linguistic features on\ncross-lingual transfer performance. In this study, we investigate the manner in\nwhich this effect can be mapped onto the representation space. While past\nstudies have focused on the impact on cross-lingual alignment in multilingual\nlanguage models during fine-tuning, this study examines the absolute evolution\nof the respective language representation spaces produced by MLLMs. We place a\nspecific emphasis on the role of linguistic characteristics and investigate\ntheir inter-correlation with the impact on representation spaces and\ncross-lingual transfer performance. Additionally, this paper provides\npreliminary evidence of how these findings can be leveraged to enhance transfer\nto linguistically distant languages.\n","authors":["Fred Philippy","Siwen Guo","Shohreh Haddadan"],"pdf_url":"https://arxiv.org/pdf/2305.02151v2.pdf","comment":"SIGTYP Workshop 2023 (co-located with EACL 2023)"},{"id":"http://arxiv.org/abs/2403.18350v1","updated":"2024-03-27T08:42:31Z","published":"2024-03-27T08:42:31Z","title":"Evaluation of Semantic Search and its Role in\n  Retrieved-Augmented-Generation (RAG) for Arabic Language","summary":"  The latest advancements in machine learning and deep learning have brought\nforth the concept of semantic similarity, which has proven immensely beneficial\nin multiple applications and has largely replaced keyword search. However,\nevaluating semantic similarity and conducting searches for a specific query\nacross various documents continue to be a complicated task. This complexity is\ndue to the multifaceted nature of the task, the lack of standard benchmarks,\nwhereas these challenges are further amplified for Arabic language. This paper\nendeavors to establish a straightforward yet potent benchmark for semantic\nsearch in Arabic. Moreover, to precisely evaluate the effectiveness of these\nmetrics and the dataset, we conduct our assessment of semantic search within\nthe framework of retrieval augmented generation (RAG).\n","authors":["Ali Mahboub","Muhy Eddin Za'ter","Bashar Alfrou","Yazan Estaitia","Adnan Jaljuli","Asma Hakouz"],"pdf_url":"https://arxiv.org/pdf/2403.18350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18349v1","updated":"2024-03-27T08:39:56Z","published":"2024-03-27T08:39:56Z","title":"Rejection Improves Reliability: Training LLMs to Refuse Unknown\n  Questions Using RL from Knowledge Feedback","summary":"  Large Language Models (LLMs) often generate erroneous outputs, known as\nhallucinations, due to their limitations in discerning questions beyond their\nknowledge scope. While addressing hallucination has been a focal point in\nresearch, previous efforts primarily concentrate on enhancing correctness\nwithout giving due consideration to the significance of rejection mechanisms.\nIn this paper, we conduct a comprehensive examination of the role of rejection,\nintroducing the notion of model reliability along with corresponding metrics.\nThese metrics measure the model's ability to provide accurate responses while\nadeptly rejecting questions exceeding its knowledge boundaries, thereby\nminimizing hallucinations. To improve the inherent reliability of LLMs, we\npresent a novel alignment framework called Reinforcement Learning from\nKnowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically\ndetermine the model's knowledge boundary and trains a reliable reward model to\nencourage the refusal of out-of-knowledge questions. Experimental results on\nmathematical questions affirm the substantial efficacy of RLKF in significantly\nenhancing LLM reliability.\n","authors":["Hongshen Xu","Zichen Zhu","Da Ma","Situo Zhang","Shuai Fan","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.18349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18346v1","updated":"2024-03-27T08:38:49Z","published":"2024-03-27T08:38:49Z","title":"Quantifying and Mitigating Unimodal Biases in Multimodal Large Language\n  Models: A Causal Perspective","summary":"  Recent advancements in Large Language Models (LLMs) have facilitated the\ndevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,\nMLLMs often suffer from an over-reliance on unimodal biases (e.g., language\nbias and vision bias), leading to incorrect answers in complex multimodal\ntasks. To investigate this issue, we propose a causal framework to interpret\nthe biases in Visual Question Answering (VQA) problems. Within our framework,\nwe devise a causal graph to elucidate the predictions of MLLMs on VQA problems,\nand assess the causal effect of biases through an in-depth causal analysis.\nMotivated by the causal graph, we introduce a novel MORE dataset, consisting of\n12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,\nnecessitating multi-hop reasoning and the surmounting of unimodal biases.\nFurthermore, we propose two strategies to mitigate unimodal biases and enhance\nMLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)\nframework for limited-access MLLMs and the refinement of open-source MLLMs\nthrough fine-tuning. Extensive quantitative and qualitative experiments offer\nvaluable insights for future research.\n","authors":["Meiqi Chen","Yixin Cao","Yan Zhang","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18341v1","updated":"2024-03-27T08:32:19Z","published":"2024-03-27T08:32:19Z","title":"IterAlign: Iterative Constitutional Alignment of Large Language Models","summary":"  With the rapid development of large language models (LLMs), aligning LLMs\nwith human values and societal norms to ensure their reliability and safety has\nbecome crucial. Reinforcement learning with human feedback (RLHF) and\nConstitutional AI (CAI) have been proposed for LLM alignment. However, these\nmethods require either heavy human annotations or explicitly pre-defined\nconstitutions, which are labor-intensive and resource-consuming. To overcome\nthese drawbacks, we study constitution-based LLM alignment and propose a\ndata-driven constitution discovery and self-alignment framework called\nIterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM\nand automatically discovers new constitutions using a stronger LLM. These\nconstitutions are then used to guide self-correction of the base LLM. Such a\nconstitution discovery pipeline can be run iteratively and automatically to\ndiscover new constitutions that specifically target the alignment gaps in the\ncurrent LLM. Empirical results on several safety benchmark datasets and\nmultiple base LLMs show that IterAlign successfully improves truthfulness,\nhelpfulness, harmlessness and honesty, improving the LLM alignment by up to\n$13.5\\%$ in harmlessness.\n","authors":["Xiusi Chen","Hongzhi Wen","Sreyashi Nag","Chen Luo","Qingyu Yin","Ruirui Li","Zheng Li","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18341v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18336v1","updated":"2024-03-27T08:21:01Z","published":"2024-03-27T08:21:01Z","title":"A Dataset for Pharmacovigilance in German, French, and Japanese:\n  Annotating Adverse Drug Reactions across Languages","summary":"  User-generated data sources have gained significance in uncovering Adverse\nDrug Reactions (ADRs), with an increasing number of discussions occurring in\nthe digital world. However, the existing clinical corpora predominantly revolve\naround scientific articles in English. This work presents a multilingual corpus\nof texts concerning ADRs gathered from diverse sources, including patient fora,\nsocial media, and clinical reports in German, French, and Japanese. Our corpus\ncontains annotations covering 12 entity types, four attribute types, and 13\nrelation types. It contributes to the development of real-world multilingual\nlanguage models for healthcare. We provide statistics to highlight certain\nchallenges associated with the corpus and conduct preliminary experiments\nresulting in strong baselines for extracting entities and relations between\nthese entities, both within and across languages.\n","authors":["Lisa Raithel","Hui-Syuan Yeh","Shuntaro Yada","Cyril Grouin","Thomas Lavergne","Aurélie Névéol","Patrick Paroubek","Philippe Thomas","Tomohiro Nishiyama","Sebastian Möller","Eiji Aramaki","Yuji Matsumoto","Roland Roller","Pierre Zweigenbaum"],"pdf_url":"https://arxiv.org/pdf/2403.18336v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18327v1","updated":"2024-03-27T08:08:00Z","published":"2024-03-27T08:08:00Z","title":"Can LLMs Converse Formally? Automatically Assessing LLMs in Translating\n  and Interpreting Formal Specifications","summary":"  Stakeholders often describe system requirements using natural language which\nare then converted to formal syntax by a domain-expert leading to increased\ndesign costs. This paper assesses the capabilities of Large Language Models\n(LLMs) in converting between natural language descriptions and formal\nspecifications. Existing work has evaluated the capabilities of LLMs in\ngenerating formal syntax such as source code but such experiments are typically\nhand-crafted and use problems that are likely to be in the training set of\nLLMs, and often require human-annotated datasets. We propose an approach that\ncan use two copies of an LLM in conjunction with an off-the-shelf verifier to\nautomatically evaluate its translation abilities without any additional human\ninput. Our approach generates formal syntax using language grammars to\nautomatically generate a dataset. We conduct an empirical evaluation to measure\nthe accuracy of this translation task and show that SOTA LLMs cannot adequately\nsolve this task, limiting their current utility in the design of complex\nsystems.\n","authors":["Rushang Karia","Daksh Dobhal","Daniel Bramblett","Pulkit Verma","Siddharth Srivastava"],"pdf_url":"https://arxiv.org/pdf/2403.18327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18314v1","updated":"2024-03-27T07:34:44Z","published":"2024-03-27T07:34:44Z","title":"Chinese Offensive Language Detection:Current Status and Future\n  Directions","summary":"  Despite the considerable efforts being made to monitor and regulate\nuser-generated content on social media platforms, the pervasiveness of\noffensive language, such as hate speech or cyberbullying, in the digital space\nremains a significant challenge. Given the importance of maintaining a\ncivilized and respectful online environment, there is an urgent and growing\nneed for automatic systems capable of detecting offensive speech in real time.\nHowever, developing effective systems for processing languages such as Chinese\npresents a significant challenge, owing to the language's complex and nuanced\nnature, which makes it difficult to process automatically. This paper provides\na comprehensive overview of offensive language detection in Chinese, examining\ncurrent benchmarks and approaches and highlighting specific models and tools\nfor addressing the unique challenges of detecting offensive language in this\ncomplex language. The primary objective of this survey is to explore the\nexisting techniques and identify potential avenues for further research that\ncan address the cultural and linguistic complexities of Chinese.\n","authors":["Yunze Xiao","Houda Bouamor","Wajdi Zaghouani"],"pdf_url":"https://arxiv.org/pdf/2403.18314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11399v2","updated":"2024-03-27T07:05:22Z","published":"2024-03-18T01:14:47Z","title":"X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment","summary":"  The impressive development of large language models (LLMs) is expanding into\nthe realm of large multimodal models (LMMs), which incorporate multiple types\nof data beyond text. However, the nature of multimodal models leads to\nsignificant expenses in the creation of training data. Furthermore,\nconstructing multilingual data for LMMs presents its own set of challenges due\nto language diversity and complexity. Therefore, in this study, we propose two\ncost-effective methods to solve this problem: (1) vocabulary expansion and\npretraining of multilingual LLM for specific languages, and (2) automatic and\nelaborate construction of multimodal datasets using GPT4-V. Based on015 these\nmethods, we constructed a 91K English-Korean-Chinese multilingual, multimodal\ntraining dataset. Additionally, we developed a bilingual multimodal model that\nexhibits excellent performance in both Korean and English, surpassing existing\napproaches.\n","authors":["Dongjae Shin","Hyunseok Lim","Inho Won","Changsu Choi","Minjun Kim","Seungwoo Song","Hangyeol Yoo","Sangmin Kim","Kyungtae Lim"],"pdf_url":"https://arxiv.org/pdf/2403.11399v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12468v3","updated":"2024-03-27T06:46:56Z","published":"2023-02-24T05:48:53Z","title":"Adapting Knowledge for Few-shot Table-to-Text Generation","summary":"  Pretrained language models (PLMs) have made remarkable progress in\ntable-to-text generation tasks. However, the lack of domain-specific knowledge\nmakes it challenging to bridge the topological gap between tabular data and\ntext, especially in real-world applications with limited resources. To mitigate\nthe limitation of insufficient labeled data, we propose a novel framework:\nAdapt-Knowledge-to-Generate (AKG). The core insight of AKG is to adapt\nunlabeled domain-specific knowledge into the model, which brings at least three\nbenefits: (1) it injects representation of normal table-related descriptions to\nbridge the topological gap between tabular data and texts; (2) it enables us to\nuse large amounts of unlabeled domain-specific knowledge fully, which can\nalleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it\nallows us to design various tasks to employ the domain-specific knowledge.\nExtensive experiments and analyses are conducted on three open-domain, few-shot\nnatural language generation (NLG) data sets: Humans, Songs, and Books. Compared\nto previous state-of-the-art approaches, our model achieves superior\nperformance in terms of both fluency and accuracy as judged by human and\nautomatic evaluations.\n","authors":["Zhixin Guo","Minyxuan Yan","Jiexing Qi","Jianping Zhou","Ziwei He","Guanjie Zheng","Xinbing Wang"],"pdf_url":"https://arxiv.org/pdf/2302.12468v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2302.04415"},{"id":"http://arxiv.org/abs/2403.18295v1","updated":"2024-03-27T06:43:58Z","published":"2024-03-27T06:43:58Z","title":"Dual Instruction Tuning with Large Language Models for Mathematical\n  Reasoning","summary":"  Recent advancements highlight the success of instruction tuning with large\nlanguage models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical\nreasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as\nincorrect, missing, and redundant steps in CoT generation leading to\ninaccuracies in answer predictions. To alleviate this problem, we propose a\ndual instruction tuning strategy to meticulously model mathematical reasoning\nfrom both forward and reverse directions. This involves introducing the\nIntermediate Reasoning State Prediction task (forward reasoning) and the\nInstruction Reconstruction task (reverse reasoning) to enhance the LLMs'\nunderstanding and execution of instructions. Training instances for these tasks\nare constructed based on existing mathematical instruction tuning datasets.\nSubsequently, LLMs undergo multi-task fine-tuning using both existing\nmathematical instructions and the newly created data. Comprehensive experiments\nvalidate the effectiveness and domain generalization of the dual instruction\ntuning strategy across various mathematical reasoning tasks.\n","authors":["Yongwei Zhou","Tiejun Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.18295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06201v3","updated":"2024-03-27T06:31:42Z","published":"2024-01-11T15:45:11Z","title":"EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction","summary":"  To address intricate real-world tasks, there has been a rising interest in\ntool utilization in applications of large language models (LLMs). To develop\nLLM-based agents, it usually requires LLMs to understand many tool functions\nfrom different tool documentation. But these documentations could be diverse,\nredundant or incomplete, which immensely affects the capability of LLMs in\nusing tools. To solve this, we introduce EASYTOOL, a framework transforming\ndiverse and lengthy tool documentation into a unified and concise tool\ninstruction for easier tool usage. EasyTool purifies essential information from\nextensive tool documentation of different sources, and elaborates a unified\ninterface (i.e., tool instruction) to offer standardized tool descriptions and\nfunctionalities for LLM-based agents. Extensive experiments on multiple\ndifferent tasks demonstrate that EasyTool can significantly reduce token\nconsumption and improve the performance of tool utilization in real-world\nscenarios. Our code will be available at\n\\url{https://github.com/microsoft/JARVIS/} in the future.\n","authors":["Siyu Yuan","Kaitao Song","Jiangjie Chen","Xu Tan","Yongliang Shen","Ren Kan","Dongsheng Li","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2401.06201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18286v1","updated":"2024-03-27T06:25:40Z","published":"2024-03-27T06:25:40Z","title":"Few-Shot Recalibration of Language Models","summary":"  Recent work has uncovered promising ways to extract well-calibrated\nconfidence estimates from language models (LMs), where the model's confidence\nscore reflects how likely it is to be correct. However, while LMs may appear\nwell-calibrated over broad distributions, this often hides significant\nmiscalibration within narrower slices (e.g., systemic over-confidence in math\ncan balance out systemic under-confidence in history, yielding perfect\ncalibration in aggregate). To attain well-calibrated confidence estimates for\nany slice of a distribution, we propose a new framework for few-shot\nslice-specific recalibration. Specifically, we train a recalibration model that\ntakes in a few unlabeled examples from any given slice and predicts a curve\nthat remaps confidence scores to be more accurate for that slice. Our trained\nmodel can recalibrate for arbitrary new slices, without using any labeled data\nfrom that slice. This enables us to identify domain-specific confidence\nthresholds above which the LM's predictions can be trusted, and below which it\nshould abstain. Experiments show that our few-shot recalibrator consistently\noutperforms existing calibration methods, for instance improving calibration\nerror for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.\n","authors":["Xiang Lisa Li","Urvashi Khandelwal","Kelvin Guu"],"pdf_url":"https://arxiv.org/pdf/2403.18286v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.16512v2","updated":"2024-03-27T06:25:10Z","published":"2024-03-25T07:55:29Z","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","summary":"  In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages.\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2403.16512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18277v1","updated":"2024-03-27T06:13:04Z","published":"2024-03-27T06:13:04Z","title":"BlendX: Complex Multi-Intent Detection with Blended Patterns","summary":"  Task-oriented dialogue (TOD) systems are commonly designed with the\npresumption that each utterance represents a single intent. However, this\nassumption may not accurately reflect real-world situations, where users\nfrequently express multiple intents within a single utterance. While there is\nan emerging interest in multi-intent detection (MID), existing in-domain\ndatasets such as MixATIS and MixSNIPS have limitations in their formulation. To\naddress these issues, we present BlendX, a suite of refined datasets featuring\nmore diverse patterns than their predecessors, elevating both its complexity\nand diversity. For dataset construction, we utilize both rule-based heuristics\nas well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a\nsimilarity-driven strategy for utterance selection. To ensure the quality of\nthe proposed datasets, we also introduce three novel metrics that assess the\nstatistical properties of an utterance related to word count, conjunction use,\nand pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art\nMID models struggle with the challenges posed by the new datasets, highlighting\nthe need to reexamine the current state of the MID field. The dataset is\navailable at https://github.com/HYU-NLP/BlendX.\n","authors":["Yejin Yoon","Jungyeon Lee","Kangsan Kim","Chanhee Park","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2403.18277v1.pdf","comment":"Accepted to LREC-COLING2024"},{"id":"http://arxiv.org/abs/2403.18276v1","updated":"2024-03-27T06:07:05Z","published":"2024-03-27T06:07:05Z","title":"RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era\n  of Transformers","summary":"  Transformer structure has achieved great success in multiple applied machine\nlearning communities, such as natural language processing (NLP), computer\nvision (CV) and information retrieval (IR). Transformer architecture's core\nmechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$\ntime complexity in inference. Many works have been proposed to improve the\nattention mechanism's scalability, such as Flash Attention and Multi-query\nAttention. A different line of work aims to design new mechanisms to replace\nattention. Recently, a notable model structure -- Mamba, which is based on\nstate space models, has achieved transformer-equivalent performance in multiple\nsequence modeling tasks.\n  In this work, we examine \\mamba's efficacy through the lens of a classical IR\ntask -- document ranking. A reranker model takes a query and a document as\ninput, and predicts a scalar relevance score. This task demands the language\nmodel's ability to comprehend lengthy contextual inputs and to capture the\ninteraction between query and document tokens. We find that (1) Mamba models\nachieve competitive performance compared to transformer-based models with the\nsame training recipe; (2) but also have a lower training throughput in\ncomparison to efficient transformer implementations such as flash attention. We\nhope this study can serve as a starting point to explore Mamba models in other\nclassical IR tasks. Our code implementation and trained checkpoints are made\npublic to facilitate\nreproducibility.\\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.\n","authors":["Zhichao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17636v2","updated":"2024-03-27T05:55:35Z","published":"2024-03-26T12:11:29Z","title":"Mix-Initiative Response Generation with Dynamic Prefix Tuning","summary":"  Mixed initiative serves as one of the key factors in controlling conversation\ndirections. For a speaker, responding passively or leading proactively would\nresult in rather different responses. However, most dialogue systems focus on\ntraining a holistic response generation model without any distinction among\ndifferent initiatives. It leads to the cross-contamination problem, where the\nmodel confuses different initiatives and generates inappropriate responses.\nMoreover, obtaining plenty of human annotations for initiative labels can be\nexpensive. To address this issue, we propose a general mix-Initiative Dynamic\nPrefix Tuning framework (IDPT) to decouple different initiatives from the\ngeneration model, which learns initiative-aware prefixes in both supervised and\nunsupervised settings. Specifically, IDPT decouples initiative factors into\ndifferent prefix parameters and uses the attention mechanism to adjust the\nselection of initiatives in guiding generation dynamically. The prefix\nparameters can be tuned towards accurate initiative prediction as well as\nmix-initiative response generation. Extensive experiments on two public\ndialogue datasets show that the proposed IDPT outperforms previous baselines on\nboth automatic metrics and human evaluations. It also manages to generate\nappropriate responses with manipulated initiatives.\n","authors":["Yuxiang Nie","Heyan Huang","Xian-Ling Mao","Lizi Liao"],"pdf_url":"https://arxiv.org/pdf/2403.17636v2.pdf","comment":"Accepted to the main conference of NAACL 2024"},{"id":"http://arxiv.org/abs/2311.08590v2","updated":"2024-03-27T05:53:58Z","published":"2023-11-14T23:20:51Z","title":"PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language\n  Models","summary":"  Pre-trained language models (PLMs) show impressive performance in various\ndownstream NLP tasks. However, pre-training large language models demands\nsubstantial memory and training compute. Furthermore, due to the substantial\nresources required, many PLM weights are confidential. Consequently, users are\ncompelled to share their data with model owners for fine-tuning specific tasks.\nTo overcome the limitations, we introduce Plug-in External Memory Adaptation\n(PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM\nfine-tuning without requiring access to all the weights. PEMA integrates with\ncontext representations from test data during inference to perform downstream\ntasks. It uses external memory to store PLM-generated context representations\nmapped with target tokens. Our method utilizes weight matrices of LoRA-like\nbottlenecked adapter in the PLM's final layer to enhance efficiency. Our\napproach also includes Gradual Unrolling, a novel interpolation strategy to\nimprove generation quality. We validate PEMA's effectiveness through\nexperiments on syntactic and real datasets for machine translation and style\ntransfer. Our findings show that PEMA outperforms other PEFT approaches in\nmemory and latency efficiency for training, and also excels in maintaining\nsentence meaning and generating appropriate language and styles.\n","authors":["HyunJin Kim","Young Jin Kim","JinYeong Bak"],"pdf_url":"https://arxiv.org/pdf/2311.08590v2.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18260v1","updated":"2024-03-27T05:22:06Z","published":"2024-03-27T05:22:06Z","title":"Toward Interactive Regional Understanding in Vision-Large Language\n  Models","summary":"  Recent Vision-Language Pre-training (VLP) models have demonstrated\nsignificant advancements. Nevertheless, these models heavily rely on image-text\npairs that capture only coarse and global information of an image, leading to a\nlimitation in their regional understanding ability. In this work, we introduce\n\\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,\nallowing them to understand user-indicated image regions. To achieve this, we\ndesign a simple yet innovative architecture, requiring no modifications to the\nmodel architecture or objective function. Additionally, we leverage a dataset\nthat contains a novel source of information, namely Localized Narratives, which\nhas been overlooked in previous VLP research. Our experiments demonstrate that\nour single generalist model not only achieves an interactive dialogue system\nbut also exhibits superior performance on various zero-shot region\nunderstanding tasks, without compromising its ability for global image\nunderstanding.\n","authors":["Jungbeom Lee","Sanghyuk Chun","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2403.18260v1.pdf","comment":"NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2403.09131v2","updated":"2024-03-27T05:02:55Z","published":"2024-03-14T06:49:16Z","title":"ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate\n  Professional and Non-Professional Styled Text","summary":"  Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\nfine-tuning remain underexplored. This study concentrates on textual\nprofessionalism and introduces a novel methodology, named ProSwitch, which\nequips a language model with the ability to produce both professional and\nnon-professional responses through knowledge-guided instruction tuning.\nProSwitch unfolds across three phases: data preparation for gathering domain\nknowledge and training corpus; instruction tuning for optimizing language\nmodels with multiple levels of instruction formats; and comprehensive\nevaluation for assessing the professionalism discrimination and reference-based\nquality of generated text. Comparative analysis of ProSwitch against both\ngeneral and specialized language models reveals that our approach outperforms\nbaselines in switching between professional and non-professional text\ngeneration.\n","authors":["Chang Zong","Yuyan Chen","Weiming Lu","Jian Shao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.09131v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2312.07950v3","updated":"2024-03-27T04:51:51Z","published":"2023-12-13T07:56:27Z","title":"CBQ: Cross-Block Quantization for Large Language Models","summary":"  Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.\n","authors":["Xin Ding","Xiaoyu Liu","Zhijun Tu","Yun Zhang","Wei Li","Jie Hu","Hanting Chen","Yehui Tang","Zhiwei Xiong","Baoqun Yin","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07950v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18253v1","updated":"2024-03-27T04:51:42Z","published":"2024-03-27T04:51:42Z","title":"MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation","summary":"  Metaphors are ubiquitous in daily life, yet detecting them poses a\nsignificant challenge. Previous approaches often struggled with improper\napplication of language rules and overlooked the issue of data sparsity. To\naddress these challenges, we introduce knowledge distillation and prompt\nlearning into metaphor detection. Specifically, we devise a prompt learning\ntemplate tailored for the metaphor detection task. By masking target words and\nproviding relevant prompt information, we guide the model to accurately infer\nthe contextual meaning of these words. This approach not only mitigates the\ninterference from the literal meaning of target words but also ensures the\nproper utilization of MIP language rules for metaphor detection. Moreover, we\nemploy a teacher model equipped with prior knowledge to generate meaningful\nsoft labels, guiding the optimization process of the student model. The\ninclusion of soft labels, akin to label smoothing, helps alleviate the model's\ntendency towards over-confidence and effectively addresses the challenge of\ndata sparsity. Experimental results demonstrate that our proposed model\nachieves state-of-the-art performance across multiple datasets.\n","authors":["Kaidi Jia","Rongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18252v1","updated":"2024-03-27T04:49:23Z","published":"2024-03-27T04:49:23Z","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","summary":"  Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.\n","authors":["Yiwu Zhong","Zi-Yuan Hu","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18252v1.pdf","comment":"Project page: https://github.com/LaVi-Lab/Visual-Table"},{"id":"http://arxiv.org/abs/2403.18251v1","updated":"2024-03-27T04:47:10Z","published":"2024-03-27T04:47:10Z","title":"Since the Scientific Literature Is Multilingual, Our Models Should Be\n  Too","summary":"  English has long been assumed the $\\textit{lingua franca}$ of scientific\nresearch, and this notion is reflected in the natural language processing (NLP)\nresearch involving scientific document representation. In this position piece,\nwe quantitatively show that the literature is largely multilingual and argue\nthat current models and benchmarks should reflect this linguistic diversity. We\nprovide evidence that text-based models fail to create meaningful\nrepresentations for non-English papers and highlight the negative user-facing\nimpacts of using English-only models non-discriminately across a multilingual\ndomain. We end with suggestions for the NLP community on how to improve\nperformance on non-English documents.\n","authors":["Abteen Ebrahimi","Kenneth Church"],"pdf_url":"https://arxiv.org/pdf/2403.18251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18249v1","updated":"2024-03-27T04:39:18Z","published":"2024-03-27T04:39:18Z","title":"Exploring the Deceptive Power of LLM-Generated Fake News: A Study of\n  Real-World Detection Challenges","summary":"  Recent advancements in Large Language Models (LLMs) have enabled the creation\nof fake news, particularly in complex fields like healthcare. Studies highlight\nthe gap in the deceptive power of LLM-generated fake news with and without\nhuman assistance, yet the potential of prompting techniques has not been fully\nexplored. Thus, this work aims to determine whether prompting strategies can\neffectively narrow this gap. Current LLM-based fake news attacks require human\nintervention for information gathering and often miss details and fail to\nmaintain context consistency. Therefore, to better understand threat tactics,\nwe propose a strong fake news attack method called conditional\nVariational-autoencoder-Like Prompt (VLPrompt). Unlike current methods,\nVLPrompt eliminates the need for additional data collection while maintaining\ncontextual coherence and preserving the intricacies of the original text. To\npropel future research on detecting VLPrompt attacks, we created a new dataset\nnamed VLPrompt fake news (VLPFN) containing real and fake texts. Our\nexperiments, including various detection methods and novel human study metrics,\nwere conducted to assess their performance on our dataset, yielding numerous\nfindings.\n","authors":["Yanshen Sun","Jianfeng He","Limeng Cui","Shuo Lei","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2403.18249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14965v4","updated":"2024-03-27T04:38:44Z","published":"2023-05-24T09:57:37Z","title":"Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting\n  Jailbreaks","summary":"  Recent explorations with commercial Large Language Models (LLMs) have shown\nthat non-expert users can jailbreak LLMs by simply manipulating their prompts;\nresulting in degenerate output behavior, privacy and security breaches,\noffensive outputs, and violations of content regulator policies. Limited\nstudies have been conducted to formalize and analyze these attacks and their\nmitigations. We bridge this gap by proposing a formalism and a taxonomy of\nknown (and possible) jailbreaks. We survey existing jailbreak methods and their\neffectiveness on open-source and commercial LLMs (such as GPT-based models,\nOPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak\ndetection in terms of their effectiveness against known attacks. For further\nanalysis, we release a dataset of model outputs across 3700 jailbreak prompts\nover 4 tasks.\n","authors":["Abhinav Rao","Sachin Vashistha","Atharva Naik","Somak Aditya","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2305.14965v4.pdf","comment":"Accepted at LREC-COLING 2024 - The 2024 Joint International\n  Conference on Computational Linguistics, Language Resources and Evaluation"},{"id":"http://arxiv.org/abs/2206.08657v6","updated":"2024-03-27T03:53:23Z","published":"2022-06-17T09:42:35Z","title":"BridgeTower: Building Bridges Between Encoders in Vision-Language\n  Representation Learning","summary":"  Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose BridgeTower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, BridgeTower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BridgeTower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\nhttps://github.com/microsoft/BridgeTower.\n","authors":["Xiao Xu","Chenfei Wu","Shachar Rosenman","Vasudev Lal","Wanxiang Che","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2206.08657v6.pdf","comment":"Accepted by AAAI 2023, Oral"},{"id":"http://arxiv.org/abs/2306.04357v5","updated":"2024-03-27T03:06:13Z","published":"2023-06-07T11:40:07Z","title":"Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue\n  Systems","summary":"  Dialogue response selection aims to select an appropriate response from\nseveral candidates based on a given user and system utterance history. Most\nexisting works primarily focus on post-training and fine-tuning tailored for\ncross-encoders. However, there are no post-training methods tailored for dense\nencoders in dialogue response selection. We argue that when the current\nlanguage model, based on dense dialogue systems (such as BERT), is employed as\na dense encoder, it separately encodes dialogue context and response, leading\nto a struggle to achieve the alignment of both representations. Thus, we\npropose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward\nyet effective post-training technique tailored for dense encoders in dialogue\nresponse selection. Dial-MAE uses an asymmetric encoder-decoder architecture to\ncompress the dialogue semantics into dense vectors, which achieves better\nalignment between the features of the dialogue context and response. Our\nexperiments have demonstrated that Dial-MAE is highly effective, achieving\nstate-of-the-art performance on two commonly evaluated benchmarks.\n","authors":["Zhenpeng Su","Xing Wu","Wei Zhou","Guangyuan Ma","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2306.04357v5.pdf","comment":"This paper has been accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.00868v2","updated":"2024-03-27T03:03:00Z","published":"2024-03-01T04:39:16Z","title":"SoftTiger: A Clinical Foundation Model for Healthcare Workflows","summary":"  We introduce SoftTiger, a clinical large language model (CLaM) designed as a\nfoundation model for healthcare workflows. The narrative and unstructured\nnature of clinical notes is a major obstacle for healthcare intelligentization.\nWe address a critical problem of structuring clinical notes into clinical data,\naccording to international interoperability standards. We collect and annotate\ndata for three subtasks, namely, international patient summary, clinical\nimpression and medical encounter. We then supervised fine-tuned a\nstate-of-the-art LLM using public and credentialed clinical data. The training\nis orchestrated in a way that the target model can first support basic clinical\ntasks such as abbreviation expansion and temporal information extraction, and\nthen learn to perform more complex downstream clinical tasks. Moreover, we\naddress several modeling challenges in the healthcare context, e.g., extra long\ncontext window. Our blind pairwise evaluation shows that SoftTiger outperforms\nother popular open-source models and GPT-3.5, comparable to Gemini-pro, with a\nmild gap from GPT-4. We believe that LLMs may become a step-stone towards\nhealthcare digitalization and democratization. Therefore, we publicly release\nSoftTiger models at scales of 13 billion and 70 billion parameters, as well as\ndatasets and code for our innovative scalable evaluation, hopefully, making a\nsignificant contribution to the healthcare industry.\n","authors":["Ye Chen","Igor Couto","Wei Cai","Cong Fu","Bruno Dorneles"],"pdf_url":"https://arxiv.org/pdf/2403.00868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17304v2","updated":"2024-03-27T02:59:57Z","published":"2024-02-27T08:27:15Z","title":"Probing Multimodal Large Language Models for Global and Local Semantic\n  Representations","summary":"  The advancement of Multimodal Large Language Models (MLLMs) has greatly\naccelerated the development of applications in understanding integrated texts\nand images. Recent works leverage image-caption datasets to train MLLMs,\nachieving state-of-the-art performance on image-to-text tasks. However, there\nare few studies exploring which layers of MLLMs make the most effort to the\nglobal image information, which plays vital roles in multimodal comprehension\nand generation. In this study, we find that the intermediate layers of models\ncan encode more global semantic information, whose representation vectors\nperform better on visual-language entailment tasks, rather than the topmost\nlayers. We further probe models regarding local semantic representations\nthrough object recognition tasks. We find that the topmost layers may\nexcessively focus on local information, leading to a diminished ability to\nencode global information. Our code and data are released via\nhttps://github.com/kobayashikanna01/probing_MLLM_rep.\n","authors":["Mingxu Tao","Quzhe Huang","Kun Xu","Liwei Chen","Yansong Feng","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.17304v2.pdf","comment":"Accepted by LREC-COLING 2024 as a short paper (Camera Ready)"},{"id":"http://arxiv.org/abs/2403.17343v2","updated":"2024-03-27T02:49:16Z","published":"2024-03-26T03:05:20Z","title":"Language Models are Free Boosters for Biomedical Imaging Tasks","summary":"  In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.\n","authors":["Zhixin Lai","Jing Wu","Suiyao Chen","Yucheng Zhou","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2403.17343v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16915v3","updated":"2024-03-27T01:53:36Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.15764v2","updated":"2024-03-27T01:23:58Z","published":"2024-02-24T08:40:30Z","title":"Look Before You Leap: Problem Elaboration Prompting Improves\n  Mathematical Reasoning in Large Language Models","summary":"  Large language models (LLMs) still grapple with complex tasks like\nmathematical reasoning. Despite significant efforts invested in improving\nprefix prompts or reasoning process, the crucial role of problem context might\nhave been neglected. Accurate recognition of inputs is fundamental for solving\nmathematical tasks, as ill-formed problems could potentially mislead LLM's\nreasoning. In this study, we propose a new approach named Problem Elaboration\nPrompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,\nPEP decomposes and elucidates the problem context before reasoning, therefore\nenhancing the context modeling and parsing efficiency. Experiments across\ndatasets and models demonstrate promising performances: (1) PEP demonstrates an\noverall enhancement in various mathematical tasks. For instance, with the\nGPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through\ngreedy decoding and self-consistency, respectively. (2) PEP can be easily\nimplemented and integrated with other prompting methods. (3) PEP shows\nparticular strength in handling distraction problems.\n","authors":["Haoran Liao","Jidong Tian","Shaohua Hu","Hao He","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2402.15764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18182v1","updated":"2024-03-27T01:19:23Z","published":"2024-03-27T01:19:23Z","title":"ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech\n  Corpus","summary":"  We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech\ncorpus. The corpus comprises twelve hours of Zoom meetings involving multiple\nspeakers role-playing a work situation where Students brainstorm ideas for a\ncertain topic and then discuss it with an Interlocutor. The meetings cover\ndifferent topics and are divided into phases with different language setups.\nThe corpus presents a challenging set for automatic speech recognition (ASR),\nincluding two languages (Arabic and English) with Arabic spoken in multiple\nvariants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English\nused with various accents. Adding to the complexity of the corpus, there is\nalso code-switching between these languages and dialects. As part of our work,\nwe take inspiration from established sets of transcription guidelines to\npresent a set of guidelines handling issues of conversational speech,\ncode-switching and orthography of both languages. We further enrich the corpus\nwith two layers of annotations; (1) dialectness level annotation for the\nportion of the corpus where mixing occurs between different variants of Arabic,\nand (2) automatic morphological annotations, including tokenization,\nlemmatization, and part-of-speech tagging.\n","authors":["Injy Hamed","Fadhl Eryani","David Palfreyman","Nizar Habash"],"pdf_url":"https://arxiv.org/pdf/2403.18182v1.pdf","comment":"Accepted to LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2301.10856v3","updated":"2024-03-27T00:47:21Z","published":"2023-01-25T22:27:40Z","title":"Partial Mobilization: Tracking Multilingual Information Flows Amongst\n  Russian Media Outlets and Telegram","summary":"  In response to disinformation and propaganda from Russian online media\nfollowing the invasion of Ukraine, Russian media outlets such as Russia Today\nand Sputnik News were banned throughout Europe. To maintain viewership, many of\nthese Russian outlets began to heavily promote their content on messaging\nservices like Telegram. In this work, we study how 16 Russian media outlets\ninteracted with and utilized 732 Telegram channels throughout 2022. Leveraging\nthe foundational model MPNet, DP-means clustering, and Hawkes processes, we\ntrace how narratives spread between news sites and Telegram channels. We show\nthat news outlets not only propagate existing narratives through Telegram but\nthat they source material from the messaging platform. For example, across the\nwebsites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of\narticles discussed content that originated/resulted from activity on Telegram.\nFinally, tracking the spread of individual topics, we measure the rate at which\nnews outlets and Telegram channels disseminate content within the Russian media\necosystem, finding that websites like ura.news and Telegram channels such as\n@genshab are the most effective at disseminating their content.\n","authors":["Hans W. A. Hanley","Zakir Durumeric"],"pdf_url":"https://arxiv.org/pdf/2301.10856v3.pdf","comment":"Accepted to ICWSM 2024"},{"id":"http://arxiv.org/abs/2308.11138v3","updated":"2024-03-27T00:29:33Z","published":"2023-08-22T02:39:42Z","title":"NLP-based detection of systematic anomalies among the narratives of\n  consumer complaints","summary":"  We develop an NLP-based procedure for detecting systematic nonmeritorious\nconsumer complaints, simply called systematic anomalies, among complaint\nnarratives. While classification algorithms are used to detect pronounced\nanomalies, in the case of smaller and frequent systematic anomalies, the\nalgorithms may falter due to a variety of reasons, including technical ones as\nwell as natural limitations of human analysts. Therefore, as the next step\nafter classification, we convert the complaint narratives into quantitative\ndata, which are then analyzed using an algorithm for detecting systematic\nanomalies. We illustrate the entire procedure using complaint narratives from\nthe Consumer Complaint Database of the Consumer Financial Protection Bureau.\n","authors":["Peiheng Gao","Ning Sun","Xuefeng Wang","Chen Yang","Ričardas Zitikis"],"pdf_url":"https://arxiv.org/pdf/2308.11138v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18167v1","updated":"2024-03-27T00:23:03Z","published":"2024-03-27T00:23:03Z","title":"Mechanisms of non-factual hallucinations in language models","summary":"  State-of-the-art language models (LMs) sometimes generate non-factual\nhallucinations that misalign with world knowledge. Despite extensive efforts to\ndetect and mitigate hallucinations, understanding their internal mechanisms\nremains elusive. Our study investigates the mechanistic causes of\nhallucination, specifically non-factual ones where the LM incorrectly predicts\nobject attributes in response to subject-relation queries. With causal\nmediation analysis and embedding space projection, we identify two general\nmechanistic causes of hallucinations shared across LMs of various scales and\ndesigns: 1) insufficient subject attribute knowledge in lower layer MLPs, and\n2) failing to select the correct object attribute in upper layer attention\nheads and MLPs. These two mechanisms exhibit varying degrees of subject-object\nassociation, predictive uncertainty and perturbation robustness. Additionally,\nwe scrutinize LM pre-training checkpoints, revealing distinct learning dynamics\nfor the two mechanistic causes of hallucinations. We also highlight how\nattribution features from our causal analysis can effectively construct\nhallucination detectors. Our work proposes a mechanistic understanding of LM\nfactual errors.\n","authors":["Lei Yu","Meng Cao","Jackie Chi Kit Cheung","Yue Dong"],"pdf_url":"https://arxiv.org/pdf/2403.18167v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2403.18821v1","updated":"2024-03-27T17:59:56Z","published":"2024-03-27T17:59:56Z","title":"Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and\n  Benchmark","summary":"  We present a new dataset called Real Acoustic Fields (RAF) that captures real\nacoustic room data from multiple modalities. The dataset includes high-quality\nand densely captured room impulse response data paired with multi-view images,\nand precise 6DoF pose tracking data for sound emitters and listeners in the\nrooms. We used this dataset to evaluate existing methods for novel-view\nacoustic synthesis and impulse response generation which previously relied on\nsynthetic data. In our evaluation, we thoroughly assessed existing audio and\naudio-visual models against multiple criteria and proposed settings to enhance\ntheir performance on real-world data. We also conducted experiments to\ninvestigate the impact of incorporating visual data (i.e., images and depth)\ninto neural acoustic field models. Additionally, we demonstrated the\neffectiveness of a simple sim2real approach, where a model is pre-trained with\nsimulated data and fine-tuned with sparse real-world data, resulting in\nsignificant improvements in the few-shot learning approach. RAF is the first\ndataset to provide densely captured room acoustic data, making it an ideal\nresource for researchers working on audio and audio-visual neural acoustic\nfield modeling techniques. Demos and datasets are available on our project\npage: https://facebookresearch.github.io/real-acoustic-fields/\n","authors":["Ziyang Chen","Israel D. Gebru","Christian Richardt","Anurag Kumar","William Laney","Andrew Owens","Alexander Richard"],"pdf_url":"https://arxiv.org/pdf/2403.18821v1.pdf","comment":"Accepted to CVPR 2024. Project site:\n  https://facebookresearch.github.io/real-acoustic-fields/"},{"id":"http://arxiv.org/abs/2403.18820v1","updated":"2024-03-27T17:59:54Z","published":"2024-03-27T17:59:54Z","title":"MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view\n  Human Performance Capture and Rendering","summary":"  Faithful human performance capture and free-view rendering from sparse RGB\nobservations is a long-standing problem in Vision and Graphics. The main\nchallenges are the lack of observations and the inherent ambiguities of the\nsetting, e.g. occlusions and depth ambiguity. As a result, radiance fields,\nwhich have shown great promise in capturing high-frequency appearance and\ngeometry details in dense setups, perform poorly when na\\\"ively supervising\nthem on sparse camera views, as the field simply overfits to the sparse-view\ninputs. To address this, we propose MetaCap, a method for efficient and\nhigh-quality geometry recovery and novel view synthesis given very sparse or\neven a single view of the human. Our key idea is to meta-learn the radiance\nfield weights solely from potentially sparse multi-view videos, which can serve\nas a prior when fine-tuning them on sparse imagery depicting the human. This\nprior provides a good network weight initialization, thereby effectively\naddressing ambiguities in sparse-view capture. Due to the articulated structure\nof the human body and motion-induced surface deformations, learning such a\nprior is non-trivial. Therefore, we propose to meta-learn the field weights in\na pose-canonicalized space, which reduces the spatial feature range and makes\nfeature learning more effective. Consequently, one can fine-tune our field\nparameters to quickly generalize to unseen poses, novel illumination conditions\nas well as novel and sparse (even monocular) camera views. For evaluating our\nmethod under different scenarios, we collect a new dataset, WildDynaCap, which\ncontains subjects captured in, both, a dense camera dome and in-the-wild sparse\ncamera rigs, and demonstrate superior results compared to recent\nstate-of-the-art methods on both public and WildDynaCap dataset.\n","authors":["Guoxing Sun","Rishabh Dabral","Pascal Fua","Christian Theobalt","Marc Habermann"],"pdf_url":"https://arxiv.org/pdf/2403.18820v1.pdf","comment":"Project page: https://vcai.mpi-inf.mpg.de/projects/MetaCap/"},{"id":"http://arxiv.org/abs/2403.18819v1","updated":"2024-03-27T17:59:53Z","published":"2024-03-27T17:59:53Z","title":"Benchmarking Object Detectors with COCO: A New Path Forward","summary":"  The Common Objects in Context (COCO) dataset has been instrumental in\nbenchmarking object detectors over the past decade. Like every dataset, COCO\ncontains subtle errors and imperfections stemming from its annotation\nprocedure. With the advent of high-performing models, we ask whether these\nerrors of COCO are hindering its utility in reliably benchmarking further\nprogress. In search for an answer, we inspect thousands of masks from COCO\n(2017 version) and uncover different types of errors such as imprecise mask\nboundaries, non-exhaustively annotated instances, and mislabeled masks. Due to\nthe prevalence of COCO, we choose to correct these errors to maintain\ncontinuity with prior research. We develop COCO-ReM (Refined Masks), a cleaner\nset of annotations with visibly better mask quality than COCO-2017. We evaluate\nfifty object detectors and find that models that predict visually sharper masks\nscore higher on COCO-ReM, affirming that they were being incorrectly penalized\ndue to errors in COCO-2017. Moreover, our models trained using COCO-ReM\nconverge faster and score higher than their larger variants trained using\nCOCO-2017, highlighting the importance of data quality in improving object\ndetectors. With these findings, we advocate using COCO-ReM for future object\ndetection research. Our dataset is available at https://cocorem.xyz\n","authors":["Shweta Singh","Aayan Yadav","Jitesh Jain","Humphrey Shi","Justin Johnson","Karan Desai"],"pdf_url":"https://arxiv.org/pdf/2403.18819v1.pdf","comment":"Technical report. Dataset website: https://cocorem.xyz and code:\n  https://github.com/kdexd/coco-rem"},{"id":"http://arxiv.org/abs/2403.18818v1","updated":"2024-03-27T17:59:52Z","published":"2024-03-27T17:59:52Z","title":"ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object\n  Removal and Insertion","summary":"  Diffusion models have revolutionized image editing but often generate images\nthat violate physical laws, particularly the effects of objects on the scene,\ne.g., occlusions, shadows, and reflections. By analyzing the limitations of\nself-supervised approaches, we propose a practical solution centered on a\n\\q{counterfactual} dataset. Our method involves capturing a scene before and\nafter removing a single object, while minimizing other changes. By fine-tuning\na diffusion model on this dataset, we are able to not only remove objects but\nalso their effects on the scene. However, we find that applying this approach\nfor photorealistic object insertion requires an impractically large dataset. To\ntackle this challenge, we propose bootstrap supervision; leveraging our object\nremoval model trained on a small counterfactual dataset, we synthetically\nexpand this dataset considerably. Our approach significantly outperforms prior\nmethods in photorealistic object removal and insertion, particularly at\nmodeling the effects of objects on the scene.\n","authors":["Daniel Winter","Matan Cohen","Shlomi Fruchter","Yael Pritch","Alex Rav-Acha","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2403.18818v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18816v1","updated":"2024-03-27T17:59:33Z","published":"2024-03-27T17:59:33Z","title":"Garment3DGen: 3D Garment Stylization and Texture Generation","summary":"  We introduce Garment3DGen a new method to synthesize 3D garment assets from a\nbase mesh given a single input image as guidance. Our proposed approach allows\nusers to generate 3D textured clothes based on both real and synthetic images,\nsuch as those generated by text prompts. The generated assets can be directly\ndraped and simulated on human bodies. First, we leverage the recent progress of\nimage to 3D diffusion methods to generate 3D garment geometries. However, since\nthese geometries cannot be utilized directly for downstream tasks, we propose\nto use them as pseudo ground-truth and set up a mesh deformation optimization\nprocedure that deforms a base template mesh to match the generated 3D target.\nSecond, we introduce carefully designed losses that allow the input base mesh\nto freely deform towards the desired target, yet preserve mesh quality and\ntopology such that they can be simulated. Finally, a texture estimation module\ngenerates high-fidelity texture maps that are globally and locally consistent\nand faithfully capture the input guidance, allowing us to render the generated\n3D assets. With Garment3DGen users can generate the textured 3D garment of\ntheir choice without the need of artist intervention. One can provide a textual\nprompt describing the garment they desire to generate a simulation-ready 3D\nasset. We present a plethora of quantitative and qualitative comparisons on\nvarious assets both real and generated and provide use-cases of how one can\ngenerate simulation-ready 3D garments.\n","authors":["Nikolaos Sarafianos","Tuur Stuyck","Xiaoyu Xiang","Yilei Li","Jovan Popovic","Rakesh Ranjan"],"pdf_url":"https://arxiv.org/pdf/2403.18816v1.pdf","comment":"Project Page: https://nsarafianos.github.io/garment3dgen"},{"id":"http://arxiv.org/abs/2403.18814v1","updated":"2024-03-27T17:59:04Z","published":"2024-03-27T17:59:04Z","title":"Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models","summary":"  In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.\n","authors":["Yanwei Li","Yuechen Zhang","Chengyao Wang","Zhisheng Zhong","Yixin Chen","Ruihang Chu","Shaoteng Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2403.18814v1.pdf","comment":"Code and models are available at\n  https://github.com/dvlab-research/MiniGemini"},{"id":"http://arxiv.org/abs/2403.18811v1","updated":"2024-03-27T17:57:02Z","published":"2024-03-27T17:57:02Z","title":"Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance\n  Accompaniment","summary":"  We introduce a novel task within the field of 3D dance generation, termed\ndance accompaniment, which necessitates the generation of responsive movements\nfrom a dance partner, the \"follower\", synchronized with the lead dancer's\nmovements and the underlying musical rhythm. Unlike existing solo or group\ndance generation tasks, a duet dance scenario entails a heightened degree of\ninteraction between the two participants, requiring delicate coordination in\nboth pose and position. To support this task, we first build a large-scale and\ndiverse duet interactive dance dataset, DD100, by recording about 117 minutes\nof professional dancers' performances. To address the challenges inherent in\nthis task, we propose a GPT-based model, Duolando, which autoregressively\npredicts the subsequent tokenized motion conditioned on the coordinated\ninformation of the music, the leader's and the follower's movements. To further\nenhance the GPT's capabilities of generating stable results on unseen\nconditions (music and leader motions), we devise an off-policy reinforcement\nlearning strategy that allows the model to explore viable trajectories from\nout-of-distribution samplings, guided by human-defined rewards. Based on the\ncollected dataset and proposed method, we establish a benchmark with several\ncarefully designed metrics.\n","authors":["Li Siyao","Tianpei Gu","Zhitao Yang","Zhengyu Lin","Ziwei Liu","Henghui Ding","Lei Yang","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2403.18811v1.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18807v1","updated":"2024-03-27T17:53:30Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://github.com/Aradhye2002/EcoDepth.\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2311.10319v4","updated":"2024-03-27T17:41:50Z","published":"2023-11-17T04:04:29Z","title":"Shifting to Machine Supervision: Annotation-Efficient Semi and\n  Self-Supervised Learning for Automatic Medical Image Segmentation and\n  Classification","summary":"  Advancements in clinical treatment are increasingly constrained by the\nlimitations of supervised learning techniques, which depend heavily on large\nvolumes of annotated data. The annotation process is not only costly but also\ndemands substantial time from clinical specialists. Addressing this issue, we\nintroduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)\npipeline, a novel approach that leverages advancements in self-supervised and\nsemi-supervised learning. These techniques engage in auxiliary tasks that do\nnot require labeling, thus simplifying the scaling of machine supervision\ncompared to fully-supervised methods. Our study benchmarks these techniques on\nthree distinct medical imaging datasets to evaluate their effectiveness in\nclassification and segmentation tasks. Notably, we observed that self\nsupervised learning significantly surpassed the performance of supervised\nmethods in the classification of all evaluated datasets. Remarkably, the\nsemi-supervised approach demonstrated superior outcomes in segmentation,\noutperforming fully-supervised methods while using 50% fewer labels across all\ndatasets. In line with our commitment to contributing to the scientific\ncommunity, we have made the S4MI code openly accessible, allowing for broader\napplication and further development of these methods.\n","authors":["Pranav Singh","Raviteja Chukkapalli","Shravan Chaudhari","Luoyao Chen","Mei Chen","Jinqian Pan","Craig Smuda","Jacopo Cirrone"],"pdf_url":"https://arxiv.org/pdf/2311.10319v4.pdf","comment":"Seventeen pages (incl. references), five figures, and one table.\n  (Under Review)"},{"id":"http://arxiv.org/abs/2403.18795v1","updated":"2024-03-27T17:40:14Z","published":"2024-03-27T17:40:14Z","title":"Gamba: Marry Gaussian Splatting with Mamba for single view 3D\n  reconstruction","summary":"  We tackle the challenge of efficiently reconstructing a 3D asset from a\nsingle image with growing demands for automated 3D content creation pipelines.\nPrevious methods primarily rely on Score Distillation Sampling (SDS) and Neural\nRadiance Fields (NeRF). Despite their significant success, these approaches\nencounter practical limitations due to lengthy optimization and considerable\nmemory usage. In this report, we introduce Gamba, an end-to-end amortized 3D\nreconstruction model from single-view images, emphasizing two main insights:\n(1) 3D representation: leveraging a large number of 3D Gaussians for an\nefficient 3D Gaussian splatting process; (2) Backbone design: introducing a\nMamba-based sequential network that facilitates context-dependent reasoning and\nlinear scalability with the sequence (token) length, accommodating a\nsubstantial number of Gaussians. Gamba incorporates significant advancements in\ndata preprocessing, regularization design, and training methodologies. We\nassessed Gamba against existing optimization-based and feed-forward 3D\ngeneration approaches using the real-world scanned OmniObject3D dataset. Here,\nGamba demonstrates competitive generation capabilities, both qualitatively and\nquantitatively, while achieving remarkable speed, approximately 0.6 second on a\nsingle NVIDIA A100 GPU.\n","authors":["Qiuhong Shen","Xuanyu Yi","Zike Wu","Pan Zhou","Hanwang Zhang","Shuicheng Yan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18791v1","updated":"2024-03-27T17:35:24Z","published":"2024-03-27T17:35:24Z","title":"Object Pose Estimation via the Aggregation of Diffusion Features","summary":"  Estimating the pose of objects from images is a crucial task of 3D scene\nunderstanding, and recent approaches have shown promising results on very large\nbenchmarks. However, these methods experience a significant performance drop\nwhen dealing with unseen objects. We believe that it results from the limited\ngeneralizability of image features. To address this problem, we have an\nin-depth analysis on the features of diffusion models, e.g. Stable Diffusion,\nwhich hold substantial potential for modeling unseen objects. Based on this\nanalysis, we then innovatively introduce these diffusion features for object\npose estimation. To achieve this, we propose three distinct architectures that\ncan effectively capture and aggregate diffusion features of different\ngranularity, greatly improving the generalizability of object pose estimation.\nOur approach outperforms the state-of-the-art methods by a considerable margin\non three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our\nmethod achieves higher accuracy than the previous best arts on unseen objects:\n98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the\nstrong generalizability of our method. Our code is released at\nhttps://github.com/Tianfu18/diff-feats-pose.\n","authors":["Tianfu Wang","Guosheng Hu","Hongguang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18791v1.pdf","comment":"Accepted to CVPR2024"},{"id":"http://arxiv.org/abs/2403.18784v1","updated":"2024-03-27T17:32:04Z","published":"2024-03-27T17:32:04Z","title":"SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable\n  Surface","summary":"  We present SplatFace, a novel Gaussian splatting framework designed for 3D\nhuman face reconstruction without reliance on accurate pre-determined geometry.\nOur method is designed to simultaneously deliver both high-quality novel view\nrendering and accurate 3D mesh reconstructions. We incorporate a generic 3D\nMorphable Model (3DMM) to provide a surface geometric structure, making it\npossible to reconstruct faces with a limited set of input images. We introduce\na joint optimization strategy that refines both the Gaussians and the morphable\nsurface through a synergistic non-rigid alignment process. A novel distance\nmetric, splat-to-surface, is proposed to improve alignment by considering both\nthe Gaussian position and covariance. The surface information is also utilized\nto incorporate a world-space densification process, resulting in superior\nreconstruction quality. Our experimental analysis demonstrates that the\nproposed method is competitive with both other Gaussian splatting techniques in\nnovel view synthesis and other 3D reconstruction methods in producing 3D face\nmeshes with high geometric precision.\n","authors":["Jiahao Luo","Jing Liu","James Davis"],"pdf_url":"https://arxiv.org/pdf/2403.18784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18775v1","updated":"2024-03-27T17:23:39Z","published":"2024-03-27T17:23:39Z","title":"ImageNet-D: Benchmarking Neural Network Robustness on Diffusion\n  Synthetic Object","summary":"  We establish rigorous benchmarks for visual perception robustness. Synthetic\nimages such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific\ntype of evaluation over synthetic corruptions, backgrounds, and textures, yet\nthose robustness benchmarks are restricted in specified variations and have low\nsynthetic quality. In this work, we introduce generative model as a data source\nfor synthesizing hard images that benchmark deep models' robustness. Leveraging\ndiffusion models, we are able to generate images with more diversified\nbackgrounds, textures, and materials than any prior work, where we term this\nbenchmark as ImageNet-D. Experimental results show that ImageNet-D results in a\nsignificant accuracy drop to a range of vision models, from the standard ResNet\nvisual classifier to the latest foundation models like CLIP and MiniGPT-4,\nsignificantly reducing their accuracy by up to 60\\%. Our work suggests that\ndiffusion models can be an effective source to test vision models. The code and\ndataset are available at https://github.com/chenshuang-zhang/imagenet_d.\n","authors":["Chenshuang Zhang","Fei Pan","Junmo Kim","In So Kweon","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2403.18775v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2312.01220v2","updated":"2024-03-27T17:23:16Z","published":"2023-12-02T20:11:48Z","title":"Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation","summary":"  Detecting objects in low-light scenarios presents a persistent challenge, as\ndetectors trained on well-lit data exhibit significant performance degradation\non low-light data due to low visibility. Previous methods mitigate this issue\nby exploring image enhancement or object detection techniques with real\nlow-light image datasets. However, the progress is impeded by the inherent\ndifficulties about collecting and annotating low-light images. To address this\nchallenge, we propose to boost low-light object detection with zero-shot\nday-night domain adaptation, which aims to generalize a detector from well-lit\nscenarios to low-light ones without requiring real low-light data. Revisiting\nRetinex theory in the low-level vision, we first design a reflectance\nrepresentation learning module to learn Retinex-based illumination invariance\nin images with a carefully designed illumination invariance reinforcement\nstrategy. Next, an interchange-redecomposition-coherence procedure is\nintroduced to improve over the vanilla Retinex image decomposition process by\nperforming two sequential image decompositions and introducing a\nredecomposition cohering loss. Extensive experiments on ExDark, DARK FACE, and\nCODaN datasets show strong low-light generalizability of our method. Our code\nis available at https://github.com/ZPDu/DAI-Net.\n","authors":["Zhipeng Du","Miaojing Shi","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2312.01220v2.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.06054v4","updated":"2024-03-27T17:06:10Z","published":"2024-03-10T00:47:05Z","title":"Decoupled Data Consistency with Diffusion Purification for Image\n  Restoration","summary":"  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n","authors":["Xiang Li","Soo Min Kwon","Ismail R. Alkhouri","Saiprasad Ravishankar","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2403.06054v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18762v1","updated":"2024-03-27T17:01:10Z","published":"2024-03-27T17:01:10Z","title":"ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place\n  Recognition","summary":"  Place recognition is an important task for robots and autonomous cars to\nlocalize themselves and close loops in pre-built maps. While single-modal\nsensor-based methods have shown satisfactory performance, cross-modal place\nrecognition that retrieving images from a point-cloud database remains a\nchallenging problem. Current cross-modal methods transform images into 3D\npoints using depth estimation for modality conversion, which are usually\ncomputationally intensive and need expensive labeled data for depth\nsupervision. In this work, we introduce a fast and lightweight framework to\nencode images and point clouds into place-distinctive descriptors. We propose\nan effective Field of View (FoV) transformation module to convert point clouds\ninto an analogous modality as images. This module eliminates the necessity for\ndepth estimation and helps subsequent modules achieve real-time performance. We\nfurther design a non-negative factorization-based encoder to extract mutually\nconsistent semantic features between point clouds and images. This encoder\nyields more distinctive global descriptors for retrieval. Experimental results\non the KITTI dataset show that our proposed methods achieve state-of-the-art\nperformance while running in real time. Additional evaluation on the HAOMO\ndataset covering a 17 km trajectory further shows the practical generalization\ncapabilities. We have released the implementation of our methods as open source\nat: https://github.com/haomo-ai/ModaLink.git.\n","authors":["Weidong Xie","Lun Luo","Nanfei Ye","Yi Ren","Shaoyi Du","Minhang Wang","Jintao Xu","Rui Ai","Weihao Gu","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18762v1.pdf","comment":"8 pages, 11 figures, conference"},{"id":"http://arxiv.org/abs/2403.18756v1","updated":"2024-03-27T16:56:14Z","published":"2024-03-27T16:56:14Z","title":"Detection of subclinical atherosclerosis by image-based deep learning on\n  chest x-ray","summary":"  Aims. To develop a deep-learning based system for recognition of subclinical\natherosclerosis on a plain frontal chest x-ray. Methods and Results. A\ndeep-learning algorithm to predict coronary artery calcium (CAC) score (the\nAI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%\ninternal validation cohort) of primary prevention patients (58.4% male, median\nage 63 [51-74] years) with available paired chest x-ray and chest computed\ntomography (CT) indicated for any clinical reason and performed within 3\nmonths. The CAC score calculated on chest CT was used as ground truth. The\nmodel was validated on an temporally-independent cohort of 90 patients from the\nsame institution (external validation). The diagnostic accuracy of the AI-CAC\nmodel assessed by the area under the curve (AUC) was the primary outcome.\nOverall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.\nAUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation\ncohort and 0.77 in the external validation cohort. Sensitivity was consistently\nabove 92% in both cohorts. In the overall cohort (n=540), among patients with\nAI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with\nAI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events\n(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to\naccurately detect subclinical atherosclerosis on chest x-ray with elevated\nsensitivity, and to predict ASCVD events with elevated negative predictive\nvalue. Adoption of the AI-CAC model to refine CV risk stratification or as an\nopportunistic screening tool requires prospective evaluation.\n","authors":["Guglielmo Gallone","Francesco Iodice","Alberto Presta","Davide Tore","Ovidio de Filippo","Michele Visciano","Carlo Alberto Barbano","Alessandro Serafini","Paola Gorrini","Alessandro Bruno","Walter Grosso Marra","James Hughes","Mario Iannaccone","Paolo Fonio","Attilio Fiandrotti","Alessandro Depaoli","Marco Grangetto","Gaetano Maria de Ferrari","Fabrizio D'Ascenzo"],"pdf_url":"https://arxiv.org/pdf/2403.18756v1.pdf","comment":"Submitted to European Heart Journal - Cardiovascular Imaging Added\n  also the additional material 44 pages (30 main paper, 14 additional\n  material), 14 figures (5 main manuscript, 9 additional material)"},{"id":"http://arxiv.org/abs/2303.09817v2","updated":"2024-03-27T16:52:59Z","published":"2023-03-17T07:53:18Z","title":"Interpretable machine learning for time-to-event prediction in medicine\n  and healthcare","summary":"  Time-to-event prediction, e.g. cancer survival analysis or hospital length of\nstay, is a highly prominent machine learning task in medical and healthcare\napplications. However, only a few interpretable machine learning methods comply\nwith its challenges. To facilitate a comprehensive explanatory analysis of\nsurvival models, we formally introduce time-dependent feature effects and\nglobal feature importance explanations. We show how post-hoc interpretation\nmethods allow for finding biases in AI systems predicting length of stay using\na novel multi-modal dataset created from 1235 X-ray images with textual\nradiology reports annotated by human experts. Moreover, we evaluate cancer\nsurvival models beyond predictive performance to include the importance of\nmulti-omics feature groups based on a large-scale benchmark comprising 11\ndatasets from The Cancer Genome Atlas (TCGA). Model developers can use the\nproposed methods to debug and improve machine learning algorithms, while\nphysicians can discover disease biomarkers and assess their significance. We\nhope the contributed open data and code resources facilitate future work in the\nemerging research direction of explainable survival analysis.\n","authors":["Hubert Baniecki","Bartlomiej Sobieski","Patryk Szatkowski","Przemyslaw Bombinski","Przemyslaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2303.09817v2.pdf","comment":"An extended version of an AIME 2023 paper submitted to Artificial\n  Intelligence in Medicine"},{"id":"http://arxiv.org/abs/2403.14623v2","updated":"2024-03-27T16:49:35Z","published":"2024-03-21T17:59:41Z","title":"Simplified Diffusion Schrödinger Bridge","summary":"  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/checkcrab/SDSB.\n","authors":["Zhicong Tang","Tiankai Hang","Shuyang Gu","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11107v2","updated":"2024-03-27T16:48:34Z","published":"2024-03-17T06:21:21Z","title":"Self-supervised co-salient object detection via feature correspondence\n  at multiple scales","summary":"  Our paper introduces a novel two-stage self-supervised approach for detecting\nco-occurring salient objects (CoSOD) in image groups without requiring\nsegmentation annotations. Unlike existing unsupervised methods that rely solely\non patch-level information (e.g. clustering patch descriptors) or on\ncomputation heavy off-the-shelf components for CoSOD, our lightweight model\nleverages feature correspondences at both patch and region levels,\nsignificantly improving prediction performance. In the first stage, we train a\nself-supervised network that detects co-salient regions by computing local\npatch-level feature correspondences across images. We obtain the segmentation\npredictions using confidence-based adaptive thresholding. In the next stage, we\nrefine these intermediate segmentations by eliminating the detected regions\n(within each image) whose averaged feature representations are dissimilar to\nthe foreground feature representation averaged across all the cross-attention\nmaps (from the previous stage). Extensive experiments on three CoSOD benchmark\ndatasets show that our self-supervised model outperforms the corresponding\nstate-of-the-art models by a huge margin (e.g. on the CoCA dataset, our model\nhas a 13.7% F-measure gain over the SOTA unsupervised CoSOD model). Notably,\nour self-supervised model also outperforms several recent fully supervised\nCoSOD models on the three test datasets (e.g., on the CoCA dataset, our model\nhas a 4.6% F-measure gain over a recent supervised CoSOD model).\n","authors":["Souradeep Chakraborty","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2403.11107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18734v1","updated":"2024-03-27T16:22:45Z","published":"2024-03-27T16:22:45Z","title":"A vascular synthetic model for improved aneurysm segmentation and\n  detection via Deep Neural Networks","summary":"  We hereby present a full synthetic model, able to mimic the various\nconstituents of the cerebral vascular tree: the cerebral arteries, the\nbifurcations and the intracranial aneurysms. By building this model, our goal\nwas to provide a substantial dataset of brain arteries which could be used by a\n3D Convolutional Neural Network (CNN) to either segment or detect/recognize\nvarious vascular diseases (such as artery dissection/thrombosis) or even some\nportions of the cerebral vasculature, such as the bifurcations or aneurysms. In\nthis study, we will particularly focus on Intra-Cranial Aneurysm (ICA)\ndetection and segmentation. The cerebral aneurysms most often occur on a\nparticular structure of the vascular tree named the Circle of Willis. Various\nstudies have been conducted to detect and monitor the ICAs and those based on\nDeep Learning (DL) achieve the best performances. Specifically, in this work,\nwe propose a full synthetic 3D model able to mimic the brain vasculature as\nacquired by Magnetic Resonance Angiography (MRA), and more particularly the\nTime Of Flight (TOF) principle. Among the various MRI modalities, the MRA-TOF\nallows to have a relatively good rendering of the blood vessels and is\nnon-invasive (no contrast liquid injection). Our model has been designed to\nsimultaneously mimic the arteries geometry, the ICA shape and the background\nnoise. The geometry of the vascular tree is modeled thanks to an interpolation\nwith 3D Spline functions, and the statistical properties of the background MRI\nnoise is collected from MRA acquisitions and reproduced within the model. In\nthis work, we thoroughly describe the synthetic vasculature model, we build up\na neural network designed for ICA segmentation and detection, and finally, we\ncarry out an in-depth evaluation of the performance gap gained thanks to the\nsynthetic model data augmentation.\n","authors":["Rafic Nader","Florent Autrusseau","Vincent L'Allinec","Romain Bourcier"],"pdf_url":"https://arxiv.org/pdf/2403.18734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18731v1","updated":"2024-03-27T16:21:24Z","published":"2024-03-27T16:21:24Z","title":"Enhancing Manufacturing Quality Prediction Models through the\n  Integration of Explainability Methods","summary":"  This research presents a method that utilizes explainability techniques to\namplify the performance of machine learning (ML) models in forecasting the\nquality of milling processes, as demonstrated in this paper through a\nmanufacturing use case. The methodology entails the initial training of ML\nmodels, followed by a fine-tuning phase where irrelevant features identified\nthrough explainability methods are eliminated. This procedural refinement\nresults in performance enhancements, paving the way for potential reductions in\nmanufacturing costs and a better understanding of the trained ML models. This\nstudy highlights the usefulness of explainability techniques in both explaining\nand optimizing predictive models in the manufacturing realm.\n","authors":["Dennis Gross","Helge Spieker","Arnaud Gotlieb","Ricardo Knoblauch"],"pdf_url":"https://arxiv.org/pdf/2403.18731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18730v1","updated":"2024-03-27T16:20:55Z","published":"2024-03-27T16:20:55Z","title":"Towards Image Ambient Lighting Normalization","summary":"  Lighting normalization is a crucial but underexplored restoration task with\nbroad applications. However, existing works often simplify this task within the\ncontext of shadow removal, limiting the light sources to one and\noversimplifying the scene, thus excluding complex self-shadows and restricting\nsurface classes to smooth ones. Although promising, such simplifications hinder\ngeneralizability to more realistic settings encountered in daily use. In this\npaper, we propose a new challenging task termed Ambient Lighting Normalization\n(ALN), which enables the study of interactions between shadows, unifying image\nrestoration and shadow removal in a broader context. To address the lack of\nappropriate datasets for ALN, we introduce the large-scale high-resolution\ndataset Ambient6K, comprising samples obtained from multiple light sources and\nincluding self-shadows resulting from complex geometries, which is the first of\nits kind. For benchmarking, we select various mainstream methods and rigorously\nevaluate them on Ambient6K. Additionally, we propose IFBlend, a novel strong\nbaseline that maximizes Image-Frequency joint entropy to selectively restore\nlocal areas under different lighting conditions, without relying on shadow\nlocalization priors. Experiments show that IFBlend achieves SOTA scores on\nAmbient6K and exhibits competitive performance on conventional shadow removal\nbenchmarks compared to shadow-specific models with mask priors. The dataset,\nbenchmark, and code are available at https://github.com/fvasluianu97/IFBlend.\n","authors":["Florin-Alexandru Vasluianu","Tim Seizinger","Zongwei Wu","Rakesh Ranjan","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2403.18730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09992v3","updated":"2024-03-27T16:20:52Z","published":"2023-03-17T14:07:55Z","title":"LION: Implicit Vision Prompt Tuning","summary":"  Despite recent competitive performance across a range of vision tasks, vision\nTransformers still have an issue of heavy computational costs. Recently, vision\nprompt learning has provided an economic solution to this problem without\nfine-tuning the whole large-scale models. However, the efficiency of existing\nmodels are still far from satisfactory due to insertion of extensive prompts\nblocks and trick prompt designs. In this paper, we propose an efficient vision\nmodel named impLicit vIsion prOmpt tuNing (LION), which is motivated by deep\nimplicit models with stable memory costs for various complex tasks. In\nparticular, we merely insect two equilibrium implicit layers in two ends of the\npre-trained main backbone with parameters in the backbone frozen. Moreover, we\nprune the parameters in these two layers according to lottery hypothesis. The\nperformance obtained by our LION are promising on a wide range of datasets. In\nparticular, our LION reduces up to 11.5% of training parameter numbers while\nobtaining higher performance compared with the state-of-the-art baseline VPT,\nespecially under challenging scenes. Furthermore, we find that our proposed\nLION had a good generalization performance, making it an easy way to boost\ntransfer learning in the future.\n","authors":["Haixin Wang","Jianlong Chang","Xiao Luo","Jinan Sun","Zhouchen Lin","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2303.09992v3.pdf","comment":"Accepted by AAAI2024; 9 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.18717v1","updated":"2024-03-27T16:06:37Z","published":"2024-03-27T16:06:37Z","title":"Semi-Supervised Learning for Deep Causal Generative Models","summary":"  Developing models that can answer questions of the form \"How would $x$ change\nif $y$ had been $z$?\" is fundamental for advancing medical image analysis.\nTraining causal generative models that address such counterfactual questions,\nthough, currently requires that all relevant variables have been observed and\nthat corresponding labels are available in training data. However, clinical\ndata may not have complete records for all patients and state of the art causal\ngenerative models are unable to take full advantage of this. We thus develop,\nfor the first time, a semi-supervised deep causal generative model that\nexploits the causal relationships between variables to maximise the use of all\navailable data. We explore this in the setting where each sample is either\nfully labelled or fully unlabelled, as well as the more clinically realistic\ncase of having different labels missing for each sample. We leverage techniques\nfrom causal inference to infer missing values and subsequently generate\nrealistic counterfactuals, even for samples with incomplete labels.\n","authors":["Yasin Ibrahim","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2403.18717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18715v1","updated":"2024-03-27T16:04:47Z","published":"2024-03-27T16:04:47Z","title":"Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding","summary":"  Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.\n","authors":["Xintong Wang","Jingheng Pan","Liang Ding","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2403.18715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18714v1","updated":"2024-03-27T16:02:00Z","published":"2024-03-27T16:02:00Z","title":"Bringing Textual Prompt to AI-Generated Image Quality Assessment","summary":"  AI-Generated Images (AGIs) have inherent multimodal nature. Unlike\ntraditional image quality assessment (IQA) on natural scenarios, AGIs quality\nassessment (AGIQA) takes the correspondence of image and its textual prompt\ninto consideration. This is coupled in the ground truth score, which confuses\nthe unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs\nQuality Assessment via Image and Prompt), a multimodal framework for AGIQA via\ncorresponding image and prompt incorporation. Specifically, we propose a novel\nincremental pretraining task named Image2Prompt for better understanding of\nAGIs and their corresponding textual prompts. An effective and efficient\nimage-prompt fusion module, along with a novel special [QA] token, are also\napplied. Both are plug-and-play and beneficial for the cooperation of image and\nits corresponding prompt. Experiments demonstrate that our IP-IQA achieves the\nstate-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.\n","authors":["Bowen Qu","Haohui Li","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18714v1.pdf","comment":"6 pages, 3 figures, accepted by ICME2024"},{"id":"http://arxiv.org/abs/2403.18711v1","updated":"2024-03-27T15:58:25Z","published":"2024-03-27T15:58:25Z","title":"SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable\n  Transient-Free 3D reconstruction from Satellite Imagery","summary":"  Current stereo-vision pipelines produce high accuracy 3D reconstruction when\nusing multiple pairs or triplets of satellite images. However, these pipelines\nare sensitive to the changes between images that can occur as a result of\nmulti-date acquisitions. Such variations are mainly due to variable shadows,\nreflexions and transient objects (cars, vegetation). To take such changes into\naccount, Neural Radiance Fields (NeRF) have recently been applied to multi-date\nsatellite imagery. However, Neural methods are very compute-intensive, taking\ndozens of hours to learn, compared with minutes for standard stereo-vision\npipelines. Following the ideas of Instant Neural Graphics Primitives we propose\nto use an efficient sampling strategy and multi-resolution hash encoding to\naccelerate the learning. Our model, Satellite Neural Graphics Primitives\n(SAT-NGP) decreases the learning time to 15 minutes while maintaining the\nquality of the 3D reconstruction.\n","authors":["Camille Billouard","Dawa Derksen","Emmanuelle Sarrazin","Bruno Vallet"],"pdf_url":"https://arxiv.org/pdf/2403.18711v1.pdf","comment":"5 pages, 3 figures, 1 table; Accepted to International Geoscience and\n  Remote Sensing Symposium (IGARSS) 2024; Code available at\n  https://github.com/Ellimac0/SAT-NGP"},{"id":"http://arxiv.org/abs/2403.18708v1","updated":"2024-03-27T15:56:42Z","published":"2024-03-27T15:56:42Z","title":"Dense Vision Transformer Compression with Few Samples","summary":"  Few-shot model compression aims to compress a large model into a more compact\none with only a tiny training set (even without labels). Block-level pruning\nhas recently emerged as a leading technique in achieving high accuracy and low\nlatency in few-shot CNN compression. But, few-shot compression for Vision\nTransformers (ViT) remains largely unexplored, which presents a new challenge.\nIn particular, the issue of sparse compression exists in traditional CNN\nfew-shot methods, which can only produce very few compressed models of\ndifferent model sizes. This paper proposes a novel framework for few-shot ViT\ncompression named DC-ViT. Instead of dropping the entire block, DC-ViT\nselectively eliminates the attention module while retaining and reusing\nportions of the MLP module. DC-ViT enables dense compression, which outputs\nnumerous compressed models that densely populate the range of model complexity.\nDC-ViT outperforms state-of-the-art few-shot compression methods by a\nsignificant margin of 10 percentage points, along with lower latency in the\ncompression of ViT and its variants.\n","authors":["Hanxiao Zhang","Yifan Zhou","Guo-Hua Wang","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18708v1.pdf","comment":"Accepted to CVPR 2024. Note: Jianxin Wu is a contributing author for\n  the arXiv version of this paper but is not listed as an author in the CVPR\n  version due to his role as Program Chair"},{"id":"http://arxiv.org/abs/2401.15120v2","updated":"2024-03-27T15:49:52Z","published":"2024-01-26T03:44:58Z","title":"Incorporating simulated spatial context information improves the\n  effectiveness of contrastive learning models","summary":"  Visual learning often occurs in a specific context, where an agent acquires\nskills through exploration and tracking of its location in a consistent\nenvironment. The historical spatial context of the agent provides a similarity\nsignal for self-supervised contrastive learning. We present a unique approach,\ntermed Environmental Spatial Similarity (ESS), that complements existing\ncontrastive learning methods. Using images from simulated, photorealistic\nenvironments as an experimental setting, we demonstrate that ESS outperforms\ntraditional instance discrimination approaches. Moreover, sampling additional\ndata from the same environment substantially improves accuracy and provides new\naugmentations. ESS allows remarkable proficiency in room classification and\nspatial prediction tasks, especially in unfamiliar environments. This learning\nparadigm has the potential to enable rapid visual learning in agents operating\nin new environments with unique visual characteristics. Potentially\ntransformative applications span from robotics to space exploration. Our proof\nof concept demonstrates improved efficiency over methods that rely on\nextensive, disconnected datasets.\n","authors":["Lizhen Zhu","James Z. Wang","Wonseuk Lee","Brad Wyble"],"pdf_url":"https://arxiv.org/pdf/2401.15120v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12091v3","updated":"2024-03-27T15:44:25Z","published":"2023-03-21T09:07:15Z","title":"Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised\n  Learning","summary":"  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled\ndata and test data are from the same distribution. Open-set semi-supervised\nlearning (Open-set SSL) considers a more practical scenario, where unlabeled\ndata and test data contain new categories (outliers) not observed in labeled\ndata (inliers). Most previous works focused on outlier detection via binary\nclassifiers, which suffer from insufficient scalability and inability to\ndistinguish different types of uncertainty. In this paper, we propose a novel\nframework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these\nlimitations. Concretely, we first introduce evidential deep learning (EDL) as\nan outlier detector to quantify different types of uncertainty, and design\ndifferent uncertainty metrics for self-training and inference. Furthermore, we\npropose a novel adaptive negative optimization strategy, making EDL more\ntailored to the unlabeled dataset containing both inliers and outliers. As\ndemonstrated empirically, our proposed method outperforms existing\nstate-of-the-art methods across four datasets.\n","authors":["Yang Yu","Danruo Deng","Furui Liu","Yueming Jin","Qi Dou","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.12091v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.18690v1","updated":"2024-03-27T15:41:23Z","published":"2024-03-27T15:41:23Z","title":"Annolid: Annotate, Segment, and Track Anything You Need","summary":"  Annolid is a deep learning-based software package designed for the\nsegmentation, labeling, and tracking of research targets within video files,\nfocusing primarily on animal behavior analysis. Based on state-of-the-art\ninstance segmentation methods, Annolid now harnesses the Cutie video object\nsegmentation model to achieve resilient, markerless tracking of multiple\nanimals from single annotated frames, even in environments in which they may be\npartially or entirely concealed by environmental features or by one another.\nOur integration of Segment Anything and Grounding-DINO strategies additionally\nenables the automatic masking and segmentation of recognizable animals and\nobjects by text command, removing the need for manual annotation. Annolid's\ncomprehensive approach to object segmentation flexibly accommodates a broad\nspectrum of behavior analysis applications, enabling the classification of\ndiverse behavioral states such as freezing, digging, pup huddling, and social\ninteractions in addition to the tracking of animals and their body parts.\n","authors":["Chen Yang","Thomas A. Cleland"],"pdf_url":"https://arxiv.org/pdf/2403.18690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08479v2","updated":"2024-03-27T15:38:27Z","published":"2023-12-13T19:38:50Z","title":"Vision Transformer-Based Deep Learning for Histologic Classification of\n  Endometrial Cancer","summary":"  Endometrial cancer, the fourth most common cancer in females in the United\nStates, with the lifetime risk for developing this disease is approximately\n2.8% in women. Precise histologic evaluation and molecular classification of\nendometrial cancer is important for effective patient management and\ndetermining the best treatment modalities. This study introduces EndoNet, which\nuses convolutional neural networks for extracting histologic features and a\nvision transformer for aggregating these features and classifying slides based\non their visual characteristics into high- and low- grade. The model was\ntrained on 929 digitized hematoxylin and eosin-stained whole-slide images of\nendometrial cancer from hysterectomy cases at Dartmouth-Health. It classifies\nthese slides into low-grade (Endometroid Grades 1 and 2) and high-grade\n(endometroid carcinoma FIGO grade 3, uterine serous carcinoma, carcinosarcoma)\ncategories. EndoNet was evaluated on an internal test set of 110 patients and\nan external test set of 100 patients from the public TCGA database. The model\nachieved a weighted average F1-score of 0.91 (95% CI: 0.86-0.95) and an AUC of\n0.95 (95% CI: 0.89-0.99) on the internal test, and 0.86 (95% CI: 0.80-0.94) for\nF1-score and 0.86 (95% CI: 0.75-0.93) for AUC on the external test. Pending\nfurther validation, EndoNet has the potential to support pathologists without\nthe need of manual annotations in classifying the grades of gynecologic\npathology tumors.\n","authors":["Manu Goyal","Laura J. Tafe","James X. Feng","Kristen E. Muller","Liesbeth Hondelink","Jessica L. Bentz","Saeed Hassanpour"],"pdf_url":"https://arxiv.org/pdf/2312.08479v2.pdf","comment":"4 Tables and 3 Figures"},{"id":"http://arxiv.org/abs/2308.06098v2","updated":"2024-03-27T15:26:44Z","published":"2023-08-11T12:18:53Z","title":"Automated Construction of Time-Space Diagrams for Traffic Analysis Using\n  Street-View Video Sequence","summary":"  Time-space diagrams are essential tools for analyzing traffic patterns and\noptimizing transportation infrastructure and traffic management strategies.\nTraditional data collection methods for these diagrams have limitations in\nterms of temporal and spatial coverage. Recent advancements in camera\ntechnology have overcome these limitations and provided extensive urban data.\nIn this study, we propose an innovative approach to constructing time-space\ndiagrams by utilizing street-view video sequences captured by cameras mounted\non moving vehicles. Using the state-of-the-art YOLOv5, StrongSORT, and\nphotogrammetry techniques for distance calculation, we can infer vehicle\ntrajectories from the video data and generate time-space diagrams. To evaluate\nthe effectiveness of our proposed method, we utilized datasets from the KITTI\ncomputer vision benchmark suite. The evaluation results demonstrate that our\napproach can generate trajectories from video data, although there are some\nerrors that can be mitigated by improving the performance of the detector,\ntracker, and distance calculation components. In conclusion, the utilization of\nstreet-view video sequences captured by cameras mounted on moving vehicles,\ncombined with state-of-the-art computer vision techniques, has immense\npotential for constructing comprehensive time-space diagrams. These diagrams\noffer valuable insights into traffic patterns and contribute to the design of\ntransportation infrastructure and traffic management strategies.\n","authors":["Tanay Rastogi","Mårten Björkman"],"pdf_url":"https://arxiv.org/pdf/2308.06098v2.pdf","comment":"The paper is published in 2023 IEEE 26th International Conference on\n  Intelligent Transportation Systems (ITSC)"},{"id":"http://arxiv.org/abs/2403.18674v1","updated":"2024-03-27T15:17:10Z","published":"2024-03-27T15:17:10Z","title":"Deep Learning for Robust and Explainable Models in Computer Vision","summary":"  Recent breakthroughs in machine and deep learning (ML and DL) research have\nprovided excellent tools for leveraging enormous amounts of data and optimizing\nhuge models with millions of parameters to obtain accurate networks for image\nprocessing. These developments open up tremendous opportunities for using\nartificial intelligence (AI) in the automation and human assisted AI industry.\nHowever, as more and more models are deployed and used in practice, many\nchallenges have emerged. This thesis presents various approaches that address\nrobustness and explainability challenges for using ML and DL in practice.\n  Robustness and reliability are the critical components of any model before\ncertification and deployment in practice. Deep convolutional neural networks\n(CNNs) exhibit vulnerability to transformations of their inputs, such as\nrotation and scaling, or intentional manipulations as described in the\nadversarial attack literature. In addition, building trust in AI-based models\nrequires a better understanding of current models and developing methods that\nare more explainable and interpretable a priori.\n  This thesis presents developments in computer vision models' robustness and\nexplainability. Furthermore, this thesis offers an example of using vision\nmodels' feature response visualization (models' interpretations) to improve\nrobustness despite interpretability and robustness being seemingly unrelated in\nthe related research. Besides methodological developments for robust and\nexplainable vision models, a key message of this thesis is introducing model\ninterpretation techniques as a tool for understanding vision models and\nimproving their design and robustness. In addition to the theoretical\ndevelopments, this thesis demonstrates several applications of ML and DL in\ndifferent contexts, such as medical imaging and affective computing.\n","authors":["Mohammadreza Amirian"],"pdf_url":"https://arxiv.org/pdf/2403.18674v1.pdf","comment":"150 pages, 37 figures, 12 tables"},{"id":"http://arxiv.org/abs/2311.15803v3","updated":"2024-03-27T15:05:19Z","published":"2023-11-27T13:25:47Z","title":"SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using\n  Neural Radiance Fields","summary":"  In rapidly-evolving domains such as autonomous driving, the use of multiple\nsensors with different modalities is crucial to ensure high operational\nprecision and stability. To correctly exploit the provided information by each\nsensor in a single common frame, it is essential for these sensors to be\naccurately calibrated. In this paper, we leverage the ability of Neural\nRadiance Fields (NeRF) to represent different sensors modalities in a common\nvolumetric representation to achieve robust and accurate spatio-temporal sensor\ncalibration. By designing a partitioning approach based on the visible part of\nthe scene for each sensor, we formulate the calibration problem using only the\noverlapping areas. This strategy results in a more robust and accurate\ncalibration that is less prone to failure. We demonstrate that our approach\nworks on outdoor urban scenes by validating it on multiple established driving\ndatasets. Results show that our method is able to get better accuracy and\nrobustness compared to existing methods.\n","authors":["Quentin Herau","Nathan Piasco","Moussab Bennehar","Luis Roldão","Dzmitry Tsishkou","Cyrille Migniot","Pascal Vasseur","Cédric Demonceaux"],"pdf_url":"https://arxiv.org/pdf/2311.15803v3.pdf","comment":"Accepted at CVPR 2024. Project page: https://qherau.github.io/SOAC/"},{"id":"http://arxiv.org/abs/2403.18660v1","updated":"2024-03-27T15:03:38Z","published":"2024-03-27T15:03:38Z","title":"InstructBrush: Learning Attention-based Instruction Optimization for\n  Image Editing","summary":"  In recent years, instruction-based image editing methods have garnered\nsignificant attention in image editing. However, despite encompassing a wide\nrange of editing priors, these methods are helpless when handling editing tasks\nthat are challenging to accurately describe through language. We propose\nInstructBrush, an inversion method for instruction-based image editing methods\nto bridge this gap. It extracts editing effects from exemplar image pairs as\nediting instructions, which are further applied for image editing. Two key\ntechniques are introduced into InstructBrush, Attention-based Instruction\nOptimization and Transformation-oriented Instruction Initialization, to address\nthe limitations of the previous method in terms of inversion effects and\ninstruction generalization. To explore the ability of instruction inversion\nmethods to guide image editing in open scenarios, we establish a\nTransformationOriented Paired Benchmark (TOP-Bench), which contains a rich set\nof scenes and editing types. The creation of this benchmark paves the way for\nfurther exploration of instruction inversion. Quantitatively and qualitatively,\nour approach achieves superior performance in editing and is more semantically\nconsistent with the target editing effects.\n","authors":["Ruoyu Zhao","Qingnan Fan","Fei Kou","Shuai Qin","Hong Gu","Wei Wu","Pengcheng Xu","Mingrui Zhu","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18660v1.pdf","comment":"Project Page: https://royzhao926.github.io/InstructBrush/"},{"id":"http://arxiv.org/abs/2311.12386v3","updated":"2024-03-27T15:01:44Z","published":"2023-11-21T06:55:21Z","title":"Point, Segment and Count: A Generalized Framework for Object Counting","summary":"  Class-agnostic object counting aims to count all objects in an image with\nrespect to example boxes or class names, \\emph{a.k.a} few-shot and zero-shot\ncounting. In this paper, we propose a generalized framework for both few-shot\nand zero-shot object counting based on detection. Our framework combines the\nsuperior advantages of two foundation models without compromising their\nzero-shot capability: (\\textbf{i}) SAM to segment all possible objects as mask\nproposals, and (\\textbf{ii}) CLIP to classify proposals to obtain accurate\nobject counts. However, this strategy meets the obstacles of efficiency\noverhead and the small crowded objects that cannot be localized and\ndistinguished. To address these issues, our framework, termed PseCo, follows\nthree steps: point, segment, and count. Specifically, we first propose a\nclass-agnostic object localization to provide accurate but least point prompts\nfor SAM, which consequently not only reduces computation costs but also avoids\nmissing small objects. Furthermore, we propose a generalized object\nclassification that leverages CLIP image/text embeddings as the classifier,\nfollowing a hierarchical knowledge distillation to obtain discriminative\nclassifications among hierarchical mask proposals. Extensive experimental\nresults on FSC-147, COCO, and LVIS demonstrate that PseCo achieves\nstate-of-the-art performance in both few-shot/zero-shot object\ncounting/detection. Code: https://github.com/Hzzone/PseCo\n","authors":["Zhizhong Huang","Mingliang Dai","Yi Zhang","Junping Zhang","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2311.12386v3.pdf","comment":"Accepted by CVPR 2024. Camera ready"},{"id":"http://arxiv.org/abs/2311.17532v3","updated":"2024-03-27T15:01:22Z","published":"2023-11-29T11:10:40Z","title":"Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech\n  Gesture Generation","summary":"  Generating vivid and emotional 3D co-speech gestures is crucial for virtual\navatar animation in human-machine interaction applications. While the existing\nmethods enable generating the gestures to follow a single emotion label, they\noverlook that long gesture sequence modeling with emotion transition is more\npractical in real scenes. In addition, the lack of large-scale available\ndatasets with emotional transition speech and corresponding 3D human gestures\nalso limits the addressing of this task. To fulfill this goal, we first\nincorporate the ChatGPT-4 and an audio inpainting approach to construct the\nhigh-fidelity emotion transition human speeches. Considering obtaining the\nrealistic 3D pose annotations corresponding to the dynamically inpainted\nemotion transition audio is extremely difficult, we propose a novel weakly\nsupervised training strategy to encourage authority gesture transitions.\nSpecifically, to enhance the coordination of transition gestures w.r.t\ndifferent emotional ones, we model the temporal association representation\nbetween two different emotional gesture sequences as style guidance and infuse\nit into the transition generation. We further devise an emotion mixture\nmechanism that provides weak supervision based on a learnable mixed emotion\nlabel for transition gestures. Last, we present a keyframe sampler to supply\neffective initial posture cues in long sequences, enabling us to generate\ndiverse gestures. Extensive experiments demonstrate that our method outperforms\nthe state-of-the-art models constructed by adapting single emotion-conditioned\ncounterparts on our newly defined emotion transition task and datasets. Our\ncode and dataset will be released on the project page:\nhttps://xingqunqi-lab.github.io/Emo-Transition-Gesture/.\n","authors":["Xingqun Qi","Jiahao Pan","Peng Li","Ruibin Yuan","Xiaowei Chi","Mengfei Li","Wenhan Luo","Wei Xue","Shanghang Zhang","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2311.17532v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18649v1","updated":"2024-03-27T14:56:44Z","published":"2024-03-27T14:56:44Z","title":"Addressing Data Annotation Challenges in Multiple Sensors: A Solution\n  for Scania Collected Datasets","summary":"  Data annotation in autonomous vehicles is a critical step in the development\nof Deep Neural Network (DNN) based models or the performance evaluation of the\nperception system. This often takes the form of adding 3D bounding boxes on\ntime-sequential and registered series of point-sets captured from active\nsensors like Light Detection and Ranging (LiDAR) and Radio Detection and\nRanging (RADAR). When annotating multiple active sensors, there is a need to\nmotion compensate and translate the points to a consistent coordinate frame and\ntimestamp respectively. However, highly dynamic objects pose a unique\nchallenge, as they can appear at different timestamps in each sensor's data.\nWithout knowing the speed of the objects, their position appears to be\ndifferent in different sensor outputs. Thus, even after motion compensation,\nhighly dynamic objects are not matched from multiple sensors in the same frame,\nand human annotators struggle to add unique bounding boxes that capture all\nobjects. This article focuses on addressing this challenge, primarily within\nthe context of Scania collected datasets. The proposed solution takes a track\nof an annotated object as input and uses the Moving Horizon Estimation (MHE) to\nrobustly estimate its speed. The estimated speed profile is utilized to correct\nthe position of the annotated box and add boxes to object clusters missed by\nthe original annotation.\n","authors":["Ajinkya Khoche","Aron Asefaw","Alejandro Gonzalez","Bogdan Timus","Sina Sharif Mansouri","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.18649v1.pdf","comment":"Accepted to European Control Conference 2024"},{"id":"http://arxiv.org/abs/2403.18637v1","updated":"2024-03-27T14:42:08Z","published":"2024-03-27T14:42:08Z","title":"Transformers-based architectures for stroke segmentation: A review","summary":"  Stroke remains a significant global health concern, necessitating precise and\nefficient diagnostic tools for timely intervention and improved patient\noutcomes. The emergence of deep learning methodologies has transformed the\nlandscape of medical image analysis. Recently, Transformers, initially designed\nfor natural language processing, have exhibited remarkable capabilities in\nvarious computer vision applications, including medical image analysis. This\ncomprehensive review aims to provide an in-depth exploration of the\ncutting-edge Transformer-based architectures applied in the context of stroke\nsegmentation. It commences with an exploration of stroke pathology, imaging\nmodalities, and the challenges associated with accurate diagnosis and\nsegmentation. Subsequently, the review delves into the fundamental ideas of\nTransformers, offering detailed insights into their architectural intricacies\nand the underlying mechanisms that empower them to effectively capture complex\nspatial information within medical images. The existing literature is\nsystematically categorized and analyzed, discussing various approaches that\nleverage Transformers for stroke segmentation. A critical assessment is\nprovided, highlighting the strengths and limitations of these methods,\nincluding considerations of performance and computational efficiency.\nAdditionally, this review explores potential avenues for future research and\ndevelopment\n","authors":["Yalda Zafari-Ghadim","Essam A. Rashed","Mohamed Mabrok"],"pdf_url":"https://arxiv.org/pdf/2403.18637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.11041v3","updated":"2024-03-27T14:29:27Z","published":"2022-04-23T10:19:58Z","title":"Learning by Erasing: Conditional Entropy based Transferable\n  Out-Of-Distribution Detection","summary":"  Out-of-distribution (OOD) detection is essential to handle the distribution\nshifts between training and test scenarios. For a new in-distribution (ID)\ndataset, existing methods require retraining to capture the dataset-specific\nfeature representation or data distribution. In this paper, we propose a deep\ngenerative models (DGM) based transferable OOD detection method, which is\nunnecessary to retrain on a new ID dataset. We design an image erasing strategy\nto equip exclusive conditional entropy distribution for each ID dataset, which\ndetermines the discrepancy of DGM's posteriori ucertainty distribution on\ndifferent ID datasets. Owing to the powerful representation capacity of\nconvolutional neural networks, the proposed model trained on complex dataset\ncan capture the above discrepancy between ID datasets without retraining and\nthus achieve transferable OOD detection. We validate the proposed method on\nfive datasets and verity that ours achieves comparable performance to the\nstate-of-the-art group based OOD detection methods that need to be retrained to\ndeploy on new ID datasets. Our code is available at\nhttps://github.com/oOHCIOo/CETOOD.\n","authors":["Meng Xing","Zhiyong Feng","Yong Su","Changjae Oh"],"pdf_url":"https://arxiv.org/pdf/2204.11041v3.pdf","comment":"update new experimental results"},{"id":"http://arxiv.org/abs/2403.18605v1","updated":"2024-03-27T14:24:30Z","published":"2024-03-27T14:24:30Z","title":"FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image\n  Editing","summary":"  Our work addresses limitations seen in previous approaches for object-centric\nediting problems, such as unrealistic results due to shape discrepancies and\nlimited control in object replacement or insertion. To this end, we introduce\nFlexEdit, a flexible and controllable editing framework for objects where we\niteratively adjust latents at each denoising step using our FlexEdit block.\nInitially, we optimize latents at test time to align with specified object\nconstraints. Then, our framework employs an adaptive mask, automatically\nextracted during denoising, to protect the background while seamlessly blending\nnew content into the target image. We demonstrate the versatility of FlexEdit\nin various object editing tasks and curate an evaluation test suite with\nsamples from both real and synthetic images, along with novel evaluation\nmetrics designed for object-centric editing. We conduct extensive experiments\non different editing scenarios, demonstrating the superiority of our editing\nframework over recent advanced text-guided image editing methods. Our project\npage is published at https://flex-edit.github.io/.\n","authors":["Trong-Tung Nguyen","Duc-Anh Nguyen","Anh Tran","Cuong Pham"],"pdf_url":"https://arxiv.org/pdf/2403.18605v1.pdf","comment":"Our project page: https://flex-edit.github.io/"},{"id":"http://arxiv.org/abs/2403.18600v1","updated":"2024-03-27T14:22:40Z","published":"2024-03-27T14:22:40Z","title":"RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in\n  Instructional Videos","summary":"  Procedure Planning in instructional videos entails generating a sequence of\naction steps based on visual observations of the initial and target states.\nDespite the rapid progress in this task, there remain several critical\nchallenges to be solved: (1) Adaptive procedures: Prior works hold an\nunrealistic assumption that the number of action steps is known and fixed,\nleading to non-generalizable models in real-world scenarios where the sequence\nlength varies. (2) Temporal relation: Understanding the step temporal relation\nknowledge is essential in producing reasonable and executable plans. (3)\nAnnotation cost: Annotating instructional videos with step-level labels (i.e.,\ntimestamp) or sequence-level labels (i.e., action category) is demanding and\nlabor-intensive, limiting its generalizability to large-scale datasets.In this\nwork, we propose a new and practical setting, called adaptive procedure\nplanning in instructional videos, where the procedure length is not fixed or\npre-determined. To address these challenges we introduce Retrieval-Augmented\nPlanner (RAP) model. Specifically, for adaptive procedures, RAP adaptively\ndetermines the conclusion of actions using an auto-regressive model\narchitecture. For temporal relation, RAP establishes an external memory module\nto explicitly retrieve the most relevant state-action pairs from the training\nvideos and revises the generated procedures. To tackle high annotation cost,\nRAP utilizes a weakly-supervised learning manner to expand the training dataset\nto other task-relevant, unannotated videos by generating pseudo labels for\naction steps. Experiments on CrossTask and COIN benchmarks show the superiority\nof RAP over traditional fixed-length models, establishing it as a strong\nbaseline solution for adaptive procedure planning.\n","authors":["Ali Zare","Yulei Niu","Hammad Ayyubi","Shih-fu Chang"],"pdf_url":"https://arxiv.org/pdf/2403.18600v1.pdf","comment":"23 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2403.18593v1","updated":"2024-03-27T14:18:09Z","published":"2024-03-27T14:18:09Z","title":"Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote\n  Sensing Image Understanding","summary":"  The tokenizer, as one of the fundamental components of large models, has long\nbeen overlooked or even misunderstood in visual tasks. One key factor of the\ngreat comprehension power of the large language model is that natural language\ntokenizers utilize meaningful words or subwords as the basic elements of\nlanguage. In contrast, mainstream visual tokenizers, represented by patch-based\nmethods such as Patch Embed, rely on meaningless rectangular patches as basic\nelements of vision, which cannot serve as effectively as words or subwords in\nlanguage. Starting from the essence of the tokenizer, we defined semantically\nindependent regions (SIRs) for vision. We designed a simple HOmogeneous visual\ntOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception\nModule (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,\nthe OPM splits the image into 4*4 pixel seeds and then utilizes the attention\nmechanism to perceive SIRs. The OVM employs cross-attention to merge seeds\nwithin the same SIR. To achieve adaptability, the OVM defines a variable number\nof learnable vectors as cross-attention queries, allowing for the adjustment of\ntoken quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19\nclassification dataset, and GID5 segmentation dataset for sparse and dense\ntasks. The results demonstrate that the visual tokens obtained by HOOK\ncorrespond to individual objects, which demonstrates homogeneity. HOOK\noutperformed Patch Embed by 6\\% and 10\\% in the two tasks and achieved\nstate-of-the-art performance compared to the baselines used for comparison.\nCompared to Patch Embed, which requires more than one hundred tokens for one\nimage, HOOK requires only 6 and 8 tokens for sparse and dense tasks,\nrespectively, resulting in efficiency improvements of 1.5 to 2.8 times. The\ncode is available at https://github.com/GeoX-Lab/Hook.\n","authors":["Run Shao","Zhaoyang Zhang","Chao Tao","Yunsheng Zhang","Chengli Peng","Haifeng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18593v1.pdf","comment":"20 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2403.18589v1","updated":"2024-03-27T14:12:56Z","published":"2024-03-27T14:12:56Z","title":"Users prefer Jpegli over same-sized libjpeg-turbo or MozJPEG","summary":"  We performed pairwise comparisons by human raters of JPEG images from\nMozJPEG, libjpeg-turbo and our new Jpegli encoder. When compressing images at a\nquality similar to libjpeg-turbo quality 95, the Jpegli images were 54% likely\nto be preferred over both libjpeg-turbo and MozJPEG images, but used only 2.8\nbits per pixel compared to libjpeg-turbo and MozJPEG that used 3.8 and 3.5 bits\nper pixel respectively. The raw ratings and source images are publicly\navailable for further analysis and study.\n","authors":["Martin Bruse","Luca Versari","Zoltan Szabadka","Jyrki Alakuijala"],"pdf_url":"https://arxiv.org/pdf/2403.18589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18587v1","updated":"2024-03-27T14:11:23Z","published":"2024-03-27T14:11:23Z","title":"The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency\n  Attacks in Computer Vision","summary":"  Resource efficiency plays an important role for machine learning nowadays.\nThe energy and decision latency are two critical aspects to ensure a\nsustainable and practical application. Unfortunately, the energy consumption\nand decision latency are not robust against adversaries. Researchers have\nrecently demonstrated that attackers can compute and submit so-called sponge\nexamples at inference time to increase the energy consumption and decision\nlatency of neural networks. In computer vision, the proposed strategy crafts\ninputs with less activation sparsity which could otherwise be used to\naccelerate the computation. In this paper, we analyze the mechanism how these\nenergy-latency attacks reduce activation sparsity. In particular, we find that\ninput uniformity is a key enabler. A uniform image, that is, an image with\nmostly flat, uniformly colored surfaces, triggers more activations due to a\nspecific interplay of convolution, batch normalization, and ReLU activation.\nBased on these insights, we propose two new simple, yet effective strategies\nfor crafting sponge examples: sampling images from a probability distribution\nand identifying dense, yet inconspicuous inputs in natural datasets. We\nempirically examine our findings in a comprehensive evaluation with multiple\nimage classification models and show that our attack achieves the same sparsity\neffect as prior sponge-example methods, but at a fraction of computation\neffort. We also show that our sponge examples transfer between different neural\nnetworks. Finally, we discuss applications of our findings for the good by\nimproving efficiency by increasing sparsity.\n","authors":["Andreas Müller","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2403.18587v1.pdf","comment":"Accepted at the DLSP 2024"},{"id":"http://arxiv.org/abs/2312.07264v2","updated":"2024-03-27T14:09:10Z","published":"2023-12-12T13:44:53Z","title":"Dual Structure-Aware Image Filterings for Semi-supervised Medical Image\n  Segmentation","summary":"  Semi-supervised image segmentation has attracted great attention recently.\nThe key is how to leverage unlabeled images in the training process. Most\nmethods maintain consistent predictions of the unlabeled images under\nvariations (e.g., adding noise/perturbations, or creating alternative versions)\nin the image and/or model level. In most image-level variation, medical images\noften have prior structure information, which has not been well explored. In\nthis paper, we propose novel dual structure-aware image filterings (DSAIF) as\nthe image-level variations for semi-supervised medical image segmentation.\nMotivated by connected filtering that simplifies image via filtering in\nstructure-aware tree-based image representation, we resort to the dual contrast\ninvariant Max-tree and Min-tree representation. Specifically, we propose a\nnovel connected filtering that removes topologically equivalent nodes (i.e.\nconnected components) having no siblings in the Max/Min-tree. This results in\ntwo filtered images preserving topologically critical structure. Applying the\nproposed DSAIF to mutually supervised networks decreases the consensus of their\nerroneous predictions on unlabeled images. This helps to alleviate the\nconfirmation bias issue of overfitting to noisy pseudo labels of unlabeled\nimages, and thus effectively improves the segmentation performance. Extensive\nexperimental results on three benchmark datasets demonstrate that the proposed\nmethod significantly/consistently outperforms some state-of-the-art methods.\nThe source codes will be publicly available.\n","authors":["Yuliang Gu","Zhichao Sun","Tian Chen","Xin Xiao","Yepeng Liu","Yongchao Xu","Laurent Najman"],"pdf_url":"https://arxiv.org/pdf/2312.07264v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18575v1","updated":"2024-03-27T13:56:08Z","published":"2024-03-27T13:56:08Z","title":"HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional\n  Synthesis and Sampling of Hand-Object Interactions","summary":"  Reconstructing 3D hand mesh robustly from a single image is very challenging,\ndue to the lack of diversity in existing real-world datasets. While data\nsynthesis helps relieve the issue, the syn-to-real gap still hinders its usage.\nIn this work, we present HandBooster, a new approach to uplift the data\ndiversity and boost the 3D hand-mesh reconstruction performance by training a\nconditional generative space on hand-object interactions and purposely sampling\nthe space to synthesize effective data samples. First, we construct versatile\ncontent-aware conditions to guide a diffusion model to produce realistic images\nwith diverse hand appearances, poses, views, and backgrounds; favorably,\naccurate 3D annotations are obtained for free. Then, we design a novel\ncondition creator based on our similarity-aware distribution sampling\nstrategies to deliberately find novel and realistic interaction poses that are\ndistinctive from the training set. Equipped with our method, several baselines\ncan be significantly improved beyond the SOTA on the HO3D and DexYCB\nbenchmarks. Our code will be released on\nhttps://github.com/hxwork/HandBooster_Pytorch.\n","authors":["Hao Xu","Haipeng Li","Yinqiao Wang","Shuaicheng Liu","Chi-Wing Fu"],"pdf_url":"https://arxiv.org/pdf/2403.18575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07636v3","updated":"2024-03-27T13:51:59Z","published":"2024-03-12T13:18:22Z","title":"Decomposing Disease Descriptions for Enhanced Pathology Detection: A\n  Multi-Aspect Vision-Language Pre-training Framework","summary":"  Medical vision language pre-training (VLP) has emerged as a frontier of\nresearch, enabling zero-shot pathological recognition by comparing the query\nimage with the textual descriptions for each disease. Due to the complex\nsemantics of biomedical texts, current methods struggle to align medical images\nwith key pathological findings in unstructured reports. This leads to the\nmisalignment with the target disease's textual representation. In this paper,\nwe introduce a novel VLP framework designed to dissect disease descriptions\ninto their fundamental aspects, leveraging prior knowledge about the visual\nmanifestations of pathologies. This is achieved by consulting a large language\nmodel and medical experts. Integrating a Transformer module, our approach\naligns an input image with the diverse elements of a disease, generating\naspect-centric image representations. By consolidating the matches from each\naspect, we improve the compatibility between an image and its associated\ndisease. Additionally, capitalizing on the aspect-oriented representations, we\npresent a dual-head Transformer tailored to process known and unknown diseases,\noptimizing the comprehensive detection efficacy. Conducting experiments on\nseven downstream datasets, ours improves the accuracy of recent methods by up\nto 8.56% and 17.0% for seen and unseen categories, respectively. Our code is\nreleased at https://github.com/HieuPhan33/MAVL.\n","authors":["Vu Minh Hieu Phan","Yutong Xie","Yuankai Qi","Lingqiao Liu","Liyang Liu","Bowen Zhang","Zhibin Liao","Qi Wu","Minh-Son To","Johan W. Verjans"],"pdf_url":"https://arxiv.org/pdf/2403.07636v3.pdf","comment":"Accepted at CVPR2024. Pre-print before final camera-ready version"},{"id":"http://arxiv.org/abs/2403.18565v1","updated":"2024-03-27T13:46:01Z","published":"2024-03-27T13:46:01Z","title":"Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images\n  with Deep Learning -- A Review","summary":"  Deep learning based approaches have been used to improve image quality in\ncone-beam computed tomography (CBCT), a medical imaging technique often used in\napplications such as image-guided radiation therapy, implant dentistry or\northopaedics. In particular, while deep learning methods have been applied to\nreduce various types of CBCT image artifacts arising from motion, metal\nobjects, or low-dose acquisition, a comprehensive review summarizing the\nsuccesses and shortcomings of these approaches, with a primary focus on the\ntype of artifacts rather than the architecture of neural networks, is lacking\nin the literature. In this review, the data generation and simulation\npipelines, and artifact reduction techniques are specifically investigated for\neach type of artifact. We provide an overview of deep learning techniques that\nhave successfully been shown to reduce artifacts in 3D, as well as in\ntime-resolved (4D) CBCT through the use of projection- and/or volume-domain\noptimizations, or by introducing neural networks directly within the CBCT\nreconstruction algorithms. Research gaps are identified to suggest avenues for\nfuture exploration. One of the key findings of this work is an observed trend\ntowards the use of generative models including GANs and score-based or\ndiffusion models, accompanied with the need for more diverse and open training\ndatasets and simulations.\n","authors":["Mohammadreza Amirian","Daniel Barco","Ivo Herzig","Frank-Peter Schilling"],"pdf_url":"https://arxiv.org/pdf/2403.18565v1.pdf","comment":"16 pages, 4 figures, 1 Table, published in IEEE Access Journal"},{"id":"http://arxiv.org/abs/2403.09700v2","updated":"2024-03-27T13:42:25Z","published":"2024-03-05T22:19:21Z","title":"Shapley Values-Powered Framework for Fair Reward Split in Content\n  Produced by GenAI","summary":"  It is evident that, currently, generative models are surpassed in quality by\nhuman professionals. However, with the advancements in Artificial Intelligence,\nthis gap will narrow, leading to scenarios where individuals who have dedicated\nyears of their lives to mastering a skill become obsolete due to their high\ncosts, which are inherently linked to the time they require to complete a task\n-- a task that AI could accomplish in minutes or seconds. To avoid future\nsocial upheavals, we must, even now, contemplate how to fairly assess the\ncontributions of such individuals in training generative models and how to\ncompensate them for the reduction or complete loss of their incomes. In this\nwork, we propose a method to structure collaboration between model developers\nand data providers. To achieve this, we employ Shapley Values to quantify the\ncontribution of artist(s) in an image generated by the Stable Diffusion-v1.5\nmodel and to equitably allocate the reward among them.\n","authors":["Alex Glinsky","Alexey Sokolsky"],"pdf_url":"https://arxiv.org/pdf/2403.09700v2.pdf","comment":"36 pages, 32 figures"},{"id":"http://arxiv.org/abs/2403.18554v1","updated":"2024-03-27T13:33:14Z","published":"2024-03-27T13:33:14Z","title":"CosalPure: Learning Concept from Group Images for Robust Co-Saliency\n  Detection","summary":"  Co-salient object detection (CoSOD) aims to identify the common and salient\n(usually in the foreground) regions across a given group of images. Although\nachieving significant progress, state-of-the-art CoSODs could be easily\naffected by some adversarial perturbations, leading to substantial accuracy\nreduction. The adversarial perturbations can mislead CoSODs but do not change\nthe high-level semantic information (e.g., concept) of the co-salient objects.\nIn this paper, we propose a novel robustness enhancement framework by first\nlearning the concept of the co-salient objects based on the input group images\nand then leveraging this concept to purify adversarial perturbations, which are\nsubsequently fed to CoSODs for robustness enhancement. Specifically, we propose\nCosalPure containing two modules, i.e., group-image concept learning and\nconcept-guided diffusion purification. For the first module, we adopt a\npre-trained text-to-image diffusion model to learn the concept of co-salient\nobjects within group images where the learned concept is robust to adversarial\nexamples. For the second module, we map the adversarial image to the latent\nspace and then perform diffusion generation by embedding the learned concept\ninto the noise prediction function as an extra condition. Our method can\neffectively alleviate the influence of the SOTA adversarial attack containing\ndifferent adversarial patterns, including exposure and noise. The extensive\nresults demonstrate that our method could enhance the robustness of CoSODs\nsignificantly.\n","authors":["Jiayi Zhu","Qing Guo","Felix Juefei-Xu","Yihao Huang","Yang Liu","Geguang Pu"],"pdf_url":"https://arxiv.org/pdf/2403.18554v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.18551v1","updated":"2024-03-27T13:31:39Z","published":"2024-03-27T13:31:39Z","title":"Attention Calibration for Disentangled Text-to-Image Personalization","summary":"  Recent thrilling progress in large-scale text-to-image (T2I) models has\nunlocked unprecedented synthesis quality of AI-generated content (AIGC)\nincluding image generation, 3D and video composition. Further, personalized\ntechniques enable appealing customized production of a novel concept given only\nseveral images as reference. However, an intriguing problem persists: Is it\npossible to capture multiple, novel concepts from one single reference image?\nIn this paper, we identify that existing approaches fail to preserve visual\nconsistency with the reference image and eliminate cross-influence from\nconcepts. To alleviate this, we propose an attention calibration mechanism to\nimprove the concept-level understanding of the T2I model. Specifically, we\nfirst introduce new learnable modifiers bound with classes to capture\nattributes of multiple concepts. Then, the classes are separated and\nstrengthened following the activation of the cross-attention operation,\nensuring comprehensive and self-contained concepts. Additionally, we suppress\nthe attention activation of different classes to mitigate mutual influence\namong concepts. Together, our proposed method, dubbed DisenDiff, can learn\ndisentangled multiple concepts from one single image and produce novel\ncustomized images with learned concepts. We demonstrate that our method\noutperforms the current state of the art in both qualitative and quantitative\nevaluations. More importantly, our proposed techniques are compatible with LoRA\nand inpainting pipelines, enabling more interactive experiences.\n","authors":["Yanbing Zhang","Mengping Yang","Qin Zhou","Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18551v1.pdf","comment":"Accepted to CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18550v1","updated":"2024-03-27T13:30:48Z","published":"2024-03-27T13:30:48Z","title":"OrCo: Towards Better Generalization via Orthogonality and Contrast for\n  Few-Shot Class-Incremental Learning","summary":"  Few-Shot Class-Incremental Learning (FSCIL) introduces a paradigm in which\nthe problem space expands with limited data. FSCIL methods inherently face the\nchallenge of catastrophic forgetting as data arrives incrementally, making\nmodels susceptible to overwriting previously acquired knowledge. Moreover,\ngiven the scarcity of labeled samples available at any given time, models may\nbe prone to overfitting and find it challenging to strike a balance between\nextensive pretraining and the limited incremental data. To address these\nchallenges, we propose the OrCo framework built on two core principles:\nfeatures' orthogonality in the representation space, and contrastive learning.\nIn particular, we improve the generalization of the embedding space by\nemploying a combination of supervised and self-supervised contrastive losses\nduring the pretraining phase. Additionally, we introduce OrCo loss to address\nchallenges arising from data limitations during incremental sessions. Through\nfeature space perturbations and orthogonality between classes, the OrCo loss\nmaximizes margins and reserves space for the following incremental data. This,\nin turn, ensures the accommodation of incoming classes in the feature space\nwithout compromising previously acquired knowledge. Our experimental results\nshowcase state-of-the-art performance across three benchmark datasets,\nincluding mini-ImageNet, CIFAR100, and CUB datasets. Code is available at\nhttps://github.com/noorahmedds/OrCo\n","authors":["Noor Ahmed","Anna Kukleva","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2403.18550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18548v1","updated":"2024-03-27T13:27:02Z","published":"2024-03-27T13:27:02Z","title":"A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency\n  Aware and Realistic Brightness Constraint","summary":"  Existing research based on deep learning has extensively explored the problem\nof daytime image dehazing. However, few studies have considered the\ncharacteristics of nighttime hazy scenes. There are two distinctions between\nnighttime and daytime haze. First, there may be multiple active colored light\nsources with lower illumination intensity in nighttime scenes, which may cause\nhaze, glow and noise with localized, coupled and frequency inconsistent\ncharacteristics. Second, due to the domain discrepancy between simulated and\nreal-world data, unrealistic brightness may occur when applying a dehazing\nmodel trained on simulated data to real-world data. To address the above two\nissues, we propose a semi-supervised model for real-world nighttime dehazing.\nFirst, the spatial attention and frequency spectrum filtering are implemented\nas a spatial-frequency domain information interaction module to handle the\nfirst issue. Second, a pseudo-label-based retraining strategy and a local\nwindow-based brightness loss for semi-supervised training process is designed\nto suppress haze and glow while achieving realistic brightness. Experiments on\npublic benchmarks validate the effectiveness of the proposed method and its\nsuperiority over state-of-the-art methods. The source code and Supplementary\nMaterials are placed in the https://github.com/Xiaofeng-life/SFSNiD.\n","authors":["Xiaofeng Cong","Jie Gui","Jing Zhang","Junming Hou","Hao Shen"],"pdf_url":"https://arxiv.org/pdf/2403.18548v1.pdf","comment":"This paper is accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.18546v1","updated":"2024-03-27T13:24:58Z","published":"2024-03-27T13:24:58Z","title":"Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes","summary":"  Fast and robust object grasping in clutter is a crucial component of\nrobotics. Most current works resort to the whole observed point cloud for 6-Dof\ngrasp generation, ignoring the guidance information excavated from global\nsemantics, thus limiting high-quality grasp generation and real-time\nperformance. In this work, we show that the widely used heatmaps are\nunderestimated in the efficiency of 6-Dof grasp generation. Therefore, we\npropose an effective local grasp generator combined with grasp heatmaps as\nguidance, which infers in a global-to-local semantic-to-point way.\nSpecifically, Gaussian encoding and the grid-based strategy are applied to\npredict grasp heatmaps as guidance to aggregate local points into graspable\nregions and provide global semantic information. Further, a novel non-uniform\nanchor sampling mechanism is designed to improve grasp accuracy and diversity.\nBenefiting from the high-efficiency encoding in the image space and focusing on\npoints in local graspable regions, our framework can perform high-quality grasp\ndetection in real-time and achieve state-of-the-art results. In addition, real\nrobot experiments demonstrate the effectiveness of our method with a success\nrate of 94% and a clutter completion rate of 100%. Our code is available at\nhttps://github.com/THU-VCLab/HGGD.\n","authors":["Siang Chen","Wei Tang","Pengwei Xie","Wenming Yang","Guijin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18546v1.pdf","comment":"Extensive results on GraspNet-1B dataset"},{"id":"http://arxiv.org/abs/2310.15081v3","updated":"2024-03-27T13:23:28Z","published":"2023-10-23T16:41:13Z","title":"E4S: Fine-grained Face Swapping via Editing With Regional GAN Inversion","summary":"  This paper proposes a novel approach to face swapping from the perspective of\nfine-grained facial editing, dubbed \"editing for swapping\" (E4S). The\ntraditional face swapping methods rely on global feature extraction and fail to\npreserve the detailed source identity. In contrast, we propose a Regional GAN\nInversion (RGI) method, which allows the explicit disentanglement of shape and\ntexture. Specifically, our E4S performs face swapping in the latent space of a\npretrained StyleGAN, where a multi-scale mask-guided encoder is applied to\nproject the texture of each facial component into regional style codes and a\nmask-guided injection module manipulating feature maps with the style codes.\nBased on this disentanglement, face swapping can be simplified as style and\nmask swapping. Besides, due to the large lighting condition gap, transferring\nthe source skin into the target image may lead to disharmony lighting. We\npropose a re-coloring network to make the swapped face maintain the target\nlighting condition while preserving the source skin. Further, to deal with the\npotential mismatch areas during mask exchange, we design a face inpainting\nmodule to refine the face shape. The extensive comparisons with\nstate-of-the-art methods demonstrate that our E4S outperforms existing methods\nin preserving texture, shape, and lighting. Our implementation is available at\nhttps://github.com/e4s2024/E4S2024.\n","authors":["Maomao Li","Ge Yuan","Cairong Wang","Zhian Liu","Yong Zhang","Yongwei Nie","Jue Wang","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2310.15081v3.pdf","comment":"Project Page: https://e4s2024.github.io/ ;. arXiv admin note: text\n  overlap with arXiv:2211.14068"},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2403.18514v1","updated":"2024-03-27T12:44:57Z","published":"2024-03-27T12:44:57Z","title":"CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection\n  of Pathological Pulmonary CT scans","summary":"  Unsupervised pathology detection can be implemented by training a model on\nhealthy data only and measuring the deviation from the training set upon\ninference, for example with CNN-based feature extraction and one-class\nclassifiers, or reconstruction-score-based methods such as AEs, GANs and\nDiffusion models. Normalizing Flows (NF) have the ability to directly learn the\nprobability distribution of training examples through an invertible\narchitecture. We leverage this property in a novel 3D NF-based model named\nCT-3DFlow, specifically tailored for patient-level pulmonary pathology\ndetection in chest CT data. Our model is trained unsupervised on healthy 3D\npulmonary CT patches, and detects deviations from its log-likelihood\ndistribution as anomalies. We aggregate patches-level likelihood values from a\npatient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.\nOut-of-distribution detection performance is evaluated using expert annotations\non a separate chest CT test dataset, outperforming other state-of-the-art\nmethods.\n","authors":["Aissam Djahnine","Alexandre Popoff","Emilien Jupin-Delevaux","Vincent Cottin","Olivier Nempont","Loic Boussel"],"pdf_url":"https://arxiv.org/pdf/2403.18514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04344v3","updated":"2024-03-27T12:44:55Z","published":"2023-06-07T11:18:53Z","title":"ViDA: Homeostatic Visual Domain Adapter for Continual Test Time\n  Adaptation","summary":"  Since real-world machine systems are running in non-stationary environments,\nContinual Test-Time Adaptation (CTTA) task is proposed to adapt the pre-trained\nmodel to continually changing target domains. Recently, existing methods mainly\nfocus on model-based adaptation, which aims to leverage a self-training manner\nto extract the target domain knowledge. However, pseudo labels can be noisy and\nthe updated model parameters are unreliable under dynamic data distributions,\nleading to error accumulation and catastrophic forgetting in the continual\nadaptation process. To tackle these challenges and maintain the model\nplasticity, we design a Visual Domain Adapter (ViDA) for CTTA, explicitly\nhandling both domain-specific and domain-shared knowledge. Specifically, we\nfirst comprehensively explore the different domain representations of the\nadapters with trainable high-rank or low-rank embedding spaces. Then we inject\nViDAs into the pre-trained model, which leverages high-rank and low-rank\nfeatures to adapt the current domain distribution and maintain the continual\ndomain-shared knowledge, respectively. To exploit the low-rank and high-rank\nViDAs more effectively, we further propose a Homeostatic Knowledge Allotment\n(HKA) strategy, which adaptively combines different knowledge from each ViDA.\nExtensive experiments conducted on four widely used benchmarks demonstrate that\nour proposed method achieves state-of-the-art performance in both\nclassification and segmentation CTTA tasks. Note that, our method can be\nregarded as a novel transfer paradigm for large-scale models, delivering\npromising results in adaptation to continually changing distributions. Project\npage: https://sites.google.com/view/iclr2024-vida/home.\n","authors":["Jiaming Liu","Senqiao Yang","Peidong Jia","Renrui Zhang","Ming Lu","Yandong Guo","Wei Xue","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.04344v3.pdf","comment":"Accepted by ICLR2024"},{"id":"http://arxiv.org/abs/2403.18512v1","updated":"2024-03-27T12:41:30Z","published":"2024-03-27T12:41:30Z","title":"ParCo: Part-Coordinating Text-to-Motion Synthesis","summary":"  We study a challenging task: text-to-motion synthesis, aiming to generate\nmotions that align with textual descriptions and exhibit coordinated movements.\nCurrently, the part-based methods introduce part partition into the motion\nsynthesis process to achieve finer-grained generation. However, these methods\nencounter challenges such as the lack of coordination between different part\nmotions and difficulties for networks to understand part concepts. Moreover,\nintroducing finer-grained part concepts poses computational complexity\nchallenges. In this paper, we propose Part-Coordinating Text-to-Motion\nSynthesis (ParCo), endowed with enhanced capabilities for understanding part\nmotions and communication among different part motion generators, ensuring a\ncoordinated and fined-grained motion synthesis. Specifically, we discretize\nwhole-body motion into multiple part motions to establish the prior concept of\ndifferent parts. Afterward, we employ multiple lightweight generators designed\nto synthesize different part motions and coordinate them through our part\ncoordination module. Our approach demonstrates superior performance on common\nbenchmarks with economic computations, including HumanML3D and KIT-ML,\nproviding substantial evidence of its effectiveness. Code is available at\nhttps://github.com/qrzou/ParCo .\n","authors":["Qiran Zou","Shangyuan Yuan","Shian Du","Yu Wang","Chang Liu","Yi Xu","Jie Chen","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16516v2","updated":"2024-03-27T12:32:31Z","published":"2024-03-25T08:00:43Z","title":"Visually Guided Generative Text-Layout Pre-training for Document\n  Intelligence","summary":"  Prior study shows that pre-training techniques can boost the performance of\nvisual document understanding (VDU), which typically requires models to gain\nabilities to perceive and reason both document texts and layouts (e.g.,\nlocations of texts and table-cells). To this end, we propose visually guided\ngenerative text-layout pre-training, named ViTLP. Given a document image, the\nmodel optimizes hierarchical language and layout modeling objectives to\ngenerate the interleaved text and layout sequence. In addition, to address the\nlimitation of processing long documents by Transformers, we introduce a\nstraightforward yet effective multi-segment generative pre-training scheme,\nfacilitating ViTLP to process word-intensive documents of any length. ViTLP can\nfunction as a native OCR model to localize and recognize texts of document\nimages. Besides, ViTLP can be effectively applied to various downstream VDU\ntasks. Extensive experiments show that ViTLP achieves competitive performance\nover existing baselines on benchmark VDU tasks, including information\nextraction, document classification, and document question answering.\n","authors":["Zhiming Mao","Haoli Bai","Lu Hou","Jiansheng Wei","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16516v2.pdf","comment":"Accepted to NAACL 2024 main conference. The first version of this\n  paper was submitted to OpenReview\n  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023"},{"id":"http://arxiv.org/abs/2312.06358v2","updated":"2024-03-27T12:24:29Z","published":"2023-12-11T13:05:54Z","title":"Intraoperative 2D/3D Image Registration via Differentiable X-ray\n  Rendering","summary":"  Surgical decisions are informed by aligning rapid portable 2D intraoperative\nimages (e.g., X-rays) to a high-fidelity 3D preoperative reference scan (e.g.,\nCT). 2D/3D image registration often fails in practice: conventional\noptimization methods are prohibitively slow and susceptible to local minima,\nwhile neural networks trained on small datasets fail on new patients or require\nimpractical landmark supervision. We present DiffPose, a self-supervised\napproach that leverages patient-specific simulation and differentiable\nphysics-based rendering to achieve accurate 2D/3D registration without relying\non manually labeled data. Preoperatively, a CNN is trained to regress the pose\nof a randomly oriented synthetic X-ray rendered from the preoperative CT. The\nCNN then initializes rapid intraoperative test-time optimization that uses the\ndifferentiable X-ray renderer to refine the solution. Our work further proposes\nseveral geometrically principled methods for sampling camera poses from\n$\\mathbf{SE}(3)$, for sparse differentiable rendering, and for driving\nregistration in the tangent space $\\mathfrak{se}(3)$ with geodesic and\nmultiscale locality-sensitive losses. DiffPose achieves sub-millimeter accuracy\nacross surgical datasets at intraoperative speeds, improving upon existing\nunsupervised methods by an order of magnitude and even outperforming supervised\nbaselines. Our code is available at https://github.com/eigenvivek/DiffPose.\n","authors":["Vivek Gopalakrishnan","Neel Dey","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2312.06358v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18501v1","updated":"2024-03-27T12:24:20Z","published":"2024-03-27T12:24:20Z","title":"HEMIT: H&E to Multiplex-immunohistochemistry Image Translation with\n  Dual-Branch Pix2pix Generator","summary":"  Computational analysis of multiplexed immunofluorescence histology data is\nemerging as an important method for understanding the tumour micro-environment\nin cancer. This work presents HEMIT, a dataset designed for translating\nHematoxylin and Eosin (H&E) sections to multiplex-immunohistochemistry (mIHC)\nimages, featuring DAPI, CD3, and panCK markers. Distinctively, HEMIT's mIHC\nimages are multi-component and cellular-level aligned with H&E, enriching\nsupervised stain translation tasks. To our knowledge, HEMIT is the first\npublicly available cellular-level aligned dataset that enables H&E to\nmulti-target mIHC image translation. This dataset provides the computer vision\ncommunity with a valuable resource to develop novel computational methods which\nhave the potential to gain new insights from H&E slide archives.\n  We also propose a new dual-branch generator architecture, using residual\nConvolutional Neural Networks (CNNs) and Swin Transformers which achieves\nbetter translation outcomes than other popular algorithms. When evaluated on\nHEMIT, it outperforms pix2pixHD, pix2pix, U-Net, and ResNet, achieving the\nhighest overall score on key metrics including the Structural Similarity Index\nMeasure (SSIM), Pearson correlation score (R), and Peak signal-to-noise Ratio\n(PSNR). Additionally, downstream analysis has been used to further validate the\nquality of the generated mIHC images. These results set a new benchmark in the\nfield of stain translation tasks.\n","authors":["Chang Bian","Beth Philips","Tim Cootes","Martin Fergie"],"pdf_url":"https://arxiv.org/pdf/2403.18501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04698v3","updated":"2024-03-27T12:24:17Z","published":"2023-11-08T14:10:19Z","title":"Challenging Common Paradigms in Multi-Task Learning","summary":"  While multi-task learning (MTL) has gained significant attention in recent\nyears, its underlying mechanisms remain poorly understood. Recent methods did\nnot yield consistent performance improvements over single task learning (STL)\nbaselines, underscoring the importance of gaining more profound insights about\nchallenges specific to MTL. In our study, we challenge paradigms in MTL in the\ncontext of STL: First, the impact of the choice of optimizer has only been\nmildly investigated in MTL. We show the pivotal role of common STL tools such\nas the Adam optimizer in MTL empirically in various experiments. To further\ninvestigate Adam's effectiveness, we theoretical derive a partial loss-scale\ninvariance under mild assumptions. Second, the notion of gradient conflicts has\noften been phrased as a specific problem in MTL. We delve into the role of\ngradient conflicts in MTL and compare it to STL. For angular gradient alignment\nwe find no evidence that this is a unique problem in MTL. We emphasize\ndifferences in gradient magnitude as the main distinguishing factor. Lastly, we\ncompare the transferability of features learned through MTL and STL on common\nimage corruptions, and find light evidence that MTL can lead to superior\ntransferability. Overall, we find surprising similarities between STL and MTL\nsuggesting to consider methods from both fields in a broader context.\n","authors":["Cathrin Elich","Lukas Kirchdorfer","Jan M. Köhler","Lukas Schott"],"pdf_url":"https://arxiv.org/pdf/2311.04698v3.pdf","comment":"-"},{"id":"http://arxiv.org/abs/2403.18495v1","updated":"2024-03-27T12:15:22Z","published":"2024-03-27T12:15:22Z","title":"Direct mineral content prediction from drill core images via transfer\n  learning","summary":"  Deep subsurface exploration is important for mining, oil and gas industries,\nas well as in the assessment of geological units for the disposal of chemical\nor nuclear waste, or the viability of geothermal energy systems. Typically,\ndetailed examinations of subsurface formations or units are performed on\ncuttings or core materials extracted during drilling campaigns, as well as on\ngeophysical borehole data, which provide detailed information about the\npetrophysical properties of the rocks. Depending on the volume of rock samples\nand the analytical program, the laboratory analysis and diagnostics can be very\ntime-consuming. This study investigates the potential of utilizing machine\nlearning, specifically convolutional neural networks (CNN), to assess the\nlithology and mineral content solely from analysis of drill core images, aiming\nto support and expedite the subsurface geological exploration. The paper\noutlines a comprehensive methodology, encompassing data preprocessing, machine\nlearning methods, and transfer learning techniques. The outcome reveals a\nremarkable 96.7% accuracy in the classification of drill core segments into\ndistinct formation classes. Furthermore, a CNN model was trained for the\nevaluation of mineral content using a learning data set from multidimensional\nlog analysis data (silicate, total clay, carbonate). When benchmarked against\nlaboratory XRD measurements on samples from the cores, both the advanced\nmultidimensional log analysis model and the neural network approach developed\nhere provide equally good performance. This work demonstrates that deep\nlearning and particularly transfer learning can support extracting\npetrophysical properties, including mineral content and formation\nclassification, from drill core images, thus offering a road map for enhancing\nmodel performance and data set quality in image-based analysis of drill cores.\n","authors":["Romana Boiger","Sergey V. Churakov","Ignacio Ballester Llagaria","Georg Kosakowski","Raphael Wüst","Nikolaos I. Prasianakis"],"pdf_url":"https://arxiv.org/pdf/2403.18495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02203v5","updated":"2024-03-27T12:12:45Z","published":"2023-07-05T10:54:50Z","title":"Neural Fields for Interactive Visualization of Statistical Dependencies\n  in 3D Simulation Ensembles","summary":"  We present the first neural network that has learned to compactly represent\nand can efficiently reconstruct the statistical dependencies between the values\nof physical variables at different spatial locations in large 3D simulation\nensembles. Going beyond linear dependencies, we consider mutual information as\na measure of non-linear dependence. We demonstrate learning and reconstruction\nwith a large weather forecast ensemble comprising 1000 members, each storing\nmultiple physical variables at a 250 x 352 x 20 simulation grid. By\ncircumventing compute-intensive statistical estimators at runtime, we\ndemonstrate significantly reduced memory and computation requirements for\nreconstructing the major dependence structures. This enables embedding the\nestimator into a GPU-accelerated direct volume renderer and interactively\nvisualizing all mutual dependencies for a selected domain point.\n","authors":["Fatemeh Farokhmanesh","Kevin Höhlein","Christoph Neuhauser","Tobias Necker","Martin Weissmann","Takemasa Miyoshi","Rüdiger Westermann"],"pdf_url":"https://arxiv.org/pdf/2307.02203v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18493v1","updated":"2024-03-27T12:08:41Z","published":"2024-03-27T12:08:41Z","title":"VersaT2I: Improving Text-to-Image Models with Versatile Reward","summary":"  Recent text-to-image (T2I) models have benefited from large-scale and\nhigh-quality data, demonstrating impressive performance. However, these T2I\nmodels still struggle to produce images that are aesthetically pleasing,\ngeometrically accurate, faithful to text, and of good low-level quality. We\npresent VersaT2I, a versatile training framework that can boost the performance\nwith multiple rewards of any T2I model. We decompose the quality of the image\ninto several aspects such as aesthetics, text-image alignment, geometry,\nlow-level quality, etc. Then, for every quality aspect, we select high-quality\nimages in this aspect generated by the model as the training set to finetune\nthe T2I model using the Low-Rank Adaptation (LoRA). Furthermore, we introduce a\ngating function to combine multiple quality aspects, which can avoid conflicts\nbetween different quality aspects. Our method is easy to extend and does not\nrequire any manual annotation, reinforcement learning, or model architecture\nchanges. Extensive experiments demonstrate that VersaT2I outperforms the\nbaseline methods across various quality criteria.\n","authors":["Jianshu Guo","Wenhao Chai","Jie Deng","Hsiang-Wei Huang","Tian Ye","Yichen Xu","Jiawei Zhang","Jenq-Neng Hwang","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18490v1","updated":"2024-03-27T12:05:22Z","published":"2024-03-27T12:05:22Z","title":"I2CKD : Intra- and Inter-Class Knowledge Distillation for Semantic\n  Segmentation","summary":"  This paper proposes a new knowledge distillation method tailored for image\nsemantic segmentation, termed Intra- and Inter-Class Knowledge Distillation\n(I2CKD). The focus of this method is on capturing and transferring knowledge\nbetween the intermediate layers of teacher (cumbersome model) and student\n(compact model). For knowledge extraction, we exploit class prototypes derived\nfrom feature maps. To facilitate knowledge transfer, we employ a triplet loss\nin order to minimize intra-class variances and maximize inter-class variances\nbetween teacher and student prototypes. Consequently, I2CKD enables the student\nto better mimic the feature representation of the teacher for each class,\nthereby enhancing the segmentation performance of the compact network.\nExtensive experiments on three segmentation datasets, i.e., Cityscapes, Pascal\nVOC and CamVid, using various teacher-student network pairs demonstrate the\neffectiveness of the proposed method.\n","authors":["Ayoub Karine","Thibault Napoléon","Maher Jridi"],"pdf_url":"https://arxiv.org/pdf/2403.18490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16943v2","updated":"2024-03-27T11:46:36Z","published":"2023-12-28T10:40:11Z","title":"SAR-Net: Multi-scale Direction-aware SAR Network via Global Information\n  Fusion","summary":"  Deep learning has driven significant progress in object detection using\nSynthetic Aperture Radar (SAR) imagery. Existing methods, while achieving\npromising results, often struggle to effectively integrate local and global\ninformation, particularly direction-aware features. This paper proposes\nSAR-Net, a novel framework specifically designed for global fusion of\ndirection-aware information in SAR object detection. SAR-Net leverages two key\ninnovations: the Unity Compensation Mechanism (UCM) and the Direction-aware\nAttention Module (DAM). UCM facilitates the establishment of complementary\nrelationships among features across different scales, enabling efficient global\ninformation fusion. Among them, Multi-scale Alignment Module (MAM) and distinct\nMulti-level Fusion Module (MFM) enhance feature integration by capturing both\ntexture detail and semantic information. Then, Multi-feature Embedding Module\n(MEM) feeds back global features into the primary branches, further improving\ninformation transmission. Additionally, DAM, through bidirectional attention\npolymerization, captures direction-aware information, effectively eliminating\nbackground interference. Extensive experiments demonstrate the effectiveness of\nSAR-Net, achieving state-of-the-art results on aircraft (SAR-AIRcraft-1.0) and\nship datasets (SSDD, HRSID), confirming its generalization capability and\nrobustness.\n","authors":["Mingxiang Cao","Jie Lei","Weiying Xie","Jiaqing Zhang","Daixun Li","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2312.16943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18476v1","updated":"2024-03-27T11:45:08Z","published":"2024-03-27T11:45:08Z","title":"Modeling uncertainty for Gaussian Splatting","summary":"  We present Stochastic Gaussian Splatting (SGS): the first framework for\nuncertainty estimation using Gaussian Splatting (GS). GS recently advanced the\nnovel-view synthesis field by achieving impressive reconstruction quality at a\nfraction of the computational cost of Neural Radiance Fields (NeRF). However,\ncontrary to the latter, it still lacks the ability to provide information about\nthe confidence associated with their outputs. To address this limitation, in\nthis paper, we introduce a Variational Inference-based approach that seamlessly\nintegrates uncertainty prediction into the common rendering pipeline of GS.\nAdditionally, we introduce the Area Under Sparsification Error (AUSE) as a new\nterm in the loss function, enabling optimization of uncertainty estimation\nalongside image reconstruction. Experimental results on the LLFF dataset\ndemonstrate that our method outperforms existing approaches in terms of both\nimage rendering quality and uncertainty estimation accuracy. Overall, our\nframework equips practitioners with valuable insights into the reliability of\nsynthesized views, facilitating safer decision-making in real-world\napplications.\n","authors":["Luca Savant","Diego Valsesia","Enrico Magli"],"pdf_url":"https://arxiv.org/pdf/2403.18476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12028v2","updated":"2024-03-27T11:43:28Z","published":"2023-11-20T18:59:51Z","title":"Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose\n  Estimation","summary":"  Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a plug-and-play pruning-and-recovering framework,\ncalled Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose\nestimation from videos. Our HoT begins with pruning pose tokens of redundant\nframes and ends with recovering full-length tokens, resulting in a few pose\ntokens in the intermediate transformer blocks and thus improving the model\nefficiency. To effectively achieve this, we propose a token pruning cluster\n(TPC) that dynamically selects a few representative tokens with high semantic\ndiversity while eliminating the redundancy of video frames. In addition, we\ndevelop a token recovering attention (TRA) to restore the detailed\nspatio-temporal information based on the selected tokens, thereby expanding the\nnetwork output to the original full-length temporal resolution for fast\ninference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and\nMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and\nestimation accuracy compared to the original VPT models. For instance, applying\nto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs\nwithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,\nrespectively. Code and models are available at\nhttps://github.com/NationalGAILab/HoT.\n","authors":["Wenhao Li","Mengyuan Liu","Hong Liu","Pichao Wang","Jialun Cai","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2311.12028v2.pdf","comment":"Accepted by CVPR 2024, Open Sourced"},{"id":"http://arxiv.org/abs/2403.18471v1","updated":"2024-03-27T11:32:44Z","published":"2024-03-27T11:32:44Z","title":"DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face\n  Forgery Analysis","summary":"  The rapid progress in deep learning has given rise to hyper-realistic facial\nforgery methods, leading to concerns related to misinformation and security\nrisks. Existing face forgery datasets have limitations in generating\nhigh-quality facial images and addressing the challenges posed by evolving\ngenerative techniques. To combat this, we present DiffusionFace, the first\ndiffusion-based face forgery dataset, covering various forgery categories,\nincluding unconditional and Text Guide facial image generation, Img2Img,\nInpaint, and Diffusion-based facial exchange algorithms. Our DiffusionFace\ndataset stands out with its extensive collection of 11 diffusion models and the\nhigh-quality of the generated images, providing essential metadata and a\nreal-world internet-sourced forgery facial image dataset for evaluation.\nAdditionally, we provide an in-depth analysis of the data and introduce\npractical evaluation protocols to rigorously assess discriminative models'\neffectiveness in detecting counterfeit facial images, aiming to enhance\nsecurity in facial image authentication processes. The dataset is available for\ndownload at \\url{https://github.com/Rapisurazurite/DiffFace}.\n","authors":["Zhongxi Chen","Ke Sun","Ziyin Zhou","Xianming Lin","Xiaoshuai Sun","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18469v1","updated":"2024-03-27T11:28:57Z","published":"2024-03-27T11:28:57Z","title":"Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain\n  Adaptive Segmentation of 3D Point Clouds","summary":"  3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to\nannotating new domains. Self-training is a competitive approach for this task,\nbut its performance is limited by different sensor sampling patterns (i.e.,\nvariations in point density) and incomplete training strategies. In this work,\nwe propose a density-guided translator (DGT), which translates point density\nbetween domains, and integrates it into a two-stage self-training pipeline\nnamed DGT-ST. First, in contrast to existing works that simultaneously conduct\ndata generation and feature/output alignment within unstable adversarial\ntraining, we employ the non-learnable DGT to bridge the domain gap at the input\nlevel. Second, to provide a well-initialized model for self-training, we\npropose a category-level adversarial network in stage one that utilizes the\nprototype to prevent negative transfer. Finally, by leveraging the designs\nabove, a domain-mixed self-training method with source-aware consistency loss\nis proposed in stage two to narrow the domain gap further. Experiments on two\nsynthetic-to-real segmentation tasks (SynLiDAR $\\rightarrow$ semanticKITTI and\nSynLiDAR $\\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms\nstate-of-the-art methods, achieving 9.4$\\%$ and 4.3$\\%$ mIoU improvements,\nrespectively. Code is available at \\url{https://github.com/yuan-zm/DGT-ST}.\n","authors":["Zhimin Yuan","Wankang Zeng","Yanfei Su","Weiquan Liu","Ming Cheng","Yulan Guo","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18469v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2403.18468v1","updated":"2024-03-27T11:28:32Z","published":"2024-03-27T11:28:32Z","title":"Deep Learning Segmentation and Classification of Red Blood Cells Using a\n  Large Multi-Scanner Dataset","summary":"  Digital pathology has recently been revolutionized by advancements in\nartificial intelligence, deep learning, and high-performance computing. With\nits advanced tools, digital pathology can help improve and speed up the\ndiagnostic process, reduce human errors, and streamline the reporting step. In\nthis paper, we report a new large red blood cell (RBC) image dataset and\npropose a two-stage deep learning framework for RBC image segmentation and\nclassification. The dataset is a highly diverse dataset of more than 100K RBCs\ncontaining eight different classes. The dataset, which is considerably larger\nthan any publicly available hematopathology dataset, was labeled independently\nby two hematopathologists who also manually created masks for RBC cell\nsegmentation. Subsequently, in the proposed framework, first, a U-Net model was\ntrained to achieve automatic RBC image segmentation. Second, an EfficientNetB0\nmodel was trained to classify RBC images into one of the eight classes using a\ntransfer learning approach with a 5X2 cross-validation scheme. An IoU of 98.03%\nand an average classification accuracy of 96.5% were attained on the test set.\nMoreover, we have performed experimental comparisons against several prominent\nCNN models. These comparisons show the superiority of the proposed model with a\ngood balance between performance and computational cost.\n","authors":["Mohamed Elmanna","Ahmed Elsafty","Yomna Ahmed","Muhammad Rushdi","Ahmed Morsy"],"pdf_url":"https://arxiv.org/pdf/2403.18468v1.pdf","comment":"15 pages, 12 figures, 8 tables"},{"id":"http://arxiv.org/abs/2403.18461v1","updated":"2024-03-27T11:19:34Z","published":"2024-03-27T11:19:34Z","title":"DiffStyler: Diffusion-based Localized Image Style Transfer","summary":"  Image style transfer aims to imbue digital imagery with the distinctive\nattributes of style targets, such as colors, brushstrokes, shapes, whilst\nconcurrently preserving the semantic integrity of the content. Despite the\nadvancements in arbitrary style transfer methods, a prevalent challenge remains\nthe delicate equilibrium between content semantics and style attributes. Recent\ndevelopments in large-scale text-to-image diffusion models have heralded\nunprecedented synthesis capabilities, albeit at the expense of relying on\nextensive and often imprecise textual descriptions to delineate artistic\nstyles. Addressing these limitations, this paper introduces DiffStyler, a novel\napproach that facilitates efficient and precise arbitrary image style transfer.\nDiffStyler lies the utilization of a text-to-image Stable Diffusion model-based\nLoRA to encapsulate the essence of style targets. This approach, coupled with\nstrategic cross-LoRA feature and attention injection, guides the style transfer\nprocess. The foundation of our methodology is rooted in the observation that\nLoRA maintains the spatial feature consistency of UNet, a discovery that\nfurther inspired the development of a mask-wise style transfer technique. This\ntechnique employs masks extracted through a pre-trained FastSAM model,\nutilizing mask prompts to facilitate feature fusion during the denoising\nprocess, thereby enabling localized style transfer that preserves the original\nimage's unaffected regions. Moreover, our approach accommodates multiple style\ntargets through the use of corresponding masks. Through extensive\nexperimentation, we demonstrate that DiffStyler surpasses previous methods in\nachieving a more harmonious balance between content preservation and style\nintegration.\n","authors":["Shaoxu Li"],"pdf_url":"https://arxiv.org/pdf/2403.18461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10522v4","updated":"2024-03-27T11:18:51Z","published":"2023-11-17T13:43:43Z","title":"Enhancing Object Coherence in Layout-to-Image Synthesis","summary":"  Layout-to-image synthesis is an emerging technique in conditional image\ngeneration. It aims to generate complex scenes, where users require fine\ncontrol over the layout of the objects in a scene. However, it remains\nchallenging to control the object coherence, including semantic coherence\n(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the\nhand and the racket should not be misaligned). In this paper, we propose a\nnovel diffusion model with effective global semantic fusion (GSF) and\nself-similarity feature enhancement modules to guide the object coherence for\nthis task. For semantic coherence, we argue that the image caption contains\nrich information for defining the semantic relationship within the objects in\nthe images. Instead of simply employing cross-attention between captions and\ngenerated images, which addresses the highly relevant layout restriction and\nsemantic coherence separately and thus leads to unsatisfying results shown in\nour experiments, we develop GSF to fuse the supervision from the layout\nrestriction and semantic coherence requirement and exploit it to guide the\nimage synthesis process. Moreover, to improve the physical coherence, we\ndevelop a Self-similarity Coherence Attention (SCA) module to explicitly\nintegrate local contextual physical coherence into each pixel's generation\nprocess. Specifically, we adopt a self-similarity map to encode the coherence\nrestrictions and employ it to extract coherent features from text embedding.\nThrough visualization of our self-similarity map, we explore the essence of\nSCA, revealing that its effectiveness is not only in capturing reliable\nphysical coherence patterns but also in enhancing complex texture generation.\nExtensive experiments demonstrate the superiority of our proposed method in\nboth image generation quality and controllability.\n","authors":["Yibin Wang","Weizhong Zhang","Jianwei Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2311.10522v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18454v1","updated":"2024-03-27T11:13:20Z","published":"2024-03-27T11:13:20Z","title":"Scaling Vision-and-Language Navigation With Offline RL","summary":"  The study of vision-and-language navigation (VLN) has typically relied on\nexpert trajectories, which may not always be available in real-world situations\ndue to the significant effort required to collect them. On the other hand,\nexisting approaches to training VLN agents that go beyond available expert data\ninvolve data augmentations or online exploration which can be tedious and\nrisky. In contrast, it is easy to access large repositories of suboptimal\noffline trajectories. Inspired by research in offline reinforcement learning\n(ORL), we introduce a new problem setup of VLN-ORL which studies VLN using\nsuboptimal demonstration data. We introduce a simple and effective\nreward-conditioned approach that can account for dataset suboptimality for\ntraining VLN agents, as well as benchmarks to evaluate progress and promote\nresearch in this area. We empirically study various noise models for\ncharacterizing dataset suboptimality among other unique challenges in VLN-ORL\nand instantiate it for the VLN$\\circlearrowright$BERT and MTVM architectures in\nthe R2R and RxR environments. Our experiments demonstrate that the proposed\nreward-conditioned approach leads to significant performance improvements, even\nin complex and intricate environments.\n","authors":["Valay Bundele","Mahesh Bhupati","Biplab Banerjee","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2403.18454v1.pdf","comment":"Published in Transactions on Machine Learning Research (04/2024)"},{"id":"http://arxiv.org/abs/2403.18452v1","updated":"2024-03-27T11:11:08Z","published":"2024-03-27T11:11:08Z","title":"SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model","summary":"  There are five types of trajectory prediction tasks: deterministic,\nstochastic, domain adaptation, momentary observation, and few-shot. These\nassociated tasks are defined by various factors, such as the length of input\npaths, data split and pre-processing methods. Interestingly, even though they\ncommonly take sequential coordinates of observations as input and infer future\npaths in the same coordinates as output, designing specialized architectures\nfor each task is still necessary. For the other task, generality issues can\nlead to sub-optimal performances. In this paper, we propose SingularTrajectory,\na diffusion-based universal trajectory prediction framework to reduce the\nperformance gap across the five tasks. The core of SingularTrajectory is to\nunify a variety of human dynamics representations on the associated tasks. To\ndo this, we first build a Singular space to project all types of motion\npatterns from each task into one embedding space. We next propose an adaptive\nanchor working in the Singular space. Unlike traditional fixed anchor methods\nthat sometimes yield unacceptable paths, our adaptive anchor enables correct\nanchors, which are put into a wrong location, based on a traversability map.\nFinally, we adopt a diffusion-based predictor to further enhance the prototype\npaths using a cascaded denoising process. Our unified framework ensures the\ngenerality across various benchmark settings such as input modality, and\ntrajectory lengths. Extensive experiments on five public benchmarks demonstrate\nthat SingularTrajectory substantially outperforms existing models, highlighting\nits effectiveness in estimating general dynamics of human movements. Code is\npublicly available at https://github.com/inhwanbae/SingularTrajectory .\n","authors":["Inhwan Bae","Young-Jae Park","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18452v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18443v1","updated":"2024-03-27T11:00:33Z","published":"2024-03-27T11:00:33Z","title":"$\\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation\n  via Optical Flow Consistency and Feature Map Synthesis","summary":"  Self-supervised monocular depth estimation methods have been increasingly\ngiven much attention due to the benefit of not requiring large, labelled\ndatasets. Such self-supervised methods require high-quality salient features\nand consequently suffer from severe performance drop for indoor scenes, where\nlow-textured regions dominant in the scenes are almost indiscriminative. To\naddress the issue, we propose a self-supervised indoor monocular depth\nestimation framework called $\\mathrm{F^2Depth}$. A self-supervised optical flow\nestimation network is introduced to supervise depth learning. To improve\noptical flow estimation performance in low-textured areas, only some patches of\npoints with more discriminative features are adopted for finetuning based on\nour well-designed patch-based photometric loss. The finetuned optical flow\nestimation network generates high-accuracy optical flow as a supervisory signal\nfor depth estimation. Correspondingly, an optical flow consistency loss is\ndesigned. Multi-scale feature maps produced by finetuned optical flow\nestimation network perform warping to compute feature map synthesis loss as\nanother supervisory signal for depth learning. Experimental results on the NYU\nDepth V2 dataset demonstrate the effectiveness of the framework and our\nproposed losses. To evaluate the generalization ability of our\n$\\mathrm{F^2Depth}$, we collect a Campus Indoor depth dataset composed of\napproximately 1500 points selected from 99 images in 18 scenes. Zero-shot\ngeneralization experiments on 7-Scenes dataset and Campus Indoor achieve\n$\\delta_1$ accuracy of 75.8% and 76.0% respectively. The accuracy results show\nthat our model can generalize well to monocular images captured in unknown\nindoor scenes.\n","authors":["Xiaotong Guo","Huijie Zhao","Shuwei Shao","Xudong Li","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.17126v2","updated":"2024-03-27T10:50:54Z","published":"2022-11-30T16:03:24Z","title":"BEVUDA: Multi-geometric Space Alignments for Domain Adaptive BEV 3D\n  Object Detection","summary":"  Vision-centric bird-eye-view (BEV) perception has shown promising potential\nin autonomous driving. Recent works mainly focus on improving efficiency or\naccuracy but neglect the challenges when facing environment changing, resulting\nin severe degradation of transfer performance. For BEV perception, we figure\nout the significant domain gaps existing in typical real-world cross-domain\nscenarios and comprehensively solve the Domain Adaption (DA) problem for\nmulti-view 3D object detection. Since BEV perception approaches are complicated\nand contain several components, the domain shift accumulation on multiple\ngeometric spaces (i.e., 2D, 3D Voxel, BEV) makes BEV DA even challenging. In\nthis paper, we propose a Multi-space Alignment Teacher-Student (MATS) framework\nto ease the domain shift accumulation, which consists of a Depth-Aware Teacher\n(DAT) and a Geometric-space Aligned Student (GAS) model. DAT tactfully combines\ntarget lidar and reliable depth prediction to construct depth-aware\ninformation, extracting target domain-specific knowledge in Voxel and BEV\nfeature spaces. It then transfers the sufficient domain knowledge of multiple\nspaces to the student model. In order to jointly alleviate the domain shift,\nGAS projects multi-geometric space features to a shared geometric embedding\nspace and decreases data distribution distance between two domains. To verify\nthe effectiveness of our method, we conduct BEV 3D object detection experiments\non three cross-domain scenarios and achieve state-of-the-art performance.\n","authors":["Jiaming Liu","Rongyu Zhang","Xiaoqi Li","Xiaowei Chi","Zehui Chen","Ming Lu","Yandong Guo","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2211.17126v2.pdf","comment":"Accepted by ICRA2024"},{"id":"http://arxiv.org/abs/2403.18442v1","updated":"2024-03-27T10:50:24Z","published":"2024-03-27T10:50:24Z","title":"Backpropagation-free Network for 3D Test-time Adaptation","summary":"  Real-world systems often encounter new data over time, which leads to\nexperiencing target domain shifts. Existing Test-Time Adaptation (TTA) methods\ntend to apply computationally heavy and memory-intensive backpropagation-based\napproaches to handle this. Here, we propose a novel method that uses a\nbackpropagation-free approach for TTA for the specific case of 3D data. Our\nmodel uses a two-stream architecture to maintain knowledge about the source\ndomain as well as complementary target-domain-specific information. The\nbackpropagation-free property of our model helps address the well-known\nforgetting problem and mitigates the error accumulation issue. The proposed\nmethod also eliminates the need for the usually noisy process of\npseudo-labeling and reliance on costly self-supervised training. Moreover, our\nmethod leverages subspace learning, effectively reducing the distribution\nvariance between the two domains. Furthermore, the source-domain-specific and\nthe target-domain-specific streams are aligned using a novel entropy-based\nadaptive fusion strategy. Extensive experiments on popular benchmarks\ndemonstrate the effectiveness of our method. The code will be available at\nhttps://github.com/abie-e/BFTT3D.\n","authors":["Yanshuo Wang","Ali Cheraghian","Zeeshan Hayder","Jie Hong","Sameera Ramasinghe","Shafin Rahman","David Ahmedt-Aristizabal","Xuesong Li","Lars Petersson","Mehrtash Harandi"],"pdf_url":"https://arxiv.org/pdf/2403.18442v1.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2311.18113v2","updated":"2024-03-27T10:46:59Z","published":"2023-11-29T21:58:41Z","title":"Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D\n  Features","summary":"  With the immense growth of dataset sizes and computing resources in recent\nyears, so-called foundation models have become popular in NLP and vision tasks.\nIn this work, we propose to explore foundation models for the task of keypoint\ndetection on 3D shapes. A unique characteristic of keypoint detection is that\nit requires semantic and geometric awareness while demanding high localization\naccuracy. To address this problem, we propose, first, to back-project features\nfrom large pre-trained 2D vision models onto 3D shapes and employ them for this\ntask. We show that we obtain robust 3D features that contain rich semantic\ninformation and analyze multiple candidate features stemming from different 2D\nfoundation models. Second, we employ a keypoint candidate optimization module\nwhich aims to match the average observed distribution of keypoints on the shape\nand is guided by the back-projected features. The resulting approach achieves a\nnew state of the art for few-shot keypoint detection on the KeyPointNet\ndataset, almost doubling the performance of the previous best methods.\n","authors":["Thomas Wimmer","Peter Wonka","Maks Ovsjanikov"],"pdf_url":"https://arxiv.org/pdf/2311.18113v2.pdf","comment":"Accepted to CVPR 2024, Project page:\n  https://wimmerth.github.io/back-to-3d.html"},{"id":"http://arxiv.org/abs/2401.08742v2","updated":"2024-03-27T10:33:02Z","published":"2024-01-16T18:58:36Z","title":"Fast Dynamic 3D Object Generation from a Single-view Video","summary":"  Generating dynamic 3D object from a single-view video is challenging due to\nthe lack of 4D labeled data. Extending image-to-3D pipelines by transferring\noff-the-shelf image generation models such as score distillation sampling,\nexisting methods tend to be slow and expensive to scale due to the need for\nback-propagating the information-limited supervision signals through a large\npretrained model. To address this, we propose an efficient video-to-4D object\ngeneration framework called Efficient4D. It generates high-quality\nspacetime-consistent images under different camera views, and then uses them as\nlabeled data to directly train a novel 4D Gaussian splatting model with\nexplicit point cloud geometry, enabling real-time rendering under continuous\ncamera trajectories. Extensive experiments on synthetic and real videos show\nthat Efficient4D offers a remarkable 20-fold increase in speed when compared to\nprior art alternatives while preserving the quality of novel view synthesis.\nFor example, Efficient4D takes only 6 mins to model a dynamic object, vs 120\nmins by Consistent4D.\n","authors":["Zijie Pan","Zeyu Yang","Xiatian Zhu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.08742v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2403.18425v1","updated":"2024-03-27T10:26:42Z","published":"2024-03-27T10:26:42Z","title":"U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models","summary":"  Diffusion models have demonstrated remarkable performance in text-to-image\nsynthesis, producing realistic and high resolution images that faithfully\nadhere to the corresponding text-prompts. Despite their great success, they\nstill fall behind in sketch-to-image synthesis tasks, where in addition to\ntext-prompts, the spatial layout of the generated images has to closely follow\nthe outlines of certain reference sketches. Employing an MLP latent edge\npredictor to guide the spatial layout of the synthesized image by predicting\nedge maps at each denoising step has been recently proposed. Despite yielding\npromising results, the pixel-wise operation of the MLP does not take into\naccount the spatial layout as a whole, and demands numerous denoising\niterations to produce satisfactory images, leading to time inefficiency. To\nthis end, we introduce U-Sketch, a framework featuring a U-Net type latent edge\npredictor, which is capable of efficiently capturing both local and global\nfeatures, as well as spatial correlations between pixels. Moreover, we propose\nthe addition of a sketch simplification network that offers the user the choice\nof preprocessing and simplifying input sketches for enhanced outputs. The\nexperimental results, corroborated by user feedback, demonstrate that our\nproposed U-Net latent edge predictor leads to more realistic results, that are\nbetter aligned with the spatial outlines of the reference sketches, while\ndrastically reducing the number of required denoising steps and, consequently,\nthe overall execution time.\n","authors":["Ilias Mitsouras","Eleftherios Tsonis","Paraskevi Tzouveli","Athanasios Voulodimos"],"pdf_url":"https://arxiv.org/pdf/2403.18425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15098v2","updated":"2024-03-27T10:26:23Z","published":"2024-03-22T10:36:50Z","title":"UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction","summary":"  Vehicle trajectory prediction has increasingly relied on data-driven\nsolutions, but their ability to scale to different data domains and the impact\nof larger dataset sizes on their generalization remain under-explored. While\nthese questions can be studied by employing multiple datasets, it is\nchallenging due to several discrepancies, e.g., in data formats, map\nresolution, and semantic annotation types. To address these challenges, we\nintroduce UniTraj, a comprehensive framework that unifies various datasets,\nmodels, and evaluation criteria, presenting new opportunities for the vehicle\ntrajectory prediction field. In particular, using UniTraj, we conduct extensive\nexperiments and find that model performance significantly drops when\ntransferred to other datasets. However, enlarging data size and diversity can\nsubstantially improve performance, leading to a new state-of-the-art result for\nthe nuScenes dataset. We provide insights into dataset characteristics to\nexplain these findings. The code can be found here:\nhttps://github.com/vita-epfl/UniTraj\n","authors":["Lan Feng","Mohammadhossein Bahari","Kaouther Messaoud Ben Amor","Éloi Zablocki","Matthieu Cord","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2403.15098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12359v2","updated":"2024-03-27T10:18:04Z","published":"2023-12-19T17:40:27Z","title":"CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary\n  semantic segmentation","summary":"  The popular CLIP model displays impressive zero-shot capabilities thanks to\nits seamless interaction with arbitrary text prompts. However, its lack of\nspatial awareness makes it unsuitable for dense computer vision tasks, e.g.,\nsemantic segmentation, without an additional fine-tuning step that often uses\nannotations and can potentially suppress its original open-vocabulary\nproperties. Meanwhile, self-supervised representation methods have demonstrated\ngood localization properties without human-made annotations nor explicit\nsupervision. In this work, we take the best of both worlds and propose an\nopen-vocabulary semantic segmentation method, which does not require any\nannotations. We propose to locally improve dense MaskCLIP features, which are\ncomputed with a simple modification of CLIP's last pooling layer, by\nintegrating localization priors extracted from self-supervised features. By\ndoing so, we greatly improve the performance of MaskCLIP and produce smooth\noutputs. Moreover, we show that the used self-supervised feature properties can\ndirectly be learnt from CLIP features. Our method CLIP-DINOiser needs only a\nsingle forward pass of CLIP and two light convolutional layers at inference, no\nextra supervision nor extra memory and reaches state-of-the-art results on\nchallenging and fine-grained benchmarks such as COCO, Pascal Context,\nCityscapes and ADE20k. The code to reproduce our results is available at\nhttps://github.com/wysoczanska/clip_dinoiser.\n","authors":["Monika Wysoczańska","Oriane Siméoni","Michaël Ramamonjisoa","Andrei Bursuc","Tomasz Trzciński","Patrick Pérez"],"pdf_url":"https://arxiv.org/pdf/2312.12359v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12480v2","updated":"2024-03-27T10:12:32Z","published":"2023-12-19T15:34:52Z","title":"Continual-MAE: Adaptive Distribution Masked Autoencoders for Continual\n  Test-Time Adaptation","summary":"  Continual Test-Time Adaptation (CTTA) is proposed to migrate a source\npre-trained model to continually changing target distributions, addressing\nreal-world dynamism. Existing CTTA methods mainly rely on entropy minimization\nor teacher-student pseudo-labeling schemes for knowledge extraction in\nunlabeled target domains. However, dynamic data distributions cause\nmiscalibrated predictions and noisy pseudo-labels in existing self-supervised\nlearning methods, hindering the effective mitigation of error accumulation and\ncatastrophic forgetting problems during the continual adaptation process. To\ntackle these issues, we propose a continual self-supervised method, Adaptive\nDistribution Masked Autoencoders (ADMA), which enhances the extraction of\ntarget domain knowledge while mitigating the accumulation of distribution\nshifts. Specifically, we propose a Distribution-aware Masking (DaM) mechanism\nto adaptively sample masked positions, followed by establishing consistency\nconstraints between the masked target samples and the original target samples.\nAdditionally, for masked tokens, we utilize an efficient decoder to reconstruct\na hand-crafted feature descriptor (e.g., Histograms of Oriented Gradients),\nleveraging its invariant properties to boost task-relevant representations.\nThrough conducting extensive experiments on four widely recognized benchmarks,\nour proposed method attains state-of-the-art performance in both classification\nand segmentation CTTA tasks. Our project page:\nhttps://sites.google.com/view/continual-mae/home.\n","authors":["Jiaming Liu","Ran Xu","Senqiao Yang","Renrui Zhang","Qizhe Zhang","Zehui Chen","Yandong Guo","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.12480v2.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2403.18417v1","updated":"2024-03-27T10:09:38Z","published":"2024-03-27T10:09:38Z","title":"ECNet: Effective Controllable Text-to-Image Diffusion Models","summary":"  The conditional text-to-image diffusion models have garnered significant\nattention in recent years. However, the precision of these models is often\ncompromised mainly for two reasons, ambiguous condition input and inadequate\ncondition guidance over single denoising loss. To address the challenges, we\nintroduce two innovative solutions. Firstly, we propose a Spatial Guidance\nInjector (SGI) which enhances conditional detail by encoding text inputs with\nprecise annotation information. This method directly tackles the issue of\nambiguous control inputs by providing clear, annotated guidance to the model.\nSecondly, to overcome the issue of limited conditional supervision, we\nintroduce Diffusion Consistency Loss (DCL), which applies supervision on the\ndenoised latent code at any given time step. This encourages consistency\nbetween the latent code at each time step and the input signal, thereby\nenhancing the robustness and accuracy of the output. The combination of SGI and\nDCL results in our Effective Controllable Network (ECNet), which offers a more\naccurate controllable end-to-end text-to-image generation framework with a more\nprecise conditioning input and stronger controllable supervision. We validate\nour approach through extensive experiments on generation under various\nconditions, such as human body skeletons, facial landmarks, and sketches of\ngeneral objects. The results consistently demonstrate that our method\nsignificantly enhances the controllability and robustness of the generated\nimages, outperforming existing state-of-the-art controllable text-to-image\nmodels.\n","authors":["Sicheng Li","Keqiang Sun","Zhixin Lai","Xiaoshi Wu","Feng Qiu","Haoran Xie","Kazunori Miyata","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06075v2","updated":"2024-03-27T09:51:15Z","published":"2023-09-12T09:12:37Z","title":"A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel\n  Segmentation via Two-Phase Training Angiography-to-Venography Translation","summary":"  We present a semi-supervised domain adaptation framework for brain vessel\nsegmentation from different image modalities. Existing state-of-the-art methods\nfocus on a single modality, despite the wide range of available cerebrovascular\nimaging techniques. This can lead to significant distribution shifts that\nnegatively impact the generalization across modalities. By relying on annotated\nangiographies and a limited number of annotated venographies, our framework\naccomplishes image-to-image translation and semantic segmentation, leveraging a\ndisentangled and semantically rich latent space to represent heterogeneous data\nand perform image-level adaptation from source to target domains. Moreover, we\nreduce the typical complexity of cycle-based architectures and minimize the use\nof adversarial training, which allows us to build an efficient and intuitive\nmodel with stable training. We evaluate our method on magnetic resonance\nangiographies and venographies. While achieving state-of-the-art performance in\nthe source domain, our method attains a Dice score coefficient in the target\ndomain that is only 8.9% lower, highlighting its promising potential for robust\ncerebrovascular image segmentation across different modalities.\n","authors":["Francesco Galati","Daniele Falcetta","Rosa Cortese","Barbara Casolla","Ferran Prados","Ninon Burgos","Maria A. Zuluaga"],"pdf_url":"https://arxiv.org/pdf/2309.06075v2.pdf","comment":"Accepted at the 34th British Machine Vision Conference (BMVC)"},{"id":"http://arxiv.org/abs/2403.18407v1","updated":"2024-03-27T09:49:37Z","published":"2024-03-27T09:49:37Z","title":"A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is\n  Critical for Semi-supervised Classification","summary":"  Semi-supervised learning (SSL) is a practical challenge in computer vision.\nPseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of\nThe Art (SOTA) performances in SSL. These approaches employ a\nthreshold-to-pseudo-label (T2L) process to generate PLs by truncating the\nconfidence scores of unlabeled data predicted by the self-training method.\nHowever, self-trained models typically yield biased and high-variance\npredictions, especially in the scenarios when a little labeled data are\nsupplied. To address this issue, we propose a lightweight channel-based\nensemble method to effectively consolidate multiple inferior PLs into the\ntheoretically guaranteed unbiased and low-variance one. Importantly, our\napproach can be readily extended to any SSL framework, such as FixMatch or\nFreeMatch. Experimental results demonstrate that our method significantly\noutperforms state-of-the-art techniques on CIFAR10/100 in terms of\neffectiveness and efficiency.\n","authors":["Jiaqi Wu","Junbiao Pang","Baochang Zhang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.05262v2","updated":"2024-03-27T09:43:41Z","published":"2024-03-08T12:35:07Z","title":"Debiasing Multimodal Large Language Models","summary":"  In the realms of computer vision and natural language processing, Large\nVision-Language Models (LVLMs) have become indispensable tools, proficient in\ngenerating textual descriptions based on visual inputs. Despite their\nadvancements, our investigation reveals a noteworthy bias in the generated\ncontent, where the output is primarily influenced by the underlying Large\nLanguage Models (LLMs) prior rather than the input image. Our empirical\nexperiments underscore the persistence of this bias, as LVLMs often provide\nconfident answers even in the absence of relevant images or given incongruent\nvisual input. To rectify these biases and redirect the model's focus toward\nvision information, we introduce two simple, training-free strategies. Firstly,\nfor tasks such as classification or multi-choice question-answering (QA), we\npropose a ``calibration'' step through affine transformation to adjust the\noutput distribution. This ``Post-Hoc debias'' approach ensures uniform scores\nfor each answer when the image is absent, serving as an effective\nregularization technique to alleviate the influence of LLM priors. For more\nintricate open-ended generation tasks, we extend this method to ``Debias\nsampling'', drawing inspirations from contrastive decoding methods.\nFurthermore, our investigation sheds light on the instability of LVLMs across\nvarious decoding configurations. Through systematic exploration of different\nsettings, we significantly enhance performance, surpassing reported results and\nraising concerns about the fairness of existing evaluations. Comprehensive\nexperiments substantiate the effectiveness of our proposed strategies in\nmitigating biases. These strategies not only prove beneficial in minimizing\nhallucinations but also contribute to the generation of more helpful and\nprecise illustrations.\n","authors":["Yi-Fan Zhang","Weichen Yu","Qingsong Wen","Xue Wang","Zhang Zhang","Liang Wang","Rong Jin","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2403.05262v2.pdf","comment":"38 pages, 17 figures"},{"id":"http://arxiv.org/abs/2401.01647v2","updated":"2024-03-27T09:39:41Z","published":"2024-01-03T09:46:43Z","title":"SIGNeRF: Scene Integrated Generation for Neural Radiance Fields","summary":"  Advances in image diffusion models have recently led to notable improvements\nin the generation of high-quality images. In combination with Neural Radiance\nFields (NeRFs), they enabled new opportunities in 3D generation. However, most\ngenerative 3D approaches are object-centric and applying them to editing\nexisting photorealistic scenes is not trivial. We propose SIGNeRF, a novel\napproach for fast and controllable NeRF scene editing and scene-integrated\nobject generation. A new generative update strategy ensures 3D consistency\nacross the edited images, without requiring iterative optimization. We find\nthat depth-conditioned diffusion models inherently possess the capability to\ngenerate 3D consistent views by requesting a grid of images instead of single\nviews. Based on these insights, we introduce a multi-view reference sheet of\nmodified images. Our method updates an image collection consistently based on\nthe reference sheet and refines the original NeRF with the newly generated\nimage set in one go. By exploiting the depth conditioning mechanism of the\nimage diffusion model, we gain fine control over the spatial location of the\nedit and enforce shape guidance by a selected region or an external mesh.\n","authors":["Jan-Niklas Dihlmann","Andreas Engelhardt","Hendrik Lensch"],"pdf_url":"https://arxiv.org/pdf/2401.01647v2.pdf","comment":"Project Page: https://signerf.jdihlmann.com"},{"id":"http://arxiv.org/abs/2403.18397v1","updated":"2024-03-27T09:35:56Z","published":"2024-03-27T09:35:56Z","title":"Colour and Brush Stroke Pattern Recognition in Abstract Art using\n  Modified Deep Convolutional Generative Adversarial Networks","summary":"  Abstract Art is an immensely popular, discussed form of art that often has\nthe ability to depict the emotions of an artist. Many researchers have made\nattempts to study abstract art in the form of edge detection, brush stroke and\nemotion recognition algorithms using machine and deep learning. This papers\ndescribes the study of a wide distribution of abstract paintings using\nGenerative Adversarial Neural Networks(GAN). GANs have the ability to learn and\nreproduce a distribution enabling researchers and scientists to effectively\nexplore and study the generated image space. However, the challenge lies in\ndeveloping an efficient GAN architecture that overcomes common training\npitfalls. This paper addresses this challenge by introducing a modified-DCGAN\n(mDCGAN) specifically designed for high-quality artwork generation. The\napproach involves a thorough exploration of the modifications made, delving\ninto the intricate workings of DCGANs, optimisation techniques, and\nregularisation methods aimed at improving stability and realism in art\ngeneration enabling effective study of generated patterns. The proposed mDCGAN\nincorporates meticulous adjustments in layer configurations and architectural\nchoices, offering tailored solutions to the unique demands of art generation\nwhile effectively combating issues like mode collapse and gradient vanishing.\nFurther this paper explores the generated latent space by performing random\nwalks to understand vector relationships between brush strokes and colours in\nthe abstract art space and a statistical analysis of unstable outputs after a\ncertain period of GAN training and compare its significant difference. These\nfindings validate the effectiveness of the proposed approach, emphasising its\npotential to revolutionise the field of digital art generation and digital art\necosystem.\n","authors":["Srinitish Srinivasan","Varenya Pathak"],"pdf_url":"https://arxiv.org/pdf/2403.18397v1.pdf","comment":"28 pages, 5 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.11656v2","updated":"2024-03-27T09:34:44Z","published":"2024-03-18T10:53:00Z","title":"LocalStyleFool: Regional Video Style Transfer Attack Using Segment\n  Anything Model","summary":"  Previous work has shown that well-crafted adversarial perturbations can\nthreaten the security of video recognition systems. Attackers can invade such\nmodels with a low query budget when the perturbations are semantic-invariant,\nsuch as StyleFool. Despite the query efficiency, the naturalness of the minutia\nareas still requires amelioration, since StyleFool leverages style transfer to\nall pixels in each frame. To close the gap, we propose LocalStyleFool, an\nimproved black-box video adversarial attack that superimposes regional\nstyle-transfer-based perturbations on videos. Benefiting from the popularity\nand scalably usability of Segment Anything Model (SAM), we first extract\ndifferent regions according to semantic information and then track them through\nthe video stream to maintain the temporal consistency. Then, we add\nstyle-transfer-based perturbations to several regions selected based on the\nassociative criterion of transfer-based gradient information and regional area.\nPerturbation fine adjustment is followed to make stylized videos adversarial.\nWe demonstrate that LocalStyleFool can improve both intra-frame and inter-frame\nnaturalness through a human-assessed survey, while maintaining competitive\nfooling rate and query efficiency. Successful experiments on the\nhigh-resolution dataset also showcase that scrupulous segmentation of SAM helps\nto improve the scalability of adversarial attacks under high-resolution data.\n","authors":["Yuxin Cao","Jinghao Li","Xi Xiao","Derui Wang","Minhui Xue","Hao Ge","Wei Liu","Guangwu Hu"],"pdf_url":"https://arxiv.org/pdf/2403.11656v2.pdf","comment":"Accepted to 2024 IEEE Security and Privacy Workshops (SPW)"},{"id":"http://arxiv.org/abs/2403.18388v1","updated":"2024-03-27T09:25:20Z","published":"2024-03-27T09:25:20Z","title":"FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion","summary":"  Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient\ncomputing compared with Artificial Neural Networks (ANNs), closely mirroring\nbiological neural processes. However, this potential comes with inherent\nchallenges in directly training SNNs through spatio-temporal backpropagation --\nstemming from the temporal dynamics of spiking neurons and their discrete\nsignal processing -- which necessitates alternative ways of training, most\nnotably through ANN-SNN conversion. In this work, we introduce a lightweight\nForward Temporal Bias Correction (FTBC) technique, aimed at enhancing\nconversion accuracy without the computational overhead. We ground our method on\nprovided theoretical findings that through proper temporal bias calibration the\nexpected error of ANN-SNN conversion can be reduced to be zero after each time\nstep. We further propose a heuristic algorithm for finding the temporal bias\nonly in the forward pass, thus eliminating the computational burden of\nbackpropagation and we evaluate our method on CIFAR-10/100 and ImageNet\ndatasets, achieving a notable increase in accuracy on all datasets. Codes are\nreleased at a GitHub repository.\n","authors":["Xiaofeng Wu","Velibor Bojkovic","Bin Gu","Kun Suo","Kai Zou"],"pdf_url":"https://arxiv.org/pdf/2403.18388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06733v3","updated":"2024-03-27T09:24:56Z","published":"2023-12-11T10:43:28Z","title":"TULIP: Transformer for Upsampling of LiDAR Point Cloud","summary":"  LiDAR Upsampling is a challenging task for the perception systems of robots\nand autonomous vehicles, due to the sparse and irregular structure of\nlarge-scale scene contexts. Recent works propose to solve this problem by\nconverting LiDAR data from 3D Euclidean space into an image super-resolution\nproblem in 2D image space. Although their methods can generate high-resolution\nrange images with fine-grained details, the resulting 3D point clouds often\nblur out details and predict invalid points. In this paper, we propose TULIP, a\nnew method to reconstruct high-resolution LiDAR point clouds from\nlow-resolution LiDAR input. We also follow a range image-based approach but\nspecifically modify the patch and window geometries of a Swin-Transformer-based\nnetwork to better fit the characteristics of range images. We conducted several\nexperiments on three public real-world and simulated datasets. TULIP\noutperforms state-of-the-art methods in all relevant metrics and generates\nrobust and more realistic point clouds than prior works.\n","authors":["Bin Yang","Patrick Pfreundschuh","Roland Siegwart","Marco Hutter","Peyman Moghadam","Vaishakh Patil"],"pdf_url":"https://arxiv.org/pdf/2312.06733v3.pdf","comment":"The paper was accepted by CVPR20224"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2403.18766v1","updated":"2024-03-27T17:05:03Z","published":"2024-03-27T17:05:03Z","title":"Superior Parallel Big Data Clustering through Competitive Stochastic\n  Sample Size Optimization in Big-means","summary":"  This paper introduces a novel K-means clustering algorithm, an advancement on\nthe conventional Big-means methodology. The proposed method efficiently\nintegrates parallel processing, stochastic sampling, and competitive\noptimization to create a scalable variant designed for big data applications.\nIt addresses scalability and computation time challenges typically faced with\ntraditional techniques. The algorithm adjusts sample sizes dynamically for each\nworker during execution, optimizing performance. Data from these sample sizes\nare continually analyzed, facilitating the identification of the most efficient\nconfiguration. By incorporating a competitive element among workers using\ndifferent sample sizes, efficiency within the Big-means algorithm is further\nstimulated. In essence, the algorithm balances computational time and\nclustering quality by employing a stochastic, competitive sampling strategy in\na parallel computing setting.\n","authors":["Rustam Mussabayev","Ravil Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2403.18766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18684v1","updated":"2024-03-27T15:27:36Z","published":"2024-03-27T15:27:36Z","title":"Scaling Laws For Dense Retrieval","summary":"  Scaling up neural models has yielded significant advancements in a wide array\nof tasks, particularly in language generation. Previous studies have found that\nthe performance of neural models frequently adheres to predictable scaling\nlaws, correlated with factors such as training set size and model size. This\ninsight is invaluable, especially as large-scale experiments grow increasingly\nresource-intensive. Yet, such scaling law has not been fully explored in dense\nretrieval due to the discrete nature of retrieval metrics and complex\nrelationships between training data and model sizes in retrieval tasks. In this\nstudy, we investigate whether the performance of dense retrieval models follows\nthe scaling law as other neural models. We propose to use contrastive\nlog-likelihood as the evaluation metric and conduct extensive experiments with\ndense retrieval models implemented with different numbers of parameters and\ntrained with different amounts of annotated data. Results indicate that, under\nour settings, the performance of dense retrieval models follows a precise\npower-law scaling related to the model size and the number of annotations.\nAdditionally, we examine scaling with prevalent data augmentation methods to\nassess the impact of annotation quality, and apply the scaling law to find the\nbest resource allocation strategy under a budget constraint. We believe that\nthese insights will significantly contribute to understanding the scaling\neffect of dense retrieval models and offer meaningful guidance for future\nresearch endeavors.\n","authors":["Yan Fang","Jingtao Zhan","Qingyao Ai","Jiaxin Mao","Weihang Su","Jia Chen","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18684v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.18667v1","updated":"2024-03-27T15:11:00Z","published":"2024-03-27T15:11:00Z","title":"Improving Content Recommendation: Knowledge Graph-Based Semantic\n  Contrastive Learning for Diversity and Cold-Start Users","summary":"  Addressing the challenges related to data sparsity, cold-start problems, and\ndiversity in recommendation systems is both crucial and demanding. Many current\nsolutions leverage knowledge graphs to tackle these issues by combining both\nitem-based and user-item collaborative signals. A common trend in these\napproaches focuses on improving ranking performance at the cost of escalating\nmodel complexity, reducing diversity, and complicating the task. It is\nessential to provide recommendations that are both personalized and diverse,\nrather than solely relying on achieving high rank-based performance, such as\nClick-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task\nlearning approach, training on user-item and item-item interactions. We apply\nitem-based contrastive learning on descriptive text, sampling positive and\nnegative pairs based on item metadata. Our approach allows the model to better\nunderstand the relationships between entities within the knowledge graph by\nutilizing semantic information from text. It leads to more accurate, relevant,\nand diverse user recommendations and a benefit that extends even to cold-start\nusers who have few interactions with items. We perform extensive experiments on\ntwo widely used datasets to validate the effectiveness of our approach. Our\nfindings demonstrate that jointly training user-item interactions and\nitem-based signals using synopsis text is highly effective. Furthermore, our\nresults provide evidence that item-based contrastive learning enhances the\nquality of entity embeddings, as indicated by metrics such as uniformity and\nalignment.\n","authors":["Yejin Kim","Scott Rome","Kevin Foley","Mayur Nankani","Rimon Melamed","Javier Morales","Abhay Yadav","Maria Peifer","Sardar Hamidian","H. Howie Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18667v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18628v1","updated":"2024-03-27T14:37:01Z","published":"2024-03-27T14:37:01Z","title":"To Recommend or Not: Recommendability Identification in Conversations\n  with Pre-trained Language Models","summary":"  Most current recommender systems primarily focus on what to recommend,\nassuming users always require personalized recommendations. However, with the\nwidely spread of ChatGPT and other chatbots, a more crucial problem in the\ncontext of conversational systems is how to minimize user disruption when we\nprovide recommendation services for users. While previous research has\nextensively explored different user intents in dialogue systems, fewer efforts\nare made to investigate whether recommendations should be provided. In this\npaper, we formally define the recommendability identification problem, which\naims to determine whether recommendations are necessary in a specific scenario.\nFirst, we propose and define the recommendability identification task, which\ninvestigates the need for recommendations in the current conversational\ncontext. A new dataset is constructed. Subsequently, we discuss and evaluate\nthe feasibility of leveraging pre-trained language models (PLMs) for\nrecommendability identification. Finally, through comparative experiments, we\ndemonstrate that directly employing PLMs with zero-shot results falls short of\nmeeting the task requirements. Besides, fine-tuning or utilizing soft prompt\ntechniques yields comparable results to traditional classification methods. Our\nwork is the first to study recommendability before recommendation and provides\npreliminary ways to make it a fundamental component of the future\nrecommendation system.\n","authors":["Zhefan Wang","Weizhi Ma","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18623v1","updated":"2024-03-27T14:34:22Z","published":"2024-03-27T14:34:22Z","title":"Antitrust, Amazon, and Algorithmic Auditing","summary":"  In digital markets, antitrust law and special regulations aim to ensure that\nmarkets remain competitive despite the dominating role that digital platforms\nplay today in everyone's life. Unlike traditional markets, market participant\nbehavior is easily observable in these markets. We present a series of\nempirical investigations into the extent to which Amazon engages in practices\nthat are typically described as self-preferencing. We discuss how the computer\nscience tools used in this paper can be used in a regulatory environment that\nis based on algorithmic auditing and requires regulating digital markets at\nscale.\n","authors":["Abhisek Dash","Abhijnan Chakraborty","Saptarshi Ghosh","Animesh Mukherjee","Jens Frankenreiter","Stefan Bechtold","Krishna P. Gummadi"],"pdf_url":"https://arxiv.org/pdf/2403.18623v1.pdf","comment":"The paper has been accepted to appear at Journal of Institutional and\n  Theoretical Economics (JITE) 2024"},{"id":"http://arxiv.org/abs/2403.18604v1","updated":"2024-03-27T14:24:28Z","published":"2024-03-27T14:24:28Z","title":"Modeling Sustainable City Trips: Integrating CO2 Emissions, Popularity,\n  and Seasonality into Tourism Recommender Systems","summary":"  In an era of information overload and complex decision-making processes,\nRecommender Systems (RS) have emerged as indispensable tools across diverse\ndomains, particularly travel and tourism. These systems simplify trip planning\nby offering personalized recommendations that consider individual preferences\nand address broader challenges like seasonality, travel regulations, and\ncapacity constraints. The intricacies of the tourism domain, characterized by\nmultiple stakeholders, including consumers, item providers, platforms, and\nsociety, underscore the complexity of achieving balance among diverse\ninterests. Although previous research has focused on fairness in Tourism\nRecommender Systems (TRS) from a multistakeholder perspective, limited work has\nfocused on generating sustainable recommendations.\n  Our paper introduces a novel approach for assigning a sustainability\nindicator (SF index) for city trips accessible from the users' starting point,\nintegrating Co2e analysis, destination popularity, and seasonal demand. Our\nmethodology involves comprehensive data gathering on transportation modes and\nemissions, complemented by analyses of destination popularity and seasonal\ndemand. A user study validates our index, showcasing its practicality and\nefficacy in providing well-rounded and sustainable city trip recommendations.\nOur findings contribute significantly to the evolution of responsible tourism\nstrategies, harmonizing the interests of tourists, local communities, and the\nenvironment while paving the way for future research in responsible and\nequitable tourism practices.\n","authors":["Ashmi Banerjee","Tunar Mahmudov","Emil Adler","Fitri Nur Aisyah","Wolfgang Wörndl"],"pdf_url":"https://arxiv.org/pdf/2403.18604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12997v3","updated":"2024-03-27T13:59:57Z","published":"2024-02-20T13:25:16Z","title":"Towards Trustworthy Reranking: A Simple yet Effective Abstention\n  Mechanism","summary":"  Neural Information Retrieval (NIR) has significantly improved upon\nheuristic-based IR systems. Yet, failures remain frequent, the models used\noften being unable to retrieve documents relevant to the user's query. We\naddress this challenge by proposing a lightweight abstention mechanism tailored\nfor real-world constraints, with particular emphasis placed on the reranking\nphase. We introduce a protocol for evaluating abstention strategies in a\nblack-box scenario, demonstrating their efficacy, and propose a simple yet\neffective data-driven mechanism. We provide open-source code for experiment\nreplication and abstention implementation, fostering wider adoption and\napplication in diverse contexts.\n","authors":["Hippolyte Gisserot-Boukhlef","Manuel Faysse","Emmanuel Malherbe","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2402.12997v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18536v1","updated":"2024-03-27T13:12:41Z","published":"2024-03-27T13:12:41Z","title":"A Novel Behavior-Based Recommendation System for E-commerce","summary":"  The majority of existing recommender systems rely on user ratings, which are\nlimited by the lack of user collaboration and the sparsity problem. To address\nthese issues, this study proposes a behavior-based recommender system that\nleverages customers' natural behaviors, such as browsing and clicking, on\ne-commerce platforms. The proposed recommendation system involves clustering\nactive customers, determining neighborhoods, collecting similar users,\ncalculating product reputation based on similar users, and recommending\nhigh-reputation products. To overcome the complexity of customer behaviors and\ntraditional clustering methods, an unsupervised clustering approach based on\nproduct categories is developed to enhance the recommendation methodology. This\nstudy makes notable contributions in several aspects. Firstly, a groundbreaking\nbehavior-based recommendation methodology is developed, incorporating customer\nbehavior to generate accurate and tailored recommendations leading to improved\ncustomer satisfaction and engagement. Secondly, an original unsupervised\nclustering method, focusing on product categories, enables more precise\nclustering and facilitates accurate recommendations. Finally, an approach to\ndetermine neighborhoods for active customers within clusters is established,\nensuring grouping of customers with similar behavioral patterns to enhance\nrecommendation accuracy and relevance. The proposed recommendation methodology\nand clustering method contribute to improved recommendation performance,\noffering valuable insights for researchers and practitioners in the field of\ne-commerce recommendation systems. Additionally, the proposed method\noutperforms benchmark methods in experiments conducted using a behavior dataset\nfrom the well-known e-commerce site Alibaba.\n","authors":["Reza Barzegar Nozari","Mahdi Divsalar","Sepehr Akbarzadeh Abkenar","Mohammadreza Fadavi Amiri","Ali Divsalar"],"pdf_url":"https://arxiv.org/pdf/2403.18536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18480v1","updated":"2024-03-27T11:49:58Z","published":"2024-03-27T11:49:58Z","title":"Enhanced Generative Recommendation via Content and Collaboration\n  Integration","summary":"  Generative recommendation has emerged as a promising paradigm aimed at\naugmenting recommender systems with recent advancements in generative\nartificial intelligence. This task has been formulated as a\nsequence-to-sequence generation process, wherein the input sequence encompasses\ndata pertaining to the user's previously interacted items, and the output\nsequence denotes the generative identifier for the suggested item. However,\nexisting generative recommendation approaches still encounter challenges in (i)\neffectively integrating user-item collaborative signals and item content\ninformation within a unified generative framework, and (ii) executing an\nefficient alignment between content information and collaborative signals.\n  In this paper, we introduce content-based collaborative generation for\nrecommender systems, denoted as ColaRec. To capture collaborative signals, the\ngenerative item identifiers are derived from a pretrained collaborative\nfiltering model, while the user is represented through the aggregation of\ninteracted items' content. Subsequently, the aggregated textual description of\nitems is fed into a language model to encapsulate content information. This\nintegration enables ColaRec to amalgamate collaborative signals and content\ninformation within an end-to-end framework. Regarding the alignment, we propose\nan item indexing task to facilitate the mapping between the content-based\nsemantic space and the interaction-based collaborative space. Additionally, a\ncontrastive loss is introduced to ensure that items with similar collaborative\nGIDs possess comparable content representations, thereby enhancing alignment.\nTo validate the efficacy of ColaRec, we conduct experiments on three benchmark\ndatasets. Empirical results substantiate the superior performance of ColaRec.\n","authors":["Yidan Wang","Zhaochun Ren","Weiwei Sun","Jiyuan Yang","Zhixiang Liang","Xin Chen","Ruobing Xie","Su Yan","Xu Zhang","Pengjie Ren","Zhumin Chen","Xin Xin"],"pdf_url":"https://arxiv.org/pdf/2403.18480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18479v1","updated":"2024-03-27T11:49:55Z","published":"2024-03-27T11:49:55Z","title":"Lightweight Embeddings for Graph Collaborative Filtering","summary":"  Graph neural networks (GNNs) are currently one of the most performant\ncollaborative filtering methods. Meanwhile, owing to the use of an embedding\ntable to represent each user/item as a distinct vector, GNN-based recommenders\nhave inherited the long-standing defect of parameter inefficiency. As a common\npractice for scalable embeddings, parameter sharing enables the use of fewer\nembedding vectors (i.e., meta-embeddings). When assigning meta-embeddings, most\nexisting methods are a heuristically designed, predefined mapping from each\nuser's/item's ID to the corresponding meta-embedding indexes, thus simplifying\nthe optimization problem into learning only the meta-embeddings. However, in\nthe context of GNN-based collaborative filtering, such a fixed mapping omits\nthe semantic correlations between entities that are evident in the user-item\ninteraction graph, leading to suboptimal recommendation performance. To this\nend, we propose Lightweight Embeddings for Graph Collaborative Filtering\n(LEGCF), a parameter-efficient embedding framework dedicated to GNN-based\nrecommenders. LEGCF innovatively introduces an assignment matrix as an extra\nlearnable component on top of meta-embeddings. To jointly optimize these two\nheavily entangled components, aside from learning the meta-embeddings by\nminimizing the recommendation loss, LEGCF further performs efficient assignment\nupdate by enforcing a novel semantic similarity constraint and finding its\nclosed-form solution based on matrix pseudo-inverse. The meta-embeddings and\nassignment matrix are alternately updated, where the latter is sparsified on\nthe fly to ensure negligible storage overhead. Extensive experiments on three\nbenchmark datasets have verified LEGCF's smallest trade-off between size and\nperformance, with consistent accuracy gain over state-of-the-art baselines. The\ncodebase of LEGCF is available in https://github.com/xurong-liang/LEGCF.\n","authors":["Xurong Liang","Tong Chen","Lizhen Cui","Yang Wang","Meng Wang","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2403.18479v1.pdf","comment":"Accepted by SIGIR '24"},{"id":"http://arxiv.org/abs/2311.07838v3","updated":"2024-03-27T11:36:46Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v3.pdf","comment":"Accepted by NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.18462v1","updated":"2024-03-27T11:20:48Z","published":"2024-03-27T11:20:48Z","title":"Decoy Effect In Search Interaction: Understanding User Behavior and\n  Measuring System Vulnerability","summary":"  This study examines the decoy effect's underexplored influence on user search\ninteractions and methods for measuring information retrieval (IR) systems'\nvulnerability to this effect. It explores how decoy results alter users'\ninteractions on search engine result pages, focusing on metrics like\nclick-through likelihood, browsing time, and perceived document usefulness. By\nanalyzing user interaction logs from multiple datasets, the study demonstrates\nthat decoy results significantly affect users' behavior and perceptions.\nFurthermore, it investigates how different levels of task difficulty and user\nknowledge modify the decoy effect's impact, finding that easier tasks and lower\nknowledge levels lead to higher engagement with target documents. In terms of\nIR system evaluation, the study introduces the DEJA-VU metric to assess\nsystems' susceptibility to the decoy effect, testing it on specific retrieval\ntasks. The results show differences in systems' effectiveness and\nvulnerability, contributing to our understanding of cognitive biases in search\nbehavior and suggesting pathways for creating more balanced and bias-aware IR\nevaluations.\n","authors":["Nuo Chen","Jiqun Liu","Hanpei Fang","Yuankai Luo","Tetsuya Sakai","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18435v1","updated":"2024-03-27T10:40:14Z","published":"2024-03-27T10:40:14Z","title":"DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via\n  Structural Word Alignment","summary":"  Recent research demonstrates the effectiveness of using pre-trained language\nmodels for legal case retrieval. Most of the existing works focus on improving\nthe representation ability for the contextualized embedding of the [CLS] token\nand calculate relevance using textual semantic similarity. However, in the\nlegal domain, textual semantic similarity does not always imply that the cases\nare relevant enough. Instead, relevance in legal cases primarily depends on the\nsimilarity of key facts that impact the final judgment. Without proper\ntreatments, the discriminative ability of learned representations could be\nlimited since legal cases are lengthy and contain numerous non-key facts. To\nthis end, we introduce DELTA, a discriminative model designed for legal case\nretrieval. The basic idea involves pinpointing key facts in legal cases and\npulling the contextualized embedding of the [CLS] token closer to the key facts\nwhile pushing away from the non-key facts, which can warm up the case embedding\nspace in an unsupervised manner. To be specific, this study brings the word\nalignment mechanism to the contextual masked auto-encoder. First, we leverage\nshallow decoders to create information bottlenecks, aiming to enhance the\nrepresentation ability. Second, we employ the deep decoder to enable\ntranslation between different structures, with the goal of pinpointing key\nfacts to enhance discriminative ability. Comprehensive experiments conducted on\npublicly available legal benchmarks show that our approach can outperform\nexisting state-of-the-art methods in legal case retrieval. It provides a new\nperspective on the in-depth understanding and processing of legal case\ndocuments.\n","authors":["Haitao Li","Qingyao Ai","Xinyan Han","Jia Chen","Qian Dong","Yiqun Liu","Chong Chen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2403.18435v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.06747v5","updated":"2024-03-27T10:32:29Z","published":"2024-03-11T14:13:41Z","title":"MetaSplit: Meta-Split Network for Limited-Stock Product Recommendation","summary":"  Compared to business-to-consumer (B2C) e-commerce systems,\nconsumer-to-consumer (C2C) e-commerce platforms usually encounter the\nlimited-stock problem, that is, a product can only be sold one time in a C2C\nsystem. This poses several unique challenges for click-through rate (CTR)\nprediction. Due to limited user interactions for each product (i.e. item), the\ncorresponding item embedding in the CTR model may not easily converge. This\nmakes the conventional sequence modeling based approaches cannot effectively\nutilize user history information since historical user behaviors contain a\nmixture of items with different volume of stocks. Particularly, the attention\nmechanism in a sequence model tends to assign higher score to products with\nmore accumulated user interactions, making limited-stock products being ignored\nand contribute less to the final output. To this end, we propose the Meta-Split\nNetwork (MSNet) to split user history sequence regarding to the volume of stock\nfor each product, and adopt differentiated modeling approaches for different\nsequences. As for the limited-stock products, a meta-learning approach is\napplied to address the problem of inconvergence, which is achieved by designing\nmeta scaling and shifting networks with ID and side information. In addition,\ntraditional approach can hardly update item embedding once the product is\nconsumed. Thereby, we propose an auxiliary loss that makes the parameters\nupdatable even when the product is no longer in distribution. To the best of\nour knowledge, this is the first solution addressing the recommendation of\nlimited-stock product. Experimental results on the production dataset and\nonline A/B testing demonstrate the effectiveness of our proposed method.\n","authors":["Wenhao Wu","Jialiang Zhou","Ailong He","Shuguang Han","Jufeng Chen","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.06747v5.pdf","comment":"Accepted at WWW 2024. This work has already been deployed on the\n  Xianyu platform in Alibaba. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2403.18348v1","updated":"2024-03-27T08:39:42Z","published":"2024-03-27T08:39:42Z","title":"Sequential Recommendation with Latent Relations based on Large Language\n  Model","summary":"  Sequential recommender systems predict items that may interest users by\nmodeling their preferences based on historical interactions. Traditional\nsequential recommendation methods rely on capturing implicit collaborative\nfiltering signals among items. Recent relation-aware sequential recommendation\nmodels have achieved promising performance by explicitly incorporating item\nrelations into the modeling of user historical sequences, where most relations\nare extracted from knowledge graphs. However, existing methods rely on manually\npredefined relations and suffer the sparsity issue, limiting the generalization\nability in diverse scenarios with varied item relations. In this paper, we\npropose a novel relation-aware sequential recommendation framework with Latent\nRelation Discovery (LRD). Different from previous relation-aware models that\nrely on predefined rules, we propose to leverage the Large Language Model (LLM)\nto provide new types of relations and connections between items. The motivation\nis that LLM contains abundant world knowledge, which can be adopted to mine\nlatent relations of items for recommendation. Specifically, inspired by that\nhumans can describe relations between items using natural language, LRD\nharnesses the LLM that has demonstrated human-like knowledge to obtain language\nknowledge representations of items. These representations are fed into a latent\nrelation discovery module based on the discrete state variational autoencoder\n(DVAE). Then the self-supervised relation discovery tasks and recommendation\ntasks are jointly optimized. Experimental results on multiple public datasets\ndemonstrate our proposed latent relations discovery method can be incorporated\nwith existing relation-aware sequential recommendation models and significantly\nimprove the performance. Further analysis experiments indicate the\neffectiveness and reliability of the discovered latent relations.\n","authors":["Shenghao Yang","Weizhi Ma","Peijie Sun","Qingyao Ai","Yiqun Liu","Mingchen Cai","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18348v1.pdf","comment":"Accepted by SIGIR 2024"},{"id":"http://arxiv.org/abs/2403.18325v1","updated":"2024-03-27T08:06:56Z","published":"2024-03-27T08:06:56Z","title":"Common Sense Enhanced Knowledge-based Recommendation with Large Language\n  Model","summary":"  Knowledge-based recommendation models effectively alleviate the data sparsity\nissue leveraging the side information in the knowledge graph, and have achieved\nconsiderable performance. Nevertheless, the knowledge graphs used in previous\nwork, namely metadata-based knowledge graphs, are usually constructed based on\nthe attributes of items and co-occurring relations (e.g., also buy), in which\nthe former provides limited information and the latter relies on sufficient\ninteraction data and still suffers from cold start issue. Common sense, as a\nform of knowledge with generality and universality, can be used as a supplement\nto the metadata-based knowledge graph and provides a new perspective for\nmodeling users' preferences. Recently, benefiting from the emergent world\nknowledge of the large language model, efficient acquisition of common sense\nhas become possible. In this paper, we propose a novel knowledge-based\nrecommendation framework incorporating common sense, CSRec, which can be\nflexibly coupled to existing knowledge-based methods. Considering the challenge\nof the knowledge gap between the common sense-based knowledge graph and\nmetadata-based knowledge graph, we propose a knowledge fusion approach based on\nmutual information maximization theory. Experimental results on public datasets\ndemonstrate that our approach significantly improves the performance of\nexisting knowledge-based recommendation models.\n","authors":["Shenghao Yang","Weizhi Ma","Peijie Sun","Min Zhang","Qingyao Ai","Yiqun Liu","Mingchen Cai"],"pdf_url":"https://arxiv.org/pdf/2403.18325v1.pdf","comment":"Accepted by DASFAA 2024"},{"id":"http://arxiv.org/abs/2403.18317v1","updated":"2024-03-27T07:40:05Z","published":"2024-03-27T07:40:05Z","title":"A Situation-aware Enhancer for Personalized Recommendation","summary":"  When users interact with Recommender Systems (RecSys), current situations,\nsuch as time, location, and environment, significantly influence their\npreferences. Situations serve as the background for interactions, where\nrelationships between users and items evolve with situation changes. However,\nexisting RecSys treat situations, users, and items on the same level. They can\nonly model the relations between situations and users/items respectively,\nrather than the dynamic impact of situations on user-item associations (i.e.,\nuser preferences). In this paper, we provide a new perspective that takes\nsituations as the preconditions for users' interactions. This perspective\nallows us to separate situations from user/item representations, and capture\nsituations' influences over the user-item relationship, offering a more\ncomprehensive understanding of situations. Based on it, we propose a novel\nSituation-Aware Recommender Enhancer (SARE), a pluggable module to integrate\nsituations into various existing RecSys. Since users' perception of situations\nand situations' impact on preferences are both personalized, SARE includes a\nPersonalized Situation Fusion (PSF) and a User-Conditioned Preference Encoder\n(UCPE) to model the perception and impact of situations, respectively. We\nconduct experiments of applying SARE on seven backbones in various settings on\ntwo real-world datasets. Experimental results indicate that SARE improves the\nrecommendation performances significantly compared with backbones and SOTA\nsituation-aware baselines.\n","authors":["Jiayu Li","Peijie Sun","Chumeng Jiang","Weizhi Ma","Qingyao Ai","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18317v1.pdf","comment":"Accepted at the International Conference on Database Systems for\n  Advanced Applications (DASFAA 2024)"},{"id":"http://arxiv.org/abs/2402.18355v2","updated":"2024-03-27T07:22:40Z","published":"2024-02-28T14:26:52Z","title":"COPR -- Efficient, large-scale log storage and retrieval","summary":"  Modern, large scale monitoring systems have to process and store vast amounts\nof log data in near real-time. At query time the systems have to find relevant\nlogs based on the content of the log message using support structures that can\nscale to these amounts of data while still being efficient to use. We present\nour novel Compressed Probabilistic Retrieval algorithm (COPR), capable of\nanswering Multi-Set Multi-Membership-Queries, that can be used as an\nalternative to existing indexing structures for streamed log data. In our\nexperiments, COPR required up to 93% less storage space than the tested\nstate-of-the-art inverted index and had up to four orders of magnitude less\nfalse-positives than the tested state-of-the-art membership sketch.\nAdditionally, COPR achieved up to 250 times higher query throughput than the\ntested inverted index and up to 240 times higher query throughput than the\ntested membership sketch.\n","authors":["Julian Reichinger","Thomas Krismayer","Jan Rellermeyer"],"pdf_url":"https://arxiv.org/pdf/2402.18355v2.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.18305v1","updated":"2024-03-27T06:59:39Z","published":"2024-03-27T06:59:39Z","title":"A Recommender System for NFT Collectibles with Item Feature","summary":"  Recommender systems have been actively studied and applied in various domains\nto deal with information overload. Although there are numerous studies on\nrecommender systems for movies, music, and e-commerce, comparatively less\nattention has been paid to the recommender system for NFTs despite the\ncontinuous growth of the NFT market. This paper presents a recommender system\nfor NFTs that utilizes a variety of data sources, from NFT transaction records\nto external item features, to generate precise recommendations that cater to\nindividual preferences. We develop a data-efficient graph-based recommender\nsystem to efficiently capture the complex relationship between each item and\nusers and generate node(item) embeddings which incorporate both node feature\ninformation and graph structure. Furthermore, we exploit inputs beyond\nuser-item interactions, such as image feature, text feature, and price feature.\nNumerical experiments verify the performance of the graph-based recommender\nsystem improves significantly after utilizing all types of item features as\nside information, thereby outperforming all other baselines.\n","authors":["Minjoo Choi","Seonmi Kim","Yejin Kim","Youngbin Lee","Joohwan Hong","Yongjae Lee"],"pdf_url":"https://arxiv.org/pdf/2403.18305v1.pdf","comment":"Presented at the AAAI 2023 Bridge on AI for Financial Services\n  (https://sites.google.com/view/aaai-ai-fin/home)"},{"id":"http://arxiv.org/abs/2403.17421v2","updated":"2024-03-27T06:28:53Z","published":"2024-03-26T06:34:23Z","title":"MA4DIV: Multi-Agent Reinforcement Learning for Search Result\n  Diversification","summary":"  The objective of search result diversification (SRD) is to ensure that\nselected documents cover as many different subtopics as possible. Existing\nmethods primarily utilize a paradigm of \"greedy selection\", i.e., selecting one\ndocument with the highest diversity score at a time. These approaches tend to\nbe inefficient and are easily trapped in a suboptimal state. In addition, some\nother methods aim to approximately optimize the diversity metric, such as\n$\\alpha$-NDCG, but the results still remain suboptimal. To address these\nchallenges, we introduce Multi-Agent reinforcement learning (MARL) for search\nresult DIVersity, which called MA4DIV. In this approach, each document is an\nagent and the search result diversification is modeled as a cooperative task\namong multiple agents. This approach allows for directly optimizing the\ndiversity metrics, such as $\\alpha$-NDCG, while achieving high training\nefficiency. We conducted preliminary experiments on public TREC datasets to\ndemonstrate the effectiveness and potential of MA4DIV. Considering the limited\nnumber of queries in public TREC datasets, we construct a large-scale dataset\nfrom industry sources and show that MA4DIV achieves substantial improvements in\nboth effectiveness and efficiency than existing baselines on a industrial scale\ndataset.\n","authors":["Yiqun Chen","Jiaxin Mao","Yi Zhang","Dehong Ma","Long Xia","Jun Fan","Daiting Shi","Zhicong Cheng","Simiu Gu","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2403.17421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18280v1","updated":"2024-03-27T06:16:14Z","published":"2024-03-27T06:16:14Z","title":"Improving Out-of-Vocabulary Handling in Recommendation Systems","summary":"  Recommendation systems (RS) are an increasingly relevant area for both\nacademic and industry researchers, given their widespread impact on the daily\nonline experiences of billions of users. One common issue in real RS is the\ncold-start problem, where users and items may not contain enough information to\nproduce high-quality recommendations. This work focuses on a complementary\nproblem: recommending new users and items unseen (out-of-vocabulary, or OOV) at\ntraining time. This setting is known as the inductive setting and is especially\nproblematic for factorization-based models, which rely on encoding only those\nusers/items seen at training time with fixed parameter vectors. Many existing\nsolutions applied in practice are often naive, such as assigning OOV\nusers/items to random buckets. In this work, we tackle this problem and propose\napproaches that better leverage available user/item features to improve OOV\nhandling at the embedding table level. We discuss general-purpose plug-and-play\napproaches that are easily applicable to most RS models and improve inductive\nperformance without negatively impacting transductive model performance. We\nextensively evaluate 9 OOV embedding methods on 5 models across 4 datasets\n(spanning different domains). One of these datasets is a proprietary production\ndataset from a prominent RS employed by a large social platform serving\nhundreds of millions of daily active users. In our experiments, we find that\nseveral proposed methods that exploit feature similarity using LSH consistently\noutperform alternatives on most model-dataset combinations, with the best\nmethod showing a mean improvement of 3.74% over the industry standard baseline\nin inductive performance. We release our code and hope our work helps\npractitioners make more informed decisions when handling OOV for their RS and\nfurther inspires academic research into improving OOV support in RS.\n","authors":["William Shiao","Mingxuan Ju","Zhichun Guo","Xin Chen","Evangelos Papalexakis","Tong Zhao","Neil Shah","Yozen Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18280v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.18276v1","updated":"2024-03-27T06:07:05Z","published":"2024-03-27T06:07:05Z","title":"RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era\n  of Transformers","summary":"  Transformer structure has achieved great success in multiple applied machine\nlearning communities, such as natural language processing (NLP), computer\nvision (CV) and information retrieval (IR). Transformer architecture's core\nmechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$\ntime complexity in inference. Many works have been proposed to improve the\nattention mechanism's scalability, such as Flash Attention and Multi-query\nAttention. A different line of work aims to design new mechanisms to replace\nattention. Recently, a notable model structure -- Mamba, which is based on\nstate space models, has achieved transformer-equivalent performance in multiple\nsequence modeling tasks.\n  In this work, we examine \\mamba's efficacy through the lens of a classical IR\ntask -- document ranking. A reranker model takes a query and a document as\ninput, and predicts a scalar relevance score. This task demands the language\nmodel's ability to comprehend lengthy contextual inputs and to capture the\ninteraction between query and document tokens. We find that (1) Mamba models\nachieve competitive performance compared to transformer-based models with the\nsame training recipe; (2) but also have a lower training throughput in\ncomparison to efficient transformer implementations such as flash attention. We\nhope this study can serve as a starting point to explore Mamba models in other\nclassical IR tasks. Our code implementation and trained checkpoints are made\npublic to facilitate\nreproducibility.\\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.\n","authors":["Zhichao Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18227v1","updated":"2024-03-27T03:31:05Z","published":"2024-03-27T03:31:05Z","title":"One Backpropagation in Two Tower Recommendation Models","summary":"  Recent years have witnessed extensive researches on developing two tower\nrecommendation models for relieving information overload. Four building modules\ncan be identified in such models, namely, user-item encoding, negative\nsampling, loss computing and back-propagation updating. To the best of our\nknowledge, existing algorithms have researched only on the first three modules,\nyet neglecting the backpropagation module. They all adopt a kind of two\nbackpropagation strategy, which are based on an implicit assumption of equally\ntreating users and items in the training phase. In this paper, we challenge\nsuch an equal training assumption and propose a novel one backpropagation\nupdating strategy, which keeps the normal gradient backpropagation for the item\nencoding tower, but cuts off the backpropagation for the user encoding tower.\nInstead, we propose a moving-aggregation updating strategy to update a user\nencoding in each training epoch. Except the proposed backpropagation updating\nmodule, we implement the other three modules with the most straightforward\nchoices. Experiments on four public datasets validate the effectiveness and\nefficiency of our model in terms of improved recommendation performance and\nreduced computation overload over the state-of-the-art competitors.\n","authors":["Erjia Chen","Bang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18227v1.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.16915v3","updated":"2024-03-27T01:53:36Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18183v1","updated":"2024-03-27T01:21:48Z","published":"2024-03-27T01:21:48Z","title":"Can AI Models Appreciate Document Aesthetics? An Exploration of\n  Legibility and Layout Quality in Relation to Prediction Confidence","summary":"  A well-designed document communicates not only through its words but also\nthrough its visual eloquence. Authors utilize aesthetic elements such as\ncolors, fonts, graphics, and layouts to shape the perception of information.\nThoughtful document design, informed by psychological insights, enhances both\nthe visual appeal and the comprehension of the content. While state-of-the-art\ndocument AI models demonstrate the benefits of incorporating layout and image\ndata, it remains unclear whether the nuances of document aesthetics are\neffectively captured. To bridge the gap between human cognition and AI\ninterpretation of aesthetic elements, we formulated hypotheses concerning AI\nbehavior in document understanding tasks, specifically anchored in document\ndesign principles. With a focus on legibility and layout quality, we tested\nfour aspects of aesthetic effects: noise, font-size contrast, alignment, and\ncomplexity, on model confidence using correlational analysis. The results and\nobservations highlight the value of model analysis rooted in document design\ntheories. Our work serves as a trailhead for further studies and we advocate\nfor continued research in this topic to deepen our understanding of how AI\ninterprets document aesthetics.\n","authors":["Hsiu-Wei Yang","Abhinav Agrawal","Pavlos Fragkogiannis","Shubham Nitin Mulay"],"pdf_url":"https://arxiv.org/pdf/2403.18183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18173v1","updated":"2024-03-27T01:01:09Z","published":"2024-03-27T01:01:09Z","title":"LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval\n  and Responsible Research Practices","summary":"  Efficient and accurate information extraction from scientific papers is\nsignificant in the rapidly developing human-computer interaction research in\nthe literature review process. Our paper introduces and analyses a new\ninformation retrieval system using state-of-the-art Large Language Models\n(LLMs) in combination with structured text analysis techniques to extract\nexperimental data from HCI literature, emphasizing key elements. Then We\nanalyze the challenges and risks of using LLMs in the world of research. We\nperformed a comprehensive analysis on our conducted dataset, which contained\nthe specified information of 300 CHI 2020-2022 papers, to evaluate the\nperformance of the two large language models, GPT-3.5 (text-davinci-003) and\nLlama-2-70b, paired with structured text analysis techniques. The GPT-3.5 model\ngains an accuracy of 58\\% and a mean absolute error of 7.00. In contrast, the\nLlama2 model indicates an accuracy of 56\\% with a mean absolute error of 7.63.\nThe ability to answer questions was also included in the system in order to\nwork with streamlined data. By evaluating the risks and opportunities presented\nby LLMs, our work contributes to the ongoing dialogue on establishing\nmethodological validity and ethical guidelines for LLM use in HCI data work.\n","authors":["Neda Taghizadeh Serajeh","Iman Mohammadi","Vittorio Fuccella","Mattia De Rosa"],"pdf_url":"https://arxiv.org/pdf/2403.18173v1.pdf","comment":"5 pages, CHI2024 Workshop on LLMs as Research Tools: Applications and\n  Evaluations in HCI Data Work"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.18807v1","updated":"2024-03-27T17:53:30Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://github.com/Aradhye2002/EcoDepth.\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2403.18802v1","updated":"2024-03-27T17:48:55Z","published":"2024-03-27T17:48:55Z","title":"Long-form factuality in large language models","summary":"  Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.\n","authors":["Jerry Wei","Chengrun Yang","Xinying Song","Yifeng Lu","Nathan Hu","Dustin Tran","Daiyi Peng","Ruibo Liu","Da Huang","Cosmo Du","Quoc V. Le"],"pdf_url":"https://arxiv.org/pdf/2403.18802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13525v2","updated":"2024-03-27T17:47:56Z","published":"2023-05-22T22:41:49Z","title":"A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs","summary":"  Large communication costs are a critical bottleneck in training\nstate-of-the-art neural networks on distributed systems. This paper introduces\nAxoNN, a novel four-dimensional (4D) parallelization approach, inspired by\nAgarwal's algorithm for matrix multiplication, for parallelizing tensor\ncomputations in deep learning, AxoNN employs two key strategies to minimize\ncommunication overhead. First, we optimize communication by overlapping\nexpensive collective operations (reduce-scatter, all-gather, all-reduce) with\ncomputations. Our experiments with a 20-billion parameter transformer model\ndemonstrate that these optimizations deliver nearly 53\\% improvement. Second,\nwe present an analytical model to assist users in identifying\ncommunication-minimizing configurations within the vast search space defined by\nour 4D algorithm. This model empowers practitioners by simplifying the tuning\nprocess for their specific training workloads. When training an 80-billion\nparameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a\nstate-of-the-art framework, by a significant 26%. Additionally, it achieves 57%\nof the theoretical peak FLOP/s.\n","authors":["Siddharth Singh","Prajwal Singhania","Aditya K. Ranjan","Zack Sating","Abhinav Bhatele"],"pdf_url":"https://arxiv.org/pdf/2305.13525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13483v4","updated":"2024-03-27T17:38:27Z","published":"2023-02-27T02:42:27Z","title":"CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems","summary":"  We present CrystalBox, a novel, model-agnostic, posthoc explainability\nframework for Deep Reinforcement Learning (DRL) controllers in the large family\nof input-driven environments which includes computer systems. We combine the\nnatural decomposability of reward functions in input-driven environments with\nthe explanatory power of decomposed returns. We propose an efficient algorithm\nto generate future-based explanations across both discrete and continuous\ncontrol environments. Using applications such as adaptive bitrate streaming and\ncongestion control, we demonstrate CrystalBox's capability to generate\nhigh-fidelity explanations. We further illustrate its higher utility across\nthree practical use cases: contrastive explanations, network observability, and\nguided reward design, as opposed to prior explainability techniques that\nidentify salient features.\n","authors":["Sagar Patel","Sangeetha Abdu Jyothi","Nina Narodytska"],"pdf_url":"https://arxiv.org/pdf/2302.13483v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18775v1","updated":"2024-03-27T17:23:39Z","published":"2024-03-27T17:23:39Z","title":"ImageNet-D: Benchmarking Neural Network Robustness on Diffusion\n  Synthetic Object","summary":"  We establish rigorous benchmarks for visual perception robustness. Synthetic\nimages such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific\ntype of evaluation over synthetic corruptions, backgrounds, and textures, yet\nthose robustness benchmarks are restricted in specified variations and have low\nsynthetic quality. In this work, we introduce generative model as a data source\nfor synthesizing hard images that benchmark deep models' robustness. Leveraging\ndiffusion models, we are able to generate images with more diversified\nbackgrounds, textures, and materials than any prior work, where we term this\nbenchmark as ImageNet-D. Experimental results show that ImageNet-D results in a\nsignificant accuracy drop to a range of vision models, from the standard ResNet\nvisual classifier to the latest foundation models like CLIP and MiniGPT-4,\nsignificantly reducing their accuracy by up to 60\\%. Our work suggests that\ndiffusion models can be an effective source to test vision models. The code and\ndataset are available at https://github.com/chenshuang-zhang/imagenet_d.\n","authors":["Chenshuang Zhang","Fei Pan","Junmo Kim","In So Kweon","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2403.18775v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2309.04381v2","updated":"2024-03-27T17:07:47Z","published":"2023-09-08T15:23:40Z","title":"Generalization Bounds: Perspectives from Information Theory and\n  PAC-Bayes","summary":"  A fundamental question in theoretical machine learning is generalization.\nOver the past decades, the PAC-Bayesian approach has been established as a\nflexible framework to address the generalization capabilities of machine\nlearning algorithms, and design new ones. Recently, it has garnered increased\ninterest due to its potential applicability for a variety of learning\nalgorithms, including deep neural networks. In parallel, an\ninformation-theoretic view of generalization has developed, wherein the\nrelation between generalization and various information measures has been\nestablished. This framework is intimately connected to the PAC-Bayesian\napproach, and a number of results have been independently discovered in both\nstrands. In this monograph, we highlight this strong connection and present a\nunified treatment of PAC-Bayesian and information-theoretic generalization\nbounds. We present techniques and results that the two perspectives have in\ncommon, and discuss the approaches and interpretations that differ. In\nparticular, we demonstrate how many proofs in the area share a modular\nstructure, through which the underlying ideas can be intuited. We pay special\nattention to the conditional mutual information (CMI) framework; analytical\nstudies of the information complexity of learning algorithms; and the\napplication of the proposed methods to deep learning. This monograph is\nintended to provide a comprehensive introduction to information-theoretic\ngeneralization bounds and their connection to PAC-Bayes, serving as a\nfoundation from which the most recent developments are accessible. It is aimed\nbroadly towards researchers with an interest in generalization and theoretical\nmachine learning.\n","authors":["Fredrik Hellström","Giuseppe Durisi","Benjamin Guedj","Maxim Raginsky"],"pdf_url":"https://arxiv.org/pdf/2309.04381v2.pdf","comment":"228 pages"},{"id":"http://arxiv.org/abs/2403.06054v4","updated":"2024-03-27T17:06:10Z","published":"2024-03-10T00:47:05Z","title":"Decoupled Data Consistency with Diffusion Purification for Image\n  Restoration","summary":"  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n","authors":["Xiang Li","Soo Min Kwon","Ismail R. Alkhouri","Saiprasad Ravishankar","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2403.06054v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18766v1","updated":"2024-03-27T17:05:03Z","published":"2024-03-27T17:05:03Z","title":"Superior Parallel Big Data Clustering through Competitive Stochastic\n  Sample Size Optimization in Big-means","summary":"  This paper introduces a novel K-means clustering algorithm, an advancement on\nthe conventional Big-means methodology. The proposed method efficiently\nintegrates parallel processing, stochastic sampling, and competitive\noptimization to create a scalable variant designed for big data applications.\nIt addresses scalability and computation time challenges typically faced with\ntraditional techniques. The algorithm adjusts sample sizes dynamically for each\nworker during execution, optimizing performance. Data from these sample sizes\nare continually analyzed, facilitating the identification of the most efficient\nconfiguration. By incorporating a competitive element among workers using\ndifferent sample sizes, efficiency within the Big-means algorithm is further\nstimulated. In essence, the algorithm balances computational time and\nclustering quality by employing a stochastic, competitive sampling strategy in\na parallel computing setting.\n","authors":["Rustam Mussabayev","Ravil Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2403.18766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18765v1","updated":"2024-03-27T17:03:31Z","published":"2024-03-27T17:03:31Z","title":"CaT: Constraints as Terminations for Legged Locomotion Reinforcement\n  Learning","summary":"  Deep Reinforcement Learning (RL) has demonstrated impressive results in\nsolving complex robotic tasks such as quadruped locomotion. Yet, current\nsolvers fail to produce efficient policies respecting hard constraints. In this\nwork, we advocate for integrating constraints into robot learning and present\nConstraints as Terminations (CaT), a novel constrained RL algorithm. Departing\nfrom classical constrained RL formulations, we reformulate constraints through\nstochastic terminations during policy learning: any violation of a constraint\ntriggers a probability of terminating potential future rewards the RL agent\ncould attain. We propose an algorithmic approach to this formulation, by\nminimally modifying widely used off-the-shelf RL algorithms in robot learning\n(such as Proximal Policy Optimization). Our approach leads to excellent\nconstraint adherence without introducing undue complexity and computational\noverhead, thus mitigating barriers to broader adoption. Through empirical\nevaluation on the real quadruped robot Solo crossing challenging obstacles, we\ndemonstrate that CaT provides a compelling solution for incorporating\nconstraints into RL frameworks. Videos and code are available at\nhttps://constraints-as-terminations.github.io.\n","authors":["Elliot Chane-Sane","Pierre-Alexandre Leziart","Thomas Flayols","Olivier Stasse","Philippe Souères","Nicolas Mansard"],"pdf_url":"https://arxiv.org/pdf/2403.18765v1.pdf","comment":"Project webpage: https://constraints-as-terminations.github.io"},{"id":"http://arxiv.org/abs/2311.01483v3","updated":"2024-03-27T16:56:23Z","published":"2023-11-02T14:47:06Z","title":"FedSN: A Novel Federated Learning Framework over LEO Satellite Networks","summary":"  Recently, a large number of Low Earth Orbit (LEO) satellites have been\nlaunched and deployed successfully in space by commercial companies, such as\nSpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve\nnot only for communication but also for various machine learning applications,\nsuch as space modulation recognition, remote sensing image classification, etc.\nHowever, the ground station (GS) may be incapable of downloading such a large\nvolume of raw sensing data for centralized model training due to the limited\ncontact time with LEO satellites (e.g. 5 minutes). Therefore, federated\nlearning (FL) has emerged as the promising solution to address this problem via\non-device training. Unfortunately, to enable FL on LEO satellites, we still\nface three critical challenges that are i) heterogeneous computing and memory\ncapabilities, ii) limited uplink rate, and iii) model staleness. To this end,\nwe propose FedSN as a general FL framework to tackle the above challenges, and\nfully explore data diversity on LEO satellites. Specifically, we first present\na novel sub-structure scheme to enable heterogeneous local model training\nconsidering different computing, memory, and communication constraints on LEO\nsatellites. Additionally, we propose a pseudo-synchronous model aggregation\nstrategy to dynamically schedule model aggregation for compensating model\nstaleness. To further demonstrate the effectiveness of the FedSN, we evaluate\nit using space modulation recognition and remote sensing image classification\ntasks by leveraging the data from real-world satellite networks. Extensive\nexperimental results demonstrate that FedSN framework achieves higher accuracy,\nlower computing, and communication overhead than the state-of-the-art\nbenchmarks and the effectiveness of each components in FedSN.\n","authors":["Zheng Lin","Zhe Chen","Zihan Fang","Xianhao Chen","Xiong Wang","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2311.01483v3.pdf","comment":"14 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.18756v1","updated":"2024-03-27T16:56:14Z","published":"2024-03-27T16:56:14Z","title":"Detection of subclinical atherosclerosis by image-based deep learning on\n  chest x-ray","summary":"  Aims. To develop a deep-learning based system for recognition of subclinical\natherosclerosis on a plain frontal chest x-ray. Methods and Results. A\ndeep-learning algorithm to predict coronary artery calcium (CAC) score (the\nAI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%\ninternal validation cohort) of primary prevention patients (58.4% male, median\nage 63 [51-74] years) with available paired chest x-ray and chest computed\ntomography (CT) indicated for any clinical reason and performed within 3\nmonths. The CAC score calculated on chest CT was used as ground truth. The\nmodel was validated on an temporally-independent cohort of 90 patients from the\nsame institution (external validation). The diagnostic accuracy of the AI-CAC\nmodel assessed by the area under the curve (AUC) was the primary outcome.\nOverall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.\nAUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation\ncohort and 0.77 in the external validation cohort. Sensitivity was consistently\nabove 92% in both cohorts. In the overall cohort (n=540), among patients with\nAI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with\nAI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events\n(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to\naccurately detect subclinical atherosclerosis on chest x-ray with elevated\nsensitivity, and to predict ASCVD events with elevated negative predictive\nvalue. Adoption of the AI-CAC model to refine CV risk stratification or as an\nopportunistic screening tool requires prospective evaluation.\n","authors":["Guglielmo Gallone","Francesco Iodice","Alberto Presta","Davide Tore","Ovidio de Filippo","Michele Visciano","Carlo Alberto Barbano","Alessandro Serafini","Paola Gorrini","Alessandro Bruno","Walter Grosso Marra","James Hughes","Mario Iannaccone","Paolo Fonio","Attilio Fiandrotti","Alessandro Depaoli","Marco Grangetto","Gaetano Maria de Ferrari","Fabrizio D'Ascenzo"],"pdf_url":"https://arxiv.org/pdf/2403.18756v1.pdf","comment":"Submitted to European Heart Journal - Cardiovascular Imaging Added\n  also the additional material 44 pages (30 main paper, 14 additional\n  material), 14 figures (5 main manuscript, 9 additional material)"},{"id":"http://arxiv.org/abs/2403.14623v2","updated":"2024-03-27T16:49:35Z","published":"2024-03-21T17:59:41Z","title":"Simplified Diffusion Schrödinger Bridge","summary":"  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/checkcrab/SDSB.\n","authors":["Zhicong Tang","Tiankai Hang","Shuyang Gu","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03683v2","updated":"2024-03-27T16:44:22Z","published":"2023-11-07T03:19:16Z","title":"Preventing Arbitrarily High Confidence on Far-Away Data in\n  Point-Estimated Discriminative Neural Networks","summary":"  Discriminatively trained, deterministic neural networks are the de facto\nchoice for classification problems. However, even though they achieve\nstate-of-the-art results on in-domain test sets, they tend to be overconfident\non out-of-distribution (OOD) data. For instance, ReLU networks - a popular\nclass of neural network architectures - have been shown to almost always yield\nhigh confidence predictions when the test data are far away from the training\nset, even when they are trained with OOD data. We overcome this problem by\nadding a term to the output of the neural network that corresponds to the logit\nof an extra class, that we design to dominate the logits of the original\nclasses as we move away from the training data.This technique provably prevents\narbitrarily high confidence on far-away test data while maintaining a simple\ndiscriminative point-estimate training. Evaluation on various benchmarks\ndemonstrates strong performance against competitive baselines on both far-away\nand realistic OOD data.\n","authors":["Ahmad Rashid","Serena Hacker","Guojun Zhang","Agustinus Kristiadi","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2311.03683v2.pdf","comment":"Accepted at AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.18742v1","updated":"2024-03-27T16:39:28Z","published":"2024-03-27T16:39:28Z","title":"Understanding the Learning Dynamics of Alignment with Human Feedback","summary":"  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n","authors":["Shawn Im","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2403.18742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18739v1","updated":"2024-03-27T16:32:32Z","published":"2024-03-27T16:32:32Z","title":"Usage-Specific Survival Modeling Based on Operational Data and Neural\n  Networks","summary":"  Accurate predictions of when a component will fail are crucial when planning\nmaintenance, and by modeling the distribution of these failure times, survival\nmodels have shown to be particularly useful in this context. The presented\nmethodology is based on conventional neural network-based survival models that\nare trained using data that is continuously gathered and stored at specific\ntimes, called snapshots. An important property of this type of training data is\nthat it can contain more than one snapshot from a specific individual which\nresults in that standard maximum likelihood training can not be directly\napplied since the data is not independent. However, the papers show that if the\ndata is in a specific format where all snapshot times are the same for all\nindividuals, called homogeneously sampled, maximum likelihood training can be\napplied and produce desirable results. In many cases, the data is not\nhomogeneously sampled and in this case, it is proposed to resample the data to\nmake it homogeneously sampled. How densely the dataset is sampled turns out to\nbe an important parameter; it should be chosen large enough to produce good\nresults, but this also increases the size of the dataset which makes training\nslow. To reduce the number of samples needed during training, the paper also\nproposes a technique to, instead of resampling the dataset once before the\ntraining starts, randomly resample the dataset at the start of each epoch\nduring the training. The proposed methodology is evaluated on both a simulated\ndataset and an experimental dataset of starter battery failures. The results\nshow that if the data is homogeneously sampled the methodology works as\nintended and produces accurate survival models. The results also show that\nrandomly resampling the dataset on each epoch is an effective way to reduce the\nsize of the training data.\n","authors":["Olov Holmer","Mattias Krysander","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2403.18739v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.18735v1","updated":"2024-03-27T16:24:26Z","published":"2024-03-27T16:24:26Z","title":"Nonlinear model reduction for operator learning","summary":"  Operator learning provides methods to approximate mappings between\ninfinite-dimensional function spaces. Deep operator networks (DeepONets) are a\nnotable architecture in this field. Recently, an extension of DeepONet based on\nmodel reduction and neural networks, proper orthogonal decomposition\n(POD)-DeepONet, has been able to outperform other architectures in terms of\naccuracy for several benchmark tests. We extend this idea towards nonlinear\nmodel order reduction by proposing an efficient framework that combines neural\nnetworks with kernel principal component analysis (KPCA) for operator learning.\nOur results demonstrate the superior performance of KPCA-DeepONet over\nPOD-DeepONet.\n","authors":["Hamidreza Eivazi","Stefan Wittek","Andreas Rausch"],"pdf_url":"https://arxiv.org/pdf/2403.18735v1.pdf","comment":"Published as a Tiny Paper at ICLR 2024 (Notable)"},{"id":"http://arxiv.org/abs/2403.18731v1","updated":"2024-03-27T16:21:24Z","published":"2024-03-27T16:21:24Z","title":"Enhancing Manufacturing Quality Prediction Models through the\n  Integration of Explainability Methods","summary":"  This research presents a method that utilizes explainability techniques to\namplify the performance of machine learning (ML) models in forecasting the\nquality of milling processes, as demonstrated in this paper through a\nmanufacturing use case. The methodology entails the initial training of ML\nmodels, followed by a fine-tuning phase where irrelevant features identified\nthrough explainability methods are eliminated. This procedural refinement\nresults in performance enhancements, paving the way for potential reductions in\nmanufacturing costs and a better understanding of the trained ML models. This\nstudy highlights the usefulness of explainability techniques in both explaining\nand optimizing predictive models in the manufacturing realm.\n","authors":["Dennis Gross","Helge Spieker","Arnaud Gotlieb","Ricardo Knoblauch"],"pdf_url":"https://arxiv.org/pdf/2403.18731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03100v2","updated":"2024-03-27T16:14:34Z","published":"2024-03-05T16:35:25Z","title":"NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models","summary":"  While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility, and achieves on-par quality with human recordings.\nFurthermore, we achieve better performance by scaling to 1B parameters and 200K\nhours of training data.\n","authors":["Zeqian Ju","Yuancheng Wang","Kai Shen","Xu Tan","Detai Xin","Dongchao Yang","Yanqing Liu","Yichong Leng","Kaitao Song","Siliang Tang","Zhizheng Wu","Tao Qin","Xiang-Yang Li","Wei Ye","Shikun Zhang","Jiang Bian","Lei He","Jinyu Li","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.03100v2.pdf","comment":"Achieving human-level quality and naturalness on multi-speaker\n  datasets (e.g., LibriSpeech) in a zero-shot way"},{"id":"http://arxiv.org/abs/2402.07868v2","updated":"2024-03-27T16:12:43Z","published":"2024-02-12T18:29:17Z","title":"Nesting Particle Filters for Experimental Design in Dynamical Systems","summary":"  In this paper, we propose a novel approach to Bayesian experimental design\nfor non-exchangeable data that formulates it as risk-sensitive policy\noptimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential\nMonte Carlo technique to infer optimal designs, and embed it into a particle\nMarkov chain Monte Carlo framework to perform gradient-based policy\namortization. Our approach is distinct from other amortized experimental design\ntechniques, as it does not rely on contrastive estimators. Numerical validation\non a set of dynamical systems showcases the efficacy of our method in\ncomparison to other state-of-the-art strategies.\n","authors":["Sahel Iqbal","Adrien Corenflos","Simo Särkkä","Hany Abdulsamad"],"pdf_url":"https://arxiv.org/pdf/2402.07868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11798v3","updated":"2024-03-27T16:12:18Z","published":"2023-09-21T06:04:06Z","title":"A Comprehensive Review of Community Detection in Graphs","summary":"  The study of complex networks has significantly advanced our understanding of\ncommunity structures which serves as a crucial feature of real-world graphs.\nDetecting communities in graphs is a challenging problem with applications in\nsociology, biology, and computer science. Despite the efforts of an\ninterdisciplinary community of scientists, a satisfactory solution to this\nproblem has not yet been achieved. This review article delves into the topic of\ncommunity detection in graphs, which serves as a thorough exposition of various\ncommunity detection methods from perspectives of modularity-based method,\nspectral clustering, probabilistic modelling, and deep learning. Along with the\nmethods, a new community detection method designed by us is also presented.\nAdditionally, the performance of these methods on the datasets with and without\nground truth is compared. In conclusion, this comprehensive review provides a\ndeep understanding of community detection in graphs.\n","authors":["Jiakang Li","Songning Lai","Zhihao Shuai","Yuan Tan","Yifan Jia","Mianyang Yu","Zichen Song","Xiaokang Peng","Ziyang Xu","Yongxin Ni","Haifeng Qiu","Jiayu Yang","Yutong Liu","Yonggang Lu"],"pdf_url":"https://arxiv.org/pdf/2309.11798v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18717v1","updated":"2024-03-27T16:06:37Z","published":"2024-03-27T16:06:37Z","title":"Semi-Supervised Learning for Deep Causal Generative Models","summary":"  Developing models that can answer questions of the form \"How would $x$ change\nif $y$ had been $z$?\" is fundamental for advancing medical image analysis.\nTraining causal generative models that address such counterfactual questions,\nthough, currently requires that all relevant variables have been observed and\nthat corresponding labels are available in training data. However, clinical\ndata may not have complete records for all patients and state of the art causal\ngenerative models are unable to take full advantage of this. We thus develop,\nfor the first time, a semi-supervised deep causal generative model that\nexploits the causal relationships between variables to maximise the use of all\navailable data. We explore this in the setting where each sample is either\nfully labelled or fully unlabelled, as well as the more clinically realistic\ncase of having different labels missing for each sample. We leverage techniques\nfrom causal inference to infer missing values and subsequently generate\nrealistic counterfactuals, even for samples with incomplete labels.\n","authors":["Yasin Ibrahim","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2403.18717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07494v3","updated":"2024-03-27T16:06:34Z","published":"2024-01-15T06:26:53Z","title":"Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering\n  Tasks","summary":"  Computational efficiency and non-adversarial robustness are critical factors\nin real-world engineering applications. Yet, conventional neural networks often\nfall short in addressing both simultaneously, or even separately. Drawing\ninsights from natural physical systems and existing literature, it is known\nthat an input convex architecture enhances computational efficiency, while a\nLipschitz-constrained architecture bolsters non-adversarial robustness. By\nleveraging the strengths of convexity and Lipschitz continuity, we develop a\nnovel network architecture, termed Input Convex Lipschitz Recurrent Neural\nNetworks. This model is explicitly designed for fast and robust\noptimization-based tasks and outperforms existing recurrent units across a\nspectrum of engineering tasks in terms of computational efficiency and\nnon-adversarial robustness, including real-world solar irradiance prediction\nfor Solar PV system planning at LHT Holdings in Singapore and real-time Model\nPredictive Control optimization for a nonlinear chemical reactor.\n","authors":["Zihao Wang","P S Pravin","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2401.07494v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03123v3","updated":"2024-03-27T16:03:32Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v3.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.17878v2","updated":"2024-03-27T16:01:00Z","published":"2024-03-26T17:10:15Z","title":"Empowering Data Mesh with Federated Learning","summary":"  The evolution of data architecture has seen the rise of data lakes, aiming to\nsolve the bottlenecks of data management and promote intelligent\ndecision-making. However, this centralized architecture is limited by the\nproliferation of data sources and the growing demand for timely analysis and\nprocessing. A new data paradigm, Data Mesh, is proposed to overcome these\nchallenges. Data Mesh treats domains as a first-class concern by distributing\nthe data ownership from the central team to each data domain, while keeping the\nfederated governance to monitor domains and their data products. Many\nmulti-million dollar organizations like Paypal, Netflix, and Zalando have\nalready transformed their data analysis pipelines based on this new\narchitecture. In this decentralized architecture where data is locally\npreserved by each domain team, traditional centralized machine learning is\nincapable of conducting effective analysis across multiple domains, especially\nfor security-sensitive organizations. To this end, we introduce a pioneering\napproach that incorporates Federated Learning into Data Mesh. To the best of\nour knowledge, this is the first open-source applied work that represents a\ncritical advancement toward the integration of federated learning methods into\nthe Data Mesh paradigm, underscoring the promising prospects for\nprivacy-preserving and decentralized data analysis strategies within Data Mesh\narchitecture.\n","authors":["Haoyuan Li","Salman Toor"],"pdf_url":"https://arxiv.org/pdf/2403.17878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18710v1","updated":"2024-03-27T15:57:42Z","published":"2024-03-27T15:57:42Z","title":"Deep Learning for Traffic Flow Prediction using Cellular Automata-based\n  Model and CNN-LSTM architecture","summary":"  Recent works have attempted to use deep learning to predict future states of\ntraffic flow, but have met with mixed results. These approaches face two key\nchallenges. First, training deep learning neural networks requires large\namounts of training data which are not yet easily available for traffic flow\nsystems. Second, even when data is available, the neural networks require\naccess to historical data that covers most possible traffic flow dynamics to\nsuccessfully predict future traffic states. Specifically, these deep learning\napproaches do not fully leverage domain-knowledge about traffic flow dynamics,\ndespite a significant existing knowledge-base. In this work, we propose to\nsolve both issues using a Convolutional Neural Network (CNNs) with Long Short\nTerm Memory (LSTM) deep learning architecture to successfully predict traffic\nflow, while leveraging a cellular automata-based statistical mechanics model of\ntraffic flow to generate training and test data. Another major contribution of\nthis paper is the insight that training data for a large traffic system can\nactually be sampled from the simulations of a much smaller traffic system. This\nis achieved through observing that the normalized energy distribution of the\nstatistical mechanics model is scale invariant, which significantly eases the\nburden of data generation for large scale traffic systems. The resulting\nsimulations indicate good agreement between the predicted and the true traffic\nflow dynamics.\n","authors":["Zhaohui Yang","Kshitij Jerath"],"pdf_url":"https://arxiv.org/pdf/2403.18710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18705v1","updated":"2024-03-27T15:54:55Z","published":"2024-03-27T15:54:55Z","title":"Conditional Wasserstein Distances with Applications in Bayesian OT Flow\n  Matching","summary":"  In inverse problems, many conditional generative models approximate the\nposterior measure by minimizing a distance between the joint measure and its\nlearned approximation. While this approach also controls the distance between\nthe posterior measures in the case of the Kullback--Leibler divergence, this is\nin general not hold true for the Wasserstein distance. In this paper, we\nintroduce a conditional Wasserstein distance via a set of restricted couplings\nthat equals the expected Wasserstein distance of the posteriors. Interestingly,\nthe dual formulation of the conditional Wasserstein-1 flow resembles losses in\nthe conditional Wasserstein GAN literature in a quite natural way. We derive\ntheoretical properties of the conditional Wasserstein distance, characterize\nthe corresponding geodesics and velocity fields as well as the flow ODEs.\nSubsequently, we propose to approximate the velocity fields by relaxing the\nconditional Wasserstein distance. Based on this, we propose an extension of OT\nFlow Matching for solving Bayesian inverse problems and demonstrate its\nnumerical advantages on an inverse problem and class-conditional image\ngeneration.\n","authors":["Jannis Chemseddine","Paul Hagemann","Christian Wald","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2403.18705v1.pdf","comment":"This paper supersedes arXiv:2310.13433"},{"id":"http://arxiv.org/abs/2403.18703v1","updated":"2024-03-27T15:52:54Z","published":"2024-03-27T15:52:54Z","title":"Fpga-Based Neural Thrust Controller for UAVs","summary":"  The advent of unmanned aerial vehicles (UAVs) has improved a variety of\nfields by providing a versatile, cost-effective and accessible platform for\nimplementing state-of-the-art algorithms. To accomplish a broader range of\ntasks, there is a growing need for enhanced on-board computing to cope with\nincreasing complexity and dynamic environmental conditions. Recent advances\nhave seen the application of Deep Neural Networks (DNNs), particularly in\ncombination with Reinforcement Learning (RL), to improve the adaptability and\nperformance of UAVs, especially in unknown environments. However, the\ncomputational requirements of DNNs pose a challenge to the limited computing\nresources available on many UAVs. This work explores the use of Field\nProgrammable Gate Arrays (FPGAs) as a viable solution to this challenge,\noffering flexibility, high performance, energy and time efficiency. We propose\na novel hardware board equipped with an Artix-7 FPGA for a popular open-source\nmicro-UAV platform. We successfully validate its functionality by implementing\nan RL-based low-level controller using real-world experiments.\n","authors":["Sharif Azem","David Scheunert","Mengguang Li","Jonas Gehrunger","Kai Cui","Christian Hochberger","Heinz Koepp"],"pdf_url":"https://arxiv.org/pdf/2403.18703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11800v3","updated":"2024-03-27T15:48:29Z","published":"2024-02-19T03:08:02Z","title":"Stochastic Approximation with Delayed Updates: Finite-Time Rates under\n  Markovian Sampling","summary":"  Motivated by applications in large-scale and multi-agent reinforcement\nlearning, we study the non-asymptotic performance of stochastic approximation\n(SA) schemes with delayed updates under Markovian sampling. While the effect of\ndelays has been extensively studied for optimization, the manner in which they\ninteract with the underlying Markov process to shape the finite-time\nperformance of SA remains poorly understood. In this context, our first main\ncontribution is to show that under time-varying bounded delays, the delayed SA\nupdate rule guarantees exponentially fast convergence of the \\emph{last\niterate} to a ball around the SA operator's fixed point. Notably, our bound is\n\\emph{tight} in its dependence on both the maximum delay $\\tau_{max}$, and the\nmixing time $\\tau_{mix}$. To achieve this tight bound, we develop a novel\ninductive proof technique that, unlike various existing delayed-optimization\nanalyses, relies on establishing uniform boundedness of the iterates. As such,\nour proof may be of independent interest. Next, to mitigate the impact of the\nmaximum delay on the convergence rate, we provide the first finite-time\nanalysis of a delay-adaptive SA scheme under Markovian sampling. In particular,\nwe show that the exponent of convergence of this scheme gets scaled down by\n$\\tau_{avg}$, as opposed to $\\tau_{max}$ for the vanilla delayed SA rule; here,\n$\\tau_{avg}$ denotes the average delay across all iterations. Moreover, the\nadaptive scheme requires no prior knowledge of the delay sequence for step-size\ntuning. Our theoretical findings shed light on the finite-time effects of\ndelays for a broad class of algorithms, including TD learning, Q-learning, and\nstochastic gradient descent under Markovian sampling.\n","authors":["Arman Adibi","Nicolo Dal Fabbro","Luca Schenato","Sanjeev Kulkarni","H. Vincent Poor","George J. Pappas","Hamed Hassani","Aritra Mitra"],"pdf_url":"https://arxiv.org/pdf/2402.11800v3.pdf","comment":"Accepted to the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2024!"},{"id":"http://arxiv.org/abs/2403.18699v1","updated":"2024-03-27T15:48:16Z","published":"2024-03-27T15:48:16Z","title":"Contrastive Learning with Orthonormal Anchors (CLOA)","summary":"  This study focuses on addressing the instability issues prevalent in\ncontrastive learning, specifically examining the InfoNCE loss function and its\nderivatives. We reveal a critical observation that these loss functions exhibit\na restrictive behavior, leading to a convergence phenomenon where embeddings\ntend to merge into a singular point. This \"over-fusion\" effect detrimentally\naffects classification accuracy in subsequent supervised-learning tasks.\nThrough theoretical analysis, we demonstrate that embeddings, when equalized or\nconfined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In\nresponse to this challenge, our research introduces an innovative strategy that\nleverages the same or fewer labeled data than typically used in the fine-tuning\nphase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to\ndisentangle embedding clusters, significantly enhancing the distinctiveness of\neach embedding while simultaneously ensuring their aggregation into dense,\nwell-defined clusters. Our method demonstrates remarkable improvements with\njust a fraction of the conventional label requirements, as evidenced by our\nresults on CIFAR10 and CIFAR100 datasets.\n","authors":["Huanran Li","Daniel Pimentel-Alarcón"],"pdf_url":"https://arxiv.org/pdf/2403.18699v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.12091v3","updated":"2024-03-27T15:44:25Z","published":"2023-03-21T09:07:15Z","title":"Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised\n  Learning","summary":"  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled\ndata and test data are from the same distribution. Open-set semi-supervised\nlearning (Open-set SSL) considers a more practical scenario, where unlabeled\ndata and test data contain new categories (outliers) not observed in labeled\ndata (inliers). Most previous works focused on outlier detection via binary\nclassifiers, which suffer from insufficient scalability and inability to\ndistinguish different types of uncertainty. In this paper, we propose a novel\nframework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these\nlimitations. Concretely, we first introduce evidential deep learning (EDL) as\nan outlier detector to quantify different types of uncertainty, and design\ndifferent uncertainty metrics for self-training and inference. Furthermore, we\npropose a novel adaptive negative optimization strategy, making EDL more\ntailored to the unlabeled dataset containing both inliers and outliers. As\ndemonstrated empirically, our proposed method outperforms existing\nstate-of-the-art methods across four datasets.\n","authors":["Yang Yu","Danruo Deng","Furui Liu","Yueming Jin","Qi Dou","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.12091v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.18687v1","updated":"2024-03-27T15:34:27Z","published":"2024-03-27T15:34:27Z","title":"InceptionTime vs. Wavelet -- A comparison for time series classification","summary":"  Neural networks were used to classify infrasound data. Two different\napproaches were compared. One based on the direct classification of time series\ndata, using a custom implementation of the InceptionTime network. For the other\napproach, we generated 2D images of the wavelet transformation of the signals,\nwhich were subsequently classified using a ResNet implementation. Choosing\nappropriate hyperparameter settings, both achieve a classification accuracy of\nabove 90 %, with the direct approach reaching 95.2 %.\n","authors":["Daniel Klenkert","Daniel Schaeffer","Julian Stauch"],"pdf_url":"https://arxiv.org/pdf/2403.18687v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.18685v1","updated":"2024-03-27T15:29:08Z","published":"2024-03-27T15:29:08Z","title":"Representatividad Muestral en la Incertidumbre Simétrica Multivariada\n  para la Selección de Atributos","summary":"  In this work, we analyze the behavior of the multivariate symmetric\nuncertainty (MSU) measure through the use of statistical simulation techniques\nunder various mixes of informative and non-informative randomly generated\nfeatures. Experiments show how the number of attributes, their cardinalities,\nand the sample size affect the MSU. In this thesis, through observation of\nresults, it is proposed an heuristic condition that preserves good quality in\nthe MSU under different combinations of these three factors, providing a new\nuseful criterion to help drive the process of dimension reduction.\n  --\n  En el presente trabajo hemos analizado el comportamiento de una versi\\'on\nmultivariada de la incertidumbre sim\\'etrica a trav\\'es de t\\'ecnicas de\nsimulaci\\'on estad\\'isticas sobre varias combinaciones de atributos\ninformativos y no-informativos generados de forma aleatoria. Los experimentos\nmuestran como el n\\'umero de atributos, sus cardinalidades y el tama\\~no\nmuestral afectan al MSU como medida. En esta tesis, mediante la observaci\\'on\nde resultados hemos propuesto una condici\\'on que preserva una buena calidad en\nel MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual\nprovee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\\'on\nde dimensionalidad.\n","authors":["Gustavo Sosa-Cabrera"],"pdf_url":"https://arxiv.org/pdf/2403.18685v1.pdf","comment":"52 pages, in Spanish. Advisors: Miguel Garc\\'ia-Torres, Santiago\n  G\\'omez-Guerrero, Christian E. Schaerer Serra"},{"id":"http://arxiv.org/abs/2403.18681v1","updated":"2024-03-27T15:24:54Z","published":"2024-03-27T15:24:54Z","title":"TransFusion: Contrastive Learning with Transformers","summary":"  This paper proposes a novel framework, TransFusion, designed to make the\nprocess of contrastive learning more analytical and explainable. TransFusion\nconsists of attention blocks whose softmax being replaced by ReLU, and its\nfinal block's weighted-sum operation is truncated to leave the adjacency matrix\nas the output. The model is trained by minimizing the Jensen-Shannon Divergence\nbetween its output and the target affinity matrix, which indicates whether each\npair of samples belongs to the same or different classes. The main contribution\nof TransFusion lies in defining a theoretical limit for answering two\nfundamental questions in the field: the maximum level of data augmentation and\nthe minimum batch size required for effective contrastive learning.\nFurthermore, experimental results indicate that TransFusion successfully\nextracts features that isolate clusters from complex real-world data, leading\nto improved classification accuracy in downstream tasks.\n","authors":["Huanran Li","Daniel Pimentel-Alarcón"],"pdf_url":"https://arxiv.org/pdf/2403.18681v1.pdf","comment":"17 pages, 4 figures,"},{"id":"http://arxiv.org/abs/2403.18680v1","updated":"2024-03-27T15:22:16Z","published":"2024-03-27T15:22:16Z","title":"NL-ITI: Optimizing Probing and Intervention for Improvement of ITI\n  Method","summary":"  Large Language Models (LLM) are prone to returning false information. It\nconstitutes one of major challenges in the AI field. In our work, we explore\nparadigm introduced by Inference-Time-Intervention (ITI). In first stage, it\nidentifies attention heads, which contain the highest amount of desired type of\nknowledge (e.g., truthful). Afterwards, during inference, LLM activations are\nshifted for chosen subset of attention heads. We further improved the ITI\nframework by introducing a nonlinear probing and multi-token intervention -\nNon-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice\nbenchmarks, including TruthfulQA, on which we report around 14% MC1 metric\nimprovement with respect to the baseline ITI results. NL-ITI achieves also\nencouraging results on other testsets - on Business Ethics subdomain of MMLU,\naround 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI\nperforms better while being less invasive in the behavior of LLM at the same\ntime (as measured by Kullback-Leibler divergence).\n","authors":["Jakub Hoscilowicz","Adam Wiacek","Jan Chojnacki","Adam Cieslak","Leszek Michon","Vitalii Urbanevych","Artur Janicki"],"pdf_url":"https://arxiv.org/pdf/2403.18680v1.pdf","comment":"Code is available at https://github.com/Samsung/NL-ITI"},{"id":"http://arxiv.org/abs/2403.17143v2","updated":"2024-03-27T15:15:16Z","published":"2024-03-25T19:40:26Z","title":"Guided Distant Supervision for Multilingual Relation Extraction Data:\n  Adapting to a New Language","summary":"  Relation extraction is essential for extracting and understanding\nbiographical information in the context of digital humanities and related\nsubjects. There is a growing interest in the community to build datasets\ncapable of training machine learning models to extract relationships. However,\nannotating such datasets can be expensive and time-consuming, in addition to\nbeing limited to English. This paper applies guided distant supervision to\ncreate a large biographical relationship extraction dataset for German. Our\ndataset, composed of more than 80,000 instances for nine relationship types, is\nthe largest biographical German relationship extraction dataset. We also create\na manually annotated dataset with 2000 instances to evaluate the models and\nrelease it together with the dataset compiled using guided distant supervision.\nWe train several state-of-the-art machine learning models on the automatically\ncreated dataset and release them as well. Furthermore, we experiment with\nmultilingual and cross-lingual experiments that could benefit many low-resource\nlanguages.\n","authors":["Alistair Plum","Tharindu Ranasinghe","Christoph Purschke"],"pdf_url":"https://arxiv.org/pdf/2403.17143v2.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.18671v1","updated":"2024-03-27T15:15:14Z","published":"2024-03-27T15:15:14Z","title":"Fact Checking Beyond Training Set","summary":"  Evaluating the veracity of everyday claims is time consuming and in some\ncases requires domain expertise. We empirically demonstrate that the commonly\nused fact checking pipeline, known as the retriever-reader, suffers from\nperformance deterioration when it is trained on the labeled data from one\ndomain and used in another domain. Afterwards, we delve into each component of\nthe pipeline and propose novel algorithms to address this problem. We propose\nan adversarial algorithm to make the retriever component robust against\ndistribution shift. Our core idea is to initially train a bi-encoder on the\nlabeled source data, and then, to adversarially train two separate document and\nclaim encoders using unlabeled target data. We then focus on the reader\ncomponent and propose to train it such that it is insensitive towards the order\nof claims and evidence documents. Our empirical evaluations support the\nhypothesis that such a reader shows a higher robustness against distribution\nshift. To our knowledge, there is no publicly available multi-topic fact\nchecking dataset. Thus, we propose a simple automatic method to re-purpose two\nwell-known fact checking datasets. We then construct eight fact checking\nscenarios from these datasets, and compare our model to a set of strong\nbaseline models, including recent domain adaptation models that use GPT4 for\ngenerating synthetic data.\n","authors":["Payam Karisani","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18671v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18668v1","updated":"2024-03-27T15:11:07Z","published":"2024-03-27T15:11:07Z","title":"Aiming for Relevance","summary":"  Vital signs are crucial in intensive care units (ICUs). They are used to\ntrack the patient's state and to identify clinically significant changes.\nPredicting vital sign trajectories is valuable for early detection of adverse\nevents. However, conventional machine learning metrics like RMSE often fail to\ncapture the true clinical relevance of such predictions. We introduce novel\nvital sign prediction performance metrics that align with clinical contexts,\nfocusing on deviations from clinical norms, overall trends, and trend\ndeviations. These metrics are derived from empirical utility curves obtained in\na previous study through interviews with ICU clinicians. We validate the\nmetrics' usefulness using simulated and real clinical datasets (MIMIC and\neICU). Furthermore, we employ these metrics as loss functions for neural\nnetworks, resulting in models that excel in predicting clinically significant\nevents. This research paves the way for clinically relevant machine learning\nmodel evaluation and optimization, promising to improve ICU patient care. 10\npages, 9 figures.\n","authors":["Bar Eini Porat","Danny Eytan","Uri Shalit"],"pdf_url":"https://arxiv.org/pdf/2403.18668v1.pdf","comment":"10 pages, 9 figures, AMIA Informatics 2024"},{"id":"http://arxiv.org/abs/2403.18664v1","updated":"2024-03-27T15:08:00Z","published":"2024-03-27T15:08:00Z","title":"Neural Network-Based Piecewise Survival Models","summary":"  In this paper, a family of neural network-based survival models is presented.\nThe models are specified based on piecewise definitions of the hazard function\nand the density function on a partitioning of the time; both constant and\nlinear piecewise definitions are presented, resulting in a family of four\nmodels. The models can be seen as an extension of the commonly used\ndiscrete-time and piecewise exponential models and thereby add flexibility to\nthis set of standard models. Using a simulated dataset the models are shown to\nperform well compared to the highly expressive, state-of-the-art energy-based\nmodel, while only requiring a fraction of the computation time.\n","authors":["Olov Holmer","Erik Frisk","Mattias Krysander"],"pdf_url":"https://arxiv.org/pdf/2403.18664v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.13374v3","updated":"2024-03-27T14:57:54Z","published":"2024-03-20T08:15:08Z","title":"Byzantine-resilient Federated Learning With Adaptivity to Data\n  Heterogeneity","summary":"  This paper deals with federated learning (FL) in the presence of malicious\nByzantine attacks and data heterogeneity. A novel Robust Average Gradient\nAlgorithm (RAGA) is proposed, which leverages the geometric median for\naggregation and can freely select the round number for local updating.\nDifferent from most existing resilient approaches, which perform convergence\nanalysis based on strongly-convex loss function or homogeneously distributed\ndataset, we conduct convergence analysis for not only strongly-convex but also\nnon-convex loss function over heterogeneous dataset. According to our\ntheoretical analysis, as long as the fraction of dataset from malicious users\nis less than half, RAGA can achieve convergence at rate\n$\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and\n$\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for\nstrongly-convex loss function. Moreover, stationary point or global optimal\nsolution is proved to obtainable as data heterogeneity vanishes. Experimental\nresults corroborate the robustness of RAGA to Byzantine attacks and verifies\nthe advantage of RAGA over baselines on convergence performance under various\nintensity of Byzantine attacks, for heterogeneous dataset.\n","authors":["Shiyuan Zuo","Xingrun Yan","Rongfei Fan","Han Hu","Hangguan Shan","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2403.13374v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17251v2","updated":"2024-03-27T14:48:48Z","published":"2023-03-30T09:29:53Z","title":"Demystifying Misconceptions in Social Bots Research","summary":"  Research on social bots aims at advancing knowledge and providing solutions\nto one of the most debated forms of online manipulation. Yet, social bot\nresearch is plagued by widespread biases, hyped results, and misconceptions\nthat set the stage for ambiguities, unrealistic expectations, and seemingly\nirreconcilable findings. Overcoming such issues is instrumental towards\nensuring reliable solutions and reaffirming the validity of the scientific\nmethod. In this contribution, we review some recent results in social bots\nresearch, highlighting and revising factual errors as well as methodological\nand conceptual biases. More importantly, we demystify common misconceptions,\naddressing fundamental points on how social bots research is discussed. Our\nanalysis surfaces the need to discuss research about online disinformation and\nmanipulation in a rigorous, unbiased, and responsible way. This article\nbolsters such effort by identifying and refuting common fallacious arguments\nused by both proponents and opponents of social bots research, as well as\nproviding directions toward sound methodologies for future research in the\nfield.\n","authors":["Stefano Cresci","Kai-Cheng Yang","Angelo Spognardi","Roberto Di Pietro","Filippo Menczer","Marinella Petrocchi"],"pdf_url":"https://arxiv.org/pdf/2303.17251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12882v2","updated":"2024-03-27T14:47:41Z","published":"2023-08-23T17:42:00Z","title":"LCANets++: Robust Audio Classification using Multi-layer Neural Networks\n  with Lateral Competition","summary":"  Audio classification aims at recognizing audio signals, including speech\ncommands or sound events. However, current audio classifiers are susceptible to\nperturbations and adversarial attacks. In addition, real-world audio\nclassification tasks often suffer from limited labeled data. To help bridge\nthese gaps, previous work developed neuro-inspired convolutional neural\nnetworks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)\nin the first layer (i.e., LCANets) for computer vision. LCANets learn in a\ncombination of supervised and unsupervised learning, reducing dependency on\nlabeled samples. Motivated by the fact that auditory cortex is also sparse, we\nextend LCANets to audio recognition tasks and introduce LCANets++, which are\nCNNs that perform sparse coding in multiple layers via LCA. We demonstrate that\nLCANets++ are more robust than standard CNNs and LCANets against perturbations,\ne.g., background noise, as well as black-box and white-box attacks, e.g.,\nevasion and fast gradient sign (FGSM) attacks.\n","authors":["Sayanton V. Dibbo","Juston S. Moore","Garrett T. Kenyon","Michael A. Teti"],"pdf_url":"https://arxiv.org/pdf/2308.12882v2.pdf","comment":"Accepted at 2024 IEEE International Conference on Acoustics, Speech\n  and Signal Processing Workshops (ICASSPW)"},{"id":"http://arxiv.org/abs/2403.18637v1","updated":"2024-03-27T14:42:08Z","published":"2024-03-27T14:42:08Z","title":"Transformers-based architectures for stroke segmentation: A review","summary":"  Stroke remains a significant global health concern, necessitating precise and\nefficient diagnostic tools for timely intervention and improved patient\noutcomes. The emergence of deep learning methodologies has transformed the\nlandscape of medical image analysis. Recently, Transformers, initially designed\nfor natural language processing, have exhibited remarkable capabilities in\nvarious computer vision applications, including medical image analysis. This\ncomprehensive review aims to provide an in-depth exploration of the\ncutting-edge Transformer-based architectures applied in the context of stroke\nsegmentation. It commences with an exploration of stroke pathology, imaging\nmodalities, and the challenges associated with accurate diagnosis and\nsegmentation. Subsequently, the review delves into the fundamental ideas of\nTransformers, offering detailed insights into their architectural intricacies\nand the underlying mechanisms that empower them to effectively capture complex\nspatial information within medical images. The existing literature is\nsystematically categorized and analyzed, discussing various approaches that\nleverage Transformers for stroke segmentation. A critical assessment is\nprovided, highlighting the strengths and limitations of these methods,\nincluding considerations of performance and computational efficiency.\nAdditionally, this review explores potential avenues for future research and\ndevelopment\n","authors":["Yalda Zafari-Ghadim","Essam A. Rashed","Mohamed Mabrok"],"pdf_url":"https://arxiv.org/pdf/2403.18637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18635v1","updated":"2024-03-27T14:40:25Z","published":"2024-03-27T14:40:25Z","title":"Fusion approaches for emotion recognition from speech using acoustic and\n  text-based features","summary":"  In this paper, we study different approaches for classifying emotions from\nspeech using acoustic and text-based features. We propose to obtain\ncontextualized word embeddings with BERT to represent the information contained\nin speech transcriptions and show that this results in better performance than\nusing Glove embeddings. We also propose and compare different strategies to\ncombine the audio and text modalities, evaluating them on IEMOCAP and\nMSP-PODCAST datasets. We find that fusing acoustic and text-based systems is\nbeneficial on both datasets, though only subtle differences are observed across\nthe evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect\nthat the criteria used to define the cross-validation folds have on results. In\nparticular, the standard way of creating folds for this dataset results in a\nhighly optimistic estimation of performance for the text-based system,\nsuggesting that some previous works may overestimate the advantage of\nincorporating transcriptions.\n","authors":["Leonardo Pepino","Pablo Riera","Luciana Ferrer","Agustin Gravano"],"pdf_url":"https://arxiv.org/pdf/2403.18635v1.pdf","comment":"5 pages. Accepted in ICASSP 2020"},{"id":"http://arxiv.org/abs/2403.18631v1","updated":"2024-03-27T14:38:02Z","published":"2024-03-27T14:38:02Z","title":"First Experiences with the Identification of People at Risk for Diabetes\n  in Argentina using Machine Learning Techniques","summary":"  Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for\nmedicine due to the absence of pathogenic symptoms and the lack of known\nassociated risk factors. Even though some proposals for machine learning models\nenable the identification of people at risk, the nature of the condition makes\nit so that a model suitable for one population may not necessarily be suitable\nfor another. In this article, the development and assessment of predictive\nmodels to identify people at risk for T2D and PD specifically in Argentina are\ndiscussed. First, the database was thoroughly preprocessed and three specific\ndatasets were generated considering a compromise between the number of records\nand the amount of available variables. After applying 5 different\nclassification models, the results obtained show that a very good performance\nwas observed for two datasets with some of these models. In particular, RF, DT,\nand ANN demonstrated great classification power, with good values for the\nmetrics under consideration. Given the lack of this type of tool in Argentina,\nthis work represents the first step towards the development of more\nsophisticated models.\n","authors":["Enzo Rucci","Gonzalo Tittarelli","Franco Ronchetti","Jorge F. Elgart","Laura Lanzarini","Juan José Gagliardino"],"pdf_url":"https://arxiv.org/pdf/2403.18631v1.pdf","comment":"Accepted for publication in Computer Science - CACIC 2023"},{"id":"http://arxiv.org/abs/2403.16451v3","updated":"2024-03-27T14:36:21Z","published":"2024-03-25T06:30:54Z","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","summary":"  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n","authors":["Xiang-Li Lu","Hwai-Jung Hsu","Che-Wei Chou","H. T. Kung","Chen-Hsin Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16451v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18613v1","updated":"2024-03-27T14:28:44Z","published":"2024-03-27T14:28:44Z","title":"Scalable Lipschitz Estimation for CNNs","summary":"  Estimating the Lipschitz constant of deep neural networks is of growing\ninterest as it is useful for informing on generalisability and adversarial\nrobustness. Convolutional neural networks (CNNs) in particular, underpin much\nof the recent success in computer vision related applications. However,\nalthough existing methods for estimating the Lipschitz constant can be tight,\nthey have limited scalability when applied to CNNs. To tackle this, we propose\na novel method to accelerate Lipschitz constant estimation for CNNs. The core\nidea is to divide a large convolutional block via a joint layer and width-wise\npartition, into a collection of smaller blocks. We prove an upper-bound on the\nLipschitz constant of the larger block in terms of the Lipschitz constants of\nthe smaller blocks. Through varying the partition factor, the resulting method\ncan be adjusted to prioritise either accuracy or scalability and permits\nparallelisation. We demonstrate an enhanced scalability and comparable accuracy\nto existing baselines through a range of experiments.\n","authors":["Yusuf Sulehman","Tingting Mu"],"pdf_url":"https://arxiv.org/pdf/2403.18613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18597v1","updated":"2024-03-27T14:20:11Z","published":"2024-03-27T14:20:11Z","title":"Heterogeneous Peridynamic Neural Operators: Discover Biotissue\n  Constitutive Law and Microstructure From Digital Image Correlation\n  Measurements","summary":"  Human tissues are highly organized structures with specific collagen fiber\narrangements varying from point to point. The effects of such heterogeneity\nplay an important role for tissue function, and hence it is of critical to\ndiscover and understand the distribution of such fiber orientations from\nexperimental measurements, such as the digital image correlation data. To this\nend, we introduce the heterogeneous peridynamic neural operator (HeteroPNO)\napproach, for data-driven constitutive modeling of heterogeneous anisotropic\nmaterials. The goal is to learn both a nonlocal constitutive law together with\nthe material microstructure, in the form of a heterogeneous fiber orientation\nfield, from loading field-displacement field measurements. To this end, we\npropose a two-phase learning approach. Firstly, we learn a homogeneous\nconstitutive law in the form of a neural network-based kernel function and a\nnonlocal bond force, to capture complex homogeneous material responses from\ndata. Then, in the second phase we reinitialize the learnt bond force and the\nkernel function, and training them together with a fiber orientation field for\neach material point. Owing to the state-based peridynamic skeleton, our\nHeteroPNO-learned material models are objective and have the balance of linear\nand angular momentum guaranteed. Moreover, the effects from heterogeneity and\nnonlinear constitutive relationship are captured by the kernel function and the\nbond force respectively, enabling physical interpretability. As a result, our\nHeteroPNO architecture can learn a constitutive model for a biological tissue\nwith anisotropic heterogeneous response undergoing large deformation regime.\nMoreover, the framework is capable to provide displacement and stress field\npredictions for new and unseen loading instances.\n","authors":["Siavash Jafarzadeh","Stewart Silling","Lu Zhang","Colton Ross","Chung-Hao Lee","S. M. Rakibur Rahman","Shuodao Wang","Yue Yu"],"pdf_url":"https://arxiv.org/pdf/2403.18597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18587v1","updated":"2024-03-27T14:11:23Z","published":"2024-03-27T14:11:23Z","title":"The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency\n  Attacks in Computer Vision","summary":"  Resource efficiency plays an important role for machine learning nowadays.\nThe energy and decision latency are two critical aspects to ensure a\nsustainable and practical application. Unfortunately, the energy consumption\nand decision latency are not robust against adversaries. Researchers have\nrecently demonstrated that attackers can compute and submit so-called sponge\nexamples at inference time to increase the energy consumption and decision\nlatency of neural networks. In computer vision, the proposed strategy crafts\ninputs with less activation sparsity which could otherwise be used to\naccelerate the computation. In this paper, we analyze the mechanism how these\nenergy-latency attacks reduce activation sparsity. In particular, we find that\ninput uniformity is a key enabler. A uniform image, that is, an image with\nmostly flat, uniformly colored surfaces, triggers more activations due to a\nspecific interplay of convolution, batch normalization, and ReLU activation.\nBased on these insights, we propose two new simple, yet effective strategies\nfor crafting sponge examples: sampling images from a probability distribution\nand identifying dense, yet inconspicuous inputs in natural datasets. We\nempirically examine our findings in a comprehensive evaluation with multiple\nimage classification models and show that our attack achieves the same sparsity\neffect as prior sponge-example methods, but at a fraction of computation\neffort. We also show that our sponge examples transfer between different neural\nnetworks. Finally, we discuss applications of our findings for the good by\nimproving efficiency by increasing sparsity.\n","authors":["Andreas Müller","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2403.18587v1.pdf","comment":"Accepted at the DLSP 2024"},{"id":"http://arxiv.org/abs/2403.18582v1","updated":"2024-03-27T14:03:41Z","published":"2024-03-27T14:03:41Z","title":"One flow to correct them all: improving simulations in high-energy\n  physics with a single normalising flow and a switch","summary":"  Simulated events are key ingredients in almost all high-energy physics\nanalyses. However, imperfections in the simulation can lead to sizeable\ndifferences between the observed data and simulated events. The effects of such\nmismodelling on relevant observables must be corrected either effectively via\nscale factors, with weights or by modifying the distributions of the\nobservables and their correlations. We introduce a correction method that\ntransforms one multidimensional distribution (simulation) into another one\n(data) using a simple architecture based on a single normalising flow with a\nboolean condition. We demonstrate the effectiveness of the method on a\nphysics-inspired toy dataset with non-trivial mismodelling of several\nobservables and their correlations.\n","authors":["Caio Cesar Daumann","Mauro Donega","Johannes Erdmann","Massimiliano Galli","Jan Lukas Späh","Davide Valsecchi"],"pdf_url":"https://arxiv.org/pdf/2403.18582v1.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2306.09459v3","updated":"2024-03-27T14:02:58Z","published":"2023-06-15T19:29:08Z","title":"Recurrent Action Transformer with Memory","summary":"  Recently, the use of transformers in offline reinforcement learning has\nbecome a rapidly developing area. This is due to their ability to treat the\nagent's trajectory in the environment as a sequence, thereby reducing the\npolicy learning problem to sequence modeling. In environments where the agent's\ndecisions depend on past events, it is essential to capture both the event\nitself and the decision point in the context of the model. However, the\nquadratic complexity of the attention mechanism limits the potential for\ncontext expansion. One solution to this problem is to enhance transformers with\nmemory mechanisms. In this paper, we propose the Recurrent Action Transformer\nwith Memory (RATE) - a model that incorporates recurrent memory. To evaluate\nour model, we conducted extensive experiments on both memory-intensive\nenvironments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo\ncontrol environments. The results show that the use of memory can significantly\nimprove performance in memory-intensive environments while maintaining or\nimproving results in classic environments. We hope that our findings will\nstimulate research on memory mechanisms for transformers applicable to offline\nreinforcement learning.\n","authors":["Alexey Staroverov","Egor Cherepanov","Dmitry Yudin","Alexey K. Kovalev","Aleksandr I. Panov"],"pdf_url":"https://arxiv.org/pdf/2306.09459v3.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2309.11427v2","updated":"2024-03-27T14:02:57Z","published":"2023-09-20T16:01:45Z","title":"Generative Pre-Training of Time-Series Data for Unsupervised Fault\n  Detection in Semiconductor Manufacturing","summary":"  This paper introduces TRACE-GPT, which stands for Time-seRies\nAnomaly-detection with Convolutional Embedding and Generative Pre-trained\nTransformers. TRACE-GPT is designed to pre-train univariate time-series sensor\ndata and detect faults on unlabeled datasets in semiconductor manufacturing. In\nsemiconductor industry, classifying abnormal time-series sensor data from\nnormal data is important because it is directly related to wafer defect.\nHowever, small, unlabeled, and even mixed training data without enough\nanomalies make classification tasks difficult. In this research, we capture\nfeatures of time-series data with temporal convolutional embedding and\nGenerative Pre-trained Transformer (GPT) to classify abnormal sequences from\nnormal sequences using cross entropy loss. We prove that our model shows better\nperformance than previous unsupervised models with both an open dataset, the\nUniversity of California Riverside (UCR) time-series classification archive,\nand the process log of our Chemical Vapor Deposition (CVD) equipment. Our model\nhas the highest F1 score at Equal Error Rate (EER) across all datasets and is\nonly 0.026 below the supervised state-of-the-art baseline on the open dataset.\n","authors":["Sewoong Lee","JinKyou Choi","Min Su Kim"],"pdf_url":"https://arxiv.org/pdf/2309.11427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18579v1","updated":"2024-03-27T13:59:09Z","published":"2024-03-27T13:59:09Z","title":"On Optimizing Hyperparameters for Quantum Neural Networks","summary":"  The increasing capabilities of Machine Learning (ML) models go hand in hand\nwith an immense amount of data and computational power required for training.\nTherefore, training is usually outsourced into HPC facilities, where we have\nstarted to experience limits in scaling conventional HPC hardware, as theorized\nby Moore's law. Despite heavy parallelization and optimization efforts, current\nstate-of-the-art ML models require weeks for training, which is associated with\nan enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum\nMachine Learning (QML), can offer significant theoretical speed-ups and\nenhanced expressive power. However, training QML models requires tuning various\nhyperparameters, which is a nontrivial task and suboptimal choices can highly\naffect the trainability and performance of the models. In this study, we\nidentify the most impactful hyperparameters and collect data about the\nperformance of QML models. We compare different configurations and provide\nresearchers with performance data and concrete suggestions for hyperparameter\nselection.\n","authors":["Sabrina Herbst","Vincenzo De Maio","Ivona Brandic"],"pdf_url":"https://arxiv.org/pdf/2403.18579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18578v1","updated":"2024-03-27T13:59:05Z","published":"2024-03-27T13:59:05Z","title":"SteinGen: Generating Fidelitous and Diverse Graph Samples","summary":"  Generating graphs that preserve characteristic structures while promoting\nsample diversity can be challenging, especially when the number of graph\nobservations is small. Here, we tackle the problem of graph generation from\nonly one observed graph. The classical approach of graph generation from\nparametric models relies on the estimation of parameters, which can be\ninconsistent or expensive to compute due to intractable normalisation\nconstants. Generative modelling based on machine learning techniques to\ngenerate high-quality graph samples avoids parameter estimation but usually\nrequires abundant training samples. Our proposed generating procedure,\nSteinGen, which is phrased in the setting of graphs as realisations of\nexponential random graph models, combines ideas from Stein's method and MCMC by\nemploying Markovian dynamics which are based on a Stein operator for the target\nmodel. SteinGen uses the Glauber dynamics associated with an estimated Stein\noperator to generate a sample, and re-estimates the Stein operator from the\nsample after every sampling step. We show that on a class of exponential random\ngraph models this novel \"estimation and re-estimation\" generation strategy\nyields high distributional similarity (high fidelity) to the original data,\ncombined with high sample diversity.\n","authors":["Gesine Reinert","Wenkai Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09283v3","updated":"2024-03-27T13:55:14Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18570v1","updated":"2024-03-27T13:51:26Z","published":"2024-03-27T13:51:26Z","title":"Physics-Informed Graph Neural Networks for Water Distribution Systems","summary":"  Water distribution systems (WDS) are an integral part of critical\ninfrastructure which is pivotal to urban development. As 70% of the world's\npopulation will likely live in urban environments in 2050, efficient simulation\nand planning tools for WDS play a crucial role in reaching UN's sustainable\ndevelopmental goal (SDG) 6 - \"Clean water and sanitation for all\". In this\nrealm, we propose a novel and efficient machine learning emulator, more\nprecisely, a physics-informed deep learning (DL) model, for hydraulic state\nestimation in WDS. Using a recursive approach, our model only needs a few graph\nconvolutional neural network (GCN) layers and employs an innovative algorithm\nbased on message passing. Unlike conventional machine learning tasks, the model\nuses hydraulic principles to infer two additional hydraulic state features in\nthe process of reconstructing the available ground truth feature in an\nunsupervised manner. To the best of our knowledge, this is the first DL\napproach to emulate the popular hydraulic simulator EPANET, utilizing no\nadditional information. Like most DL models and unlike the hydraulic simulator,\nour model demonstrates vastly faster emulation times that do not increase\ndrastically with the size of the WDS. Moreover, we achieve high accuracy on the\nground truth and very similar results compared to the hydraulic simulator as\ndemonstrated through experiments on five real-world WDS datasets.\n","authors":["Inaam Ashraf","Janine Strotherm","Luca Hermes","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18570v1.pdf","comment":"Extended version of the paper with the same title published at\n  Proceedings of the AAAI Conference on Artificial Intelligence 2024"},{"id":"http://arxiv.org/abs/2403.18569v1","updated":"2024-03-27T13:50:13Z","published":"2024-03-27T13:50:13Z","title":"PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop\n  Prediction","summary":"  IR drop on the power delivery network (PDN) is closely related to PDN's\nconfiguration and cell current consumption. As the integrated circuit (IC)\ndesign is growing larger, dynamic IR drop simulation becomes computationally\nunaffordable and machine learning based IR drop prediction has been explored as\na promising solution. Although CNN-based methods have been adapted to IR drop\nprediction task in several works, the shortcomings of overlooking PDN\nconfiguration is non-negligible. In this paper, we consider not only how to\nproperly represent cell-PDN relation, but also how to model IR drop following\nits physical nature in the feature aggregation procedure. Thus, we propose a\nnovel graph structure, PDNGraph, to unify the representations of the PDN\nstructure and the fine-grained cell-PDN relation. We further propose a\ndual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN\nbranches to favorably capture the above features during the learning process.\nSeveral key designs are presented to make the dynamic IR drop prediction highly\neffective and interpretable. We are the first work to apply graph structure to\ndeep-learning based dynamic IR drop prediction method. Experiments show that\nPDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%\nreduction in prediction error and achieves 545x speedup compared to the\ncommercial tool, which demonstrates the superiority of our method.\n","authors":["Yuxiang Zhao","Zhuomin Chai","Xun Jiang","Yibo Lin","Runsheng Wang","Ru Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12370v2","updated":"2024-03-27T13:44:21Z","published":"2023-10-18T22:34:32Z","title":"No-Regret Learning in Bilateral Trade via Global Budget Balance","summary":"  Bilateral trade models the problem of intermediating between two rational\nagents -- a seller and a buyer -- both characterized by a private valuation for\nan item they want to trade. We study the online learning version of the\nproblem, in which at each time step a new seller and buyer arrive and the\nlearner has to set prices for them without any knowledge about their\n(adversarially generated) valuations.\n  In this setting, known impossibility results rule out the existence of\nno-regret algorithms when budget balanced has to be enforced at each time step.\nIn this paper, we introduce the notion of \\emph{global budget balance}, which\nonly requires the learner to fulfill budget balance over the entire time\nhorizon. Under this natural relaxation, we provide the first no-regret\nalgorithms for adversarial bilateral trade under various feedback models.\nFirst, we show that in the full-feedback model, the learner can guarantee\n$\\tilde O(\\sqrt{T})$ regret against the best fixed prices in hindsight, and\nthat this bound is optimal up to poly-logarithmic terms. Second, we provide a\nlearning algorithm guaranteeing a $\\tilde O(T^{3/4})$ regret upper bound with\none-bit feedback, which we complement with a $\\Omega(T^{5/7})$ lower bound that\nholds even in the two-bit feedback model. Finally, we introduce and analyze an\nalternative benchmark that is provably stronger than the best fixed prices in\nhindsight and is inspired by the literature on bandits with knapsacks.\n","authors":["Martino Bernasconi","Matteo Castiglioni","Andrea Celli","Federico Fusco"],"pdf_url":"https://arxiv.org/pdf/2310.12370v2.pdf","comment":"Accepted at STOC 2024"},{"id":"http://arxiv.org/abs/2403.18560v1","updated":"2024-03-27T13:42:14Z","published":"2024-03-27T13:42:14Z","title":"Noise-Robust Keyword Spotting through Self-supervised Pretraining","summary":"  Voice assistants are now widely available, and to activate them a keyword\nspotting (KWS) algorithm is used. Modern KWS systems are mainly trained using\nsupervised learning methods and require a large amount of labelled data to\nachieve a good performance. Leveraging unlabelled data through self-supervised\nlearning (SSL) has been shown to increase the accuracy in clean conditions.\nThis paper explores how SSL pretraining such as Data2Vec can be used to enhance\nthe robustness of KWS models in noisy conditions, which is under-explored.\n  Models of three different sizes are pretrained using different pretraining\napproaches and then fine-tuned for KWS. These models are then tested and\ncompared to models trained using two baseline supervised learning methods, one\nbeing standard training using clean data and the other one being multi-style\ntraining (MTR). The results show that pretraining and fine-tuning on clean data\nis superior to supervised learning on clean data across all testing conditions,\nand superior to supervised MTR for testing conditions of SNR above 5 dB. This\nindicates that pretraining alone can increase the model's robustness. Finally,\nit is found that using noisy data for pretraining models, especially with the\nData2Vec-denoising approach, significantly enhances the robustness of KWS\nmodels in noisy conditions.\n","authors":["Jacob Mørk","Holger Severin Bovbjerg","Gergely Kiss","Zheng-Hua Tan"],"pdf_url":"https://arxiv.org/pdf/2403.18560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06712v2","updated":"2024-03-27T13:38:35Z","published":"2024-01-12T17:26:51Z","title":"Few-Shot Detection of Machine-Generated Text using Style Representations","summary":"  The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.\n","authors":["Rafael Rivera Soto","Kailin Koch","Aleem Khan","Barry Chen","Marcus Bishop","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2401.06712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00117v4","updated":"2024-03-27T13:38:00Z","published":"2023-09-29T20:11:15Z","title":"ABScribe: Rapid Exploration & Organization of Multiple Writing\n  Variations in Human-AI Co-Writing Tasks using Large Language Models","summary":"  Exploring alternative ideas by rewriting text is integral to the writing\nprocess. State-of-the-art Large Language Models (LLMs) can simplify writing\nvariation generation. However, current interfaces pose challenges for\nsimultaneous consideration of multiple variations: creating new variations\nwithout overwriting text can be difficult, and pasting them sequentially can\nclutter documents, increasing workload and disrupting writers' flow. To tackle\nthis, we present ABScribe, an interface that supports rapid, yet visually\nstructured, exploration and organization of writing variations in human-AI\nco-writing tasks. With ABScribe, users can swiftly modify variations using LLM\nprompts, which are auto-converted into reusable buttons. Variations are stored\nadjacently within text fields for rapid in-place comparisons using mouse-over\ninteractions on a popup toolbar. Our user study with 12 writers shows that\nABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances\nuser perceptions of the revision process (d = 2.41, p < 0.001) compared to a\npopular baseline workflow, and provides insights into how writers explore\nvariations using LLMs.\n","authors":["Mohi Reza","Nathan Laundry","Ilya Musabirov","Peter Dushniku","Zhi Yuan \"Michael\" Yu","Kashish Mittal","Tovi Grossman","Michael Liut","Anastasia Kuzminykh","Joseph Jay Williams"],"pdf_url":"https://arxiv.org/pdf/2310.00117v4.pdf","comment":"CHI 2024"},{"id":"http://arxiv.org/abs/2403.18542v1","updated":"2024-03-27T13:22:38Z","published":"2024-03-27T13:22:38Z","title":"Attention-aware semantic relevance predicting Chinese sentence reading","summary":"  In recent years, several influential computational models and metrics have\nbeen proposed to predict how humans comprehend and process sentence. One\nparticularly promising approach is contextual semantic similarity. Inspired by\nthe attention algorithm in Transformer and human memory mechanisms, this study\nproposes an ``attention-aware'' approach for computing contextual semantic\nrelevance. This new approach takes into account the different contributions of\ncontextual parts and the expectation effect, allowing it to incorporate\ncontextual information fully. The attention-aware approach also facilitates the\nsimulation of existing reading models and evaluate them. The resulting\n``attention-aware'' metrics of semantic relevance can more accurately predict\nfixation durations in Chinese reading tasks recorded in an eye-tracking corpus\nthan those calculated by existing approaches. The study's findings further\nprovide strong support for the presence of semantic preview benefits in Chinese\nnaturalistic reading. Furthermore, the attention-aware metrics of semantic\nrelevance, being memory-based, possess high interpretability from both\nlinguistic and cognitive standpoints, making them a valuable computational tool\nfor modeling eye-movements in reading and further gaining insight into the\nprocess of language comprehension. Our approach underscores the potential of\nthese metrics to advance our comprehension of how humans understand and process\nlanguage, ultimately leading to a better understanding of language\ncomprehension and processing.\n","authors":["Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2403.18542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18540v1","updated":"2024-03-27T13:17:15Z","published":"2024-03-27T13:17:15Z","title":"skscope: Fast Sparsity-Constrained Optimization in Python","summary":"  Applying iterative solvers on sparsity-constrained optimization (SCO)\nrequires tedious mathematical deduction and careful programming/debugging that\nhinders these solvers' broad impact. In the paper, the library skscope is\nintroduced to overcome such an obstacle. With skscope, users can solve the SCO\nby just programming the objective function. The convenience of skscope is\ndemonstrated through two examples in the paper, where sparse linear regression\nand trend filtering are addressed with just four lines of code. More\nimportantly, skscope's efficient implementation allows state-of-the-art solvers\nto quickly attain the sparse solution regardless of the high dimensionality of\nparameter space. Numerical experiments reveal the available solvers in skscope\ncan achieve up to 80x speedup on the competing relaxation solutions obtained\nvia the benchmarked convex solver. skscope is published on the Python Package\nIndex (PyPI) and Conda, and its source code is available at:\nhttps://github.com/abess-team/skscope.\n","authors":["Zezhi Wang","Jin Zhu","Peng Chen","Huiyang Peng","Xiaoke Zhang","Anran Wang","Yu Zheng","Junxian Zhu","Xueqin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18540v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.18539v1","updated":"2024-03-27T13:14:29Z","published":"2024-03-27T13:14:29Z","title":"Safe and Robust Reinforcement-Learning: Principles and Practice","summary":"  Reinforcement Learning (RL) has shown remarkable success in solving\nrelatively complex tasks, yet the deployment of RL systems in real-world\nscenarios poses significant challenges related to safety and robustness. This\npaper aims to identify and further understand those challenges thorough the\nexploration of the main dimensions of the safe and robust RL landscape,\nencompassing algorithmic, ethical, and practical considerations. We conduct a\ncomprehensive review of methodologies and open problems that summarizes the\nefforts in recent years to address the inherent risks associated with RL\napplications.\n  After discussing and proposing definitions for both safe and robust RL, the\npaper categorizes existing research works into different algorithmic approaches\nthat enhance the safety and robustness of RL agents. We examine techniques such\nas uncertainty estimation, optimisation methodologies, exploration-exploitation\ntrade-offs, and adversarial training. Environmental factors, including\nsim-to-real transfer and domain adaptation, are also scrutinized to understand\nhow RL systems can adapt to diverse and dynamic surroundings. Moreover, human\ninvolvement is an integral ingredient of the analysis, acknowledging the broad\nset of roles that humans can take in this context.\n  Importantly, to aid practitioners in navigating the complexities of safe and\nrobust RL implementation, this paper introduces a practical checklist derived\nfrom the synthesized literature. The checklist encompasses critical aspects of\nalgorithm design, training environment considerations, and ethical guidelines.\nIt will serve as a resource for developers and policymakers alike to ensure the\nresponsible deployment of RL systems in many application domains.\n","authors":["Taku Yamagata","Raul Santos-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2403.18539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18535v1","updated":"2024-03-27T13:11:34Z","published":"2024-03-27T13:11:34Z","title":"Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs","summary":"  Recent studies reveal a significant theoretical link between variational\nautoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to\nestimate the theoretical upper bound of the information rate-distortion\nfunction of images. Such estimated theoretical bounds substantially exceed the\nperformance of existing neural image codecs (NICs). To narrow this gap, we\npropose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The\nproposed BG-VAE leverages the theoretical bound to guide the NIC model towards\nenhanced performance. We implement the BG-VAE using Hierarchical VAEs and\ndemonstrate its effectiveness through extensive experiments. Along with\nadvanced neural network blocks, we provide a versatile, variable-rate NIC that\noutperforms existing methods when considering both rate-distortion performance\nand computational complexity. The code is available at BG-VAE.\n","authors":["Yichi Zhang","Zhihao Duan","Yuning Huang","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.18535v1.pdf","comment":"2024 IEEE International Conference on Multimedia and Expo (ICME2024)"},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2303.10365v3","updated":"2024-03-27T12:53:12Z","published":"2023-03-18T08:48:16Z","title":"CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label\n  Learning","summary":"  Partial-label learning (PLL) is an important weakly supervised learning\nproblem, which allows each training example to have a candidate label set\ninstead of a single ground-truth label. Identification-based methods have been\nwidely explored to tackle label ambiguity issues in PLL, which regard the true\nlabel as a latent variable to be identified. However, identifying the true\nlabels accurately and completely remains challenging, causing noise in pseudo\nlabels during model training. In this paper, we propose a new method called\nCroSel, which leverages historical predictions from the model to identify true\nlabels for most training examples. First, we introduce a cross selection\nstrategy, which enables two deep models to select true labels of partially\nlabeled data for each other. Besides, we propose a novel consistency\nregularization term called co-mix to avoid sample waste and tiny noise caused\nby false selection. In this way, CroSel can pick out the true labels of most\nexamples with high precision. Extensive experiments demonstrate the superiority\nof CroSel, which consistently outperforms previous state-of-the-art methods on\nbenchmark datasets. Additionally, our method achieves over 90\\% accuracy and\nquantity for selecting true labels on CIFAR-type datasets under various\nsettings.\n","authors":["Shiyu Tian","Hongxin Wei","Yiqun Wang","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2303.10365v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18519v1","updated":"2024-03-27T12:50:27Z","published":"2024-03-27T12:50:27Z","title":"Improving Line Search Methods for Large Scale Neural Network Training","summary":"  In recent studies, line search methods have shown significant improvements in\nthe performance of traditional stochastic gradient descent techniques,\neliminating the need for a specific learning rate schedule. In this paper, we\nidentify existing issues in state-of-the-art line search methods, propose\nenhancements, and rigorously evaluate their effectiveness. We test these\nmethods on larger datasets and more complex data domains than before.\nSpecifically, we improve the Armijo line search by integrating the momentum\nterm from ADAM in its search direction, enabling efficient large-scale\ntraining, a task that was previously prone to failure using Armijo line search\nmethods. Our optimization approach outperforms both the previous Armijo\nimplementation and tuned learning rate schedules for Adam. Our evaluation\nfocuses on Transformers and CNNs in the domains of NLP and image data. Our work\nis publicly available as a Python package, which provides a hyperparameter free\nPytorch optimizer.\n","authors":["Philip Kenneweg","Tristan Kenneweg","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18517v1","updated":"2024-03-27T12:49:14Z","published":"2024-03-27T12:49:14Z","title":"Efficient Algorithms for Regularized Nonnegative Scale-invariant\n  Low-rank Approximation Models","summary":"  Regularized nonnegative low-rank approximations such as sparse Nonnegative\nMatrix Factorization or sparse Nonnegative Tucker Decomposition are an\nimportant branch of dimensionality reduction models with enhanced\ninterpretability. However, from a practical perspective, the choice of\nregularizers and regularization coefficients, as well as the design of\nefficient algorithms, is challenging because of the multifactor nature of these\nmodels and the lack of theory to back these choices. This paper aims at\nimproving upon these issues. By studying a more general model called the\nHomogeneous Regularized Scale-Invariant, we prove that the scale-invariance\ninherent to low-rank approximation models causes an implicit regularization\nwith both unexpected beneficial and detrimental effects. This observation\nallows to better understand the effect of regularization functions in low-rank\napproximation models, to guide the choice of the regularization\nhyperparameters, and to design balancing strategies to enhance the convergence\nspeed of dedicated optimization algorithms. Some of these results were already\nknown but restricted to specific instances of regularized low-rank\napproximations. We also derive a generic Majorization Minimization algorithm\nthat handles many regularized nonnegative low-rank approximations, with\nconvergence guarantees. We showcase our contributions on sparse Nonnegative\nMatrix Factorization, ridge-regularized Canonical Polyadic decomposition and\nsparse Nonnegative Tucker Decomposition.\n","authors":["Jeremy E. Cohen","Valentin Leplat"],"pdf_url":"https://arxiv.org/pdf/2403.18517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18514v1","updated":"2024-03-27T12:44:57Z","published":"2024-03-27T12:44:57Z","title":"CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection\n  of Pathological Pulmonary CT scans","summary":"  Unsupervised pathology detection can be implemented by training a model on\nhealthy data only and measuring the deviation from the training set upon\ninference, for example with CNN-based feature extraction and one-class\nclassifiers, or reconstruction-score-based methods such as AEs, GANs and\nDiffusion models. Normalizing Flows (NF) have the ability to directly learn the\nprobability distribution of training examples through an invertible\narchitecture. We leverage this property in a novel 3D NF-based model named\nCT-3DFlow, specifically tailored for patient-level pulmonary pathology\ndetection in chest CT data. Our model is trained unsupervised on healthy 3D\npulmonary CT patches, and detects deviations from its log-likelihood\ndistribution as anomalies. We aggregate patches-level likelihood values from a\npatient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.\nOut-of-distribution detection performance is evaluated using expert annotations\non a separate chest CT test dataset, outperforming other state-of-the-art\nmethods.\n","authors":["Aissam Djahnine","Alexandre Popoff","Emilien Jupin-Delevaux","Vincent Cottin","Olivier Nempont","Loic Boussel"],"pdf_url":"https://arxiv.org/pdf/2403.18514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18509v1","updated":"2024-03-27T12:39:16Z","published":"2024-03-27T12:39:16Z","title":"Distributed Maximum Consensus over Noisy Links","summary":"  We introduce a distributed algorithm, termed noise-robust distributed maximum\nconsensus (RD-MC), for estimating the maximum value within a multi-agent\nnetwork in the presence of noisy communication links. Our approach entails\nredefining the maximum consensus problem as a distributed optimization problem,\nallowing a solution using the alternating direction method of multipliers.\nUnlike existing algorithms that rely on multiple sets of noise-corrupted\nestimates, RD-MC employs a single set, enhancing both robustness and\nefficiency. To further mitigate the effects of link noise and improve\nrobustness, we apply moving averaging to the local estimates. Through extensive\nsimulations, we demonstrate that RD-MC is significantly more robust to\ncommunication link noise compared to existing maximum-consensus algorithms.\n","authors":["Ehsan Lari","Reza Arablouei","Naveen K. D. Venkategowda","Stefan Werner"],"pdf_url":"https://arxiv.org/pdf/2403.18509v1.pdf","comment":"5 pages, 7 figures, submitted to EUSIPCO 2024 conference"},{"id":"http://arxiv.org/abs/2403.18506v1","updated":"2024-03-27T12:35:23Z","published":"2024-03-27T12:35:23Z","title":"Faster Convergence for Transformer Fine-tuning with Line Search Methods","summary":"  Recent works have shown that line search methods greatly increase performance\nof traditional stochastic gradient descent methods on a variety of datasets and\narchitectures [1], [2]. In this work we succeed in extending line search\nmethods to the novel and highly popular Transformer architecture and dataset\ndomains in natural language processing. More specifically, we combine the\nArmijo line search with the Adam optimizer and extend it by subdividing the\nnetworks architecture into sensible units and perform the line search\nseparately on these local units. Our optimization method outperforms the\ntraditional Adam optimizer and achieves significant performance improvements\nfor small data sets or small training budgets, while performing equal or better\nfor other tested cases. Our work is publicly available as a python package,\nwhich provides a hyperparameter-free pytorch optimizer that is compatible with\narbitrary network architectures.\n","authors":["Philip Kenneweg","Leonardo Galli","Tristan Kenneweg","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08579v2","updated":"2024-03-27T12:28:02Z","published":"2024-03-13T14:34:34Z","title":"Machine Learning Optimized Orthogonal Basis Piecewise Polynomial\n  Approximation","summary":"  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,\nlike trajectory planning, to approximate position profiles given in the form of\na set of points. While the approximation target along with domain-specific\nrequirements, like Ck -continuity, can be formulated as a system of equations\nand a result can be computed directly, such closed-form solutions posses\nlimited flexibility with respect to polynomial degrees, polynomial bases or\nadding further domain-specific requirements. Sufficiently complex optimization\ngoals soon call for the use of numerical methods, like gradient descent. Since\ngradient descent lies at the heart of training Artificial Neural Networks\n(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set\nof gradient-based optimizers potentially suitable for a wide range of\noptimization problems beyond the training task for ANNs. Our approach is to\nutilize the versatility of PP models and combine it with the potential of\nmodern ML optimizers for the use in function approximation in 1D trajectory\nplanning in the context of electronic cam design. We utilize available\noptimizers of the ML framework TensorFlow directly, outside of the scope of\nANNs, to optimize model parameters of our PP model. In this paper, we show how\nan orthogonal polynomial basis contributes to improving approximation and\ncontinuity optimization performance. Utilizing Chebyshev polynomials of the\nfirst kind, we develop a novel regularization approach enabling clearly\nimproved convergence behavior. We show that, using this regularization\napproach, Chebyshev basis performs better than power basis for all relevant\noptimizers in the combined approximation and continuity optimization setting\nand demonstrate usability of the presented approach within the electronic cam\ndomain.\n","authors":["Hannes Waclawek","Stefan Huber"],"pdf_url":"https://arxiv.org/pdf/2403.08579v2.pdf","comment":"Submitted to LION18"},{"id":"http://arxiv.org/abs/2311.04698v3","updated":"2024-03-27T12:24:17Z","published":"2023-11-08T14:10:19Z","title":"Challenging Common Paradigms in Multi-Task Learning","summary":"  While multi-task learning (MTL) has gained significant attention in recent\nyears, its underlying mechanisms remain poorly understood. Recent methods did\nnot yield consistent performance improvements over single task learning (STL)\nbaselines, underscoring the importance of gaining more profound insights about\nchallenges specific to MTL. In our study, we challenge paradigms in MTL in the\ncontext of STL: First, the impact of the choice of optimizer has only been\nmildly investigated in MTL. We show the pivotal role of common STL tools such\nas the Adam optimizer in MTL empirically in various experiments. To further\ninvestigate Adam's effectiveness, we theoretical derive a partial loss-scale\ninvariance under mild assumptions. Second, the notion of gradient conflicts has\noften been phrased as a specific problem in MTL. We delve into the role of\ngradient conflicts in MTL and compare it to STL. For angular gradient alignment\nwe find no evidence that this is a unique problem in MTL. We emphasize\ndifferences in gradient magnitude as the main distinguishing factor. Lastly, we\ncompare the transferability of features learned through MTL and STL on common\nimage corruptions, and find light evidence that MTL can lead to superior\ntransferability. Overall, we find surprising similarities between STL and MTL\nsuggesting to consider methods from both fields in a broader context.\n","authors":["Cathrin Elich","Lukas Kirchdorfer","Jan M. Köhler","Lukas Schott"],"pdf_url":"https://arxiv.org/pdf/2311.04698v3.pdf","comment":"-"},{"id":"http://arxiv.org/abs/2403.18495v1","updated":"2024-03-27T12:15:22Z","published":"2024-03-27T12:15:22Z","title":"Direct mineral content prediction from drill core images via transfer\n  learning","summary":"  Deep subsurface exploration is important for mining, oil and gas industries,\nas well as in the assessment of geological units for the disposal of chemical\nor nuclear waste, or the viability of geothermal energy systems. Typically,\ndetailed examinations of subsurface formations or units are performed on\ncuttings or core materials extracted during drilling campaigns, as well as on\ngeophysical borehole data, which provide detailed information about the\npetrophysical properties of the rocks. Depending on the volume of rock samples\nand the analytical program, the laboratory analysis and diagnostics can be very\ntime-consuming. This study investigates the potential of utilizing machine\nlearning, specifically convolutional neural networks (CNN), to assess the\nlithology and mineral content solely from analysis of drill core images, aiming\nto support and expedite the subsurface geological exploration. The paper\noutlines a comprehensive methodology, encompassing data preprocessing, machine\nlearning methods, and transfer learning techniques. The outcome reveals a\nremarkable 96.7% accuracy in the classification of drill core segments into\ndistinct formation classes. Furthermore, a CNN model was trained for the\nevaluation of mineral content using a learning data set from multidimensional\nlog analysis data (silicate, total clay, carbonate). When benchmarked against\nlaboratory XRD measurements on samples from the cores, both the advanced\nmultidimensional log analysis model and the neural network approach developed\nhere provide equally good performance. This work demonstrates that deep\nlearning and particularly transfer learning can support extracting\npetrophysical properties, including mineral content and formation\nclassification, from drill core images, thus offering a road map for enhancing\nmodel performance and data set quality in image-based analysis of drill cores.\n","authors":["Romana Boiger","Sergey V. Churakov","Ignacio Ballester Llagaria","Georg Kosakowski","Raphael Wüst","Nikolaos I. Prasianakis"],"pdf_url":"https://arxiv.org/pdf/2403.18495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18494v1","updated":"2024-03-27T12:10:30Z","published":"2024-03-27T12:10:30Z","title":"Learning in PINNs: Phase transition, total diffusion, and generalization","summary":"  We investigate the learning dynamics of fully-connected neural networks\nthrough the lens of gradient signal-to-noise ratio (SNR), examining the\nbehavior of first-order optimizers like Adam in non-convex objectives. By\ninterpreting the drift/diffusion phases in the information bottleneck theory,\nfocusing on gradient homogeneity, we identify a third phase termed ``total\ndiffusion\", characterized by equilibrium in the learning rates and homogeneous\ngradients. This phase is marked by an abrupt SNR increase, uniform residuals\nacross the sample space and the most rapid training convergence. We propose a\nresidual-based re-weighting scheme to accelerate this diffusion in quadratic\nloss functions, enhancing generalization. We also explore the information\ncompression phenomenon, pinpointing a significant saturation-induced\ncompression of activations at the total diffusion phase, with deeper layers\nexperiencing negligible information loss. Supported by experimental data on\nphysics-informed neural networks (PINNs), which underscore the importance of\ngradient homogeneity due to their PDE-based sample inter-dependence, our\nfindings suggest that recognizing phase transitions could refine ML\noptimization strategies for improved generalization.\n","authors":["Sokratis J. Anagnostopoulos","Juan Diego Toscano","Nikolaos Stergiopulos","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2403.18494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18489v1","updated":"2024-03-27T12:01:51Z","published":"2024-03-27T12:01:51Z","title":"Impact of Employing Weather Forecast Data as Input to the Estimation of\n  Evapotranspiration by Deep Neural Network Models","summary":"  Reference Evapotranspiration (ET0) is a key parameter for designing smart\nirrigation scheduling, since it is related by a coefficient to the water needs\nof a crop. The United Nations Food and Agriculture Organization, proposed a\nstandard method for ET0 computation (FAO56PM), based on the parameterization of\nthe Penman-Monteith equation, that is widely adopted in the literature. To\ncompute ET0 using the FAO56-PM method, four main weather parameters are needed:\ntemperature, humidity, wind, and solar radiation (SR). One way to make daily\nET0 estimations for future days is to use freely available weather forecast\nservices (WFSs), where many meteorological parameters are estimated up to the\nnext 15 days. A problem with this method is that currently, SR is not provided\nas a free forecast parameter on most of those online services or, normally,\nsuch forecasts present a financial cost penalty. For this reason, several ET0\nestimation models using machine and deep learning were developed and presented\nin the literature, that use as input features a reduced set of carefully\nselected weather parameters, that are compatible with common freely available\nWFSs. However, most studies on this topic have only evaluated model performance\nusing data from weather stations (WSs), without considering the effect of using\nweather forecast data. In this study, the performance of authors' previous\nmodels is evaluated when using weather forecast data from two online WFSs, in\nthe following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)\nestimate SR by ANN model, and then use that estimation for ET0 computation,\nusing the FAO56-PM method. Employing data collected from two WFSs and a WS\nlocated in Vale do Lobo, Portugal, the latter approach achieved the best\nresult, with a coefficient of determination (R2) ranging between 0.893 and\n0.667, when considering forecasts up to 15 days.\n","authors":["Pedro J. Vaz","Gabriela Schütz","Carlos Guerrero","Pedro J. S. Cardoso"],"pdf_url":"https://arxiv.org/pdf/2403.18489v1.pdf","comment":"A partial version of the work submitted to ESRE/INTERNATIONAL\n  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY"},{"id":"http://arxiv.org/abs/2403.18486v1","updated":"2024-03-27T11:58:45Z","published":"2024-03-27T11:58:45Z","title":"Synthesizing EEG Signals from Event-Related Potential Paradigms with\n  Conditional Diffusion Models","summary":"  Data scarcity in the brain-computer interface field can be alleviated through\nthe use of generative models, specifically diffusion models. While diffusion\nmodels have previously been successfully applied to electroencephalogram (EEG)\ndata, existing models lack flexibility w.r.t.~sampling or require alternative\nrepresentations of the EEG data. To overcome these limitations, we introduce a\nnovel approach to conditional diffusion models that utilizes classifier-free\nguidance to directly generate subject-, session-, and class-specific EEG data.\nIn addition to commonly used metrics, domain-specific metrics are employed to\nevaluate the specificity of the generated samples. The results indicate that\nthe proposed model can generate EEG data that resembles real data for each\nsubject, session, and class.\n","authors":["Guido Klein","Pierre Guetschel","Gianluigi Silvestri","Michael Tangermann"],"pdf_url":"https://arxiv.org/pdf/2403.18486v1.pdf","comment":"submitted to 9th Graz BCI conference, 6 pages, 3 figures, first\n  figure is split into two subfigures, 1 table"},{"id":"http://arxiv.org/abs/2311.12028v2","updated":"2024-03-27T11:43:28Z","published":"2023-11-20T18:59:51Z","title":"Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose\n  Estimation","summary":"  Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a plug-and-play pruning-and-recovering framework,\ncalled Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose\nestimation from videos. Our HoT begins with pruning pose tokens of redundant\nframes and ends with recovering full-length tokens, resulting in a few pose\ntokens in the intermediate transformer blocks and thus improving the model\nefficiency. To effectively achieve this, we propose a token pruning cluster\n(TPC) that dynamically selects a few representative tokens with high semantic\ndiversity while eliminating the redundancy of video frames. In addition, we\ndevelop a token recovering attention (TRA) to restore the detailed\nspatio-temporal information based on the selected tokens, thereby expanding the\nnetwork output to the original full-length temporal resolution for fast\ninference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and\nMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and\nestimation accuracy compared to the original VPT models. For instance, applying\nto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs\nwithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,\nrespectively. Code and models are available at\nhttps://github.com/NationalGAILab/HoT.\n","authors":["Wenhao Li","Mengyuan Liu","Hong Liu","Pichao Wang","Jialun Cai","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2311.12028v2.pdf","comment":"Accepted by CVPR 2024, Open Sourced"},{"id":"http://arxiv.org/abs/2403.18452v1","updated":"2024-03-27T11:11:08Z","published":"2024-03-27T11:11:08Z","title":"SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model","summary":"  There are five types of trajectory prediction tasks: deterministic,\nstochastic, domain adaptation, momentary observation, and few-shot. These\nassociated tasks are defined by various factors, such as the length of input\npaths, data split and pre-processing methods. Interestingly, even though they\ncommonly take sequential coordinates of observations as input and infer future\npaths in the same coordinates as output, designing specialized architectures\nfor each task is still necessary. For the other task, generality issues can\nlead to sub-optimal performances. In this paper, we propose SingularTrajectory,\na diffusion-based universal trajectory prediction framework to reduce the\nperformance gap across the five tasks. The core of SingularTrajectory is to\nunify a variety of human dynamics representations on the associated tasks. To\ndo this, we first build a Singular space to project all types of motion\npatterns from each task into one embedding space. We next propose an adaptive\nanchor working in the Singular space. Unlike traditional fixed anchor methods\nthat sometimes yield unacceptable paths, our adaptive anchor enables correct\nanchors, which are put into a wrong location, based on a traversability map.\nFinally, we adopt a diffusion-based predictor to further enhance the prototype\npaths using a cascaded denoising process. Our unified framework ensures the\ngenerality across various benchmark settings such as input modality, and\ntrajectory lengths. Extensive experiments on five public benchmarks demonstrate\nthat SingularTrajectory substantially outperforms existing models, highlighting\nits effectiveness in estimating general dynamics of human movements. Code is\npublicly available at https://github.com/inhwanbae/SingularTrajectory .\n","authors":["Inhwan Bae","Young-Jae Park","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18452v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18451v1","updated":"2024-03-27T11:11:06Z","published":"2024-03-27T11:11:06Z","title":"CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in\n  Resource-Constrained CPS and IoT","summary":"  Foundation models (FMs) emerge as a promising solution to harness distributed\nand diverse environmental data by leveraging prior knowledge to understand the\ncomplicated temporal and spatial correlations within heterogeneous datasets.\nUnlike distributed learning frameworks such as federated learning, which often\nstruggle with multimodal data, FMs can transform diverse inputs into\nembeddings. This process facilitates the integration of information from\nvarious modalities and the application of prior learning to new domains.\nHowever, deploying FMs in resource-constrained edge systems poses significant\nchallenges. To this end, we introduce CoRAST, a novel learning framework that\nutilizes FMs for enhanced analysis of distributed, correlated heterogeneous\ndata. Utilizing a server-based FM, CoRAST can exploit existing environment\ninformation to extract temporal, spatial, and cross-modal correlations among\nsensor data. This enables CoRAST to offer context-aware insights for localized\nclient tasks through FM-powered global representation learning. Our evaluation\non real-world weather dataset demonstrates CoRAST's ability to exploit\ncorrelated heterogeneous data through environmental representation learning to\nreduce the forecast errors by up to 50.3% compared to the baselines.\n","authors":["Yi Hu","Jinhang Zuo","Alanis Zhao","Bob Iannucci","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2403.18451v1.pdf","comment":"accepted and to be published in 2024 IEEE International Workshop on\n  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)"},{"id":"http://arxiv.org/abs/2403.09267v3","updated":"2024-03-27T11:11:02Z","published":"2024-03-14T10:44:10Z","title":"Deep Limit Order Book Forecasting","summary":"  We exploit cutting-edge deep learning methodologies to explore the\npredictability of high-frequency Limit Order Book mid-price changes for a\nheterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we\nrelease `LOBFrame', an open-source code base to efficiently process large-scale\nLimit Order Book data and quantitatively assess state-of-the-art deep learning\nmodels' forecasting capabilities. Our results are twofold. We demonstrate that\nthe stocks' microstructural characteristics influence the efficacy of deep\nlearning methods and that their high forecasting power does not necessarily\ncorrespond to actionable trading signals. We argue that traditional machine\nlearning metrics fail to adequately assess the quality of forecasts in the\nLimit Order Book context. As an alternative, we propose an innovative\noperational framework that evaluates predictions' practicality by focusing on\nthe probability of accurately forecasting complete transactions. This work\noffers academics and practitioners an avenue to make informed and robust\ndecisions on the application of deep learning techniques, their scope and\nlimitations, effectively exploiting emergent statistical properties of the\nLimit Order Book.\n","authors":["Antonio Briola","Silvia Bartolucci","Tomaso Aste"],"pdf_url":"https://arxiv.org/pdf/2403.09267v3.pdf","comment":"43 pages, 14 figures, 12 Tables"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18444v1","updated":"2024-03-27T11:00:53Z","published":"2024-03-27T11:00:53Z","title":"FRESCO: Federated Reinforcement Energy System for Cooperative\n  Optimization","summary":"  The rise in renewable energy is creating new dynamics in the energy grid that\npromise to create a cleaner and more participative energy grid, where\ntechnology plays a crucial part in making the required flexibility to achieve\nthe vision of the next-generation grid. This work presents FRESCO, a framework\nthat aims to ease the implementation of energy markets using a hierarchical\ncontrol architecture of reinforcement learning agents trained using federated\nlearning. The core concept we are proving is that having greedy agents subject\nto changing conditions from a higher level agent creates a cooperative setup\nthat will allow for fulfilling all the individual objectives. This paper\npresents a general overview of the framework, the current progress, and some\ninsights we obtained from the recent results.\n","authors":["Nicolas Mauricio Cuadrado","Roberto Alejandro Gutierrez","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2403.18444v1.pdf","comment":"Tiny Paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2403.18439v1","updated":"2024-03-27T10:47:06Z","published":"2024-03-27T10:47:06Z","title":"Generalized Policy Learning for Smart Grids: FL TRPO Approach","summary":"  The smart grid domain requires bolstering the capabilities of existing energy\nmanagement systems; Federated Learning (FL) aligns with this goal as it\ndemonstrates a remarkable ability to train models on heterogeneous datasets\nwhile maintaining data privacy, making it suitable for smart grid applications,\nwhich often involve disparate data distributions and interdependencies among\nfeatures that hinder the suitability of linear models. This paper introduces a\nframework that combines FL with a Trust Region Policy Optimization (FL TRPO)\naiming to reduce energy-associated emissions and costs. Our approach reveals\nlatent interconnections and employs personalized encoding methods to capture\nunique insights, understanding the relationships between features and optimal\nstrategies, allowing our model to generalize to previously unseen data.\nExperimental results validate the robustness of our approach, affirming its\nproficiency in effectively learning policy models for smart grid challenges.\n","authors":["Yunxiang Li","Nicolas Mauricio Cuadrado","Samuel Horváth","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2403.18439v1.pdf","comment":"ICLR 2024 Workshop: Tackling Climate Change with Machine Learning"},{"id":"http://arxiv.org/abs/2403.18438v1","updated":"2024-03-27T10:45:16Z","published":"2024-03-27T10:45:16Z","title":"Global Vegetation Modeling with Pre-Trained Weather Transformers","summary":"  Accurate vegetation models can produce further insights into the complex\ninteraction between vegetation activity and ecosystem processes. Previous\nresearch has established that long-term trends and short-term variability of\ntemperature and precipitation affect vegetation activity. Motivated by the\nrecent success of Transformer-based Deep Learning models for medium-range\nweather forecasting, we adapt the publicly available pre-trained FourCastNet to\nmodel vegetation activity while accounting for the short-term dynamics of\nclimate variability. We investigate how the learned global representation of\nthe atmosphere's state can be transferred to model the normalized difference\nvegetation index (NDVI). Our model globally estimates vegetation activity at a\nresolution of \\SI{0.25}{\\degree} while relying only on meteorological data. We\ndemonstrate that leveraging pre-trained weather models improves the NDVI\nestimates compared to learning an NDVI model from scratch. Additionally, we\ncompare our results to other recent data-driven NDVI modeling approaches from\nmachine learning and ecology literature. We further provide experimental\nevidence on how much data and training time is necessary to turn FourCastNet\ninto an effective vegetation model. Code and models will be made available upon\npublication.\n","authors":["Pascal Janetzky","Florian Gallusser","Simon Hentschel","Andreas Hotho","Anna Krause"],"pdf_url":"https://arxiv.org/pdf/2403.18438v1.pdf","comment":"Tackling Climate Change with Machine Learning Workshop @ ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18436v1","updated":"2024-03-27T10:40:27Z","published":"2024-03-27T10:40:27Z","title":"Collaborative Active Learning in Conditional Trust Environment","summary":"  In this paper, we investigate collaborative active learning, a paradigm in\nwhich multiple collaborators explore a new domain by leveraging their combined\nmachine learning capabilities without disclosing their existing data and\nmodels. Instead, the collaborators share prediction results from the new domain\nand newly acquired labels. This collaboration offers several advantages: (a) it\naddresses privacy and security concerns by eliminating the need for direct\nmodel and data disclosure; (b) it enables the use of different data sources and\ninsights without direct data exchange; and (c) it promotes cost-effectiveness\nand resource efficiency through shared labeling costs. To realize these\nbenefits, we introduce a collaborative active learning framework designed to\nfulfill the aforementioned objectives. We validate the effectiveness of the\nproposed framework through simulations. The results demonstrate that\ncollaboration leads to higher AUC scores compared to independent efforts,\nhighlighting the framework's ability to overcome the limitations of individual\nmodels. These findings support the use of collaborative approaches in active\nlearning, emphasizing their potential to enhance outcomes through collective\nexpertise and shared resources. Our work provides a foundation for further\nresearch on collaborative active learning and its practical applications in\nvarious domains where data privacy, cost efficiency, and model performance are\ncritical considerations.\n","authors":["Zan-Kai Chong","Hiroyuki Ohsaki","Bryan Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18436v1.pdf","comment":"5 pages, 9 figures, conference"},{"id":"http://arxiv.org/abs/2403.18425v1","updated":"2024-03-27T10:26:42Z","published":"2024-03-27T10:26:42Z","title":"U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models","summary":"  Diffusion models have demonstrated remarkable performance in text-to-image\nsynthesis, producing realistic and high resolution images that faithfully\nadhere to the corresponding text-prompts. Despite their great success, they\nstill fall behind in sketch-to-image synthesis tasks, where in addition to\ntext-prompts, the spatial layout of the generated images has to closely follow\nthe outlines of certain reference sketches. Employing an MLP latent edge\npredictor to guide the spatial layout of the synthesized image by predicting\nedge maps at each denoising step has been recently proposed. Despite yielding\npromising results, the pixel-wise operation of the MLP does not take into\naccount the spatial layout as a whole, and demands numerous denoising\niterations to produce satisfactory images, leading to time inefficiency. To\nthis end, we introduce U-Sketch, a framework featuring a U-Net type latent edge\npredictor, which is capable of efficiently capturing both local and global\nfeatures, as well as spatial correlations between pixels. Moreover, we propose\nthe addition of a sketch simplification network that offers the user the choice\nof preprocessing and simplifying input sketches for enhanced outputs. The\nexperimental results, corroborated by user feedback, demonstrate that our\nproposed U-Net latent edge predictor leads to more realistic results, that are\nbetter aligned with the spatial outlines of the reference sketches, while\ndrastically reducing the number of required denoising steps and, consequently,\nthe overall execution time.\n","authors":["Ilias Mitsouras","Eleftherios Tsonis","Paraskevi Tzouveli","Athanasios Voulodimos"],"pdf_url":"https://arxiv.org/pdf/2403.18425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18423v1","updated":"2024-03-27T10:24:25Z","published":"2024-03-27T10:24:25Z","title":"SemRoDe: Macro Adversarial Training to Learn Representations That are\n  Robust to Word-Level Attacks","summary":"  Language models (LMs) are indispensable tools for natural language processing\ntasks, but their vulnerability to adversarial attacks remains a concern. While\ncurrent research has explored adversarial training techniques, their\nimprovements to defend against word-level attacks have been limited. In this\nwork, we propose a novel approach called Semantic Robust Defence (SemRoDe), a\nMacro Adversarial Training strategy to enhance the robustness of LMs. Drawing\ninspiration from recent studies in the image domain, we investigate and later\nconfirm that in a discrete data setting such as language, adversarial samples\ngenerated via word substitutions do indeed belong to an adversarial domain\nexhibiting a high Wasserstein distance from the base domain. Our method learns\na robust representation that bridges these two domains. We hypothesize that if\nsamples were not projected into an adversarial domain, but instead to a domain\nwith minimal shift, it would improve attack robustness. We align the domains by\nincorporating a new distance-based objective. With this, our model is able to\nlearn more generalized representations by aligning the model's high-level\noutput features and therefore better handling unseen adversarial samples. This\nmethod can be generalized across word embeddings, even when they share minimal\noverlap at both vocabulary and word-substitution levels. To evaluate the\neffectiveness of our approach, we conduct experiments on BERT and RoBERTa\nmodels on three datasets. The results demonstrate promising state-of-the-art\nrobustness.\n","authors":["Brian Formento","Wenjie Feng","Chuan Sheng Foo","Luu Anh Tuan","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18423v1.pdf","comment":"Published in NAACL 2024 (Main Track)"},{"id":"http://arxiv.org/abs/2402.01739v2","updated":"2024-03-27T10:21:24Z","published":"2024-01-29T12:05:02Z","title":"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models","summary":"  To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.\n","authors":["Fuzhao Xue","Zian Zheng","Yao Fu","Jinjie Ni","Zangwei Zheng","Wangchunshu Zhou","Yang You"],"pdf_url":"https://arxiv.org/pdf/2402.01739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01191v2","updated":"2024-03-27T10:12:31Z","published":"2023-11-02T12:36:19Z","title":"VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node\n  Classification","summary":"  Class imbalance in graph data presents significant challenges for node\nclassification. While existing methods, such as SMOTE-based approaches,\npartially mitigate this issue, they still exhibit limitations in constructing\nimbalanced graphs. Generative self-supervised learning (SSL) methods,\nexemplified by graph autoencoders (GAEs), offer a promising solution by\ndirectly generating minority nodes from the data itself, yet their potential\nremains underexplored. In this paper, we delve into the shortcomings of\nSMOTE-based approaches in the construction of imbalanced graphs. Furthermore,\nwe introduce VIGraph, a simple yet effective generative SSL approach that\nrelies on the Variational GAE as the fundamental model. VIGraph strictly\nadheres to the concept of imbalance when constructing imbalanced graphs and\ninnovatively leverages the variational inference (VI) ability of Variational\nGAE to generate nodes for minority classes. VIGraph introduces comprehensive\ntraining strategies, including cross-view contrastive learning at the decoding\nphase to capture semantic knowledge, adjacency matrix reconstruction to\npreserve graph structure, and alignment strategy to ensure stable training.\nVIGraph can generate high-quality nodes directly usable for classification,\neliminating the need to integrate the generated nodes back to the graph as well\nas additional retraining found in SMOTE-based methods. We conduct extensive\nexperiments, results from which demonstrate the superiority and generality of\nour approach.\n","authors":["Yulan Hu","Sheng Ouyang","Zhirui Yang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2311.01191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18415v1","updated":"2024-03-27T10:06:33Z","published":"2024-03-27T10:06:33Z","title":"The Topos of Transformer Networks","summary":"  The transformer neural network has significantly out-shined all other neural\nnetwork architectures as the engine behind large language models. We provide a\ntheoretical analysis of the expressivity of the transformer architecture\nthrough the lens of topos theory. From this viewpoint, we show that many common\nneural network architectures, such as the convolutional, recurrent and graph\nconvolutional networks, can be embedded in a pretopos of piecewise-linear\nfunctions, but that the transformer necessarily lives in its topos completion.\nIn particular, this suggests that the two network families instantiate\ndifferent fragments of logic: the former are first order, whereas transformers\nare higher-order reasoners. Furthermore, we draw parallels with architecture\nsearch and gradient descent, integrating our analysis in the framework of\ncybernetic agents.\n","authors":["Mattia Jacopo Villani","Peter McBurney"],"pdf_url":"https://arxiv.org/pdf/2403.18415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06075v2","updated":"2024-03-27T09:51:15Z","published":"2023-09-12T09:12:37Z","title":"A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel\n  Segmentation via Two-Phase Training Angiography-to-Venography Translation","summary":"  We present a semi-supervised domain adaptation framework for brain vessel\nsegmentation from different image modalities. Existing state-of-the-art methods\nfocus on a single modality, despite the wide range of available cerebrovascular\nimaging techniques. This can lead to significant distribution shifts that\nnegatively impact the generalization across modalities. By relying on annotated\nangiographies and a limited number of annotated venographies, our framework\naccomplishes image-to-image translation and semantic segmentation, leveraging a\ndisentangled and semantically rich latent space to represent heterogeneous data\nand perform image-level adaptation from source to target domains. Moreover, we\nreduce the typical complexity of cycle-based architectures and minimize the use\nof adversarial training, which allows us to build an efficient and intuitive\nmodel with stable training. We evaluate our method on magnetic resonance\nangiographies and venographies. While achieving state-of-the-art performance in\nthe source domain, our method attains a Dice score coefficient in the target\ndomain that is only 8.9% lower, highlighting its promising potential for robust\ncerebrovascular image segmentation across different modalities.\n","authors":["Francesco Galati","Daniele Falcetta","Rosa Cortese","Barbara Casolla","Ferran Prados","Ninon Burgos","Maria A. Zuluaga"],"pdf_url":"https://arxiv.org/pdf/2309.06075v2.pdf","comment":"Accepted at the 34th British Machine Vision Conference (BMVC)"},{"id":"http://arxiv.org/abs/2310.05723v2","updated":"2024-03-27T09:48:34Z","published":"2023-10-09T13:47:05Z","title":"Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement\n  Learning","summary":"  Offline pretraining with a static dataset followed by online fine-tuning\n(offline-to-online, or OtO) is a paradigm well matched to a real-world RL\ndeployment process. In this scenario, we aim to find the best-performing policy\nwithin a limited budget of online interactions. Previous work in the OtO\nsetting has focused on correcting for bias introduced by the policy-constraint\nmechanisms of offline RL algorithms. Such constraints keep the learned policy\nclose to the behavior policy that collected the dataset, but we show this can\nunnecessarily limit policy performance if the behavior policy is far from\noptimal. Instead, we forgo constraints and frame OtO RL as an exploration\nproblem that aims to maximize the benefit of online data-collection. We first\nstudy the major online RL exploration methods based on intrinsic rewards and\nUCB in the OtO setting, showing that intrinsic rewards add training instability\nthrough reward-function modification, and UCB methods are myopic and it is\nunclear which learned-component's ensemble to use for action selection. We then\nintroduce an algorithm for planning to go out-of-distribution (PTGOOD) that\navoids these issues. PTGOOD uses a non-myopic planning procedure that targets\nexploration in relatively high-reward regions of the state-action space\nunlikely to be visited by the behavior policy. By leveraging concepts from the\nConditional Entropy Bottleneck, PTGOOD encourages data collected online to\nprovide new information relevant to improving the final deployment policy\nwithout altering rewards. We show empirically in several continuous control\ntasks that PTGOOD significantly improves agent returns during online\nfine-tuning and avoids the suboptimal policy convergence that many of our\nbaselines exhibit in several environments.\n","authors":["Trevor McInroe","Adam Jelley","Stefano V. Albrecht","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2310.05723v2.pdf","comment":"10 pages, 17 figures, preprint"},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.18402v1","updated":"2024-03-27T09:44:50Z","published":"2024-03-27T09:44:50Z","title":"On Spectrogram Analysis in a Multiple Classifier Fusion Framework for\n  Power Grid Classification Using Electric Network Frequency","summary":"  The Electric Network Frequency (ENF) serves as a unique signature inherent to\npower distribution systems. Here, a novel approach for power grid\nclassification is developed, leveraging ENF. Spectrograms are generated from\naudio and power recordings across different grids, revealing distinctive ENF\npatterns that aid in grid classification through a fusion of classifiers. Four\ntraditional machine learning classifiers plus a Convolutional Neural Network\n(CNN), optimized using Neural Architecture Search, are developed for One-vs-All\nclassification. This process generates numerous predictions per sample, which\nare then compiled and used to train a shallow multi-label neural network\nspecifically designed to model the fusion process, ultimately leading to the\nconclusive class prediction for each sample. Experimental findings reveal that\nboth validation and testing accuracy outperform those of current\nstate-of-the-art classifiers, underlining the effectiveness and robustness of\nthe proposed methodology.\n","authors":["Georgios Tzolopoulos","Christos Korgialas","Constantine Kotropoulos"],"pdf_url":"https://arxiv.org/pdf/2403.18402v1.pdf","comment":"13th International Conference on Pattern Recognition Applications and\n  Methods (ICPRAM)"},{"id":"http://arxiv.org/abs/2403.18397v1","updated":"2024-03-27T09:35:56Z","published":"2024-03-27T09:35:56Z","title":"Colour and Brush Stroke Pattern Recognition in Abstract Art using\n  Modified Deep Convolutional Generative Adversarial Networks","summary":"  Abstract Art is an immensely popular, discussed form of art that often has\nthe ability to depict the emotions of an artist. Many researchers have made\nattempts to study abstract art in the form of edge detection, brush stroke and\nemotion recognition algorithms using machine and deep learning. This papers\ndescribes the study of a wide distribution of abstract paintings using\nGenerative Adversarial Neural Networks(GAN). GANs have the ability to learn and\nreproduce a distribution enabling researchers and scientists to effectively\nexplore and study the generated image space. However, the challenge lies in\ndeveloping an efficient GAN architecture that overcomes common training\npitfalls. This paper addresses this challenge by introducing a modified-DCGAN\n(mDCGAN) specifically designed for high-quality artwork generation. The\napproach involves a thorough exploration of the modifications made, delving\ninto the intricate workings of DCGANs, optimisation techniques, and\nregularisation methods aimed at improving stability and realism in art\ngeneration enabling effective study of generated patterns. The proposed mDCGAN\nincorporates meticulous adjustments in layer configurations and architectural\nchoices, offering tailored solutions to the unique demands of art generation\nwhile effectively combating issues like mode collapse and gradient vanishing.\nFurther this paper explores the generated latent space by performing random\nwalks to understand vector relationships between brush strokes and colours in\nthe abstract art space and a statistical analysis of unstable outputs after a\ncertain period of GAN training and compare its significant difference. These\nfindings validate the effectiveness of the proposed approach, emphasising its\npotential to revolutionise the field of digital art generation and digital art\necosystem.\n","authors":["Srinitish Srinivasan","Varenya Pathak"],"pdf_url":"https://arxiv.org/pdf/2403.18397v1.pdf","comment":"28 pages, 5 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.18393v1","updated":"2024-03-27T09:30:50Z","published":"2024-03-27T09:30:50Z","title":"Tensor-based Graph Learning with Consistency and Specificity for\n  Multi-view Clustering","summary":"  Graph learning is widely recognized as a crucial technique in multi-view\nclustering. Existing graph learning methods typically involve constructing an\nadaptive neighbor graph based on probabilistic neighbors and then learning a\nconsensus graph to for clustering, however, they are confronted with two\nlimitations. Firstly, they often rely on Euclidean distance to measure\nsimilarity when constructing the adaptive neighbor graph, which proves\ninadequate in capturing the intrinsic structure among data points in many\nreal-world scenarios. Secondly, most of these methods focus solely on consensus\ngraph, ignoring view-specific graph information. In response to the\naforementioned drawbacks, we in this paper propose a novel tensor-based graph\nlearning framework that simultaneously considers consistency and specificity\nfor multi-view clustering. Specifically, we calculate the similarity distance\non the Stiefel manifold to preserve the intrinsic structure among data points.\nBy making an assumption that the learned neighbor graph of each view comprises\nboth a consistent graph and a view-specific graph, we formulate a new\ntensor-based target graph learning paradigm. Owing to the benefits of tensor\nsingular value decomposition (t-SVD) in uncovering high-order correlations,\nthis model is capable of achieving a complete understanding of the target\ngraph. Furthermore, we develop an iterative algorithm to solve the proposed\nobjective optimization problem. Experiments conducted on real-world datasets\nhave demonstrated the superior performance of the proposed method over some\nstate-of-the-art multi-view clustering methods. The source code has been\nreleased on https://github.com/lshi91/CSTGL-Code.\n","authors":["Long Shi","Lei Cao","Yunshan Ye","Yu Zhao","Badong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18383v1","updated":"2024-03-27T09:21:07Z","published":"2024-03-27T09:21:07Z","title":"Generative Multi-modal Models are Good Class-Incremental Learners","summary":"  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic\nforgetting caused by the classifier's bias towards the current task has long\nposed a significant challenge. It is mainly caused by the characteristic of\ndiscriminative models. With the growing popularity of the generative\nmulti-modal models, we would explore replacing discriminative models with\ngenerative ones for CIL. However, transitioning from discriminative to\ngenerative models requires addressing two key challenges. The primary challenge\nlies in transferring the generated textual information into the classification\nof distinct categories. Additionally, it requires formulating the task of CIL\nwithin a generative framework. To this end, we propose a novel generative\nmulti-modal model (GMM) framework for class-incremental learning. Our approach\ndirectly generates labels for images using an adapted generative model. After\nobtaining the detailed text, we use a text encoder to extract text features and\nemploy feature matching to determine the most similar label as the\nclassification prediction. In the conventional CIL settings, we achieve\nsignificantly better results in long-sequence task scenarios. Under the\nFew-shot CIL setting, we have improved by at least 14\\% accuracy over all the\ncurrent state-of-the-art methods with significantly less forgetting. Our code\nis available at \\url{https://github.com/DoubleClass/GMM}.\n","authors":["Xusheng Cao","Haori Lu","Linlan Huang","Xialei Liu","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.18383v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18379v1","updated":"2024-03-27T09:17:50Z","published":"2024-03-27T09:17:50Z","title":"IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining\n  Useful Life Prediction","summary":"  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion\nbatteries is crucial for maintaining the safe and stable operation of\nrechargeable battery management systems. However, this task is often\nchallenging due to the complex temporal dynamics involved. Recently,\nattention-based networks, such as Transformers and Informer, have been the\npopular architecture in time series forecasting. Despite their effectiveness,\nthese models with abundant parameters necessitate substantial training time to\nunravel temporal patterns. To tackle these challenges, we propose a simple\nMLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which\nis an architecture based exclusively on multi-layer perceptrons (MLPs),\nextracting information by mixing operations along both intra-patch and\ninter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer\ncomprises parallel dual-head mixer layers: the intra-patch mixing MLP,\ncapturing local temporal patterns in the short-term period, and the inter-patch\nmixing MLP, capturing global temporal patterns in the long-term period.\nNotably, to address the varying importance of features in RUL prediction, we\nintroduce a weighted loss function in the MLP-Mixer-based architecture, marking\nthe first time such an approach has been employed. Our experiments demonstrate\nthat IIP-Mixer achieves competitive performance in battery RUL prediction,\noutperforming other popular time-series frameworks\n","authors":["Guangzai Ye","Li Feng","Jianlan Guo","Yuqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18375v1","updated":"2024-03-27T09:14:36Z","published":"2024-03-27T09:14:36Z","title":"Stragglers-Aware Low-Latency Synchronous Federated Learning via\n  Layer-Wise Model Updates","summary":"  Synchronous federated learning (FL) is a popular paradigm for collaborative\nedge learning. It typically involves a set of heterogeneous devices locally\ntraining neural network (NN) models in parallel with periodic centralized\naggregations. As some of the devices may have limited computational resources\nand varying availability, FL latency is highly sensitive to stragglers.\nConventional approaches discard incomplete intra-model updates done by\nstragglers, alter the amount of local workload and architecture, or resort to\nasynchronous settings; which all affect the trained model performance under\ntight training latency constraints. In this work, we propose straggler-aware\nlayer-wise federated learning (SALF) that leverages the optimization procedure\nof NNs via backpropagation to update the global model in a layer-wise fashion.\nSALF allows stragglers to synchronously convey partial gradients, having each\nlayer of the global model be updated independently with a different\ncontributing set of users. We provide a theoretical analysis, establishing\nconvergence guarantees for the global model under mild assumptions on the\ndistribution of the participating devices, revealing that SALF converges at the\nsame asymptotic rate as FL with no timing limitations. This insight is matched\nwith empirical observations, demonstrating the performance gains of SALF\ncompared to alternative mechanisms mitigating the device heterogeneity gap in\nFL.\n","authors":["Natalie Lang","Alejandro Cohen","Nir Shlezinger"],"pdf_url":"https://arxiv.org/pdf/2403.18375v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2403.18821v1","updated":"2024-03-27T17:59:56Z","published":"2024-03-27T17:59:56Z","title":"Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and\n  Benchmark","summary":"  We present a new dataset called Real Acoustic Fields (RAF) that captures real\nacoustic room data from multiple modalities. The dataset includes high-quality\nand densely captured room impulse response data paired with multi-view images,\nand precise 6DoF pose tracking data for sound emitters and listeners in the\nrooms. We used this dataset to evaluate existing methods for novel-view\nacoustic synthesis and impulse response generation which previously relied on\nsynthetic data. In our evaluation, we thoroughly assessed existing audio and\naudio-visual models against multiple criteria and proposed settings to enhance\ntheir performance on real-world data. We also conducted experiments to\ninvestigate the impact of incorporating visual data (i.e., images and depth)\ninto neural acoustic field models. Additionally, we demonstrated the\neffectiveness of a simple sim2real approach, where a model is pre-trained with\nsimulated data and fine-tuned with sparse real-world data, resulting in\nsignificant improvements in the few-shot learning approach. RAF is the first\ndataset to provide densely captured room acoustic data, making it an ideal\nresource for researchers working on audio and audio-visual neural acoustic\nfield modeling techniques. Demos and datasets are available on our project\npage: https://facebookresearch.github.io/real-acoustic-fields/\n","authors":["Ziyang Chen","Israel D. Gebru","Christian Richardt","Anurag Kumar","William Laney","Andrew Owens","Alexander Richard"],"pdf_url":"https://arxiv.org/pdf/2403.18821v1.pdf","comment":"Accepted to CVPR 2024. Project site:\n  https://facebookresearch.github.io/real-acoustic-fields/"},{"id":"http://arxiv.org/abs/2403.18715v1","updated":"2024-03-27T16:04:47Z","published":"2024-03-27T16:04:47Z","title":"Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding","summary":"  Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.\n","authors":["Xintong Wang","Jingheng Pan","Liang Ding","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2403.18715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18714v1","updated":"2024-03-27T16:02:00Z","published":"2024-03-27T16:02:00Z","title":"Bringing Textual Prompt to AI-Generated Image Quality Assessment","summary":"  AI-Generated Images (AGIs) have inherent multimodal nature. Unlike\ntraditional image quality assessment (IQA) on natural scenarios, AGIs quality\nassessment (AGIQA) takes the correspondence of image and its textual prompt\ninto consideration. This is coupled in the ground truth score, which confuses\nthe unimodal IQA methods. To solve this problem, we introduce IP-IQA (AGIs\nQuality Assessment via Image and Prompt), a multimodal framework for AGIQA via\ncorresponding image and prompt incorporation. Specifically, we propose a novel\nincremental pretraining task named Image2Prompt for better understanding of\nAGIs and their corresponding textual prompts. An effective and efficient\nimage-prompt fusion module, along with a novel special [QA] token, are also\napplied. Both are plug-and-play and beneficial for the cooperation of image and\nits corresponding prompt. Experiments demonstrate that our IP-IQA achieves the\nstate-of-the-art on AGIQA-1k and AGIQA-3k datasets. Code will be available.\n","authors":["Bowen Qu","Haohui Li","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18714v1.pdf","comment":"6 pages, 3 figures, accepted by ICME2024"},{"id":"http://arxiv.org/abs/2402.14326v2","updated":"2024-03-27T13:25:17Z","published":"2024-02-22T06:38:25Z","title":"Think before You Leap: Content-Aware Low-Cost Edge-Assisted Video\n  Semantic Segmentation","summary":"  Offloading computing to edge servers is a promising solution to support\ngrowing video understanding applications at resource-constrained IoT devices.\nRecent efforts have been made to enhance the scalability of such systems by\nreducing inference costs on edge servers. However, existing research is not\ndirectly applicable to pixel-level vision tasks such as video semantic\nsegmentation (VSS), partly due to the fluctuating VSS accuracy and segment\nbitrate caused by the dynamic video content. In response, we present Penance, a\nnew edge inference cost reduction framework. By exploiting softmax outputs of\nVSS models and the prediction mechanism of H.264/AVC codecs, Penance optimizes\nmodel selection and compression settings to minimize the inference cost while\nmeeting the required accuracy within the available bandwidth constraints. We\nimplement Penance in a commercial IoT device with only CPUs. Experimental\nresults show that Penance consumes a negligible 6.8% more computation resources\nthan the optimal strategy while satisfying accuracy and bandwidth constraints\nwith a low failure rate.\n","authors":["Mingxuan Yan","Yi Wang","Xuedou Xiao","Zhiqing Luo","Jianhua He","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2402.14326v2.pdf","comment":"Accepted by ACM Multimedia 2023"},{"id":"http://arxiv.org/abs/2403.18323v1","updated":"2024-03-27T07:52:51Z","published":"2024-03-27T07:52:51Z","title":"How to Cache Important Contents for Multi-modal Service in Dynamic\n  Networks: A DRL-based Caching Scheme","summary":"  With the continuous evolution of networking technologies, multi-modal\nservices that involve video, audio, and haptic contents are expected to become\nthe dominant multimedia service in the near future. Edge caching is a key\ntechnology that can significantly reduce network load and content transmission\nlatency, which is critical for the delivery of multi-modal contents. However,\nexisting caching approaches only rely on a limited number of factors, e.g.,\npopularity, to evaluate their importance for caching, which is inefficient for\ncaching multi-modal contents, especially in dynamic network environments. To\novercome this issue, we propose a content importance-based caching scheme which\nconsists of a content importance evaluation model and a caching model. By\nleveraging dueling double deep Q networks (D3QN) model, the content importance\nevaluation model can adaptively evaluate contents' importance in dynamic\nnetworks. Based on the evaluated contents' importance, the caching model can\neasily cache and evict proper contents to improve caching efficiency. The\nsimulation results show that the proposed content importance-based caching\nscheme outperforms existing caching schemes in terms of caching hit ratio (at\nleast 15% higher), reduced network load (up to 22% reduction), average number\nof hops (up to 27% lower), and unsatisfied requests ratio (more than 47%\nreduction).\n","authors":["Zhe Zhang","Marc St-Hilaire","Xin Wei","Haiwei Dong","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2403.18323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18252v1","updated":"2024-03-27T04:49:23Z","published":"2024-03-27T04:49:23Z","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","summary":"  Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.\n","authors":["Yiwu Zhong","Zi-Yuan Hu","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18252v1.pdf","comment":"Project page: https://github.com/LaVi-Lab/Visual-Table"},{"id":"http://arxiv.org/abs/2403.10066v3","updated":"2024-03-27T02:25:51Z","published":"2024-03-15T07:16:07Z","title":"Contrastive Pre-Training with Multi-View Fusion for No-Reference Point\n  Cloud Quality Assessment","summary":"  No-reference point cloud quality assessment (NR-PCQA) aims to automatically\nevaluate the perceptual quality of distorted point clouds without available\nreference, which have achieved tremendous improvements due to the utilization\nof deep neural networks. However, learning-based NR-PCQA methods suffer from\nthe scarcity of labeled data and usually perform suboptimally in terms of\ngeneralization. To solve the problem, we propose a novel contrastive\npre-training framework tailored for PCQA (CoPA), which enables the pre-trained\nmodel to learn quality-aware representations from unlabeled data. To obtain\nanchors in the representation space, we project point clouds with different\ndistortions into images and randomly mix their local patches to form mixed\nimages with multiple distortions. Utilizing the generated anchors, we constrain\nthe pre-training process via a quality-aware contrastive loss following the\nphilosophy that perceptual quality is closely related to both content and\ndistortion. Furthermore, in the model fine-tuning stage, we propose a\nsemantic-guided multi-view fusion module to effectively integrate the features\nof projected images from multiple perspectives. Extensive experiments show that\nour method outperforms the state-of-the-art PCQA methods on popular benchmarks.\nFurther investigations demonstrate that CoPA can also benefit existing\nlearning-based PCQA models.\n","authors":["Ziyu Shan","Yujie Zhang","Qi Yang","Haichen Yang","Yiling Xu","Jenq-Neng Hwang","Xiaozhong Xu","Shan Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10066v3.pdf","comment":null}]}}